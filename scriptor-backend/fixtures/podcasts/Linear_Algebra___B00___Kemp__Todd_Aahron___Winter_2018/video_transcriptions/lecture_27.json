{
    "Blurbs": {
        "- 1/3 is + 2/3 + -1 - -2/3 is 2/3 - 1 which is -1. So there's are you too. And then the last thing we need to do is normalize it so we just compute the length of U2 squared is -2/3 squared + 2/3 squared + -1/3 squared So that is 4/9 + 4/9 + 1/9. What is 9/9 for 1 miraculously this Vector was already normalized ": [
            669.9,
            707.4,
            24
        ],
        "- New Times you. V is 0 and there for you. V is different. So therefore you is orthogonal to be that's the conclusion here in general. If you have any Matrix and it has two distinct eigenvalues and eigenvectors realize I can values will be linearly independent even better. If your Matrix is symmetric, you have to eigenvalues with distinct to eigenvectors with distinct eigenvalues. They are orthogonal. Fits ": [
            2082.4,
            2112.6,
            70
        ],
        "Happy Pi Day everyone. Welcome to the last new lecture of math 18 will be finished with all the course material today. Next lecture on Friday is review for the final exam. So today we are going to finish our discussion from last time of the gram Schmidt orthogonalization procedure and the QR factorization of matrices. And then we will move on to discuss the spectral theorem the most important ": [
            7.6,
            38.2,
            0
        ],
        "Matrix remember that's so if so if they are worse than normal. Then what we got is that? Let's rewrite is q q as an orthogonal Matrix. Where Q transpose x q is the identity of eigenvectors orthogonal basis in the in this case means that to transpose is Q inverse. So this says we can write this as qdq inverse, but you inverse is the same as Q transposed. ": [
            1613.8,
            1662.0,
            56
        ],
        "So what are they? Are linearly independent. So for any Matrix if you have values are linearly independent. Well, if you have a symmetric Matrix, you get the best kind of linear Independence. Here's a fun little calculation and take their. Product to be silly by this. Nonzero number Lambda - meet me with a nonzero number cuz I'm done near distinct eigenvalues. I'm going to compute this product X ": [
            1935.3,
            1971.1,
            66
        ],
        "That's what we're looking for. We're looking for when the eigenvector Matrix is an orthogonal Matrix in which case the diagonalization is not and what's great about that is that it's a whole lot easier to find the transpose of a matrix than the inverse of a matrix. So if you knew already that your Matrix was orthogonally diagonalizable, which is a lot of syllables but just means that it ": [
            1663.3,
            1688.2,
            57
        ],
        "They look like this. That's what these pictures these orbital pictures of electron clouds. That's what they are. That's all they are there the eigenvectors of the Matrix that represents the energy of a hydrogen atom better reasons why the spectral theorem is the most important theorem in mathematics. In fact is the most important serum in all of science. Because it's the basis of quantum mechanics and quantum mechanics ": [
            2916.8,
            2941.4,
            97
        ],
        "V3 within the span of V1 and V2 then look at that third line there were taking V3 and subtracting from its orthogonal projection onto you one and it's installing a projection onto you too. But if these trees is in the span of V1 and V2, it's therefore in the span of you want and you too in which case it's orthogonal projection into the space van, but you ": [
            249.0,
            271.5,
            10
        ],
        "Vector our first collector you one hat. Is V 1/3 2/3? 1/3 + -2/3 okay. Now we need to produce the second Vector in our list. So we following Brown Schmidt with the second Vector is V2. That's why the second new Vector is you too and would like that to just be me too. And then we be done but in general we have to take the projection TV ": [
            549.9,
            584.7,
            21
        ],
        "a diagonal. So if you do it again, it reflects it back you get the original Matrix back. So that's Q D transpose. What kind of Matrix is d? It's diagonal what that means is that it's got entries on the diagonal and zeros above and below. Now. If you transpose that Matrix, I swap the above and below triangular Parts. Is it down transpose? so look, if a is ": [
            1748.5,
            1780.3,
            60
        ],
        "a fever and it's just populated by the numbers that come up. The numbers are one of the length of you one and V2. You 1 / the length of view 1 and length of you too and so on in that list. And so if you were a computer and can remember exactly how to take those numbers normalized appropriately and put them into this Matrix there. Then immediately ": [
            401.3,
            427.3,
            16
        ],
        "a system that is a linear combination of the eigenvectors because they form a basis. It's in some random state. But when you take a measurement of the energy what happens if you end up applying the prediction on one of those vectors, that's what it means to take a measurement you project onto the eigenspace and then what you measure is the corresponding eigenvalue. That's what quantum mechanics says ": [
            2862.3,
            2885.2,
            95
        ],
        "about eigenvalues two weeks ago two and a half weeks ago. What sucks about diagonalize ability? This is an important topic for the final exam. So I'll give you a square Matrix a and I can ask you is it diagonalizable? Remember what that means? I will Matrix is one that is similar to a diagonal matrix. That's what I've written. There can be written as PDP inverse Scorpios and ": [
            1511.7,
            1534.2,
            52
        ],
        "all. Then it can be diagonalized orthogonally, but that still doesn't answer the question. We wants to know can I just inspect the Matrix and see from its symmetry that it is diagonalizable and therefore for stopping leg itemizable. And the answer is yes. It's called the spectral theorem. And the ceremony is yes every symmetric Matrix is diagonalizable and therefore orthogonally diagonalizable if your Matrix is symmetric, if it's ": [
            2255.7,
            2288.3,
            75
        ],
        "alternate formulation of what that means. Which is called the spectral decomposition of such matrices because what this says is that if I takes my Matrix a my symmetric Matrix, then I can decompose it as DQ transfer at you and hat is RQ. is this and then to inverse but the beauty is I'm so on The Columns of a get turned in. Okay. Well now how does this ": [
            2354.8,
            2410.3,
            79
        ],
        "an R1 would you Grandma Schmidt on The Columns of a one with a different from The Columns of a so we have a new QR decomposition. And now from there with Mormon a 2 which is our one q12 now exactly the same thing. We just saw before our 1 is equal to one transpose a one. So this is equal to q1 inverse of a one-to-one which is ": [
            1305.6,
            1336.2,
            45
        ],
        "any such symmetric Matrix? So to move in that direction to see if that's true. Let's look at the following calculation suppose. I take two eigenvalues. So I have it suppose. I have a symmetric Matrix a and suppose that I've got two eigenvectors for it you envy two eigenvectors with two distinct eigenvalues in that situation. Sound rygin vectors, you know, we already did a calculation like this for ": [
            1872.3,
            1908.1,
            64
        ],
        "are now. I'm going to write a new Matrix down call A1 different Matrix from before because our times cute that you can reverse the meniscus cause both of the mission is to an are there End by end in the Square Matrix. They're the same size. It's typically not equal to a matrix multiplication doesn't commute. So, why would we do this? Here's why are X QR is equal ": [
            1188.5,
            1215.7,
            41
        ],
        "basis vectors in this room. All I do is I first projected onto the XY plane and then projected onto the x-axis and I scale it by the X eigenvector then I projected into the us over there and I multiply that by the way and I'm all for that by this VI. Hey, it's the easiest possible way to understand geometrically what a matrix does and it works for ": [
            2572.9,
            2600.9,
            85
        ],
        "big rooms. Okay, you're going to be able to spread out. I mean you're going to have seating assignment assignments. Have you spread out? Most of you will have no one sitting on either side of you is I'm forcing not quite possible for that. And for everyone there is twice as many seats as we need but one point eight times in a suit says we need but it's ": [
            113.8,
            131.6,
            4
        ],
        "by the diagonal entries? Announce here. We've got a matrix and I've given you as columns Columns of Islam does scaling the original use and I X Matrix written in terms of its Rose. But the nice thing there is remember how matrix multiplication works the entries of the matrix product are given by row times column. So if you work that out here, what you find is that this ": [
            2436.1,
            2476.0,
            81
        ],
        "can tell you. So if you're trying to find a zygon value, you can try to compute a ones I can buy it instead easier to compute a ones I can tell you not necessarily but what if we do it again? So now let's take a 1. I know we can do it QR decomposition. Different than the CUNY are we got last time? We have a Q on ": [
            1281.4,
            1305.6,
            44
        ],
        "different contexts. Some old word 18th century. Where is here? It comes from pictures like this. But you've seen if you've taken even in high school, but certainly in University if you've seen in physics and chemistry classes it what is the Spectrum if I take a glass tube is filled with only hydrogen gas and I have two electrodes at the end of my run a strong current through ": [
            2633.7,
            2663.1,
            87
        ],
        "easier not necessarily but I can produce a whole string of these a A1 A2 A3. a 100 and here is the serum. as n goes to Infinity a n in this sequence rapidly converges to an upper triangular Matrix and that's something you can check on Matlab. Can you can read up Google it how to do the QR decomposition of a matrix or Flop from your homework or ": [
            1357.2,
            1401.0,
            47
        ],
        "easier way to do it. Even if it's less efficient a matrix linearly independent. So I'm going to do gram Schmidt orthogonalization and produce an Orson Orville basis for the same factors as they were going to find. orthonormal basis for the column space of a so let's call these two vectors V1 and V2. So we're going to find that are normal basis using the gram Schmidt process to ": [
            472.7,
            519.4,
            19
        ],
        "every symmetric Matrix. So that decision and you do need to know these things for the final exam and test it on them on your homework. The last seven minutes of class. I would like to answer the question. Why is it called spectral? It should probably be called the projection decomposition or something like that white spectral. This is a really fancy sounding word. That means different things in ": [
            2600.9,
            2633.7,
            86
        ],
        "factorization of a matrix? Well, let me make a comment first. So the comment I want to make first is remember in the general case where we have to say is tall and skinny where it has more rows and columns those columns. Do not form a basis of RN where they live takes. The queue is also and VIPs tall and skinny. They do not form the basis of ": [
            1047.7,
            1072.6,
            36
        ],
        "faster, but that's the basic idea. So that is how you compute eigenvalues and that's the reason that you are factorization is actually really important. It's used everywhere in science anywhere you need eigenvalues QR factorization is how you actually numerically computer. Okay, so you need to understand the QR factorization? All right, so that's what I want to say about you are now I want to finish the course ": [
            1456.2,
            1481.6,
            50
        ],
        "find a matrix in general you are asked is it diagonalizable? That's just hard to check you can't tell by looking at it. Usually you just have to go through all the math to see if there are enough real eigenvalues and if the algebraic multiplicity and geometric multiplicity of each other like in vallese match the pain in the butt most of time. Where has this condition of being ": [
            1821.1,
            1844.0,
            62
        ],
        "four or five lines missing. spectral lines of hydrogen How does time went on? Physicist discovered that there are actually many more lines outside the visible spectrum that are missing infinitely many lines that are missing these form of the emission spectrum of hydrogen. When did were first discovered nobody had any idea what that could possibly mean? Okay, but some physicists in the first part the first twenty years ": [
            2689.1,
            2717.6,
            89
        ],
        "from beginning to end quantum mechanics is all linear algebra. And by the way, I just told you what the eigenvalues of a there's no spectral lines. What are the eigenvectors of functions are going to be these factors are functions in three variables functions in space. What do those functions look like? What do the eigenvectors look like? You've seen them before because you took chemistry in high school. ": [
            2885.2,
            2916.8,
            96
        ],
        "gives you this decomposition. And the point is that the Triangular structure out there gives you an opera training going to Matrix for the are there are factorization of the Matrix a Hey, so if you were a computer than you would once you've done the gram Schmidt process, you could immediately write down 2 and r. You're probably not a computer. So you can write down the Q because ": [
            427.3,
            451.0,
            17
        ],
        "going to be linearly independent, then p is less than or equal to a surface is typically a tall skinny Matrix what the gram Schmidt orthogonalization procedure does for you. It's produced these or Sonoma vectors here called that Matrix Q. And what this triangular relationship between the use in that Matrix a cute. I meant to write End by P and R is p by P. So we have ": [
            367.4,
            401.3,
            15
        ],
        "going to be roomier than the midterms have been so I hope that will make it a more pleasant experience. Also, I'm happy to tell you that the final is worth 50 points. So it is 1 2/3 times as long as the midterms were one and two thirds times as long as a designed 1 hour exam what you have 3 hours to do it. I hope that time ": [
            131.6,
            151.4,
            5
        ],
        "has an orthogonal basis of eigenvectors. You just have to find the Q in the D. You don't need to find the inverse because the inverse is just to transpose just turn the Matrix on its side. Okay. So when that happens that's what happened. That's that's what we'll get. But when will it happen? So here's the observation I want to make which is If I have such a ": [
            1688.2,
            1710.7,
            58
        ],
        "has those columns turned on their side to transpose is 2/3 1/3 - 2/3 is the first row - 2/3 2/3 - 1/3 as the second row and we multiply that by the original Matrix. And let's just work out with this 2 by 2 Matrix is the first entry is 4/3 + 1/3 - 4/3, which is 1/3. The entry below that is -4/3 + 2/3 + 2/3 is ": [
            892.3,
            939.9,
            31
        ],
        "have that something be to buy two so far is going to be a two-by-two matrix and the numbers that are up here already modulus of normalization the length of product / the length of you want so they're already there, but you don't have to remember that because we can make the following observation. The Columns of cube are orthonormal. Matrix I can encode the. Products between his columns ": [
            785.0,
            831.3,
            28
        ],
        "in this Square Matrix Q transpose Q if those columns are orthonormal as we saw two lectures ago that precisely means to transpose Q is the identity Matrix. Now, how does that help us here? We'll just look at the equation. therefore If I multiply both sides of that equation by Q transpose what to transpose Q is the identity Matrix this we just said right here. So that's just ": [
            831.3,
            867.3,
            29
        ],
        "indeed 0 As We Knew It was supposed to be because this is supposed to be the two one entry or the one to entry. I mean is 0 + 1/3 + 2/3 is 1 And The Last Story. The first century is 4/3. + 1/3 + 4/3 thank you. Okay, 4/3 + 1/3 + 4/3, which is 9/3, which is 3 thank you. I think I got the other ": [
            939.9,
            976.5,
            32
        ],
        "independent factors because they're orthogonal at 9072 Forester normalize them. You have to normalize them if they're already orthogonal then you just divide them by their links and you have worked for normal vectors. So that's the gram Schmidt process will do one more example of applying it in a moment. But the observation we made at the end of last lecture was that if you put this all together ": [
            293.3,
            315.7,
            12
        ],
        "invertible Matrix India's diagonal but that's equivalent of the right geometric way to understand. It's diagonalizable. If there is a basis of RN that is composed of eigenvectors of a Okay, great. So you get a basis of RN and you've done lots of examples where you find the eigenspaces and you see that there are enough eigenvectors to form a basis of all time. No, most of the time ": [
            1534.2,
            1558.2,
            53
        ],
        "is just equal to Lambda 1 times you won you won transpose. Plus Lambda two times you to you to transpose. Plus down the line to climb to Nunu and transpose. Spectral decomposition because think about it one more step. Well, but one more step here to identify these things. So you won you to you and these are north and normal basis of our at What is this u11 ": [
            2476.0,
            2513.1,
            82
        ],
        "is orthogonal projection onto the column space of it. But in the special case we're and was able to pee by started with a basis of our at not necessarily orthogonal and I used the groundsman process to produce an orthonormal basis The Matrix to the square. Matrix is invertible. So how does that help us here? Well, let's take a look here how that helps us is it turns ": [
            1097.8,
            1129.4,
            38
        ],
        "is the basis of all of physical science in particular chemistry this picture here up there the periodic table it was written down 80 years before the turn of the twentieth century people understood that chemical elements came in his periodic bands with with behavior that reflected each other but had no idea why it was all explained by these pictures. It was all explained by the eigenvector analysis of ": [
            2941.4,
            2968.2,
            98
        ],
        "it Matrix mechanics because the model and you've never seen this before unless you've taken higher level quantum mechanics classes, but the model of quantum mechanics, all right here that he formulated which is really still how it works. Is that here's how the physical system is modeled the hydrogen atoms that have the the state of the physical system inside that glass tube is modeled as a vector in ": [
            2747.7,
            2773.7,
            91
        ],
        "lectures to go for a general Matrix. If I have a situation like this where I have a matrix that has two distinct eigenvalues remember that we prove that the eigenvectors are there for Made of cheese what what's true about eigenvectors of eigenvectors of distinct eigenvalues? I heard an answer, but please say it louder. You weren't confident about your answer. Okay, could they be parallel to each other? ": [
            1908.1,
            1935.3,
            65
        ],
        "matrix, that's orthogonally diagonalizable that has an orthogonal basis of eigenvectors. Let's look at its transpose a transpose is equal to what we just said that we can factorize as QD to transpose that's to transpose of a product is the product of the transpose is but in the other order, this is to transpose transpose x d transpose x q transpose. No transpose, it just reflects the Matrix across ": [
            1710.7,
            1748.5,
            59
        ],
        "noticed that there's a triangular pattern up there only depends on V1 X on you wanted me to which is the same as depending on V1 and V2 and use three depends on you one U2 and V3 the same as depending on V1 V2 and V3. So you 7 is in the span of V1 through the 7th doesn't need all the other future V. So you get a ": [
            315.7,
            340.0,
            13
        ],
        "now an upper triangular Matrix. You can pick off its eigenvalues its eigenvalues are on the deck. That is how about love compute eigenvalues when you put in a in Matlab for a matrix a what it does, is it erased the QR factorization and reverses it like this over and over and picks off the diagonal entries. It does some small tweaks to this algorithm to make it even ": [
            1432.1,
            1456.2,
            49
        ],
        "now have many ways to find Bayseas they all boil down to the same thing more or less but in different situations, we have different descriptions and algorithms for how to find a basis. You're given a basis for a Subspace. How do you find an orthonormal basis for that Subspace? Okay, that's what the gram Schmidt orthogonalization or orthodontist ization procedure does for you. You're given a collection of ": [
            179.1,
            200.5,
            7
        ],
        "of a it is not an orthonormal basis of our three where those vectors lives. There's only two of them. It can't be a basis of our three. It's an orthonormal basis of a Subspace the Subspace spanned by the original vectors V1 and V2, which is the column space of a so, there is our personal know the QR decomposition composition says, okay. We know we now know that ": [
            732.2,
            756.5,
            26
        ],
        "of the Matrix. You would do gram Schmidt to find the Q and then to find the are you would just multiply Q transpose times original Matrix day. This is less efficient than just figuring out which coefficients appeared in the process and putting them in the Matrix if a computer is doing it and it knows where to look for those numbers to put in the Triangular Matrix that ": [
            1002.6,
            1023.6,
            34
        ],
        "of the twentieth century figured out that our whole model for physics was wrong. Because of these experiments and I figured out that what's going on. Is that the kind of Light energy that can be emitted and absorbed by hydrogen. It only comes in discrete quantized amounts. This was the beginning of what is now called quantum mechanics. Call quantum mechanics when it was invented by Heisenberg. He called ": [
            2717.6,
            2747.7,
            90
        ],
        "on the last flight. The transpose of a product is the product of the transpose is in the other order. So this is equal to you transpose a transpose V - you transpose a but if we're in the city. Symmetric then those two things are equal. so if a equals a transpose then this is there a So when I get is that this is non-zero number you landed ": [
            2046.9,
            2082.4,
            69
        ],
        "on those pictures and those linear Transformations. They encode all of the physical operations and measurements that we can make in particular. There's an energy operator, which is called the hamiltonian because of the measure of the energy of the system. You multiply that by that Matrix. Now that Matrix is a symmetric Matrix all the energy operators all the physical operators in quantum mechanics are symmetric matrices, which means ": [
            2797.4,
            2827.4,
            93
        ],
        "one and then the second one you take it and subtract from it. It's orthogonal projection onto the first and then you take the third one and you subtract from it. It's orthogonal projection onto the first and the second. Okay and proceed that way at that will automatically produce for you a list of orthogonal vectors know if the ventures you started with warrants linearly independent, for example, suppose ": [
            224.9,
            249.0,
            9
        ],
        "orthogonally diagonalizable then it's transpose is itself text we call as I mentioned before we call such matrices symmetric. Okay, so that's the conclusion of a symmetric. Those are the only kind of matrices we could possibly expect to have a North Enola basis of eigenvectors. Okay, great. So I give you and I give you a matrix. That is symmetric. That's an easy condition to check remember if you ": [
            1780.3,
            1821.1,
            61
        ],
        "our end that Matrix Q is typically not invertible. It's not SquarePants convertible. So well, we do have the equation that you choose that to transpose Q is the identity that does not mean that cute you transpose is the identity. There is no inverse Fork you typically and we saw already transpose what it represents that bigger Square Matrix Matrix that represents the Matrix of the linear transformation, which ": [
            1072.6,
            1097.8,
            37
        ],
        "out this can be used to very quickly and very efficiently approximate eigenvalues not I'm not going to actually show you how this works in the example of it would be fun for you to try out Matlab to see how it doesn't but here's how it works. This is what's called a QR algorithm and this is how I met Love Actually compute eigenvalues. So you start with a ": [
            1129.4,
            1150.5,
            39
        ],
        "possibility to prove it for us. It was just take more time that I want to and I'm not interested in the proof. I'm interested in the statement. Okay. So the spectral theorem says there's one class of matrices the best kind of symmetric matrices, which you can just tell by looking they are diagonalizable. And in fact, they are orthogonally diagonalizable. Let me point out one thing as an ": [
            2333.6,
            2354.8,
            78
        ],
        "pressure will not play any role. Okay, you just need to show that you learned and understood what we've done so far. All together and you're sitting room assignment have been posted and should be visible to you on TriNet. All right. So let's review what we did last time. So we're talking about orthogonalisation. So if you are given a Subspace and you find a basis for it, we ": [
            151.4,
            179.1,
            6
        ],
        "really appreciate it. You have one final MyMathLab homework set it to do tomorrow night at 11:59 p.m. On all the previous ones. There's been a one-week Grace. Afterward during which you can still do problems for 50% credit this time. I've said that Grace. To be slightly less. It's until next Wednesday. I'm guessing you guys are going to be busy anyway, but finals next week and we need ": [
            63.9,
            89.3,
            2
        ],
        "requires no flops. Item requires no conversation with all the competition is already done. Where is this matrix multiplication take some flops, but in the size of the examples you guys are doing it's easier to do that matrix multiplication then to try to remember exactly which coefficient close to where you will find that you are factorization of The Matrix. But why would you want to find the QR ": [
            1023.6,
            1047.7,
            35
        ],
        "similar to a one which is similar to a so this new Matrix A2 is also similar to the original Matrix. It also has the same eigenvalues. It's probably a totally different Matrix, but now I've got three different matrices. They all have the same eigenvalues if it's easier to compute eigenvalues. If one of them then I can choose that one. Now. Is this one going to be any ": [
            1336.2,
            1357.2,
            46
        ],
        "some big Vector space What Vector space was kind of like the one we've been confused about the most in this room the vector space p of polynomials. It's a space of functions that are called wave functions, but you don't need to worry about that at some big Vector space. And there are operators there are matrices Infinity by Infinity matrices that act on those vectors linear Transformations act ": [
            2773.7,
            2797.4,
            92
        ],
        "square Matrix at There was a square Matrix Ai and you are going to do the QR factorization of it snow for this to work exactly as stated. He had better be invertible. Okay. So let's just say is an invertible Matrix basis to start with for the car is equal to Q transposed. Now here's what mut level do next will say, okay, I found the Q in the ": [
            1150.5,
            1188.5,
            40
        ],
        "string them together. Animatrix q and we will get that a is q d q inverse which is the same as QC to transpose where this D. As usual is the diagonal matrix with the eigenvalues. Okay, so that's a nice little observation here. So if I Matrix is orthogonal diagonalizable, it must be symmetric if it is symmetric, and if we already know that it can be diagonalized at ": [
            2226.6,
            2255.7,
            74
        ],
        "symmetric is easy to check visually do I have the same answers above the dragon level up just to look and see so those that's a that's a necessary condition for having an orthogonal basis of eigenvectors. Okay, but that doesn't mean necessarily that every such Matrix that every symmetric Matrix is orthogonal diagonalizable. Well, let's see. So that's the question here. That's the converse. Can we always orthogonally diagonalize ": [
            1844.0,
            1872.3,
            63
        ],
        "that typically won't happen. He probably will have to normalize it again. But in this case it worked out. It was already a normalize vector. So therefore you two hat is the same as you to it was already normalized. So it's right here. So we have found our orthodontic basis it consists of this one and this one to be clear. That's an ordinal basis of the column space ": [
            707.4,
            732.2,
            25
        ],
        "that typically. So the question is when does it just happen? Is orthogonal basis of the best kind of basis for what kinds of matrices is it true that there is an orthogonal basis of eigenvectors or normal basis of so, let's see if we can suss out which kind of matrices might have that property. So what I'm saying is that I have a matrix diagonalizable. where the P ": [
            1580.7,
            1613.8,
            55
        ],
        "that's what gram Schmidt does. That's the hats that you produced but figuring out which coefficient to put we're in the army tricks. That's a pain in the butt to try to remember but it's okay. You don't need to okay. There's a I'm not going to make you do it cuz this is going to be on the final but there's a shortcut the alarm cut but there's an ": [
            451.0,
            472.7,
            18
        ],
        "the first thing we do is we take the first vector and we call it you want instead. That's our first Spectre D1 there and we need to normalize we compute the length of you 1 squared is 2 squared + 1 squared + -2 squared which is 9 that's what that says. Set the length of you one is \u221a 9 or 3. And so we have our first ": [
            519.4,
            549.9,
            20
        ],
        "the gas turn off the lights then I will see the two glowing and if I pass it through a prism to separated into its different frequencies, I find something remarkable scientist discovered in the late 19th century that had to had no explanation for the time which is that you get lots of colors out you get a full spectrum of colors out with a few Exceptions. There are ": [
            2663.1,
            2689.1,
            88
        ],
        "the same thing as the identity Matrix times are which is just our that's what our is. You don't need to remember how to put those triangular coefficients into the army tricks. Once you find the Q you just take to transpose times the original Matrix a and you'll get your are so let's write that down in this case. Our is Hugh transpose times. We take the Matrix that ": [
            867.3,
            892.3,
            30
        ],
        "them orthogonal doing the gram Schmidt orthogonalization procedure. That is we start with the basis of eigenvectors. They are already orthogonal that's only going to modify the vectors with any combination of eigenvectors. if is diagonalizable and symmetric then yes, there is an orthodontist basis of eigenvectors. So who's to say that's in this case, then we can find you one. Hat you to have you and hat and we ": [
            2173.0,
            2226.6,
            73
        ],
        "theorem in mathematics. Quick administrative reminders. Please do fill out your capes. I checked the response rate last night and it's hovering around 30% at the moment. So 2/3 of you haven't filled them out you have until this Friday evening to fill them out. I definitely and my department and the university would appreciate your feedback. So if you can please take 5 minutes and fill that out would ": [
            38.2,
            63.9,
            1
        ],
        "they are orthogonally diagonalizable which means that there is a basis of the wave functions consisting of eigenvectors and eigenvalues. Are the eigenvalues the eigenvalues of the energy operator for the hydrogen atom? Those are the only thing it can only be in an eigenvector State. That's when you know, you've heard this crazy phrase collapsing the wave function before in popular literature what that literally means is you have ": [
            2827.4,
            2862.3,
            94
        ],
        "they're not all distinct The Matrix might still be diagonalizable. So if I have a symmetric Matrix and it happens to be diagonalizable, then what this calculation just showed us is that the distinct eigenspaces or if each other is if I take two vectors one from the other from the from the three-dimensional eigenspace of the eigenvalue 7 it well, those might not be orthogonal but we can make ": [
            2141.4,
            2173.0,
            72
        ],
        "this Matrix. So among other things I told you that is all and everything that computer can do at the end of the day linear algebra is all in everything that the Universe can do and that's why I think this is the most important class that you've taken. See you I'll see you all on Friday for review for the finals. ": [
            2968.2,
            2995.2,
            99
        ],
        "this. Product. I'm going to do it in this really funny backwards toward away so far. It's alright Lambda times you. V - mute you. V. I'm going to write that as Lambda times you. V - you. Movie. and the reason that I'm reading it that way is that Lambda you is a you and Landon is a v so I can write this as dotted with v - ": [
            1971.1,
            2010.9,
            67
        ],
        "to Q transpose Hyundai? So this is to transpose a q. but in the special Times Square Q transfer Q means that you transpose is the inverse of Q. This is cute in verse a q. If I have a matrix and I take an invertible Matrix on the left X the inverse that in real Matrix on the right. What is that due to the Matrix? Remember what that ": [
            1215.7,
            1248.9,
            42
        ],
        "to get your grades process. So I hope you'll finish it by tomorrow night. But if you're doing a late you have only six days instead of 5 this time to do the late editions. And once again your final exam is this Saturday for days from today in three rooms listed their Galbraith hall room 242 Peterson hall room 108 and your call room 2722. Those are all nice ": [
            89.3,
            113.8,
            3
        ],
        "to show it actually at the end of the day they are real or we would need some calculus. There's a beautiful Vector calculus proof of this there. I'm so those of you who've taken math 20c and I could show you a proof If you really want not right now but in office hours or something that is a beautiful proof of it. It's not outside the realm of ": [
            2315.0,
            2333.6,
            77
        ],
        "to those one-dimensional subspaces, then your original Matrix is the linear combination of those matrices that those projection matrices. Where the linear coefficient are the eigenvalues that's called the spectral decomposition of only works for symmetric matrices, but it's beautiful because it's geometrically now easy to understand what that metrics does. Okay. It says I want to know what happens to a vector here if it's eigenvectors are the standard ": [
            2544.5,
            2572.9,
            84
        ],
        "together with this thing. We're asking about up here because we're asking can I orthogonally diagonalize can I find a basis of eigenvectors? That is actually an orthonormal basis. Well what this says if I have distinct eigenvalues, yes. Okay, so if I have all distinct real eigenvalues for my symmetric Matrix, then yes, it will have a North normal basis of eigenvectors by that calculation. No better yet. If ": [
            2112.6,
            2141.4,
            71
        ],
        "transformation is called does matrices are called. But remember this similar, this is the similarity transformation. So the Matrix A1 is similar to The Matrix. Now what does similar mean while it means exactly this but in particular it implies that a one and a they have the same eigenvalues. So even though this new Matrix a one is different from a its eigenvalues are the same as as I ": [
            1248.9,
            1281.4,
            43
        ],
        "transpose that's the Matrix of the orthogonal projection onto you want? So I can write this one more way which is this a Lambda One X the projection onto you one. Plus Lambda two times the projection onto YouTube. It is Matrix is a sum of projection matrices. It's a linear combination of projection. So you have this nice basements of eigenvectors. And if you take the orthogonal projections on ": [
            2513.1,
            2544.5,
            83
        ],
        "transpose, if you can just look at it and then you know without doing any work, but the Matrix can be orthogonally diagonalize it has an orthogonal basis of eigenvectors. I am not going to prove this there. We don't have the tools to prove this then we would need either some of the fundamental theorem of linear algebra, because we really have to deal with complex roots of polynomials ": [
            2288.3,
            2315.0,
            76
        ],
        "triangular Turtle pattern there and that triangular pattern is reflected in the fact that this can be systematizing put together to say that you have a certain Matrix factorization Matrix decomposition you start with your Matrix with the vectors v as its columns. Let's call that Matrix a Now it is well either vectors in RN so there's and Rose and RP columns here. And of course if these are ": [
            340.0,
            367.4,
            14
        ],
        "two on two Hue one. Or you want to hack if we want just a matter of whether we want to normalize before or after I'm just going to do this. We need to calculate V2 dotted with you one you wanted the same as v111. 0 + 1 - 2 written down one wrong 0 + 1 + 2 which is 302 find an orthogonal basis. So let's just ": [
            584.7,
            630.7,
            22
        ],
        "two right though. And then the last entry is -2/3 * 020 + 2/3 + 1/3, so it should just be one again. And there is RR Matrix. Okay, and if you want you can verify that Q x r is equal to a QR decomposition and that's really how you would find it. If you were asked on an exam on a homework set to find the QR factorization ": [
            976.5,
            1002.6,
            33
        ],
        "vectors linearly independent vectors. The gram Schmidt process produces from those a new list of vectors that span the same space spend the same. Subspace that the original vectors did but they are orthonormal vectors and the way you construct them is you just take the first one in the list and you can choose which is the first if you want to shuffle them around and take the first ": [
            200.5,
            224.9,
            8
        ],
        "wanted you to is itself and so that third term used to be there would be zero. So if you start with vectors that aren't linearly independent the gram Schmidt process will produce sum of 0 vectors for you. That's okay. You just have to throw them away if that happens. Okay, but it's better probably should just start with a basis of the Subspace dependent factors. You get linearly ": [
            271.5,
            293.3,
            11
        ],
        "we can write. Hey in the form Q x r where Q is the Matrix formed by the two vectors you want hat and you too hot. That's cute. An RR is going to be let's see. What size is it going to be is 3 by 2. Accu is 3 by 2. So to multiply 3 by to buy something and get a 3 by 2. You have to ": [
            756.5,
            785.0,
            27
        ],
        "when you have done that the bases that you get is not an orthogonal basis, the eigenvectors are typically not orthogonal that you could do gram Schmidt on them to produce an orthogonal basis, but then that new orthogonal basis will not be composed of eigenvectors. There's rigidity here. Hey, you can't just change the vectors tickling your combinations of them and still have them be eigenvectors. You can't do ": [
            1558.2,
            1580.7,
            54
        ],
        "with Section 7.1. So let's continue on the steam here. So we just had the QR factorization with start with a basis with over the Matrix a and we replaced it with a matrix with the same column space cute that has orthogonal normal columns. And we saw that that is useful for comparing eigenvalue still on this team of eigenvalues and orthogonal vectors. Let's think back we started talking ": [
            1481.6,
            1511.7,
            51
        ],
        "work? Well from the diagonalization when we did this in the first place how the diagonals ation arose we noted that if I take the product if I should be happier if I take a matrix in terms of its columns and I X diagonal matrix on the right what that does and this reflects the fact that those are eigenvalues what that does is it just scaled those vectors ": [
            2410.3,
            2436.1,
            80
        ],
        "write down what all this is now. minus the top part of the V2 and you one we just computer that that's 3 / the length of you 1 squared which is 9 time's the vector you want. You want half the picture you want which is 221 - 2 okay, so that is 0 1 - 1 - 2/3 1/3 and -2/3 So that gives us - 2/3. 1 ": [
            630.7,
            669.9,
            23
        ],
        "you already did that already composition of a matrix in Matlab to do the QR factorization and do the QR process or do it manually yourself if you want to write a little script, okay and calculate these matrices. They wanted to 8384 for some input Matrix and you will see that after a hundred steps. Matrix will be there anymore. Hey, there aren't actually sterile, but they're very close ": [
            1401.0,
            1432.1,
            48
        ],
        "you. It with a v How does that help us? Well? that let's just remember the definition of the top products hear X. Y is just equal to X transpose X Y, so that is equal to hey you transpose x v - you transpose x a v I know I'm just going to do one more step here, which is looking here a you transpose. We already did this ": [
            2010.9,
            2046.9,
            68
        ]
    },
    "File Name": "Linear_Algebra___B00___Kemp__Todd_Aahron___Winter_2018-lecture_27.flac",
    "Full Transcript": "Happy Pi Day everyone.  Welcome to the last new lecture of math 18 will be finished with all the course material today. Next lecture on Friday is review for the final exam.  So today we are going to  finish our discussion from last time of the gram Schmidt orthogonalization procedure and the QR factorization of matrices.  And then we will move on to discuss the spectral theorem the most important theorem in mathematics.  Quick administrative reminders. Please do fill out your capes. I checked the response rate last night and it's hovering around 30% at the moment. So 2/3 of you haven't filled them out you have until this Friday evening to fill them out. I definitely and my department and the university would appreciate your feedback. So if you can please take 5 minutes and fill that out would really appreciate it.  You have one final MyMathLab homework set it to do tomorrow night at 11:59 p.m. On all the previous ones. There's been a one-week Grace. Afterward during which you can still do problems for 50% credit this time. I've said that Grace. To be slightly less. It's until next Wednesday. I'm guessing you guys are going to be busy anyway, but finals next week and we need to get your grades process. So I hope you'll finish it by tomorrow night. But if you're doing a late you have only six days instead of 5 this time to do the late editions. And once again your final exam is this Saturday for days from today in three rooms listed their Galbraith hall room 242 Peterson hall room 108 and your call room 2722. Those are all nice big rooms. Okay, you're going to be able to spread out. I mean you're going to have seating assignment assignments. Have you spread out? Most of you will have no one sitting on either side of you is I'm forcing not quite possible for that.  And for everyone there is twice as many seats as we need but one point eight times in a suit says we need but it's going to be roomier than the midterms have been so I hope that will make it a more pleasant experience. Also, I'm happy to tell you that the final is worth 50 points. So it is 1 2/3 times as long as the midterms were one and two thirds times as long as a designed 1 hour exam what you have 3 hours to do it. I hope that time pressure will not play any role. Okay, you just need to show that you learned and understood what we've done so far.  All together and you're sitting room assignment have been posted and should be visible to you on TriNet.  All right. So let's review what we did last time.  So we're talking about orthogonalisation. So if you are given a Subspace and you find a basis for it, we now have many ways to find Bayseas they all boil down to the same thing more or less but in different situations, we have different descriptions and algorithms for how to find a basis. You're given a basis for a Subspace. How do you find an orthonormal basis for that Subspace? Okay, that's what the gram Schmidt orthogonalization or orthodontist ization procedure does for you. You're given a collection of vectors linearly independent vectors.  The gram Schmidt process produces from those a new list of vectors that span the same space spend the same.  Subspace that the original vectors did but they are orthonormal vectors and the way you construct them is you just take the first one in the list and you can choose which is the first if you want to shuffle them around and take the first one and then the second one you take it and subtract from it. It's orthogonal projection onto the first and then you take the third one and you subtract from it. It's orthogonal projection onto the first and the second.  Okay and proceed that way at that will automatically produce for you a list of orthogonal vectors know if the ventures you started with warrants linearly independent, for example, suppose V3 within the span of V1 and V2 then look at that third line there were taking V3 and subtracting from its orthogonal projection onto you one and it's installing a projection onto you too. But if these trees is in the span of V1 and V2, it's therefore in the span of you want and you too in which case it's orthogonal projection into the space van, but you wanted you to is itself and so that third term used to be there would be zero. So if you start with vectors that aren't linearly independent the gram Schmidt process will produce sum of 0 vectors for you. That's okay.  You just have to throw them away if that happens. Okay, but it's better probably should just start with a basis of the Subspace dependent factors. You get linearly independent factors because they're orthogonal at 9072 Forester normalize them. You have to normalize them if they're already orthogonal then you just divide them by their links and you have worked for normal vectors. So that's the gram Schmidt process will do one more example of applying it in a moment.  But the observation we made at the end of last lecture was that if you put this all together noticed that there's a triangular pattern up there only depends on V1 X on you wanted me to which is the same as depending on V1 and V2 and use three depends on you one U2 and V3 the same as depending on V1 V2 and V3. So you 7 is in the span of V1 through the 7th doesn't need all the other future V. So you get a triangular Turtle pattern there and that triangular pattern is reflected in the fact that this can be systematizing put together to say that you have a certain Matrix factorization Matrix decomposition you start with your Matrix with the vectors v as its columns. Let's call that Matrix a  Now it is well either vectors in RN so there's and Rose and RP columns here. And of course if these are going to be linearly independent, then p is less than or equal to a surface is typically a tall skinny Matrix what the gram Schmidt orthogonalization procedure does for you. It's produced these or Sonoma vectors here called that Matrix Q.  And what this triangular relationship between the use in that Matrix a cute. I meant to write End by P and R is p by P.  So we have a fever and it's just populated by the numbers that come up. The numbers are one of the length of you one and V2. You 1 / the length of view 1 and length of you too and so on in that list. And so if you were a computer and can remember exactly how to take those numbers normalized appropriately and put them into this Matrix there. Then immediately gives you this decomposition. And the point is that the Triangular structure out there gives you an opera training going to Matrix for the are there are factorization of the Matrix a  Hey, so if you were a computer than you would once you've done the gram Schmidt process, you could immediately write down 2 and r.  You're probably not a computer. So you can write down the Q because that's what gram Schmidt does. That's the hats that you produced but figuring out which coefficient to put we're in the army tricks. That's a pain in the butt to try to remember but it's okay. You don't need to okay. There's a I'm not going to make you do it cuz this is going to be on the final but there's a shortcut the alarm cut but there's an easier way to do it. Even if it's less efficient a matrix linearly independent.  So I'm going to do gram Schmidt orthogonalization and produce an Orson Orville basis for the same factors as they were going to find.  orthonormal basis  for the column space of a so let's call these two vectors V1 and V2. So we're going to find that are normal basis using the gram Schmidt process to the first thing we do is we take the first vector and we call it you want instead. That's our first Spectre D1 there and we need to normalize we compute the length of you 1 squared is 2 squared + 1 squared + -2 squared  which is 9  that's what that says.  Set the length of you one is \u221a 9 or 3. And so we have our first Vector our first collector you one hat.  Is V 1/3 2/3?  1/3 + -2/3  okay. Now we need to produce the second Vector in our list. So we following Brown Schmidt with the second Vector is V2.  That's why the second new Vector is you too and would like that to just be me too. And then we be done but in general we have to take the projection TV two on two Hue one.  Or you want to hack if we want just a matter of whether we want to normalize before or after I'm just going to do this.  We need to calculate V2 dotted with you one you wanted the same as v111.  0 + 1 - 2  written down one wrong 0 + 1 + 2 which is 302 find an orthogonal basis. So let's just write down what all this is now.  minus the top part of the V2 and you one we just computer that that's 3 / the length of you 1 squared which is 9  time's the vector you want. You want half the picture you want which is 221 - 2  okay, so that is  0 1 - 1 -  2/3 1/3 and -2/3  So that gives us - 2/3.  1 - 1/3 is + 2/3 + -1 - -2/3 is 2/3 - 1 which is -1.  So there's are you too. And then the last thing we need to do is normalize it so we just compute the length of U2 squared is -2/3 squared + 2/3 squared + -1/3 squared  So that is 4/9 + 4/9 + 1/9.  What is 9/9 for 1 miraculously this Vector was already normalized that typically won't happen. He probably will have to normalize it again. But in this case it worked out. It was already a normalize vector.  So therefore you two hat is the same as you to it was already normalized. So it's right here.  So we have found our orthodontic basis it consists of this one and this one to be clear. That's an ordinal basis of the column space of a it is not an orthonormal basis of our three where those vectors lives. There's only two of them. It can't be a basis of our three. It's an orthonormal basis of a Subspace the Subspace spanned by the original vectors V1 and V2, which is the column space of a so, there is our personal know the QR decomposition composition says, okay. We know we now know that we can write.  Hey in the form Q x r where Q is the Matrix formed by the two vectors you want hat and you too hot. That's cute.  An RR is going to be let's see. What size is it going to be is 3 by 2.  Accu is 3 by 2. So to multiply 3 by to buy something and get a 3 by 2. You have to have that something be to buy two so far is going to be a two-by-two matrix and the numbers that are up here already modulus of normalization the length of product / the length of you want so they're already there, but you don't have to remember that because we can make the following observation.  The Columns of cube are orthonormal.  Matrix  I can encode the. Products between his columns in this Square Matrix Q transpose Q if those columns are orthonormal as we saw two lectures ago that precisely means to transpose Q is the identity Matrix.  Now, how does that help us here? We'll just look at the equation.  therefore If I multiply both sides of that equation by Q transpose  what to transpose Q is the identity Matrix this we just said right here.  So that's just the same thing as the identity Matrix times are which is just our that's what our is.  You don't need to remember how to put those triangular coefficients into the army tricks. Once you find the Q you just take to transpose times the original Matrix a and you'll get your are so let's write that down in this case. Our is Hugh transpose times. We take the Matrix that has those columns turned on their side to transpose is 2/3 1/3 - 2/3 is the first row - 2/3 2/3 - 1/3 as the second row and we multiply that by the original Matrix.  And let's just work out with this 2 by 2 Matrix is the first entry is 4/3 + 1/3 - 4/3, which is 1/3.  The entry below that is -4/3 + 2/3 + 2/3 is indeed 0 As We Knew It was supposed to be because this is supposed to be the two one entry or the one to entry. I mean is 0 + 1/3 + 2/3 is 1  And The Last Story.  The first century is 4/3.  + 1/3  + 4/3 thank you. Okay, 4/3 + 1/3 + 4/3, which is 9/3, which is 3  thank you. I think I got the other two right though.  And then the last entry is -2/3 * 020 + 2/3 + 1/3, so it should just be one again.  And there is RR Matrix.  Okay, and if you want you can verify that Q x r is equal to a QR decomposition and that's really how you would find it. If you were asked on an exam on a homework set to find the QR factorization of the Matrix. You would do gram Schmidt to find the Q and then to find the are you would just multiply Q transpose times original Matrix day. This is less efficient than just figuring out which coefficients appeared in the process and putting them in the Matrix if a computer is doing it and it knows where to look for those numbers to put in the Triangular Matrix that requires no flops.  Item requires no conversation with all the competition is already done. Where is this matrix multiplication take some flops, but in the size of the examples you guys are doing it's easier to do that matrix multiplication then to try to remember exactly which coefficient close to where you will find that you are factorization of The Matrix.  But why would you want to find the QR factorization of a matrix? Well, let me make a comment first.  So the comment I want to make first is remember in the general case where we have to say is tall and skinny where it has more rows and columns those columns. Do not form a basis of RN where they live takes. The queue is also and VIPs tall and skinny. They do not form the basis of our end that Matrix Q is typically not invertible. It's not SquarePants convertible. So well, we do have the equation that you choose that to transpose Q is the identity that does not mean that cute you transpose is the identity. There is no inverse Fork you typically and we saw already transpose what it represents that bigger Square Matrix Matrix that represents the Matrix of the linear transformation, which is orthogonal projection onto the column space of it.  But in the special case we're and was able to pee by started with a basis of our at not necessarily orthogonal and I used the groundsman process to produce an orthonormal basis The Matrix to the square. Matrix is invertible.  So how does that help us here? Well, let's take a look here how that helps us is it turns out this can be used to very quickly and very efficiently approximate eigenvalues not I'm not going to actually show you how this works in the example of it would be fun for you to try out Matlab to see how it doesn't but here's how it works. This is what's called a QR algorithm and this is how I met Love Actually compute eigenvalues. So you start with a square Matrix at  There was a square Matrix Ai and you are going to do the QR factorization of it snow for this to work exactly as stated. He had better be invertible. Okay. So let's just say is an invertible Matrix basis to start with for the car is equal to Q transposed.  Now here's what mut level do next will say, okay, I found the Q in the are now. I'm going to write a new Matrix down call A1 different Matrix from before because our times cute that you can reverse the meniscus cause both of the mission is to an are there End by end in the Square Matrix. They're the same size. It's typically not equal to a matrix multiplication doesn't commute.  So, why would we do this? Here's why are X QR is equal to Q transpose Hyundai? So this is to transpose a q.  but  in the special Times Square  Q transfer Q means that you transpose is the inverse of Q.  This is cute in verse a q.  If I have a matrix and I take an invertible Matrix on the left X the inverse that in real Matrix on the right.  What is that due to the Matrix? Remember what that transformation is called does matrices are called.  But remember this similar, this is the similarity transformation. So the Matrix A1 is similar to The Matrix.  Now what does similar mean while it means exactly this but in particular it implies that a one and a they have the same eigenvalues. So even though this new Matrix a one is different from a its eigenvalues are the same as as I can tell you. So if you're trying to find a zygon value, you can try to compute a ones I can buy it instead easier to compute a ones I can tell you not necessarily but what if we do it again?  So now let's take a 1.  I know we can do it QR decomposition.  Different than the CUNY are we got last time? We have a Q on an R1 would you Grandma Schmidt on The Columns of a one with a different from The Columns of a so we have a new QR decomposition.  And now from there with Mormon a 2 which is our one q12 now exactly the same thing. We just saw before our 1 is equal to one transpose a one.  So this is equal to q1 inverse of a one-to-one which is similar to a one which is similar to a so this new Matrix A2 is also similar to the original Matrix. It also has the same eigenvalues. It's probably a totally different Matrix, but now I've got three different matrices. They all have the same eigenvalues if it's easier to compute eigenvalues. If one of them then I can choose that one. Now. Is this one going to be any easier not necessarily but  I can produce a whole string of these a A1 A2 A3.  a 100  and here is the serum.  as n goes to Infinity  a n in this sequence rapidly converges  to an upper triangular Matrix  and that's something you can check on Matlab. Can you can read up Google it how to do the QR decomposition of a matrix or Flop from your homework or you already did that already composition of a matrix in Matlab to do the QR factorization and do the QR process or do it manually yourself if you want to write a little script, okay and calculate these matrices. They wanted to 8384 for some input Matrix and you will see that after a hundred steps. Matrix will be there anymore.  Hey, there aren't actually sterile, but they're very close now an upper triangular Matrix. You can pick off its eigenvalues its eigenvalues are on the deck.  That is how about love compute eigenvalues when you put in a in Matlab for a matrix a what it does, is it erased the QR factorization and reverses it like this over and over and picks off the diagonal entries. It does some small tweaks to this algorithm to make it even faster, but that's the basic idea.  So that is how you compute eigenvalues and that's the reason that you are factorization is actually really important. It's used everywhere in science anywhere you need eigenvalues QR factorization is how you actually numerically computer.  Okay, so you need to understand the QR factorization?  All right, so that's what I want to say about you are now I want to finish the course with Section 7.1.  So let's continue on the steam here.  So we just had  the QR factorization with start with a basis with over the Matrix a and we replaced it with a matrix with the same column space cute that has orthogonal normal columns. And we saw that that is useful for comparing eigenvalue still on this team of eigenvalues and orthogonal vectors. Let's think back we started talking about eigenvalues two weeks ago two and a half weeks ago.  What sucks about diagonalize ability? This is an important topic for the final exam. So I'll give you a square Matrix a and I can ask you is it diagonalizable? Remember what that means? I will Matrix is one that is similar to a diagonal matrix. That's what I've written. There can be written as PDP inverse Scorpios and invertible Matrix India's diagonal but that's equivalent of the right geometric way to understand. It's diagonalizable. If there is a basis of RN that is composed of eigenvectors of a  Okay, great. So you get a basis of RN and you've done lots of examples where you find the eigenspaces and you see that there are enough eigenvectors to form a basis of all time.  No, most of the time when you have done that the bases that you get is not an orthogonal basis, the eigenvectors are typically not orthogonal that you could do gram Schmidt on them to produce an orthogonal basis, but then that new orthogonal basis will not be composed of eigenvectors. There's rigidity here.  Hey, you can't just change the vectors tickling your combinations of them and still have them be eigenvectors. You can't do that typically.  So the question is when does it just happen?  Is orthogonal basis of the best kind of basis for what kinds of matrices is it true that there is an orthogonal basis of eigenvectors or normal basis of so, let's see if we can suss out which kind of matrices might have that property. So what I'm saying is that I have a matrix diagonalizable.  where the P Matrix remember that's so if so if  they are worse than normal.  Then what we got is that?  Let's rewrite is q q as an orthogonal Matrix.  Where Q transpose x q is the identity of eigenvectors orthogonal basis in the in this case means that to transpose is Q inverse.  So this says we can write this as qdq inverse, but you inverse is the same as Q transposed.  That's what we're looking for. We're looking for when the eigenvector Matrix is an orthogonal Matrix in which case the diagonalization is not and what's great about that is that it's a whole lot easier to find the transpose of a matrix than the inverse of a matrix. So if you knew already that your Matrix was orthogonally diagonalizable, which is a lot of syllables but just means that it has an orthogonal basis of eigenvectors. You just have to find the Q in the D. You don't need to find the inverse because the inverse is just to transpose just turn the Matrix on its side. Okay. So when that happens that's what happened. That's that's what we'll get. But when will it happen? So here's the observation I want to make which is  If I have such a matrix, that's orthogonally diagonalizable that has an orthogonal basis of eigenvectors. Let's look at its transpose a transpose is equal to what we just said that we can factorize as QD to transpose that's to transpose of a product is the product of the transpose is but in the other order,  this is to transpose transpose x d transpose x q transpose.  No transpose, it just reflects the Matrix across a diagonal. So if you do it again, it reflects it back you get the original Matrix back. So that's Q D transpose.  What kind of Matrix is d?  It's diagonal what that means is that it's got entries on the diagonal and zeros above and below. Now. If you transpose that Matrix, I swap the above and below triangular Parts. Is it down transpose?  so look, if a is orthogonally diagonalizable then it's transpose is  itself  text we call as I mentioned before we call such matrices symmetric.  Okay, so that's the conclusion of a symmetric.  Those are the only kind of matrices we could possibly expect to have a North Enola basis of eigenvectors.  Okay, great. So I give you and I give you a matrix. That is symmetric. That's an easy condition to check remember if you find a matrix in general you are asked is it diagonalizable? That's just hard to check you can't tell by looking at it. Usually you just have to go through all the math to see if there are enough real eigenvalues and if the algebraic multiplicity and geometric multiplicity of each other like in vallese match the pain in the butt most of time.  Where has this condition of being symmetric is easy to check visually do I have the same answers above the dragon level up just to look and see so those that's a that's a necessary condition for having an orthogonal basis of eigenvectors. Okay, but that doesn't mean necessarily that every such Matrix that every symmetric Matrix is orthogonal diagonalizable. Well, let's see. So that's the question here. That's the converse. Can we always orthogonally diagonalize any such symmetric Matrix? So to move in that direction to see if that's true. Let's look at the following calculation suppose. I take two eigenvalues. So I have it suppose. I have a symmetric Matrix a  and suppose that I've got two eigenvectors for it you  envy  two eigenvectors with two distinct eigenvalues in that situation.  Sound rygin vectors, you know, we already did a calculation like this for lectures to go for a general Matrix. If I have a situation like this where I have a matrix that has two distinct eigenvalues remember that we prove that the eigenvectors are there for  Made of cheese what what's true about eigenvectors of eigenvectors of distinct eigenvalues?  I heard an answer, but please say it louder.  You weren't confident about your answer. Okay, could they be parallel to each other? So what are they?  Are linearly independent. So for any Matrix if you have values are linearly independent. Well, if you have a symmetric Matrix, you get the best kind of linear Independence. Here's a fun little calculation and take their. Product to be silly by this. Nonzero number Lambda - meet me with a nonzero number cuz I'm done near distinct eigenvalues.  I'm going to compute this product X this. Product. I'm going to do it in this really funny backwards toward away so far. It's alright Lambda times you. V - mute you. V.  I'm going to write that as Lambda times you. V - you. Movie.  and the reason that I'm reading it that way is that  Lambda you is a you and Landon is a v so I can write this as  dotted with v - you. It with a v  How does that help us? Well?  that  let's just remember the definition of the top products hear X. Y is just equal to X transpose X Y, so that is equal to  hey you transpose x v - you transpose x a v  I know I'm just going to do one more step here, which is looking here a you transpose.  We already did this on the last flight. The transpose of a product is the product of the transpose is in the other order.  So this is equal to you transpose a transpose V - you transpose a but if we're in the city.  Symmetric then those two things are equal.  so if a equals a transpose  then this is there a  So when I get is that this is non-zero number you landed - New Times you. V is 0 and there for you. V is different.  So therefore you is orthogonal to be that's the conclusion here in general. If you have any Matrix and it has two distinct eigenvalues and eigenvectors realize I can values will be linearly independent even better. If your Matrix is symmetric, you have to eigenvalues with distinct to eigenvectors with distinct eigenvalues. They are orthogonal.  Fits together with this thing. We're asking about up here because we're asking can I orthogonally diagonalize can I find a basis of eigenvectors? That is actually an orthonormal basis. Well what this says if I have distinct eigenvalues, yes.  Okay, so  if I have all distinct real eigenvalues for my symmetric Matrix, then yes, it will have a North normal basis of eigenvectors by that calculation. No better yet.  If they're not all distinct The Matrix might still be diagonalizable. So if I have a symmetric Matrix and it happens to be diagonalizable, then what this calculation just showed us is that the distinct eigenspaces or if each other is if I take two vectors one from the other from the from the three-dimensional eigenspace of the eigenvalue 7 it well, those might not be orthogonal but we can make them orthogonal doing the gram Schmidt orthogonalization procedure. That is we start with the basis of eigenvectors. They are already orthogonal that's only going to modify the vectors with any combination of eigenvectors.  if  is diagonalizable  and symmetric then yes, there is an orthodontist basis of eigenvectors.  So  who's to say that's in this case, then we can find you one. Hat you to have you and hat and we string them together.  Animatrix q and we will get that a is q d q inverse which is the same as QC to transpose where this D. As usual is the diagonal matrix with the eigenvalues.  Okay, so that's a nice little observation here. So if I Matrix is orthogonal diagonalizable, it must be symmetric if it is symmetric, and if we already know that it can be diagonalized at all. Then it can be diagonalized orthogonally, but that still doesn't answer the question. We wants to know can I just inspect the Matrix and see from its symmetry that it is diagonalizable and therefore for stopping leg itemizable.  And the answer is yes. It's called the spectral theorem. And the ceremony is yes every symmetric Matrix is diagonalizable and therefore orthogonally diagonalizable if your Matrix is symmetric, if it's transpose, if you can just look at it and then you know without doing any work, but the Matrix can be orthogonally diagonalize it has an orthogonal basis of eigenvectors.  I am not going to prove this there.  We don't have the tools to prove this then we would need either some of the fundamental theorem of linear algebra, because we really have to deal with complex roots of polynomials to show it actually at the end of the day they are real or we would need some calculus. There's a beautiful Vector calculus proof of this there. I'm so those of you who've taken math 20c and I could show you a proof If you really want not right now but in office hours or something that is a beautiful proof of it. It's not outside the realm of possibility to prove it for us. It was just take more time that I want to and I'm not interested in the proof. I'm interested in the statement. Okay. So the spectral theorem says there's one class of matrices the best kind of symmetric matrices, which you can just tell by looking they are diagonalizable. And in fact, they are orthogonally diagonalizable. Let me point out one thing as an alternate formulation of what that means.  Which is called the spectral decomposition of such matrices because what this says is that if I takes my Matrix a my symmetric Matrix, then I can decompose it as DQ transfer at you and hat is RQ.  is this  and then to inverse but the beauty is  I'm so on The Columns of a get turned in.  Okay. Well now how does this work?  Well from the diagonalization when we did this in the first place how the diagonals ation arose we noted that if I take the product if I should be happier if I take a matrix in terms of its columns and I X diagonal matrix on the right what that does and this reflects the fact that those are eigenvalues what that does is it just scaled those vectors by the diagonal entries?  Announce here. We've got a matrix and I've given you as columns Columns of Islam does scaling the original use  and I X Matrix written in terms of its Rose. But the nice thing there is remember how matrix multiplication works the entries of the matrix product are given by row times column. So if you work that out here, what you find is that this is just equal to Lambda 1 times you won you won transpose.  Plus Lambda two times you to you to transpose.  Plus down the line to climb to Nunu and transpose.  Spectral decomposition because think about it one more step. Well, but one more step here to identify these things. So you won you to you and these are north and normal basis of our at  What is this u11 transpose that's the Matrix of the orthogonal projection onto you want?  So I can write this one more way which is this a Lambda One X the projection onto you one.  Plus Lambda two times the projection onto YouTube.  It is Matrix is a sum of projection matrices. It's a linear combination of projection. So you have this nice basements of eigenvectors. And if you take the orthogonal projections on to those one-dimensional subspaces, then your original Matrix is the linear combination of those matrices that those projection matrices.  Where the linear coefficient are the eigenvalues that's called the spectral decomposition of only works for symmetric matrices, but it's beautiful because it's geometrically now easy to understand what that metrics does. Okay. It says I want to know what happens to a vector here if it's eigenvectors are the standard basis vectors in this room. All I do is I first projected onto the XY plane and then projected onto the x-axis and I scale it by the X eigenvector then I projected into the us over there and I multiply that by the way and I'm all for that by this VI.  Hey, it's the easiest possible way to understand geometrically what a matrix does and it works for every symmetric Matrix. So that decision and you do need to know these things for the final exam and test it on them on your homework. The last seven minutes of class. I would like to answer the question. Why is it called spectral? It should probably be called the projection decomposition or something like that white spectral. This is a really fancy sounding word. That means different things in different contexts. Some old word 18th century. Where is here? It comes from pictures like this.  But you've seen if you've taken even in high school, but certainly in University if you've seen in physics and chemistry classes it what is the Spectrum if I take a glass tube is filled with only hydrogen gas and I have two electrodes at the end of my run a strong current through the gas turn off the lights then I will see the two glowing and if I pass it through a prism to separated into its different frequencies, I find something remarkable scientist discovered in the late 19th century that had to had no explanation for the time which is that you get lots of colors out you get a full spectrum of colors out with a few Exceptions. There are four or five lines missing.  spectral lines of hydrogen  How does time went on?  Physicist discovered that there are actually many more lines outside the visible spectrum that are missing infinitely many lines that are missing these form of the emission spectrum of hydrogen.  When did were first discovered nobody had any idea what that could possibly mean? Okay, but some physicists in the first part the first twenty years of the twentieth century figured out that our whole model for physics was wrong.  Because of these experiments and I figured out that what's going on. Is that the kind of  Light energy that can be emitted and absorbed by hydrogen. It only comes in discrete quantized amounts. This was the beginning of what is now called quantum mechanics.  Call quantum mechanics when it was invented by Heisenberg. He called it Matrix mechanics because the model and you've never seen this before unless you've taken higher level quantum mechanics classes, but the model of quantum mechanics, all right here that he formulated which is really still how it works.  Is that here's how the physical system is modeled the hydrogen atoms that have the the state of the physical system inside that glass tube is modeled as a vector in some big Vector space What Vector space was kind of like the one we've been confused about the most in this room the vector space p of polynomials. It's a space of functions that are called wave functions, but you don't need to worry about that at some big Vector space.  And there are operators there are matrices Infinity by Infinity matrices that act on those vectors linear Transformations act on those pictures and those linear Transformations. They encode all of the physical operations and measurements that we can make in particular. There's an energy operator, which is called the hamiltonian because of the measure of the energy of the system. You multiply that by that Matrix.  Now that Matrix is a symmetric Matrix all the energy operators all the physical operators in quantum mechanics are symmetric matrices, which means they are orthogonally diagonalizable which means that there is a basis of the wave functions consisting of eigenvectors and eigenvalues.  Are the eigenvalues the eigenvalues of the energy operator for the hydrogen atom? Those are the only thing it can only be in an eigenvector State. That's when you know, you've heard this crazy phrase collapsing the wave function before in popular literature what that literally means is you have a system that is a linear combination of the eigenvectors because they form a basis. It's in some random state. But when you take a measurement of the energy what happens if you end up applying the prediction on one of those vectors, that's what it means to take a measurement you project onto the eigenspace and then what you measure is the corresponding eigenvalue.  That's what quantum mechanics says from beginning to end quantum mechanics is all linear algebra. And by the way, I just told you what the eigenvalues of a there's no spectral lines. What are the eigenvectors of functions are going to be these factors are functions in three variables functions in space. What do those functions look like? What do the eigenvectors look like? You've seen them before because you took chemistry in high school. They look like this.  That's what these pictures these orbital pictures of electron clouds. That's what they are. That's all they are there the eigenvectors of the Matrix that represents the energy of a hydrogen atom better reasons why the spectral theorem is the most important theorem in mathematics. In fact is the most important serum in all of science.  Because it's the basis of quantum mechanics and quantum mechanics is the basis of all of physical science in particular chemistry this picture here up there the periodic table it was written down 80 years before the turn of the twentieth century people understood that chemical elements came in his periodic bands with with behavior that reflected each other but had no idea why it was all explained by these pictures. It was all explained by the eigenvector analysis of this Matrix. So among other things I told you that is all and everything that computer can do at the end of the day linear algebra is all in everything that the Universe can do and that's why I think this is the most important class that you've taken.  See you I'll see you all on Friday for review for the finals. "
}