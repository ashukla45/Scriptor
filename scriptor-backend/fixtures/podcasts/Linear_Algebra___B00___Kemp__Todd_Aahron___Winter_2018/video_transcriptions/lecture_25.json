{
    "Blurbs": {
        "/ X dotted with x. Accidentally use the vector you instead of X here. So we're not confused with inefficient the last night if he is has been of some Vector you that this is why daughters with you / the length of you squared that's what we computed on the last slides. At least we know how it works last flight. I said the projection onto a vector you ": [
            1630.4,
            1662.4,
            46
        ],
        "/ the length of you squared time's the vector U. So let's figure out what those three numbers those two numbers. Are you. It was V is equal to 1 + 0 + 0 1 squared + 2 squared + -2 squared Okay X the factor you which is 1 to -2. So this is 1/9 of the vector 1 2-2, that's the projection. It's shorter because the vector from ": [
            1285.4,
            1329.9,
            37
        ],
        "2. and then to -4 and 4 and that's it. That's the Matrix of the orthogonal projection onto this one-dimensional Subspace. And by the way, you can go ahead and compute now that if you multiply that matrix by the vector y 1 Y 2 y 3 you'll get exactly the formula that's written up there. I'll leave that to you to verify the matrix multiplication agrees with that formula ": [
            2598.3,
            2625.0,
            74
        ],
        "And so on Down the Line. That's how you do it. Can you fix a Northland or an orthogonal basis and you project your vector onto each one of the basis vectors and then you add those up and if you think about How we calculate the coordinates of a vector in euclidean space thinking of the standard basis as an orphan Armel basis. That's exactly what we're doing here. ": [
            1815.0,
            1845.7,
            51
        ],
        "Are we? That does not tell us anything about you you transpose. I throw you you transpose. Well you is and by P you transpose is p by n. Okay. So the size of this Matrix is an bi an but this is typically false. In fact, it'll always be false. If p is less than a man. Okay that the product you you transpose. It's not transpose you is. ": [
            385.9,
            424.7,
            12
        ],
        "Happy Friday. Good morning. It is a happy Friday. Sun is shining feels like spring out there. And we got some money rosberg going on in here. It's a good time. Two administrative reminders. And once again, please fill in your capes your student Professor end and course evaluations 9 to fill those out online. Your final MyMathLab homework sets has been assigned posted last night you have until next ": [
            1.5,
            38.4,
            0
        ],
        "I project any Vector onto another one just with that simple formula. Okay. So for example got a couple down here. the vector v onto the vector you hear it with these two vectors in R3. The picture that I drew that was in the plane definition of the 376 whatever you want the projection of the vector v onto the vector you is simply asked you. It was me. ": [
            1242.3,
            1283.9,
            36
        ],
        "I'm going to do is I'm going to take the real numbers and I'm going to find the best approximation that my model gives of those numbers to adjust my model for the next iteration how to find the best approximation in my mouth that I can find to the actual numbers. I need to be able to closest point to a Subspace thing. this is the single most important ": [
            2935.8,
            2959.0,
            83
        ],
        "If you have a rectangular Matrix, well be the case that has a left inverse but not a right at first but that will be the case here too. Now. What is that was PT is equal to end. That means that this Matrix which has the columns equal to those vectors you want you to up the UN is a square Matrix. Fix now in that case. Those vectors ": [
            430.9,
            470.1,
            13
        ],
        "It has components 2 and 3 what I'm saying is I projected onto the first base and so that better is 2 * 81 + 3. * that's exactly what this tells me. So now let me make a few remarks. the bassist what's the normal? That means that those links of the use are all one. And therefore we have the slightly simpler formula. That we just get why. ": [
            1845.7,
            1896.2,
            52
        ],
        "It with you one X you want + y. It with you two times you too. Etc why. It with you pee. I'm sleepy. So that's one way that it's nicer to work with an orthodontic basis than just an orthogonal basis. Let me be clear here. This is a theorem. I haven't proved it. It's not hard to prove that one. Let me give you an idea of why ": [
            1896.2,
            1922.2,
            53
        ],
        "Lambda X. It with x and really all we need to do is figure out what is 10 / X. If I have any nonzero Vector ex that X. It was except the length of X, right. So this tells me hear that V. It with x - Lambda times. The length of a squared is equal to 0 I can solve for Lambda there Lambda is equal to be ": [
            1173.3,
            1206.8,
            34
        ],
        "Okay know if the Subspace V is one-dimensional there's not so much difference between those but actually we should verify that things are making sense of 9V. I could have used one of you is the span of 9u I could have used a scalar multiple of. Okay, so well, I have this formula here in terms of you. That means I should also make sure that they make sense ": [
            1663.5,
            1693.1,
            47
        ],
        "RN, you can't have more than any of them. So P orthogonal vectors and rnp must be less than or equal to n it could be equal to end which case you have an linearly independent vectors an RN they form a basis in which case they would form an orthogonal basis or or is it normal? Now we saw last time at the end of the lecture that there's ": [
            189.2,
            212.7,
            6
        ],
        "Remember I said if you've got an orthonormal set of vectors, that means that you transpose you is the identity but you you transpose may not be the identity. If the vectors are there fewer vectors then and in our end then you transpose what video is the orthogonal projection onto the Subspace spanned by the vectors? You started with an orthonormal basis for RN then the Subspace spanned by ": [
            2316.2,
            2347.4,
            66
        ],
        "So the ortho part is vacuous. We don't need to find to find orthogonal vectors. There's only one vector. There doesn't need to be anything. But we do need to have the normal part. Would you need to have a unit length of vector? So that means we should replace. The vector here, which I'll call you. with It's normalization which means we divide it by its length and it's ": [
            2498.4,
            2524.8,
            71
        ],
        "The other is in the park. That's exactly what we did on this last slide. We had our vector. V K and what we did was we wanted to write it. We aren't projected onto x what we did was we wrote that v as the sum of two vectors the one we called. Where is in the Subspace spanned by X. Okay. That is exactly what we're doing when ": [
            1519.0,
            1556.4,
            43
        ],
        "Thursday at 11:59 p.m. That covers the remaining material covering in this class from chapters 5 6 and 7. Your last night lab assignment is due tomorrow night at 11:59 and you're not love quiz. Just as a reminder is next week on Tuesday in the time slots that you normally have your section on Thursday. If you have a conflict than there are conflict time to set up and ": [
            38.4,
            68.1,
            1
        ],
        "Wasaga. No just like the example. We stopped here. If the vectors xnv up there are pretty close to orthogonal then if we project V down on taxes going to be pretty close to zero here to voltar that point. Let's project the vector w. On to the vector you well, that means we need to compute you. It with W / the length of you squared times the vector ": [
            1329.9,
            1356.6,
            38
        ],
        "Y-12 be the one that's inviso. We do this position and the part. That's envy that is the orthogonal projection of the vector. That's the definition. How do you actually compute it? Well, we just saw that you computed in one-dimensional case. If V is the span of some Vector X. And we saw that the projection. On TV up some Vector Y is equal to Y dotted with x ": [
            1594.6,
            1630.4,
            45
        ],
        "a Subspace this formula make sense. Okay, but still that doesn't answer the question. How do we compute the orthogonal projection onto a Subspace? You know it exists, So here's how we do it in the projection onto V is given by this formula is / is any basis vector? Any basis Vector of the Subspace V? That is how we're going to generalize I think in terms of a ": [
            1723.2,
            1769.5,
            49
        ],
        "a collection of vectors are normal transpose you is the identity Matrix is an nyp matrix by P. Identity Matrix, okay. Very large this Matrix P by P its size. It's a square Matrix is equal to the number of extras in the set. Okay, so that's that's if I want to make an important comment here which is in the typical situation where we have fewer than n vectors. ": [
            327.7,
            385.9,
            11
        ],
        "a dotted with that that Matrix there that. That's call him back after I wish I could write as y 1 - 2y 2 + 2 y 3 time is the vector. one- to two so there's a perfectly good formula for the projection of a vector into the Subspace spanned by that vector. But we have to find an orthonormal basis. Find an orthonormal basis of vee-vee is one-dimensional. ": [
            2460.0,
            2498.4,
            70
        ],
        "a e x x is equal to the linear combination of the columns. given by the vector X right Well, look at what I've written here for the orthogonal projection the orthogonal projection. Why is a certain linear combination of those? so if I asked as before if I Write you as the Matrix whose columns. Are you want you to through you pee? Then just from the definition of ": [
            2191.1,
            2227.2,
            63
        ],
        "a sort of nifty way or a very concise way to express this or condition as you went through Ups training together at The Columns of a matrix as usual so that Matrix is going to be typically taller than it is wide. It will have more rows and columns or at worst equal number of rows and columns. and what we saw was that the dot product of you ": [
            212.7,
            236.3,
            7
        ],
        "a vertical line again. Just like we did on the side where we introduced this. here in this example Is the projection of Y into V? There are lots of vectors Envy. Let's draw some other ones. Here's one. Here's one. What is the distance from each of those vectors to why the distance from this first green vectorizer to why is the length of that line right there? And ": [
            2771.4,
            2805.0,
            79
        ],
        "all the vectors are orthogonal to each other. So that means that if I take you wanted you to its. Product of 0 and staying with you too and you want by taking you want to do 3 there. Product of 0 and same with you three and you want you too and you 3 there. Product is zero and so on we find that all of the entries of ": [
            273.8,
            294.1,
            9
        ],
        "anyway, that is how you compute orthogonal projections. and here I've just restated what I said on the on the last slide there that if you have an orphan Armel basis already then That you are orthogonal projection is just given by inspector and add those guys up. Okay. Examples to compute these things in a moment, but first I want to talk a little bit more of the properties ": [
            2018.8,
            2051.4,
            57
        ],
        "are also all of a unit like they're all of length 1 normalized vectors. Okay, so that rules out the zero Vector if you have a collection of orthogonal vectors, they are linearly independent. That means that if you have an invective an RN that are orthogonal cannot be bigger than an ant if you have a bunch of orthogonal if you have a bunch of linearly independent vectors in ": [
            157.7,
            189.2,
            5
        ],
        "at it is now the first of all of the what's the normal basis of Art? Okay. All right. Let me tell you something else about these orthogonal matrices. That's pretty great. So an orthogonal Matrix want to make sure it's a square Matrix so we can think of it as a linear transformation transformation T of X is equal to 2 times x know what can we say about ": [
            656.5,
            702.0,
            19
        ],
        "basis vectors of the so it is so I'm going to leave that to you as an exercise to work out. You can also read in the textbook. You just need to take the dot product of that thing minus y with each of the basis vectors you once were you pee and computer and you will see that it's exactly or organized so that everything will cancel out. But ": [
            1993.1,
            2018.8,
            56
        ],
        "basis. So if you are going to find us the first thing you do is you find some orthogonal basis for your Subspace V by doing this you just add up the projections. Okay. Let me write this again. This is this is the projection onto the span of you want or I wrote it is just you won before of Y plus the projection onto you to of why. ": [
            1769.5,
            1813.8,
            50
        ],
        "but if you have a two-dimensional or three-dimensional space till next day, so that's going to be the topics for Mondays lecture procedure. For Subspace set for now. You're just going to have to trust me that every Subspace has orphan army bases. But before we get there. There's one thing I want to tell you one further thing. I want to tell you which is what is orthogonal projection ": [
            2689.6,
            2725.2,
            77
        ],
        "compute those numbers and add them up and you'll get some Factor some mechanically. It's a very easy, but it's important to understand what have a nice weekend. ": [
            3052.2,
            3063.3,
            86
        ],
        "describes mechanical. Yes is not that abstract. But what it means is Vertical line of a perpendicular Slide the vector along there actually mean this theorem tells us what orthogonal projection actually is. So if I give you a Subspace V on a vector why? The projection of what that really is, is it the that is closest to Hawaii. So that is let's draw. Let's do this dropping down ": [
            2725.2,
            2771.4,
            78
        ],
        "difference between that vector and why? But that's supposed to be for Thuggin all to the Subspace. That was the definition. Remember uniquely decomposing a vector has something in the Subspace plus something for appendicular the Subspace spanned properties. You need to show that this thing. I've told you here this time of wonder myself projections to fruit. One of them is immediate because it's a linear combination of the ": [
            1951.6,
            1993.1,
            55
        ],
        "distance from y to V. Is bigger than the distance from y to Z? That is z is the thing energy quiz distance to why is smallest that's really important because approximation. So if you think about these things in terms of how we talked about vectors on the first day of class is just a list of numbers S&P 500 500 5 numbers. Okay. It's hard to understand what ": [
            2860.7,
            2911.5,
            81
        ],
        "do we talk about projections on two subspaces in general? So if I've got an RN, I'm going to Vector in R and I've got a Subspace V an RN for ject into that Subspace orthogonally. What I'm going to do is the same thing I did before. I'm going to draw a line perpendicular Subspace. Okay. Well perpendicular Subspace. What is that mean? We know what that means. As ": [
            1434.5,
            1472.4,
            41
        ],
        "dotted with X over the length of x squared. And so back from property one that gives us a formula for this Vector. Why can't watch LOL right down here. Why which will call the projection onto X of the vector v is equal to the dot product of V with x / the length of x squared x? the vector X There's a formula for the projection. That's how ": [
            1206.8,
            1242.3,
            35
        ],
        "early next week. quest web page to practice final exams so you can take a look at those and start working on those know we will go through some solutions of those problems in the review session next Friday, which is what next Friday's lecture will be about okay. so today we are going to go to section 6.3 which is an orthogonal projection. And next time we'll move to ": [
            97.9,
            125.8,
            3
        ],
        "formation. So let's do an example of ourselves. So here is a vector 1-2 2 It spans a Subspace. And I want to find the orthogonal projection onto V by which I mean I want to find. The linear transformation. I want to find the Matrix that represents this linear transformation. Well first things first, we have our formula and here we don't need to do is some of things ": [
            2390.1,
            2429.8,
            68
        ],
        "haven't learned how to do that. We know how to find a basis for a Subspace typically or we have a generic procedure where you have a Subspace, you know, it's spanned by some doctors and then you can start throwing away a vector is until you find a Spanish that is linearly independent, but normal basis and normal basis just means you can do you find a spanning vector, ": [
            2663.4,
            2689.6,
            76
        ],
        "identity? Say are orthonormal. The Columns of you transpose are the rows of you. Okay. So this says the rose of you also form an orthonormal basis of RN Why should that be I don't actually have a good explanation for this other than using the linear algebra tools were developed in this course you start out with a bunch of column vectors that are orthonormal Matrix. If you look ": [
            614.4,
            656.5,
            18
        ],
        "if I replace you with 9u is this also equal to Y daughter with nine you / the length of a 9u squared * 9? Because I've got this nine on the numerator here. And this night of the numerator there and I've got two nines in the denominator down there because it's inside the square. Okay, so it doesn't back to make sense to talk about the projection onto ": [
            1693.1,
            1723.2,
            48
        ],
        "importance of production actually is Okay, so let me finish just by so I won't prove that sound. It's the shortest. So let's finish just by finding the closest points. to a Subspace spanned by those two vectors Hey, if you see a question like that, all it's asking you to do is to find. They are stalking a projection into that Subspace of the vector 1 2 3. And ": [
            2959.0,
            3014.1,
            84
        ],
        "in the represented by these orthogonal matrices which are exactly the matrices that Have an Arsenal cases as the column for the rose. Alright, so now I want to talk about orthogonal projection is so what is an orthogonal projection? Well, orthogonal projection really just means a shadow. Okay. It means a direct. Oh, so if I take my pencil here And I want to orthogonally project it down onto ": [
            947.7,
            981.9,
            28
        ],
        "is that a x you as the identity. but if you have two square matrices inverse But we know that for square matrices if I have an inverse on the left, it's also the inverse on the right and so we get there for also you a is the identity that is you you transpose is the identity. No caution. Once again, this only works for square matrices for orthogonal ": [
            536.2,
            570.3,
            16
        ],
        "it's true. This thing you're falling all projection. Why interview? Let's go Eevee. What are its properties so let's go through VSCO the same idea that we went through when we'd geometrically to find the apothem of projection onto a single Vector. Will the first property is that it must be in the Subspace. We're projecting into the Subspace and then the second property is that if I take the ": [
            1922.2,
            1951.6,
            54
        ],
        "just the zero Vector is orthogonal projection if you are already your projection. projections for Jackson's on to a single Vector but When I talked about a second ago, the projection of this bottle the line was this bottle or by my pencil onto the table. I wasn't talking about the projection onto a particular onto a two-dimensional Subspace. different well, we'll see that they're very closely related. So how ": [
            1392.8,
            1434.5,
            40
        ],
        "length is the square root of 9/8 is 3. Unmask to to like that and then what we know that thing isn't what's a normal basis that one vector is an orthodontist basis of V. and what we just saw the last night, is that the projection on TV the Matrix of the projection on where you Single column Matrix that is that normalize Vector. So that is we have ": [
            2524.8,
            2559.0,
            72
        ],
        "matrices. If you have vectors in our normal basis of our orthogonal Matrix it satisfies both. Only in the Square cases that work. Let's think about this for a second. Actually. This is kind of remarkable. This transpose is the identity. This happens here. Think about this. We said that you record the dog parks between The Columns of you last time. So what does you you transpose equal the ": [
            570.3,
            614.4,
            17
        ],
        "matrix just reminding you how those ideas work from the beginning of the course before the final exam, but actually easier to work from the definitions to see what Matrix we get here. So let's work with an orthonormal basis. Language start with an orthonormal basis you want you to up to u p of the Subspace V. And there is our formula for the orthogonal projection. I want to ": [
            2110.5,
            2133.7,
            60
        ],
        "matrix multiplication. This thing is equal to That Matrix you on YouTube up to you pee with those columns times the column Vector who's entries. Are you one transpose? Why you to transpose why I'm down to you. Transpose. Why? But now the common way that we actually compute matrix multiplication a matrix by a column Vector. What I do is I multiply the row row by column. But that's ": [
            2227.2,
            2274.2,
            64
        ],
        "of Earth Angel projection. So if I take add them up, And project that'll be the same thing as projecting each of the vectors and adding them up. Why is that just look actually at the formula that's right here everything in that formula is linear that formula is linear in why on matrix multiplication is linear. Orthogonal projection is a linear transformation. So that the function that takes a ": [
            2051.4,
            2087.4,
            58
        ],
        "of x and t of Y. Everything is preserved. That'll be the inner product of X with Y / the length of x times the length of Y. But we know what this says. This says that the cosine of the angle between X and Y is equal to the cosine of the angle between t of x and t f y So otherwise this winter transformation T. It also ": [
            856.7,
            881.4,
            25
        ],
        "orthogonalisation section 6.4. So let's review a little bit and catch up where we were before. So a collection of vectors in RNA is called orthogonal if each pair of the vectors are orthogonal hear that is if I take any two letters from that list their perpendicular to each other there. Product is zero additional where we stay there or so normal. If in addition to being orthogonal they ": [
            125.8,
            157.7,
            4
        ],
        "preserves angles. So if I take out an orthogonal Matrix and Matrix whose columns or rows of RN the kind of linear transformation it represents is one that preserves all geometry. Okay, in other words if I have some vectors and I transform them they must transform rigidly their lengths don't change and the angle between them don't change and that's the kind of which is not a 1980s Jazzercise ": [
            881.4,
            909.2,
            26
        ],
        "read I'm going to use this inner product notation instead. Well, okay, so that's just you axe inner product you why. I'll let you know just remember the definition of the inner product of the dot product. It just means that I take the transpose of the first vector and X the second vector. Okay. Now remember how the transpose works. The transpose of a product is the product of ": [
            731.9,
            763.2,
            21
        ],
        "routine. I'm just representing here a rotation. That's the kind of transformation. These are orthogonal matrices are rotations in the two by two case. This is the generalization. It is rigid like this The Preserves angles and lengths Reflections. I can't do a reflection in this room. I don't have a mirror. Okay, but that's that's what is it is a rotation or reflection. so those are reserved all geometry ": [
            909.2,
            947.7,
            27
        ],
        "that is parallel to X that I get by just dropping that easy to understand. How do we calculate? Thanks. I want to figure out what that is down here. I'm going to give it a name. I'm going to call it. Why? How do I compute the vector y well I just noticed two things about the vector y. Okay. The first is that is parallel to the vector ": [
            1061.2,
            1090.9,
            31
        ],
        "the distance from this other one is that line right there? Those lines are longer than this one. That's the statement here the statement is that so what this really is saying is that the claim is that for any vector v Envy let's give this Earth projection name. Let's call it ve like before. for any Vector V in V the distance from V 2z is I'm sorry the ": [
            2805.0,
            2860.7,
            80
        ],
        "the table what that means is I'm taking a break over head straight over the table. I look at the shadow underneath there. That's what action is. So let's do an example of that on the on the graph here. So let me draw vector. There's a vector v. And I want to compute its orthogonal projection onto the line through X here. And so here is the line through ": [
            981.9,
            1016.2,
            29
        ],
        "the transposes but in the opposite order, is there a good things to remember as we're heading into the final exam where you'll be tested on all these sort of things. Okay, so this ux transpose the same as X transpose you transpose. an owl But you transfer. So this is just X transpose time is the identity x y. Which is X transpose y but that's the definition of ": [
            763.2,
            800.9,
            22
        ],
        "the vector ex down here. Well what that means is that I drop a line perpendicular to X straight down. Have to be a little easier to draw so here is a vertical line. Straight down at a right angle. the orthogonal projection of that vector onto so that vector v onto the lines who acts is this Vector down here? Paint that picture down there. So it's the one ": [
            1016.2,
            1061.2,
            30
        ],
        "the. Product of X and Y. So what we see from here is that if we have an orthogonal Matrix you then as a linear transformation, it has the special property that it preserves. Products if I transform to vectors by you. The dot product between those vectors doesn't change. Know the codes Volta length and angles. So what we see is there for that the length of T of ": [
            800.9,
            829.0,
            23
        ],
        "they actually specify what the orthogonal projection is because the second condition X is equal to zero. That's what it means to talk to ex. But the first condition tells us that why is some multiple of x? So therefore B- Lambda X dotted with X is equal to zero. Now, let me use the properties of the. Product to expand that out. That's a v. It with x minus ": [
            1139.4,
            1173.3,
            33
        ],
        "this Matrix except on the diagonal cut on the diagonal. We have you won. You want you too. You too, and you three. You three Those are all the length squared of the vectors you want you to and you three if these are orthonormal vectors which has by the way only one Elena tubes. Then those lights are all one. So this is another concise way of saying that ": [
            294.1,
            327.7,
            10
        ],
        "this linear transformation? What kind of property is it had? Let me show you it has a pretty great property. So if that linear transformation that I get by multiplying a vector orthogonal Matrix And I take two vectors and transform them. I want to look at the dot product of those to the dot product of TX with t of Y. And actually to make the notation easier to ": [
            702.0,
            731.9,
            20
        ],
        "those vectors is all of our at which means that they are saga. No projection is just the identity. If I asked you to orthogonally project this Vector, which is already in the table or what happens to it. It stays the same. It's already there. Okay, if you're already in the Subspace yourself. This is all consistent. If your what it is is the Matrix of the orthogonal projection ": [
            2347.4,
            2390.1,
            67
        ],
        "to take 1/3 of 1-2 2 * 1/3 of 1-2 2 as a row row times, So that might look a little funny but let's take out that one 9, so this is a 3 by 3 Matrix. cave the next one we get by multiplying 1 by -2 And one by 2 then next row have the most - 2 by 1 - 2 y -2 and -2 by ": [
            2559.0,
            2598.3,
            73
        ],
        "transpose x y. so on Down the Line Okay. Now the first thing I want to note about this is let's remember the definition of matrix multiplication. Right? So if I have some Vector X let's say let's just say has three components. I have some Matrix a witch as usual right in terms of its columns from going to multiply these it better have three columns then by definition ": [
            2158.6,
            2191.1,
            62
        ],
        "vector and returns its orthogonal projection into a Subspace that isn't that is a linear transformation, which means that it has a matrix. What is the Matrix of that linear transformation? We can compute the standard Matrix of a linear transformation that one way we can do that is to apply the transformation to the standard basis vectors and we get some new batteries that are The Columns of a ": [
            2087.4,
            2110.5,
            59
        ],
        "wanted you to we can calculate that by taking you transpose times you the one to entry of you transpose x 6 entry that Matrix you transpose you is the dot product review 7 and U6 the products of all the vectors. That's great. And also it means that we can simply write what it means for a bunch of vectors to be towards the normal normal. That means that ": [
            236.3,
            273.8,
            8
        ],
        "we got but that's how you compute orthogonal projection. Okay? Okay, so I'm going to leave you now in a few minutes. slightly confused unfortunately so why is that because What I said is if we want to find the projection. Into a Subspace. The first thing we need to do is find an orthonormal basis for the Subspace. How do you find an orthonormal basis for a Subspace? We ": [
            2625.0,
            2663.4,
            75
        ],
        "we just so if I have some Vector why and I want to orthogonally projected into what I have to do is take why dotted with that Vector 1 - 2 2 / the length of that factor 1-2 2 squared time is the vector 1-2 2 and so we already computed that one that length of that sector square is 9 so it would I get is 1/9 of ": [
            2429.8,
            2460.0,
            69
        ],
        "we saw last lecture. So what's really going on here is we take our Subspace? Now it has an orthogonal complement. And so that means that they are also linearly independent from each other. What that tells me is that if I take any Vector in the big space in RN I can I can you compose it. As a sum of two vectors or one of them. is envy ": [
            1472.4,
            1518.1,
            42
        ],
        "we use the formula that we had from before you're told that this is a North normal for the Spectre. Divided by the length of that first basis Vector squared times that first basis vector. and then I add to that that projection onto the second suspect r123 dotted with - 211 / the length of - 211 squared X Factor - 211 Okay, and then from there you just ": [
            3014.1,
            3052.2,
            85
        ],
        "we're doing our projection. So what we want to do is take our Vector Subspace V and a vector that is perpendicular to the because those two. Subspaces are complementary to each other by the rank theorem their Dimensions out of the whole space and because they are linearly independent from each other we can always uniquely. Okay. So we Define the projection of the vector want to be that ": [
            1556.4,
            1594.6,
            44
        ],
        "what this says here. I'm taking you one transpose, which is the first it with Y and so on down the line. So what that thing? The vector whose Rose are you one transpose you to transpose down to transpose? Time is the vector y. This is u x you transpose x y. And that is right there. I'm here at the projection is really the linear transformation matrix transpose. ": [
            2274.2,
            2316.2,
            65
        ],
        "write the Matrix of that which means I want to write that as 8 * why for some Matrix a okay. Well first thing I'm going to do to get there is use the definition of the. Product which says that why I thought it with you one, which is the same as you one dotted with. Why is you one transpose x y And this one is you to ": [
            2133.7,
            2158.6,
            61
        ],
        "x squared. Well, that's the inner product that sell but taking x equals y in the calculation we just made that turns into the dot product of X with itself. Which is the length of x squared so this thing preserves lengths. And also therefore if I take the dot product of T of X with t a y and divide that by the product of the length of T ": [
            829.0,
            856.7,
            24
        ],
        "you but is about your ex that we started with. Right. So why is equal to some constant times the vector ex is parallel to X, but the other thing is we purple Okay, and that's where that line intersect. That's the vector v - y. And that picture is drawn perpendicular to X. So the other condition here, is that V - what is perpendicular? Now those two conditions ": [
            1090.9,
            1139.4,
            32
        ],
        "you once were you and they form a North enorma basis? Weather orthogonal vectors their normalized it was a normal vectors and there's an of them in our end. That means that they form a basis. So if I have a matrix who is columns are an orthodontist tomorrow, then we call that Matrix you an orthogonal Matrix. Okay, so that's a new word for you to put in your ": [
            470.1,
            493.4,
            14
        ],
        "you should be in touch with him at love ta about that and I didn't post it on here. But I will mention once again your final exam is one week from tomorrow at the beginning of exam week. 11:30 a.m. The rooms have been Galbraith 242 Peterson 108 and York 27225 a seat assignment in one of those rooms selected randomly, which will be posted on the course webpage ": [
            68.1,
            97.9,
            2
        ],
        "you so what is you. You daughter with w? Is equal to 1 * 0 + 2 * 1 + -2 * 1 which is 2-2, which is 0 to the zero Vector. Of course, that's what had to happen. You. It was wa0 means that you and W are orthogonal to each other. So this bottle here. It doesn't have a shadow. Okay, the shadow is right underneath. It's ": [
            1356.6,
            1392.8,
            39
        ],
        "you were talking about such a vector and projecting it except maybe what I've got is maybe I've got a model that predicts how the stocks are supposed to evolve like all of the trading firms on Wall Street 2 I might model tells me how things are supposed to evolve and then I actually look at what the numbers are. They don't quite agree with my model. So what ": [
            2911.5,
            2935.8,
            82
        ],
        "your flashcards. An orthogonal Matrix is a matrix has columns are orthonormal vectors normal basis Square metres between orthogonal Matrix. It might make more sense to call that an orthonormal Matrix, but that's not the word that's used. We call it an orthogonal Matrix transpose the identity. Spanish special case with square you transpose use the identity what that tells you is is equal to a and what we see, ": [
            493.4,
            536.2,
            15
        ]
    },
    "File Name": "Linear_Algebra___B00___Kemp__Todd_Aahron___Winter_2018-lecture_25.flac",
    "Full Transcript": "Happy Friday. Good morning.  It is a happy Friday.  Sun is shining feels like spring out there.  And we got some money rosberg going on in here. It's a good time.  Two administrative reminders. And once again, please fill in your capes your student Professor end and course evaluations 9 to fill those out online.  Your final MyMathLab homework sets has been assigned posted last night you have until next Thursday at 11:59 p.m. That covers the remaining material covering in this class from chapters 5 6 and 7.  Your last night lab assignment is due tomorrow night at 11:59 and you're not love quiz. Just as a reminder is next week on Tuesday in the time slots that you normally have your section on Thursday. If you have a conflict than there are conflict time to set up and you should be in touch with him at love ta about that and I didn't post it on here. But I will mention once again your final exam is one week from tomorrow at the beginning of exam week. 11:30 a.m. The rooms have been Galbraith 242 Peterson 108 and York 27225 a seat assignment in one of those rooms selected randomly, which will be posted on the course webpage early next week.  quest web page to practice final exams so you can take a look at those and start working on those know we will go through some solutions of those problems in the review session next Friday, which is what next Friday's lecture will be about  okay.  so today  we are going to go to section 6.3 which is an orthogonal projection. And next time we'll move to orthogonalisation section 6.4.  So let's review a little bit and catch up where we were before.  So a collection of vectors in RNA is called orthogonal if each pair of the vectors are orthogonal hear that is if I take any two letters from that list their perpendicular to each other there. Product is zero additional where we stay there or so normal. If in addition to being orthogonal they are also all of a unit like they're all of length 1 normalized vectors. Okay, so that rules out the zero Vector if you have a collection of orthogonal vectors, they are linearly independent. That means that if you have an invective an RN that are orthogonal cannot be bigger than an ant if you have a bunch of orthogonal if you have a bunch of linearly independent vectors in RN, you can't have more than any of them.  So P orthogonal vectors and rnp must be less than or equal to n it could be equal to end which case you have an linearly independent vectors an RN they form a basis in which case they would form an orthogonal basis or or is it normal?  Now we saw last time at the end of the lecture that there's a sort of nifty way or a very concise way to express this or condition as you went through Ups training together at The Columns of a matrix as usual so that Matrix is going to be typically taller than it is wide. It will have more rows and columns or at worst equal number of rows and columns.  and what we saw was that the dot product of you wanted you to  we can calculate that by taking you transpose times you the one to entry of you transpose x 6 entry that Matrix you transpose you is the dot product review 7 and U6 the products of all the vectors.  That's great. And also it means that we can simply write what it means for a bunch of vectors to be towards the normal normal. That means that all the vectors are orthogonal to each other. So that means that if I take you wanted you to its. Product of 0 and staying with you too and you want by taking you want to do 3 there. Product of 0 and same with you three and you want you too and you 3 there. Product is zero and so on we find that all of the entries of this Matrix except on the diagonal cut on the diagonal. We have you won. You want you too. You too, and you three. You three  Those are all the length squared of the vectors you want you to and you three if these are orthonormal vectors which has by the way only one Elena tubes.  Then those lights are all one.  So this is another concise way of saying that a collection of vectors are normal transpose you is the identity Matrix is an nyp matrix by P. Identity Matrix, okay.  Very large this Matrix P by P its size. It's a square Matrix is equal to the number of extras in the set.  Okay, so that's that's if I want to make an important comment here which is in the typical situation where we have fewer than n vectors. Are we?  That does not tell us anything about you you transpose.  I throw you you transpose. Well you is and by P you transpose is p by n. Okay. So the size of this Matrix is an bi an  but this is typically false. In fact, it'll always be false.  If p is less than a man.  Okay that the product you you transpose. It's not transpose you is.  If you have a rectangular Matrix, well be the case that has a left inverse but not a right at first but that will be the case here too. Now. What is that was PT is equal to end. That means that this Matrix which has the columns equal to those vectors you want you to up the UN is a square Matrix.  Fix now in that case.  Those vectors you once were you and they form a North enorma basis?  Weather orthogonal vectors their normalized it was a normal vectors and there's an of them in our end. That means that they form a basis. So if I have a matrix who is columns are an orthodontist tomorrow, then we call that Matrix you an orthogonal Matrix.  Okay, so that's a new word for you to put in your your flashcards. An orthogonal Matrix is a matrix has columns are orthonormal vectors normal basis Square metres between orthogonal Matrix. It might make more sense to call that an orthonormal Matrix, but that's not the word that's used. We call it an orthogonal Matrix transpose the identity.  Spanish special case with square  you transpose use the identity what that tells you is is equal to a and what we see, is that a x you as the identity.  but if you have two square matrices inverse  But we know that for square matrices if I have an inverse on the left, it's also the inverse on the right and so we get there for also you a is the identity that is you you transpose is the identity.  No caution. Once again, this only works for square matrices for orthogonal matrices. If you have vectors in our normal basis of our orthogonal Matrix it satisfies both.  Only in the Square cases that work.  Let's think about this for a second. Actually. This is kind of remarkable. This transpose is the identity. This happens here. Think about this. We said that you record the dog parks between The Columns of you last time.  So what does you you transpose equal the identity? Say are orthonormal.  The Columns of you transpose are the rows of you. Okay. So this says  the rose of you also form  an orthonormal basis of RN  Why should that be I don't actually have a good explanation for this other than using the linear algebra tools were developed in this course you start out with a bunch of column vectors that are orthonormal Matrix. If you look at it is now the first of all of the what's the normal basis of Art?  Okay.  All right. Let me tell you something else about these orthogonal matrices. That's pretty great.  So an orthogonal Matrix want to make sure it's a square Matrix so we can think of it as a linear transformation transformation T of X is equal to 2 times x  know what can we say about this linear transformation? What kind of property is it had? Let me show you it has a pretty great property. So if that linear transformation that I get by multiplying a vector orthogonal Matrix  And I take two vectors and transform them.  I want to look at the dot product of those to the dot product of TX with t of Y.  And actually to make the notation easier to read I'm going to use this inner product notation instead.  Well, okay, so that's just you axe inner product you why.  I'll let you know just remember the definition of the inner product of the dot product. It just means that I take the transpose of the first vector and X the second vector.  Okay.  Now remember how the transpose works.  The transpose of a product is the product of the transposes but in the opposite order, is there a good things to remember as we're heading into the final exam where you'll be tested on all these sort of things.  Okay, so this ux transpose the same as X transpose you transpose.  an owl  But you transfer.  So this is just X transpose time is the identity x y.  Which is X transpose y but that's the definition of the. Product of X and Y.  So what we see from here is that if we have an orthogonal Matrix you then as a linear transformation, it has the special property that it preserves. Products if I transform to vectors by you.  The dot product between those vectors doesn't change.  Know the codes Volta length and angles. So what we see is there for that the length of T of x squared. Well, that's the inner product that sell but taking x equals y in the calculation we just made that turns into the dot product of X with itself.  Which is the length of x squared so this thing preserves lengths.  And also therefore if I take the dot product of T of X with t a y and divide that by the product of the length of T of x and t of Y.  Everything is preserved. That'll be the inner product of X with Y / the length of x times the length of Y.  But we know what this says. This says that the cosine of the angle between X and Y is equal to the cosine of the angle between t of x and t f y  So otherwise this winter transformation T. It also preserves angles.  So if I take out an orthogonal Matrix and Matrix whose columns or rows of RN the kind of linear transformation it represents is one that preserves all geometry.  Okay, in other words if I have some vectors and I transform them they must transform rigidly their lengths don't change and the angle between them don't change and that's the kind of which is not a 1980s Jazzercise routine. I'm just representing here a rotation.  That's the kind of transformation. These are orthogonal matrices are rotations in the two by two case. This is the generalization. It is rigid like this The Preserves angles and lengths Reflections. I can't do a reflection in this room. I don't have a mirror. Okay, but that's that's what is it is a rotation or reflection.  so those are reserved all geometry in the represented by these orthogonal matrices which are exactly the matrices that  Have an Arsenal cases as the column for the rose. Alright, so now I want to talk about orthogonal projection is so what is an orthogonal projection? Well, orthogonal projection really just means a shadow. Okay. It means a direct. Oh, so if I take  my pencil here  And I want to orthogonally project it down onto the table what that means is I'm taking a break over head straight over the table. I look at the shadow underneath there. That's what action is.  So let's do an example of that on the on the graph here. So let me draw vector.  There's a vector v.  And I want to compute its orthogonal projection onto the line through X here.  And so here is the line through the vector ex down here.  Well what that means is that I drop a line perpendicular to X straight down.  Have to be a little easier to draw so here is a vertical line.  Straight down at a right angle.  the orthogonal projection of that vector  onto so that vector v onto the lines who acts is this Vector down here?  Paint that picture down there. So it's the one that is parallel to X that I get by just dropping that easy to understand. How do we calculate?  Thanks. I want to figure out what that is down here. I'm going to give it a name. I'm going to call it.  Why?  How do I compute the vector y well I just noticed two things about the vector y. Okay. The first is  that is parallel to the vector you but is about your ex that we started with. Right. So why is equal to some constant times the vector ex is parallel to X, but the other thing is we  purple  Okay, and that's where that line intersect. That's the vector v - y.  And that picture is drawn perpendicular to X. So the other condition here, is that V - what is perpendicular?  Now those two conditions they actually specify what the orthogonal projection is because the second condition X is equal to zero. That's what it means to talk to ex.  But the first condition tells us that why is some multiple of x?  So therefore B- Lambda X dotted with X is equal to zero.  Now, let me use the properties of the. Product to expand that out. That's a v. It with x minus Lambda X. It with x  and really all we need to do is figure out what is 10 / X. If I have any nonzero Vector ex that X. It was except the length of X, right. So this tells me hear that V. It with x - Lambda times. The length of a squared is equal to 0 I can solve for Lambda there Lambda is equal to be dotted with X over the length of x squared.  And so back from property one that gives us a formula for this Vector. Why can't watch LOL right down here.  Why which will call the projection onto X of the vector v is equal to the dot product of V with x / the length of x squared x?  the vector X  There's a formula for the projection. That's how I project any Vector onto another one just with that simple formula.  Okay. So for example got a couple down here.  the vector v  onto the vector you hear it with these two vectors in R3.  The picture that I drew that was in the plane definition of the 376 whatever you want the projection of the vector v onto the vector you is simply asked you. It was me.  /  the length of you squared  time's the vector U.  So let's figure out what those three numbers those two numbers. Are you. It was V is equal to 1 + 0 + 0  1 squared + 2 squared + -2 squared  Okay X the factor you which is 1 to -2.  So this is 1/9 of the vector 1 2-2, that's the projection. It's shorter because the vector from Wasaga. No just like the example. We stopped here. If the vectors xnv up there are pretty close to orthogonal then if we project V down on taxes going to be pretty close to zero here to voltar that point. Let's project the vector w.  On to the vector you well, that means we need to compute you. It with W / the length of you squared times the vector you so what is you. You daughter with w?  Is equal to 1 * 0 + 2 * 1 + -2 * 1 which is 2-2, which is 0 to the zero Vector. Of course, that's what had to happen. You. It was wa0 means that you and W are orthogonal to each other. So this bottle here.  It doesn't have a shadow.  Okay, the shadow is right underneath. It's just the zero Vector is orthogonal projection if you are already your projection.  projections for Jackson's on to a single Vector but  When I talked about a second ago, the projection of this bottle the line was this bottle or by my pencil onto the table. I wasn't talking about the projection onto a particular onto a two-dimensional Subspace.  different  well, we'll see that they're very closely related. So how do we talk about projections on two subspaces in general? So if I've got an RN, I'm going to Vector in R and I've got a Subspace V an RN for ject into that Subspace orthogonally. What I'm going to do is the same thing I did before. I'm going to draw a line perpendicular Subspace.  Okay. Well perpendicular Subspace. What is that mean? We know what that means. As we saw last lecture. So what's really going on here is we take our Subspace?  Now it has an orthogonal complement. And so that means that they are also linearly independent from each other.  What that tells me is that if I take any Vector in the big space in RN I can I can you compose it.  As a sum of two vectors or one of them.  is envy  The other is in the park.  That's exactly what we did on this last slide. We had our vector.  V K and what we did was we wanted to write it. We aren't projected onto x what we did was we wrote that v as the sum of two vectors the one we called. Where is in the Subspace spanned by X. Okay. That is exactly what we're doing when we're doing our projection. So what we want to do is take our Vector Subspace V and a vector that is perpendicular to the because those two.  Subspaces are complementary to each other by the rank theorem their Dimensions out of the whole space and because they are linearly independent from each other we can always uniquely.  Okay. So we Define the projection of the vector want to be that Y-12 be the one that's inviso. We do this position and the part. That's envy that is the orthogonal projection of the vector.  That's the definition. How do you actually compute it? Well, we just saw that you computed in one-dimensional case.  If V is the span of some Vector X.  And we saw that the projection.  On TV up some Vector Y is equal to Y dotted with x / X dotted with x.  Accidentally use the vector you instead of X here. So we're not confused with inefficient the last night if he is has been of some Vector you that this is why daughters with you / the length of you squared that's what we computed on the last slides. At least we know how it works last flight. I said the projection onto a vector you  Okay know if the Subspace V is one-dimensional there's not so much difference between those but actually we should verify that things are making sense of 9V.  I could have used one of you is the span of 9u I could have used a scalar multiple of.  Okay, so well, I have this formula here in terms of you. That means I should also make sure that they make sense if I replace you with 9u is this also equal to Y daughter with nine you / the length of a 9u squared * 9?  Because I've got this nine on the numerator here.  And this night of the numerator there and I've got two nines in the denominator down there because it's inside the square.  Okay, so it doesn't back to make sense to talk about the projection onto a Subspace this formula make sense.  Okay, but still that doesn't answer the question. How do we compute the orthogonal projection onto a Subspace? You know it exists, So here's how we do it in the projection onto V is given by this formula is / is any basis vector?  Any basis Vector of the Subspace V?  That is how we're going to generalize I think in terms of a basis. So if you are going to find us the first thing you do is you find some orthogonal basis for your Subspace V by doing this you just add up the projections.  Okay. Let me write this again. This is this is the projection onto the span of you want or I wrote it is just you won before of Y plus the projection onto you to of why.  And so on Down the Line.  That's how you do it.  Can you fix a Northland or an orthogonal basis and you project your vector onto each one of the basis vectors and then you add those up and if you think about  How we calculate the coordinates of a vector in euclidean space thinking of the standard basis as an orphan Armel basis. That's exactly what we're doing here. It has components 2 and 3 what I'm saying is I projected onto the first base and so that better is 2 * 81 + 3. * that's exactly what this tells me.  So now let me make a few remarks.  the bassist  what's the normal?  That means that those links of the use are all one.  And therefore we have the slightly simpler formula.  That we just get why. It with you one X you want + y. It with you two times you too.  Etc why. It with you pee. I'm sleepy. So that's one way that it's nicer to work with an orthodontic basis than just an orthogonal basis. Let me be clear here. This is a theorem. I haven't proved it. It's not hard to prove that one. Let me give you an idea of why it's true.  This thing you're falling all projection.  Why interview? Let's go Eevee.  What are its properties so let's go through VSCO the same idea that we went through when we'd geometrically to find the apothem of projection onto a single Vector. Will the first property is that it must be in the Subspace. We're projecting into the Subspace and then the second property is that if I take the difference between that vector and why?  But that's supposed to be for Thuggin all to the Subspace. That was the definition. Remember uniquely decomposing a vector has something in the Subspace plus something for appendicular the Subspace spanned properties. You need to show that this thing. I've told you here this time of wonder myself projections to fruit.  One of them is immediate because it's a linear combination of the basis vectors of the so it is so I'm going to leave that to you as an exercise to work out. You can also read in the textbook. You just need to take the dot product of that thing minus y with each of the basis vectors you once were you pee and computer and you will see that it's exactly or organized so that everything will cancel out.  But anyway, that is how you compute orthogonal projections.  and here I've just restated what I said on the on the last slide there that if you have an orphan Armel basis already then  That you are orthogonal projection is just given by inspector and add those guys up.  Okay.  Examples to compute these things in a moment, but first I want to talk a little bit more of the properties of Earth Angel projection. So if I take add them up,  And project that'll be the same thing as projecting each of the vectors and adding them up. Why is that just look actually at the formula that's right here everything in that formula is linear that formula is linear in why on matrix multiplication is linear.  Orthogonal projection is a linear transformation. So that the function that takes a vector and returns its orthogonal projection into a Subspace that isn't that is a linear transformation, which means that it has a matrix. What is the Matrix of that linear transformation? We can compute the standard Matrix of a linear transformation that one way we can do that is to apply the transformation to the standard basis vectors and we get some new batteries that are The Columns of a matrix just reminding you how those ideas work from the beginning of the course before the final exam, but actually easier to work from the definitions to see what Matrix we get here. So let's work with an orthonormal basis.  Language start with an orthonormal basis you want you to up to u p of the Subspace V. And there is our formula for the orthogonal projection. I want to write the Matrix of that which means I want to write that as  8 *  why for some Matrix a  okay. Well first thing I'm going to do to get there is use the definition of the. Product which says that why I thought it with you one, which is the same as you one dotted with. Why is you one transpose x y  And this one is you to transpose x y.  so on Down the Line  Okay. Now the first thing I want to note about this is let's remember the definition of matrix multiplication. Right? So if I have some Vector X let's say let's just say has three components.  I have some Matrix a witch as usual right in terms of its columns from going to multiply these it better have three columns then by definition a e x x is equal to the linear combination of the columns.  given by the vector X  right  Well, look at what I've written here for the orthogonal projection the orthogonal projection.  Why is a certain linear combination of those?  so if I asked as before if I  Write you as the Matrix whose columns. Are you want you to through you pee?  Then just from the definition of matrix multiplication. This thing is equal to  That Matrix you on YouTube up to you pee with those columns times the column Vector who's entries. Are you one transpose? Why you to transpose why I'm down to you. Transpose. Why?  But now the common way that we actually compute matrix multiplication a matrix by a column Vector. What I do is I multiply the row row by column.  But that's what this says here. I'm taking you one transpose, which is the first it with Y and so on down the line. So what that thing?  The vector whose Rose are you one transpose you to transpose down to transpose?  Time is the vector y.  This is u x you transpose x y.  And that is right there. I'm here at the projection is really  the linear transformation matrix transpose. Remember I said if you've got an orthonormal set of vectors, that means that you transpose you is the identity but you you transpose may not be the identity. If the vectors are there fewer vectors then and in our end then you transpose what video is the orthogonal projection onto the Subspace spanned by the vectors?  You started with an orthonormal basis for RN then the Subspace spanned by those vectors is all of our at which means that they are saga. No projection is just the identity. If I asked you to orthogonally project this Vector, which is already in the table or what happens to it.  It stays the same. It's already there.  Okay, if you're already in the Subspace yourself.  This is all consistent. If your what it is is the Matrix of the orthogonal projection formation. So let's do an example of ourselves.  So here is a vector 1-2 2  It spans a Subspace.  And I want to find the orthogonal projection onto V by which I mean I want to find.  The linear transformation. I want to find the Matrix that represents this linear transformation. Well first things first, we have our formula and here we don't need to do is some of things we just so if I have some Vector why and I want to orthogonally projected into what I have to do is take why dotted with that Vector 1 - 2 2 / the length of that factor 1-2 2 squared  time is the vector 1-2 2  and so we already computed that one that length of that sector square is 9 so it would I get is 1/9 of a dotted with that that Matrix there that. That's call him back after I wish I could write as y 1 - 2y 2 + 2 y 3  time is the vector.  one- to two  so there's a perfectly good formula for the projection of a vector into the Subspace spanned by that vector.  But we have to find an orthonormal basis.  Find an orthonormal basis of vee-vee is one-dimensional. So the ortho part is vacuous. We don't need to find to find orthogonal vectors. There's only one vector. There doesn't need to be anything. But we do need to have the normal part. Would you need to have a unit length of vector? So that means we should replace.  The vector here, which I'll call you.  with  It's normalization which means we divide it by its length and it's length is the square root of 9/8 is 3.  Unmask to to like that and then what we know that thing isn't what's a normal basis that one vector is an orthodontist basis of V.  and what we just saw the last night, is that the projection on TV the Matrix of the projection on where you  Single column Matrix that is that normalize Vector. So that is we have to take 1/3 of 1-2 2 * 1/3 of 1-2 2 as a row row times, So that might look a little funny but let's take out that one 9, so this is a 3 by 3 Matrix.  cave the next one we get by multiplying 1 by -2  And one by 2 then next row have the most - 2 by 1 - 2 y -2 and -2 by 2.  and then to  -4 and 4 and that's it. That's the Matrix of the orthogonal projection onto this one-dimensional Subspace. And by the way, you can go ahead and compute now that if you multiply that matrix by the vector y 1 Y 2 y 3 you'll get exactly the formula that's written up there.  I'll leave that to you to verify the matrix multiplication agrees with that formula we got but that's how you compute orthogonal projection. Okay? Okay, so I'm going to leave you now in a few minutes.  slightly confused unfortunately  so why is that because  What I said is if we want to find the projection.  Into a Subspace. The first thing we need to do is find an orthonormal basis for the Subspace.  How do you find an orthonormal basis for a Subspace?  We haven't learned how to do that. We know how to find a basis for a Subspace typically or we have a generic procedure where you have a Subspace, you know, it's spanned by some doctors and then you can start throwing away a vector is until you find a Spanish that is linearly independent, but normal basis and normal basis just means you can do you find a spanning vector, but if you have a two-dimensional or three-dimensional space till next day, so that's going to be the topics for Mondays lecture procedure.  For Subspace set for now. You're just going to have to trust me that every Subspace has orphan army bases.  But before we get there.  There's one thing I want to tell you one further thing. I want to tell you which is what is orthogonal projection describes mechanical. Yes is not that abstract. But what it means is  Vertical line of a perpendicular Slide the vector along there actually mean this theorem tells us what orthogonal projection actually is. So if I give you a Subspace V on a vector why?  The projection of what that really is, is it the that is closest to Hawaii.  So that is let's draw. Let's do this dropping down a vertical line again. Just like we did on the side where we introduced this.  here  in this example  Is the projection of Y into V?  There are lots of vectors Envy. Let's draw some other ones.  Here's one. Here's one.  What is the distance from each of those vectors to why the distance from this first green vectorizer to why is the length of that line right there? And the distance from this other one is that line right there? Those lines are longer than this one.  That's the statement here the statement is that  so what this really is saying is  that the claim is  that  for any vector v  Envy  let's give this Earth projection name. Let's call it ve like before.  for any Vector V in V  the distance  from V  2z  is I'm sorry the distance from y to V.  Is bigger than the distance from y to Z?  That is z is the thing energy quiz distance to why is smallest that's really important because approximation.  So if you think about these things in terms of how we talked about vectors on the first day of class is just a list of numbers S&P 500 500 5 numbers. Okay. It's hard to understand what you were talking about such a vector and projecting it except maybe what I've got is maybe I've got a model that predicts how the stocks are supposed to evolve like all of the trading firms on Wall Street 2  I might model tells me how things are supposed to evolve and then I actually look at what the numbers are. They don't quite agree with my model. So what I'm going to do is I'm going to take the real numbers and I'm going to find the best approximation that my model gives of those numbers to adjust my model for the next iteration how to find the best approximation in my mouth that I can find to the actual numbers. I need to be able to closest point to a Subspace thing.  this is the single most important importance of production actually is  Okay, so let me finish just by so I won't prove that sound. It's the shortest. So let's finish just by finding the closest points.  to a Subspace spanned by those two vectors  Hey, if you see a question like that, all it's asking you to do is to find.  They are stalking a projection into that Subspace of the vector 1 2 3.  And we use the formula that we had from before you're told that this is a North normal for the Spectre.  Divided by the length of that first basis Vector squared times that first basis vector.  and then I add to that that projection onto the second suspect r123 dotted with - 211 / the length of - 211 squared X Factor - 211  Okay, and then from there you just compute those numbers and add them up and you'll get some Factor some mechanically. It's a very easy, but it's important to understand what have a nice weekend. "
}