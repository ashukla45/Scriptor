{
    "Blurbs": {
        "0 squared. Which is 5 and so therefore the length of U2 is the square root of 5. And you three dotted with itself. gives 2 squared + 5 squared + 4 squared which is 4 + 25 + 16, which is 45 and therefore the length of you three. Is equal to the square root of 45, which is better written is 3 times the square root of 5, ": [
            2402.9,
            2436.1,
            67
        ],
        "C2? We're just going to repeat exactly the same calculation take that supposed to fill near combination of the use and take its. Product with you to the exact same calculation. Will the coefficients except for the times to is not 0 and therefore C2 must be zero and just keep going down the line and every one of those coefficients is 0 it only takes 10 squared steps have ": [
            2004.0,
            2041.1,
            56
        ],
        "Happy Wednesday. Let's get going. First let me briefly mention that you should all have seen by now that was graded exams regrade request system this time around it's open right now closes on Friday before midnight. So take a careful. Look if you feel like you want to request a regret of a particular problem. There's a procedure to follow within gradescope. A detailed comment has to be done ": [
            0.0,
            33.6,
            0
        ],
        "I need to do rubber duck session to solve it and see that the only zeros that's what it would mean to prove. These are linearly independent invector is an RN so it can take steps. So here's how we do that. We say therefore. If I take that Vector c1u, 1 + C to you, too. down the line cpup I'm going to take it too. Product with you ": [
            1844.0,
            1889.7,
            51
        ],
        "Okay, because given that it was not very well done on the midterm. You can be sure that it's likely to appear again on the final exam. Okay few administrative reminders first course evaluations. We really appreciate the feedback at your Tia's give detailed feedback about what you thought works. Well what you could be improved, I'd love to see all of you submit your tapes. You can do that ": [
            85.2,
            114.8,
            3
        ],
        "The Columns are right it is what's supposed it has three columns. Call Emma. They might be a millionaire. Means I take those columns and I write them as the rose. Take the transpose of the columns. Of a transpose. That's what if I calculate this thing a transpose a so I'm taking that Matrix whose rows are The Columns of a on their sides. and * the columns Well, ": [
            2790.6,
            2838.4,
            78
        ],
        "V is equal to 1 + 2 +... Cpup. Sometimes it's hard to tell the difference between Pisces and you've sorry about that. We want to find those seas and to do so usually involves solving a vector equation, but it doesn't have to in this case because if I take the dog product of V with say you won. Play I do exactly the same calculation I did last ": [
            2570.0,
            2601.0,
            72
        ],
        "Wednesday to equal zero. So when are parallel really the important Point here is Wednesday. Is equal to pi over 2. What day does 90 degrees when the vectors are perpendicular to each other? Okay. I'm from the product is equal to the product of the cosine of 90\u00b0 is 0 Two vectors are perpendicular or the commonly uses orthogonal. And by the way will often use this symbol for ": [
            354.1,
            404.1,
            12
        ],
        "a 1.82. And so on so we take the a ones dotted with a1283 in the first row. A2 dotted with a 1.83 in the second row And a $3 with a one through a three. Is a third row? And so we see that in this Matrix. A transpose a the entries are the duck product of all of The Columns of the Matrix a transpose a computer. No ": [
            2862.7,
            2910.2,
            80
        ],
        "a bassist so we can now talk about orthogonal basis. And those are going to be the best kind of face. He's so given a Subspace V of RN a basis for it is called an orthogonal basis vectors. I want them to be the best kind of linear Independence. Senior independent vectors are orthogonal most of the time not all in the same plane or whatever, but it's the ": [
            2095.0,
            2137.0,
            58
        ],
        "a different date different lecture number last time but everything we're doing here you can see slightly different game ideas for the last time around. So if you're looking for a screencast from this time, it's not there from last year's and that will be a helpful resource to you. All right, we start talking about inner product. Product last time. Okay, so the dot product I haven't even written ": [
            267.6,
            289.2,
            9
        ],
        "a thug in a little right make it crystal clear here like an upside down t Okay to say that they are orthogonal to each other. So there. Product is zero from consideration of sakurasou Theron from plane. Geometry says that if you have a right angle triangle, then the sum of the squares of the two sides some of those squares equal to the square of the hypotenuse the ": [
            404.1,
            437.9,
            13
        ],
        "and R3? We have our standard basis vectors 1 0 0 0 1 0 0 0 1 and we use those whenever we can they make computations as easy as possible for easier to work with than other vectors and one reason we can now see why that's true. Is these vectors are all orthogonal to each other? 6 + pics of the first two so if I take the ": [
            1624.9,
            1662.6,
            45
        ],
        "any two vectors u and v The square of the length of their difference which is the inner product of their difference with itself. is equal to you squared plus b squared but there's a correction turn that is minus twice. The inner product of you with v is the is equal to twice the length of the vectors u and v times the cosine of the angle between the ": [
            463.1,
            498.0,
            15
        ],
        "are length one. Then that means exactly that their length squared for example r0 R1. So what we get, is that V dotted with you Jai is equal to CJ because that you 1 squared in the denominator. That's just so if you have an orthodontist basis, do you want to find the coefficients of a vector in terms that off and on what basis they're just exactly equal to ": [
            2673.4,
            2705.1,
            75
        ],
        "as close as you'll be able to do over here. That makes a right angle with that line. That's correct. Okay. So this line over here is V perp when this one here was be. Okay, so that's a pretty relatively easy things. I understand if I give you a line in the plane, it's perpendicular Subspace. It's orthogonal complement Subspace that is perpendicular to it. That uniquely pins it ": [
            631.6,
            669.1,
            19
        ],
        "best kind of linear Independence. We can go one small step further to say and let me also insist that all the vectors have a length one that's called normalized is called normalized if it's length is one but that's actually easy to achieve whenever I have a nonzero Vector it right over here. If you is not equal to zero. Then we can normalize it. What that means is ": [
            2137.0,
            2172.9,
            59
        ],
        "by this Friday at 11:59 p.m. The one thing I will say about the midterm is that people did reasonably well on most problems, but one that caught me off guard that you didn't do as well on as I had thought you would was problem 3 problems free was all about recognizing. What is a Subspace. So this is a concept you are still having a lot of trouble ": [
            33.6,
            61.3,
            1
        ],
        "column space of a transpose another name for that is just the row space today. and here is a big show this together with the ranks Arab usually goes under the name the fundamental theorem of linear algebra important theorem the geometry of linear algebra, and it says that if I take the null-space of a matrix, what is the orthogonal complement of that? Okay, so the null-space of a ": [
            1042.1,
            1072.0,
            30
        ],
        "continue with that discussion on Friday. ": [
            3064.4,
            3066.3,
            85
        ],
        "demonstration of the ranks are going to prove this but more important it is to understand what's going on here. So what does it mean when I get that first statement there? oops What does it mean? To say that the orthogonal comment of the row space is the null space. So this means that V is a vector in the row space of a and W is a vector ": [
            1126.8,
            1167.1,
            32
        ],
        "diagonal parts. We get the lynx squared. Another way of saying a collection of vectors are orthogonal vectors. A transpose a is diagonal that's what it means to say that a collection of vectors are all mutually orthogonal. Yes. There is a relationship with that next week we go here, which is that. Well I said it was their orthogonal but what a normal if they're normal. Then all of ": [
            2947.2,
            3007.8,
            82
        ],
        "down. No, I want to come to draw in three dimensions. But okay. What if I asked you here is a Subspace of R3 is table okay the table extended in and I'll directions here. Let's suppose that this bottle here is right at the origin. So here is a Subspace of R3. What is the orthogonal complement of that Subspace? Someone like to make a guess or make a ": [
            669.1,
            696.6,
            20
        ],
        "each other all three of the orthogonal frame. The only has that property. Here's a different set of vectors are perpendicular to each other. Which two should I choose? Great. So, let's take the dumb product of you two and use three. okay, so that's just going to be - 1 * - 1/2 + 2 * -2 + 1 * 7/2 1/2 - 4 + 7/2 1/2 + 7/2 ": [
            1694.5,
            1754.0,
            47
        ],
        "each other. One thing you get immediately for free is it those vectors are linearly independent from each other or saw the body of a bunch of vectors implies linear Independence now, why is that? Well, let's see here. So just to prove this there as usual linear Independence. It says that the vectors are linearly dependent if you're going to prove something about negative Universal Property. Usually you have ": [
            1785.3,
            1813.8,
            49
        ],
        "finished my proving the cauchy Schwarz inequality, which tells you what I've written here. At least make sense that the dog products divided by the product of the lengths of the two vectors. That's a number that's between me and that something that say that is actually geometric is the angle between the two vectors. So so from there we're going to talk about now primarily is the special case ": [
            314.8,
            354.1,
            11
        ],
        "first Matrix X the column X seconds me a 1 x 1 plus a 2 x 2 + 83 x 3. And then doing the same thing, but with the second row here for the second column, that's v1x 1 + b 2 x 2. Forex to be in the North Face says that Vector there is zero recognize what's written here is exactly this first row here. Let's call ": [
            1371.5,
            1406.7,
            38
        ],
        "first one. It with the third one say well, that's going to be 1 * 0 + 0 * 0 + 0 * 1 which is gives me 0 and you can do the same thing with the first two or the last two vectors are perpendicular to each other next class. You take your your thumb your first and second fingers on a hand, I guess in physics and ": [
            1662.6,
            1694.5,
            46
        ],
        "from this lecture. The one thing that I can say is winter 2017, which are available on the podcast site. There's a bit of a difference in when what topic was done than relatives and now there's a shift of about a full lecture. We're about a whole lecture ahead of what we were back then that's what I planned it this time. So you may need to look on ": [
            243.0,
            267.6,
            8
        ],
        "get zero and there we have it. If your perpendicular to both the rose that are perpendicular to every linear combination of the Rose. In other words, you're perpendicular to the row space. There's the fundamental theorem of linear algebra and its just using the words. We've learned Now. Product orthogonal those words interpreting them in terms of what matrix multiplication says. So the orthogonal complement of the row space ": [
            1527.0,
            1552.0,
            42
        ],
        "have a vector in a plane that is perpendicular exception. Is there a vector so this is not quite right this think it has some doctors that are perpendicular to everything in the plane, but it also has some vectors that are in the plane. So this too big to the Asylum compliment. So what's It's the vector made by the bottle. That's almost exactly right except that's just one ": [
            767.6,
            801.4,
            23
        ],
        "important subspaces like the no space in the column space of a matrix. Remember that the null-space of a matrix that has them to mention the melody the column space that Matrix has them to mention the rank and we have the ranks are in which says that those two Dimensions they add up to the size of the space size of that case. So we get a compliment and ": [
            979.8,
            1007.7,
            28
        ],
        "in the null space of a Then that implies that V is perpendicular to w. In other words there. Product is zero. That's the statement that's being made here. If I take a vector from the row space and a vector from the no space those two vectors are perpendicular to each other. Those have to live in the same space so that the number of columns. The row space ": [
            1167.1,
            1203.4,
            33
        ],
        "in the plant in case they add up to the dimension of the space for an RN then a Subspace orthogonal complement their Dimensions add up to the dimension end of the whole Space. What if I tell you right now of three-dimensional space inside three dimensional space? What's a set of all vectors that are perpendicular to? Just the zero Vector orthogonal complement of a vector space in itself ": [
            887.5,
            926.6,
            26
        ],
        "in this class is next week. Don't sweat it. It's no big deal. It's very carefully and things related to the course material. It's more of a basic test of your familiarity with basic functions in Matlab. Okay, and it's scheduled for Tuesday and Thursday. So if you need it on Thursday, if your sections on Thursday at 5 p.m. Is going to be on Tuesday at 5 p.m. If ": [
            142.7,
            174.0,
            5
        ],
        "is 8/2 to the -4 + 8/2 + 1/2 is 4 so we can get Sarah and if you want to go ahead and if you want a new three, you'll see that all three of those vectors are orthogonal to each other. Okay, great. So, how does that help us what helps us in a lot of ways that are all Mutual every pair of them are orthogonal to ": [
            1754.0,
            1785.3,
            48
        ],
        "is the Subspace spanned by the elements in a given row is the number that that's what this says. Understand what's going on here. Let's look at a matrix a that is I don't know. Let's make a 2 by 3. Set another right too much. Okay, so let's write it as A1 A2 A3 B1 B2 B3 Set a reminder to Rose of The Matrix. And so what is ": [
            1203.4,
            1273.6,
            34
        ],
        "is the null space ax equals 0 which seems like a very different sort of objects in the column space in the row space Witcher just the vector spaces stand by the road. Really are comprable things the column spacer becomes face of a transpose. The row space is some Subspace of this of the space where the rose live is the orthogonal complement of that thing is a set ": [
            1552.0,
            1589.4,
            43
        ],
        "is the zero Vector Subspace. Just turn around what we just said. I saw someone actually turn around not quite what I meant. But so if the complement of everything is 0x if I'm just a zero Vector what vectors are perpendicular to it. Every Vector is perpendicular to those are more demonstrations of the fact that you get this complementarity. the the two subspaces compliment we talked about some ": [
            926.6,
            979.8,
            27
        ],
        "it actually is the same theorem. That's because of this fact here. If I have a matrix and Matrix. There are three subspaces. I guess we could say that we've talked about space the row space in the column space space space of the house. So we could also talk about the null-space of the transpose. So there are four important subspaces Associated to Matrix the column space and the ": [
            1007.7,
            1042.1,
            29
        ],
        "lengths one. I have just one wine and a bunch of zeros the sum of the squares of those things as one and those vectors are orthogonal to each other. So the standard basis is an orthonormal basis. Here's another one or rather hear is an orthogonal basis of our three are mutually orthogonal to each other. Let's just go ahead and check that so you won. It with you ": [
            2256.8,
            2282.6,
            63
        ],
        "let me check into that. You can also acquire with the Matlab ta directly about that. Okay, so that's everything. I wanted to say administrative with course material and geometry. so last day One more thing last day. Unfortunately, the video screencaps failed to record against the fourth time not pleased about that keeps failing. I was told that it's working this morning. So hopefully you will have a screencast ": [
            203.5,
            243.0,
            7
        ],
        "lots of examples like that to get it a good feeling for what's going on here and this both of these kinds of examples. in three dimensional space if I took It's a solemn compliment was one-dimensional if I took a one-dimensional Subspace two dimensional. What's going on here what's going on is that if you take a Subspace and its complement add up to three in the Space Case ": [
            840.1,
            887.5,
            25
        ],
        "matrix is a Subspace of RN or an is the number of columns. What is the row space of the Matrix of the column space of a remember? Text tells us that the rank is the dimension of the whole space. That's exactly what we were sitting on the left side with the dimension of the roast is the dimension of the whole Space. Okay. So this is just another ": [
            1072.0,
            1126.8,
            31
        ],
        "meaning being a 0 means orthogonal means that you are perpendicular injections what matrix multiplication says so we just saw that the null-space of the set of things that are perpendicular to the row space is linear combinations of those things, but that's okay because If I take any vector v which is some linear combination Lambda a plus Mew be some linear combination of the Rose. then if I ": [
            1452.0,
            1491.9,
            40
        ],
        "of vectors that are perpendicular to this and that Okay, because orthogonal complement they fill out the rest of the space. Okay, so that's orthogonal complement something we're going to use and need to understand now, I want to talk about orthogonally of sets of vectors orthogonal vectors. so here's an important example over and over and with using lots of computations. So when are three safe around in general ": [
            1589.4,
            1624.9,
            44
        ],
        "often. The Hat indicates that their length one. So if you see a vector and it has a hat on it, that means it has length one more. Generally if I give you a vector and I call it smiley face. Then smiley face hat is defined to be smiley face / the length of smiley face. quit Okay, now what's so great about having an orthodontist basis for an ": [
            2473.3,
            2498.3,
            69
        ],
        "one quick observation here. If the columns are all orthogonal to each other. Then what does this give us? Well, let's look at this entry over here. If a one and a two orthogonal What does it mean to be orthogonal zero? Okay, so I would get a zero they're down here because a 2.81 is the same as a 1.82. What we get is all zeros on the off ": [
            2910.2,
            2947.2,
            81
        ],
        "one-dimensional Subspace. So here's a vector and R2 and here is the Subspace that it generates almost Okay, so that line there is the Subspace who is which is generated by. the orthogonal complement of that Subspace Is the set of all vectors that are orthogonal to every Vector in that Subspace? The perks of Cebu pronounce that perpendicular V perp is the set of all vectors. In our end, ": [
            539.4,
            590.9,
            17
        ],
        "one. Now on the one hand that must equal zero. Right, if you won. It was zero. So this is the combination is the zero vector and therefore $0 with you on his hero, but on the other hand. Let's distribute the dot product through and that gives us to see One X you one dotted with you one plus C to you too. It with you one. plus down ": [
            1889.7,
            1917.1,
            52
        ],
        "or talked about orthogonal compliments that or thought of two vectors orthogonal A Concrete way to compute when a set of vectors are orthogonal to each other. So if I give you a matrix And this is finally going to tell us what does transpose is really useful for the dock products of The Columns of a with with a 2 by 3 Matrix. Actually. It doesn't matter what size ": [
            2740.7,
            2790.6,
            77
        ],
        "orthogonal basis it's an extension of what we talked about when we were talking about or implying implying Lanier in the pain, so if I give you a basis Vector in your vector space has a unique expansion terms of the basis of a vector in terms of a basis means solving a vector equation, but if you have it is Trivial if I have an orthogonal basis for a ": [
            2498.3,
            2533.7,
            70
        ],
        "perpendicular to this plane down here in the sense that you know, there's a right angle down here. But let's be careful here. Let's be careful here the definition of the perpendicular Subspace that are perpendicular to everything to look at this playing again here. This Plane intersects that Subspace along a line right there's a line right here. It's in both of those plants that are involved. You can't ": [
            726.4,
            767.6,
            22
        ],
        "right now and not right now, please open and will be open until 11:59 p.m. Next Friday. So we have to do it before classes end if you're going to do it at all. I'll keep reminding you of that as we go. You have a MyMathLab homework due tomorrow night by 11:59 p.m. Homework is due on Friday by 11:59 p.m. And your Matlab quiz 5% of your grade ": [
            114.8,
            142.7,
            4
        ],
        "so he was a Subspace of RN. This isn't of all vectors in RN. That are perpendicular to every vector. in the Subspace So on this particular picture, let's think about that for a second. So here I'm going to Subspace. zipline What does the perpendicular Subspace look like that are orthogonal to that line? If I see someone making this kind of motion over there that I think that's ": [
            590.9,
            631.6,
            18
        ],
        "so that leaves three is three times the length of U2. Orthonormal basis I'd have to take the vectors you one which is you want hats, which is you 1/3. You too hot, which is you two over the square root of 5 and you Three Hats, which is you 3/3 * \u221a 5 and these ones are an orthonormal basis of our three and we'll use that shorthand fairly ": [
            2436.1,
            2473.3,
            68
        ],
        "square of the length of the opposite side to the right angle. Okay, that's not going to be true in general but it's true if you have a right angle, so that's what I've written right here that use the length of the vector U Mine is the sum of the squares of what we saw last time in general. Using the properties of the inner product. Is that for ": [
            437.9,
            463.1,
            14
        ],
        "suggestion for what it looks like. But some of us know the hand was way down here. Yes. Okay, a plane perpendicular going through the origin. That's it. A great. Guess I Look to take a take a gander at that secures a piece of paper. Let's use that to represent are playing so maybe like in a plane like this. Okay. Well, you know that plane that plane is ": [
            696.6,
            726.4,
            21
        ],
        "take X dotted with v That's equal to actually called W. Cuz I called it w up here x daughter with W is equal to what at Saks dotted with Lambda A+ movie and I use those linear properties of the dot product that we saw that day. That's with a plus Mew x x. But we just saw up here that both of those are zero. And so I ": [
            1491.9,
            1527.0,
            41
        ],
        "that Vector a and let School. Vector B. Okay, I guess those are actually the transposes of the doctor's return them on their side. This is a DOT product X and this is V. Projects. What we see. Has zero. Product with the crows? X is in the null space of a means X. Each row a equals 0 in other words remember that. Part of it was a vector ": [
            1406.7,
            1452.0,
            39
        ],
        "the ER with a basis vector. Find the coefficient of a vector in terms of the standard basis. That means taking the vector and taking 1 times its first component + 0 types of the components. So it's just saving us a whole lot of work when we have an orthodontist basis. Okay. In the last five minutes here. I want to now relate back to matrix multiplication. We saw ": [
            2705.1,
            2740.7,
            76
        ],
        "the length of you 1 squared. I know it could have been that you wanted was the zero Vector in which case would just get there altogether for the theorem here. It's important. We have the statements that the vectors are not zero zero Vector is orthogonal to everything but the zero Vector can't be in a list of linearly independent vectors orthogonal then they are linearly independent and what ": [
            1948.6,
            1979.0,
            54
        ],
        "the line cpup. With you want What is a thing you too. It with you one is zero you three dotted with U10 you pee. It with you one is zero because the vectors are orthogonal to each other. And so all we're left with is this turn right here, but that one is equal to the length of you one squared. And so this whole thing is C1 times ": [
            1917.1,
            1948.6,
            53
        ],
        "the row space Aerospace? What y'all use the text book conventions the column space of a transpose? That's the span of the Rose but transposed so it's the span of the first row and the second row but treated as columns by turning them on their side. So that's what the row space is the span of those two vectors there. And what is the null Space by definition? It's ": [
            1273.6,
            1309.9,
            35
        ],
        "the set of vectors X in R34 which a x x is equal to 0. Okay. Well, let's explore that a little bit. Let's write it out in this example. So what does it mean that a x x is equal to 0 so that means well, first of all, what size is the zero in this case? Text has to be a three dimensional Vector there for a x ": [
            1309.9,
            1332.8,
            36
        ],
        "the top product of you over its length? With you over its length. Okay, but now we use the properties of the top products. We have that factor one over the length of you. It's in there twice so we can Factor it out and what is divided by the length of you squared? But you. You is the length of you swear. So this number is just what? So ": [
            2201.3,
            2230.6,
            61
        ],
        "the way matrix multiplication works as I take that first row and X that First Column there to get the first entry and then I take that first row. X at the second column to get the second entry and so on down the line. So what I'm going to get is the first entry is a 1 transpose * 8 1 which is the definition of a 1.3 is ": [
            2838.4,
            2862.7,
            79
        ],
        "then I get this. Okay, so if I want to find coefficient a vector in terms of an orthonormal basis. I don't need to do any row reduction. All I have to do is take the duck products of that Vector with the basis vectors and calculate the length of the basis vectors. And then the remark here is that if you won through you pee are normalized if there ": [
            2643.6,
            2673.4,
            74
        ],
        "those links are one. I'm so really we get this so here is a very useful statement if I want to check if a collection of vectors are orthogonal normal. That's the same thing as saying that the Matrix a who has those columns as its columns a transpose a is the identity Matrix. Be careful Here. I Am by 3 Matrix in this case. There were three columns a ": [
            3007.8,
            3036.0,
            83
        ],
        "time. And I distribute through that some. I'm taking the dog park with you one in each case. Send almost all of those terms are zero. You too. You want a zero you three. You want a zero you pee. You want is 0 and all I'm left with. Is C1 times the length of you 1 squared? I know I Divide through by the length of you one square ": [
            2601.0,
            2643.6,
            73
        ],
        "to argue by contradiction. We're going to see what goes wrong with that assumption. So if I have some linear combination C11 + C2 you to + etcetera. I have someone your combination of them equaling zero. I want to show that all of those coefficients are zero. Know if these were a bunch of column vectors what I would then do as I have a vector equation here and ": [
            1813.8,
            1844.0,
            50
        ],
        "to do an computations and. Products right for each one of these and then and this is where you're trying to isolate those coefficients and usually when you drink it depends on Okay, so orthogonally is the best kind of linear Independence. So we get linear Independence whenever we have orthogonally around and what that means is remember orthogon. Ality linear Independence is one of the two key elements of ": [
            2041.1,
            2095.0,
            57
        ],
        "to take those three columns string them together in a matrix engine from the midterm. If you wants to check vectors an RN n-dimensional Vector space nearly independent vectors in n-dimensional space Forma basis automatically. So these are three vectors in R3 to check that they form a basis. We just need to check that they are linearly independent and dependent they are not normal vectors though. So if we ": [
            2315.0,
            2373.8,
            65
        ],
        "transpose was three by m. And so we see that a transpose a is three by three entries in each call. Okay. So this you get a square Matrix A square B Square for this to be true if I have any collection of columns at all a transpose a that's going to give me the identity Matrix of the appropriate size what it means to say that I will ": [
            3036.0,
            3064.4,
            84
        ],
        "two. That's equal to 2 + 0 - 210. You one daughter with you three? Is 2 - 10 + 8 which is 0 and you too. It with you three. Is 4 + 0 - 4 which is 0 or all orthogonal to each other. And by the way, I told you there are basis and the way you would normally check that if I hadn't told you is ": [
            2282.6,
            2315.0,
            64
        ],
        "vector space for a Subspace looks like this That is a coefficient of the vector V1 place is just the dotted with you want / the length of you 1 squared and so on Down the Line. We actually basically already proved this on the last flight. Let's just do the same calculation again. So what is this mean? Just to remind you about coordinate vectors K. This means that ": [
            2533.7,
            2570.0,
            71
        ],
        "vector. What about twice that factor and 76 X Factor. So probably meant it's through the bottle. It's the Subspace spanned by the bottle. perpendicular to it What's the opposite of compliment? It's the plane down here. Okay, the things that are perpendicular to a line in R3 or a plane the things that are perpendicular to a plane form a line in R3. Okay, so you should think about ": [
            801.4,
            840.1,
            24
        ],
        "vectors are orthogonal that correction. Okay, that's quick recap of what we did last day. So now today I want to talk about this concept of orthogon Outlet e start actually of vector spaces. So if I have a Subspace of RN? and as we as we go about this, let's think concretely so for example If I have a Subspace in R&R to and let's say it is a ": [
            498.0,
            539.4,
            16
        ],
        "wanted to know we also need to check the length. So the one I take it with itself and I get 1 squared + -2 squared + -2 squared + 3 + 2 positive 2 squared which is 1 + 4 + 4 which is 9 and so that says that the length of you one is three. The length squared of U2 is 2 squared + -1 squared + ": [
            2373.8,
            2402.9,
            66
        ],
        "we replace it with you. / it's likes. So I take it back there. I compute its length \u221a a stop by itself, which is a nonzero number cuz it's not. And the reason we do that is if I calculate the length of you hat now. Get up to the square of that. The length of you had is the dot product of you hat with itself. Which is ": [
            2172.9,
            2201.3,
            60
        ],
        "we see now is why that's true because what I get is it from this equation I get from this calculation I get that 0 is equal to 61 times the length of you 1 squared the length of you once where it is and not zero because you want is not the zero factor. And so therefore we see that C10. Okay, great. See one is it what about ": [
            1979.0,
            2004.0,
            55
        ],
        "what it is defined to be. So by definition the dot product is just V transpose W number for two column vectors BMW and then we Define from that the length the length of a vector is the square root of the duck part of the vector with itself, which is just the square root of the sum of the squares of the entries and we saw last time I ": [
            289.2,
            314.8,
            10
        ],
        "with unexpected. It's just going to take another couple passes through for you guys to get it. So the one takeaway that you should take one of the takeaways you should take the midterm is before the final exam. You should become more comfortable with the abstract notion of Subspace and Vector space and how those fit together and how to check if something is a Subspace what that means? ": [
            61.3,
            85.2,
            2
        ],
        "x is going to be to buy one going to be the two dimensional zero vector. So let's write a X x so that's X1 X2 X3. So that's what it means for the vector X to be in the null space of a vector. What this says? Let's use matrix multiplication. This is going to be a two-dimensional Vector multiplying along the First Column the first row of the ": [
            1332.8,
            1371.5,
            37
        ],
        "you can always normalize a nonzero Vector. He can always divided by its length and make it a unit length sector a normal Vector orthonormal. If all of the factors in the bases are mutually orthogonal to each other and each one of those vectors has length one the canonical example of that the standard basis the vector 1 0 0 0 1 0 0 0 1 they all have ": [
            2230.6,
            2256.8,
            62
        ],
        "you have a conflict with me to do it on Monday, hopefully one of those will work for you and if not then get in touch with me. You know what? I do not know the answer to that question. So I think what you should do. The question is are you allowed to bring notes? And I don't want to give you a false answer from the instructions. So ": [
            174.0,
            203.5,
            6
        ]
    },
    "File Name": "Linear_Algebra___B00___Kemp__Todd_Aahron___Winter_2018-lecture_24.flac",
    "Full Transcript": "Happy Wednesday.  Let's get going.  First let me briefly mention that you should all have seen by now that was graded exams regrade request system this time around it's open right now closes on Friday before midnight. So take a careful. Look if you feel like you want to request a regret of a particular problem. There's a procedure to follow within gradescope. A detailed comment has to be done by this Friday at 11:59 p.m. The one thing I will say about the midterm is that people did reasonably well on most problems, but one that caught me off guard that you didn't do as well on as I had thought you would was problem 3 problems free was all about recognizing. What is a Subspace. So this is a concept you are still having a lot of trouble with unexpected.  It's just going to take another couple passes through for you guys to get it. So the one takeaway that you should take one of the takeaways you should take the midterm is before the final exam. You should become more comfortable with the abstract notion of Subspace and Vector space and how those fit together and how to check if something is a Subspace what that means? Okay, because given that it was not very well done on the midterm. You can be sure that it's likely to appear again on the final exam.  Okay few administrative reminders first course evaluations. We really appreciate the feedback at your Tia's give detailed feedback about what you thought works. Well what you could be improved, I'd love to see all of you submit your tapes. You can do that right now and not right now, please open and will be open until 11:59 p.m. Next Friday. So we have to do it before classes end if you're going to do it at all. I'll keep reminding you of that as we go.  You have a MyMathLab homework due tomorrow night by 11:59 p.m. Homework is due on Friday by 11:59 p.m. And your Matlab quiz 5% of your grade in this class is next week. Don't sweat it. It's no big deal. It's very carefully and things related to the course material. It's more of a basic test of your familiarity with basic functions in Matlab. Okay, and it's scheduled for Tuesday and Thursday. So if you need it on Thursday, if your sections on Thursday at 5 p.m. Is going to be on Tuesday at 5 p.m. If you have a conflict with me to do it on Monday, hopefully one of those will work for you and if not then get in touch with me.  You know what? I do not know the answer to that question. So I think what you should do. The question is are you allowed to bring notes? And I don't want to give you a false answer from the instructions. So let me check into that. You can also acquire with the Matlab ta directly about that.  Okay, so that's everything. I wanted to say administrative with course material and geometry.  so last day  One more thing last day. Unfortunately, the video screencaps failed to record against the fourth time not pleased about that keeps failing. I was told that it's working this morning. So hopefully you will have a screencast from this lecture. The one thing that I can say is winter 2017, which are available on the podcast site. There's a bit of a difference in when what topic was done than relatives and now there's a shift of about a full lecture. We're about a whole lecture ahead of what we were back then that's what I planned it this time. So you may need to look on a different date different lecture number last time but everything we're doing here you can see slightly different game ideas for the last time around. So if you're looking for a screencast from this time, it's not there from last year's and that will be a helpful resource to you.  All right, we start talking about inner product. Product last time. Okay, so the dot product I haven't even written what it is defined to be. So by definition the dot product is just V transpose W number for two column vectors BMW and then we Define from that the length the length of a vector is the square root of the duck part of the vector with itself, which is just the square root of the sum of the squares of the entries and we saw last time I finished my proving the cauchy Schwarz inequality, which tells you what I've written here.  At least make sense that the dog products divided by the product of the lengths of the two vectors. That's a number that's between me and that something that say that is actually geometric is the angle between the two vectors. So so from there we're going to talk about now primarily is the special case Wednesday to equal zero.  So when are parallel really the important Point here is Wednesday. Is equal to pi over 2. What day does 90 degrees when the vectors are perpendicular to each other? Okay. I'm from the product is equal to the product of the cosine of 90\u00b0 is 0  Two vectors are perpendicular or the commonly uses orthogonal. And by the way will often use this symbol for a thug in a little right make it crystal clear here like an upside down t  Okay to say that they are orthogonal to each other. So there. Product is zero from consideration of sakurasou Theron from plane. Geometry says that if you have a right angle triangle, then the sum of the squares of the two sides some of those squares equal to the square of the hypotenuse the square of the length of the opposite side to the right angle. Okay, that's not going to be true in general but it's true if you have a right angle, so that's what I've written right here that use the length of the vector U Mine is the sum of the squares of what we saw last time in general.  Using the properties of the inner product. Is that for any two vectors u and v  The square of the length of their difference which is the inner product of their difference with itself.  is equal to  you squared plus b squared but there's a correction turn that is minus twice. The inner product of you with v is the is equal to twice the length of the vectors u and v times the cosine of the angle between the vectors are orthogonal that correction.  Okay, that's quick recap of what we did last day. So now today I want to talk about this concept of orthogon Outlet e  start actually of vector spaces.  So if I have a Subspace of RN?  and as we as we go about this, let's think concretely so for example  If I have a Subspace in R&R to and let's say it is a one-dimensional Subspace. So here's a vector and R2 and  here is  the Subspace that it generates almost  Okay, so that line there is the Subspace who is which is generated by.  the orthogonal complement of that Subspace  Is the set of all vectors that are orthogonal to every Vector in that Subspace?  The perks of Cebu pronounce that perpendicular V perp is the set of all vectors.  In our end, so he was a Subspace of RN. This isn't of all vectors in RN.  That are perpendicular to every vector.  in the Subspace  So on this particular picture, let's think about that for a second.  So here I'm going to Subspace.  zipline  What does the perpendicular Subspace look like that are orthogonal to that line?  If I see someone making this kind of motion over there that I think that's as close as you'll be able to do over here.  That makes a right angle with that line. That's correct. Okay. So this line over here is V perp when this one here was be.  Okay, so that's a pretty relatively easy things. I understand if I give you a line in the plane, it's perpendicular Subspace. It's orthogonal complement Subspace that is perpendicular to it. That uniquely pins it down. No, I want to come to draw in three dimensions. But okay. What if I asked you here is a Subspace of R3 is table okay the table extended in and I'll directions here. Let's suppose that this bottle here is right at the origin. So here is a Subspace of R3. What is the orthogonal complement of that Subspace?  Someone like to make a guess or make a suggestion for what it looks like.  But some of us know the hand was way down here. Yes.  Okay, a plane perpendicular going through the origin. That's it. A great. Guess I Look to take a take a gander at that secures a piece of paper. Let's use that to represent are playing so maybe like in a plane like this.  Okay. Well, you know that plane that plane is perpendicular to this plane down here in the sense that you know, there's a right angle down here. But let's be careful here. Let's be careful here the definition of the perpendicular Subspace that are perpendicular to everything to look at this playing again here.  This Plane intersects that Subspace along a line right there's a line right here.  It's in both of those plants that are involved.  You can't have a vector in a plane that is perpendicular exception.  Is there a vector so this is not quite right this think it has some doctors that are perpendicular to everything in the plane, but it also has some vectors that are in the plane. So this too big to the Asylum compliment. So what's  It's the vector made by the bottle. That's almost exactly right except that's just one vector. What about twice that factor and 76 X Factor. So probably meant it's through the bottle. It's the Subspace spanned by the bottle.  perpendicular to it  What's the opposite of compliment?  It's the plane down here. Okay, the things that are perpendicular to a line in R3 or a plane the things that are perpendicular to a plane form a line in R3.  Okay, so you should think about lots of examples like that to get it a good feeling for what's going on here and this both of these kinds of examples.  in three dimensional space if I took  It's a solemn compliment was one-dimensional if I took a one-dimensional Subspace two dimensional.  What's going on here what's going on is that if you take a Subspace and its complement add up to three in the Space Case in the plant in case they add up to the dimension of the space for an RN then a Subspace orthogonal complement their Dimensions add up to the dimension end of the whole Space.  What if I tell you right now of three-dimensional space inside three dimensional space?  What's a set of all vectors that are perpendicular to?  Just the zero Vector orthogonal complement of a vector space in itself is the zero Vector Subspace.  Just turn around what we just said.  I saw someone actually turn around not quite what I meant. But so if the complement of everything is 0x if I'm just a zero Vector what vectors are perpendicular to it. Every Vector is perpendicular to those are more demonstrations of the fact that you get this complementarity.  the the two subspaces compliment  we talked about some important subspaces like the no space in the column space of a matrix. Remember that the null-space of a matrix that has them to mention the melody the column space that Matrix has them to mention the rank and we have the ranks are in which says that those two Dimensions they add up to the size of the space size of that case.  So we get a compliment and it actually is the same theorem. That's because of this fact here.  If I have a matrix and Matrix.  There are three subspaces. I guess we could say that we've talked about space the row space in the column space space space of the house. So we could also talk about the null-space of the transpose. So there are four important subspaces Associated to Matrix the column space and the column space of a transpose another name for that is just the row space today.  and here is a big show this together with the ranks Arab usually goes under the name the fundamental theorem of linear algebra important theorem the geometry of linear algebra, and it says that if I take the null-space of a matrix,  what is the orthogonal complement of that? Okay, so the null-space of a matrix is a Subspace of RN or an is the number of columns.  What is the row space of the Matrix of the column space of a remember?  Text tells us that the rank is the dimension of the whole space. That's exactly what we were sitting on the left side with the dimension of the roast is the dimension of the whole Space. Okay. So this is just another demonstration of the ranks are going to prove this but more important it is to understand what's going on here.  So what does it mean when I get that first statement there?  oops  What does it mean?  To say that the orthogonal comment of the row space is the null space.  So this means  that  V is a vector in the row space of a  and W is a vector in the null space of a  Then that implies that V is perpendicular to w.  In other words there. Product is zero.  That's the statement that's being made here. If I take a vector from the row space and a vector from the no space those two vectors are perpendicular to each other. Those have to live in the same space so that the number of columns.  The row space is the Subspace spanned by the elements in a given row is the number that that's what this says.  Understand what's going on here.  Let's look at a matrix a that is I don't know. Let's make a 2 by 3.  Set another right too much. Okay, so let's write it as  A1 A2 A3 B1 B2 B3  Set a reminder to Rose of The Matrix.  And so what is the row space Aerospace?  What y'all use the text book conventions the column space of a transpose?  That's the span of the Rose but transposed so it's the span of the first row and the second row but treated as columns by turning them on their side.  So that's what the row space is the span of those two vectors there. And what is the null Space by definition? It's the set of vectors X in R34 which a x x is equal to 0.  Okay. Well, let's explore that a little bit. Let's write it out in this example. So what does it mean that a x x is equal to 0 so that means well, first of all, what size is the zero in this case?  Text has to be a three dimensional Vector there for a x x is going to be to buy one going to be the two dimensional zero vector.  So let's write a  X x so that's X1 X2 X3.  So that's what it means for the vector X to be in the null space of a vector.  What this says? Let's use matrix multiplication. This is going to be a two-dimensional Vector multiplying along the First Column the first row of the first Matrix X the column X seconds me a 1 x 1 plus a 2 x 2 + 83 x 3.  And then doing the same thing, but with the second row here for the second column, that's v1x 1 + b 2 x 2.  Forex to be in the North Face says that Vector there is zero recognize what's written here is exactly  this first row here. Let's call that Vector a and let School. Vector B.  Okay, I guess those are actually the transposes of the doctor's return them on their side.  This is a DOT product X and this is V. Projects.  What we see.  Has zero. Product with the crows?  X is in the null space of a means  X. Each row  a equals 0  in other words remember that. Part of it was a vector meaning being a 0 means orthogonal means that you are perpendicular injections what matrix multiplication says  so we just saw that the null-space of the set of things that are perpendicular to the row space is linear combinations of those things, but that's okay because  If I take any vector v which is some linear combination Lambda a plus Mew be some linear combination of the Rose.  then if I take X dotted with v  That's equal to actually called W. Cuz I called it w up here x daughter with W is equal to what at Saks dotted with Lambda A+ movie and I use those linear properties of the dot product that we saw that day. That's with a plus Mew x x.  But we just saw up here that both of those are zero.  And so I get zero and there we have it. If your perpendicular to both the rose that are perpendicular to every linear combination of the Rose. In other words, you're perpendicular to the row space. There's the fundamental theorem of linear algebra and its just using the words. We've learned Now. Product orthogonal those words interpreting them in terms of what matrix multiplication says. So the orthogonal complement of the row space is the null space ax equals 0 which seems like a very different sort of objects in the column space in the row space Witcher just the vector spaces stand by the road.  Really are comprable things the column spacer becomes face of a transpose. The row space is some Subspace of this of the space where the rose live is the orthogonal complement of that thing is a set of vectors that are perpendicular to this and that  Okay, because orthogonal complement they fill out the rest of the space.  Okay, so that's orthogonal complement something we're going to use and need to understand now, I want to talk about orthogonally of sets of vectors orthogonal vectors.  so  here's an important example over and over and with using lots of computations. So when are three safe around in general and R3? We have our standard basis vectors 1 0 0 0 1 0 0 0 1 and we use those whenever we can they make computations as easy as possible for easier to work with than other vectors and one reason we can now see why that's true.  Is these vectors are all orthogonal to each other?  6 + pics of the first two  so if I take  the first one. It with the third one say  well, that's going to be 1 * 0 + 0 * 0 + 0 * 1 which is gives me 0  and you can do the same thing with the first two or the last two vectors are perpendicular to each other next class. You take your your thumb your first and second fingers on a hand, I guess in physics and each other all three of the orthogonal frame.  The only has that property.  Here's a different set of vectors are perpendicular to each other. Which two should I choose?  Great. So, let's take the dumb product of you two and use three.  okay, so that's just going to be - 1 * - 1/2 + 2 * -2 + 1 * 7/2  1/2 - 4 + 7/2  1/2 + 7/2 is 8/2 to the -4 + 8/2 + 1/2 is 4 so we can get Sarah and if you want to go ahead and if you want a new three, you'll see that all three of those vectors are orthogonal to each other.  Okay, great. So, how does that help us what helps us in a lot of ways that are all Mutual every pair of them are orthogonal to each other. One thing you get immediately for free is it those vectors are linearly independent from each other or saw the body of a bunch of vectors implies linear Independence now, why is that? Well, let's see here. So just to prove this there as usual linear Independence. It says that the vectors are linearly dependent if you're going to prove something about negative Universal Property. Usually you have to argue by contradiction. We're going to see what goes wrong with that assumption.  So if I have some linear combination C11 + C2 you to + etcetera.  I have someone your combination of them equaling zero. I want to show that all of those coefficients are zero.  Know if these were a bunch of column vectors what I would then do as I have a vector equation here and I need to do rubber duck session to solve it and see that the only zeros that's what it would mean to prove. These are linearly independent invector is an RN so it can take steps. So here's how we do that. We say therefore.  If I take that Vector c1u, 1 + C to you, too.  down the line cpup  I'm going to take it too. Product with you one.  Now on the one hand that must equal zero.  Right, if you won. It was zero. So this is the combination is the zero vector and therefore $0 with you on his hero, but on the other hand.  Let's distribute the dot product through and that gives us to see One X you one dotted with you one plus C to you too. It with you one.  plus down the line cpup. With you want  What is a thing you too. It with you one is zero you three dotted with U10 you pee. It with you one is zero because the vectors are orthogonal to each other. And so all we're left with is this turn right here, but that one is equal to the length of you one squared.  And so this whole thing is C1 times the length of you 1 squared.  I know it could have been that you wanted was the zero Vector in which case would just get there altogether for the theorem here. It's important. We have the statements that the vectors are not zero zero Vector is orthogonal to everything but the zero Vector can't be in a list of linearly independent vectors orthogonal then they are linearly independent and what we see now is why that's true because what I get is it from this equation I get from this calculation I get that 0 is equal to 61 times the length of you 1 squared the length of you once where it is and not zero because you want is not the zero factor. And so therefore we see that C10.  Okay, great. See one is it what about C2? We're just going to repeat exactly the same calculation take that supposed to fill near combination of the use and take its. Product with you to the exact same calculation. Will the coefficients except for the times to is not 0 and therefore C2 must be zero and just keep going down the line and every one of those coefficients is 0  it only takes 10 squared steps have to do an computations and. Products right for each one of these and then and this is where you're trying to isolate those coefficients and usually when you drink it depends on  Okay, so orthogonally is the best kind of linear Independence.  So we get linear Independence whenever we have orthogonally around and what that means is remember orthogon. Ality linear Independence is one of the two key elements of a bassist so we can now talk about orthogonal basis. And those are going to be the best kind of face. He's so given a Subspace V of RN a basis for it is called an orthogonal basis vectors. I want them to be the best kind of linear Independence.  Senior independent vectors are orthogonal most of the time not all in the same plane or whatever, but it's the best kind of linear Independence.  We can go one small step further to say and let me also insist that all the vectors have a length one that's called normalized is called normalized if it's length is one but that's actually easy to achieve whenever I have a nonzero Vector it right over here.  If you is not equal to zero.  Then we can normalize it.  What that means is we replace it with you. / it's likes.  So I take it back there. I compute its length \u221a a stop by itself, which is a nonzero number cuz it's not.  And the reason we do that is if I calculate the length of you hat now.  Get up to the square of that. The length of you had is the dot product of you hat with itself.  Which is the top product of you over its length?  With you over its length.  Okay, but now we use the properties of the top products. We have that factor one over the length of you. It's in there twice so we can Factor it out and what is divided by the length of you squared?  But you. You is the length of you swear. So this number is just what?  So you can always normalize a nonzero Vector. He can always divided by its length and make it a unit length sector a normal Vector orthonormal. If all of the factors in the bases are mutually orthogonal to each other and each one of those vectors has length one the canonical example of that the standard basis the vector 1 0 0 0 1 0 0 0 1 they all have lengths one. I have just one wine and a bunch of zeros the sum of the squares of those things as one and those vectors are orthogonal to each other. So the standard basis is an orthonormal basis. Here's another one or rather hear is an orthogonal basis of our three are mutually orthogonal to each other. Let's just go ahead and check that so you won. It with you two.  That's equal to 2 + 0 - 210.  You one daughter with you three?  Is 2 - 10 + 8 which is 0 and you too. It with you three.  Is 4 + 0 - 4 which is 0 or all orthogonal to each other. And by the way, I told you there are basis and the way you would normally check that if I hadn't told you is to take those three columns string them together in a matrix engine from the midterm. If you wants to check vectors an RN n-dimensional Vector space nearly independent vectors in n-dimensional space Forma basis automatically.  So these are three vectors in R3 to check that they form a basis. We just need to check that they are linearly independent and dependent they are not normal vectors though. So if we wanted to know we also need to check the length.  So the one I take it with itself and I get 1 squared + -2 squared + -2 squared + 3 + 2 positive 2 squared which is 1 + 4 + 4 which is 9 and so that says that the length of you one is three.  The length squared of U2 is 2 squared + -1 squared + 0 squared.  Which is 5 and so therefore the length of U2 is the square root of 5.  And you three dotted with itself.  gives 2 squared + 5 squared + 4 squared  which is 4 + 25 + 16, which is 45 and therefore the length of you three.  Is equal to the square root of 45, which is better written is 3 times the square root of 5, so that leaves three is three times the length of U2.  Orthonormal basis I'd have to take the vectors you one which is you want hats, which is you 1/3.  You too hot, which is you two over the square root of 5 and you Three Hats, which is you 3/3 * \u221a 5 and these ones are an orthonormal basis of our three and we'll use that shorthand fairly often.  The Hat indicates that their length one. So if you see a vector and it has a hat on it, that means it has length one more. Generally if I give you a vector and I call it smiley face. Then smiley face hat is defined to be smiley face / the length of smiley face.  quit  Okay, now what's so great about having an orthodontist basis for an orthogonal basis it's an extension of what we talked about when we were talking about or implying implying Lanier in the pain, so if I give you a basis Vector in your vector space has a unique expansion terms of the basis of a vector in terms of a basis means solving a vector equation, but if you have it is Trivial if I have an orthogonal basis for a vector space for a Subspace looks like this  That is a coefficient of the vector V1 place is just the dotted with you want / the length of you 1 squared and so on Down the Line. We actually basically already proved this on the last flight. Let's just do the same calculation again. So what is this mean?  Just to remind you about coordinate vectors K. This means that V is equal to 1 + 2 +...  Cpup. Sometimes it's hard to tell the difference between Pisces and you've sorry about that. We want to find those seas and to do so usually involves solving a vector equation, but it doesn't have to in this case because if I take the dog product of V with say you won.  Play I do exactly the same calculation I did last time.  And I distribute through that some.  I'm taking the dog park with you one in each case.  Send almost all of those terms are zero. You too. You want a zero you three. You want a zero you pee. You want is 0 and all I'm left with.  Is C1 times the length of you 1 squared?  I know I  Divide through by the length of you one square then I get this.  Okay, so if I want to find  coefficient  a vector in terms of an orthonormal basis. I don't need to do any row reduction. All I have to do is take the duck products of that Vector with the basis vectors and calculate the length of the basis vectors. And then the remark here is that if you won through you pee  are normalized if there are length one.  Then that means exactly that their length squared for example r0 R1.  So what we get, is that V dotted with you Jai is equal to CJ because that you 1 squared in the denominator. That's just so if you have an orthodontist basis, do you want to find the coefficients of a vector in terms that off and on what basis they're just exactly equal to the ER with a basis vector.  Find the coefficient of a vector in terms of the standard basis. That means taking the vector and taking 1 times its first component + 0 types of the components.  So it's just saving us a whole lot of work when we have an orthodontist basis.  Okay.  In the last five minutes here. I want to now relate back to matrix multiplication. We saw or talked about orthogonal compliments that or thought of two vectors orthogonal A Concrete way to compute when a set of vectors are orthogonal to each other. So if I give you a matrix  And this is finally going to tell us what does transpose is really useful for the dock products of The Columns of a with with a 2 by 3 Matrix. Actually. It doesn't matter what size The Columns are right it is what's supposed it has three columns.  Call Emma. They might be a millionaire.  Means I take those columns and I write them as the rose.  Take the transpose of the columns.  Of a transpose. That's what if I calculate this thing a transpose a  so I'm taking that Matrix whose rows are The Columns of a on their sides.  and * the columns  Well, the way matrix multiplication works as I take that first row and X that First Column there to get the first entry and then I take that first row.  X at the second column to get the second entry and so on down the line. So what I'm going to get is the first entry is a 1 transpose * 8 1 which is the definition of a 1.3 is a 1.82.  And so on so we take the a ones dotted with a1283 in the first row.  A2 dotted with a 1.83 in the second row  And a $3 with a one through a three.  Is a third row?  And so we see that in this Matrix.  A transpose a the entries are the duck product of all of The Columns of the Matrix a transpose a computer.  No one quick observation here.  If the columns are all orthogonal to each other.  Then what does this give us? Well, let's look at this entry over here. If a one and a two orthogonal  What does it mean to be orthogonal zero? Okay, so I would get a zero they're down here because a 2.81 is the same as a 1.82. What we get is all zeros on the off diagonal parts.  We get the lynx squared.  Another way of saying a collection of vectors are orthogonal vectors.  A transpose a is diagonal that's what it means to say that a collection of vectors are all mutually orthogonal. Yes.  There is a relationship with that next week we go here, which is that. Well I said it was their orthogonal but what a normal if they're normal.  Then all of those links are one.  I'm so really we get this so here is a very useful statement if I want to check if a collection of vectors are orthogonal normal.  That's the same thing as saying that the Matrix a who has those columns as its columns a transpose a is the identity Matrix. Be careful Here. I Am by 3 Matrix in this case. There were three columns a transpose was three by m. And so we see that a transpose a is three by three entries in each call. Okay. So this you get a square Matrix A square B Square for this to be true if I have any collection of columns at all a transpose a that's going to give me the identity Matrix of the appropriate size what it means to say that I will continue with that discussion on Friday. "
}