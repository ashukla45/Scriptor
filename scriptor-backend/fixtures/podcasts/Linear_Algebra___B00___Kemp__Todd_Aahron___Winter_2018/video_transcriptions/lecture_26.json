{
    "Blurbs": {
        "* 1 which is 2 - 1 which is what? not zero So this is not an orthogonal basis. Oh well. Now what? So I have no other procedure for finding a basis, right? I mean I could mess around with this thing is I know that if I take any two linearly independent vectors in they will form a basis so I can start taking linear combinations of those ": [
            885.0,
            915.6,
            30
        ],
        "+ V3 dotted with you one hat times. You want hot + V3 daughter with you too hot. I miss you, too hat. So what I'm doing here is on solving for the V's in terms of the use that we already produced. No, it's kind of implicit because it involves the length of the number here. Let me call that R11. These numbers down here. Let me call them ": [
            2531.8,
            2564.7,
            87
        ],
        "- x is in the original Subspace. In other words are projections all about breaking down a vector as a sum of two things one in the Subspace in one perpendicular the Subspace that means forever orthogonally projecting into the OR thought into the orthogonal complement of a vector by definition we can just get this By taking the vector V2 and subtract attracting from it the projection into the ": [
            1135.1,
            1163.9,
            39
        ],
        "18. Okay, I get the first one is V3 sided with you one, which is 18 / the length of you wants word, which is 9 times the vector you one which is 1-2 2 And the second one is the top products V3 with you to which we calculated was also 18 divided by the length of you two squared which is 8. Time's the vector you too. Can ": [
            2232.9,
            2263.7,
            76
        ],
        "2. Okay, that's a simple linear equation. And that is that says V2 is equal to the length of you two times you two hats plus the top part of V2 with you 1/2 x you want hat? Unless I'll get you three. I said, let me get myself a little more space here. You straight down here. Okay, so that was we normalized the vector that we get and ": [
            2475.0,
            2504.0,
            85
        ],
        "3 depends only on Hue one that you two had a new 3 hat. It doesn't depend on you for you 5 until we get this same triangular structure. So let me write that the Triangular structure. It really is a parent when you write things like this. If I take the same collection of vector equation is here and write it as an equation for the Matrix. So here's ": [
            2632.7,
            2657.1,
            90
        ],
        "9 So I got minus 1/9 or 30 - 9 / the length of you one square, which is also 9. Time is the vector you on the on normalize Vector you want one- to two? 9 over 9 as one so this becomes easy. So this is 1 - 1 which is 0 0 - -2 which is 2 + 4 - 2 which is two and if I ": [
            2070.5,
            2098.6,
            71
        ],
        "All right, and that's basically all the administer B. I wanted to go through so I'm going to get started now with today's course material. Let's begin by reviewing what we talked about last time. So we're now deep into orthogonal T or C normality orthonormal basis and last time we talked about orthogonal projection. So if we have a Subspace V of RN and everything in the section we're ": [
            126.4,
            156.3,
            4
        ],
        "And then I'm going to Pivot the last row. And now I'm going to subtract the second row from the first. And I'm going to be in reduced row Echelon form when I do that. So I'm subtracting not that I make a mistake on that last one. Yes could thank you, which means that this is -1 there now and I'm interested in reduced row Echelon form. And so ": [
            747.1,
            789.3,
            26
        ],
        "Another U2 remaining numbers. We need are the. Box soex dotted with you one. well X is Vector 1 2 3 so I'll just write this then so that's 1 * 2 + 2 * 5 + 3 * -1 + 2 + 10 - 3 which is 9 and ex started with you, too. is 1 * -2 + 2 * 1 + 3 * 1 which is three ": [
            470.9,
            505.5,
            16
        ],
        "As I said, this is the straight down projection. So you take the vector ex and this why this projection why which work on projects of the it is the unique Vector in the Subspace V4, which that straight down wine y - x is perpendicular to be that's the definition. And geometric see what that means because of Pythagoras Theorem geometrically what that means is that this Vector why ": [
            252.0,
            283.3,
            9
        ],
        "Now what I have to do? I can just use the formulas up there right? I take the vector X. Stop product it with Y X a but with what you want and multiply that Skillet come to you want and take the vector ex and duct products with you to I'll fly by U2. Is that right? It's wrong because you can tell by my tone of voice by ": [
            385.3,
            410.9,
            13
        ],
        "R22 and r21. And these numbers here. Let me call them r33 r31 and R32. So when we do that what we have produced is we see that I've written it as some number rkk times UK hot. + r. K k - 1 x UK - 1 Hat + r. K k - 2 U K - 2 hat and so on Down the Line. That might look pretty ": [
            2564.7,
            2610.3,
            88
        ],
        "Schmidt process organized in the right way. So if you backtrack to slides to this calculation here, it's all of these coefficients. that one that one those two that one and that one that come in just need to keep track of which one goes where? So in that example, I'm just going to copy those numbers over what we got there. Play we started with that basis for the ": [
            2787.0,
            2822.2,
            95
        ],
        "That's not how that live doesn't what MetLife does instead. Is this Matt live computes the QR factorization of your Matrix, which is a very simple algorithm know you might say hey, I know what's going on here that are Matrix is upper triangular. But caution the QR factorization. The are doesn't have the same eigenvalues as the original Matrix. If it did you would be easy to compute eigenvalues ": [
            2971.1,
            3001.5,
            101
        ],
        "YouTube / it's links. Okay, so if we wanted to we could do the normalization as we go let me write down what that would look like. So then we would say well the first step is to take you 1/2 is equal to V1 divided by its length. and then you two hat is equal to V2 - Okay. Well if we've already normalized then the projection is slightly ": [
            1816.0,
            1854.2,
            62
        ],
        "a basis for the Subspace? Oh, wow, come on, guys. It's been 9 weeks already. We've been training. Let's do it again. How do we find the basis for the Subspace? Write better know that by now for the final exam. So we're going to do row reduction on that Matrix. So first I'll subtract the first row from the second. I got 0 - 2 - 2 - 2. ": [
            715.7,
            742.6,
            25
        ],
        "a few people have posted on Piazza that you signed up for the makeup exam or that the conflict quiz time, which is today there were several times a day, but then now you don't have anymore and you can do it on your regular time. If that happens just email your ta your tea for your section is proctoring. You're not labquest. Okay, so just communicate with your ta ": [
            101.3,
            125.1,
            3
        ],
        "a sec can hold on you're talking in circles, man, because the whole point of what we're trying to do here. It's figure out how to find an orthogonal basis so that we can compute orthogonal projection. But now I'm telling you in order to find an orthogonal basis. We're going to use an orthogonal projection. How am I supposed to do that? Well valid point but there are some ": [
            1079.4,
            1102.8,
            37
        ],
        "all I need to do to be one And then I'll have no basis now. I could set that up as a linear equation now, but actually here's exactly how we're going to do it. This is the end result and it uses orthogonal projection in socks. Let's think about this what I want. I want to take a vector. in the Subspace spanned by V1 and V2 What is ": [
            1016.7,
            1044.6,
            35
        ],
        "all you end up writing down as the finals record your grade is not going to have any idea where you went wrong. Whereas if you right back and that down there and you know, what if there was if I got eight instead of 6 by accident for the length of your greater will know where you went wrong and be able to. And so that's my recommendation to ": [
            588.4,
            608.8,
            20
        ],
        "already taken account of the length of you won in both places. So you're going to use that after you want to hear you wouldn't additionally divide that doctor by its length. And then the other comments here is I'm not going to take this calculation any further. We could single Vector I'm not going to do that because this is not a course in arithmetic. In fact, if you ": [
            538.4,
            568.1,
            18
        ],
        "and I'm telling you this is an orthogonal basis and that's actually easy to verify 2 * -2 - 4 + 5 - 1 is indeed 0 compute projection. I would like to find the point. Why in this Subspace me that is closest to that back tracks. In other words. I just want to find the closest point y y is equal to the orthogonal projection. I've asked interview. ": [
            342.6,
            384.0,
            12
        ],
        "and V3. And you for choosing to be a linear combination of V1 V2 V3 and V4. Okay, as we go through this procedure, we're going to find a new bassist where each new basis Vector is a linear combination of only the previous ones. We don't have to reach further on in the bases and we'll see in a few minutes, but that's going to give us some nice ": [
            1397.9,
            1419.8,
            48
        ],
        "and it's not what knot does instead and I'll just finish with this is to say we take a and factorise that is QR and then it takes the new Matrix, which it'll call A2 and writes those two in the other order. In the QR factorization if you start with a square Matrix both q and R will be square. So you can write them in the other order ": [
            3001.5,
            3024.2,
            102
        ],
        "and one is so here is my Matrix are so kieu is ortha normal columns. an R is upper triangular. What's the QR factorization of a matrix you need to know for this course in for the final exam how to find the QR factorization of a matrix, but it's exactly the same thing as doing gram Schmidt orthogonalization. Guess what? That's the QR decomposition of a matrix. On your ": [
            2861.0,
            2898.6,
            97
        ],
        "are on the final exam in this was the question you were asked if you want to go ahead and leave it in that form. That's fine. Did not only is it fine, but it's probably preferable because the greater will be able to recognize that better than the actual answer especially if you made a numerical answer the error in your answer if you made an America Lehrer and ": [
            568.1,
            588.4,
            19
        ],
        "basis for column space of a matrix. We know how to find a basis you found some basis that basis is probably not an orthogonal basis. Okay, but you got start with some basis. What we're going to do is we're going to produce a new basis. Every basis of a given space has the same number of elements. I'm going to produce it with the following properties first. It's ": [
            1345.3,
            1372.3,
            46
        ],
        "basis vectors. Let me call them V1 and V2. How do we know that the V1 V2? Is a baker's cyst? for this place The Matrix today, okay, so I found a basis is it an orthogonal basis? Let's see. What's the dot product of those two vectors computed? B1. It with v 2 is equal to -1 * -2 + -1 * 1 + 1 * 0 + 0 ": [
            844.1,
            885.0,
            29
        ],
        "basis. So how do we find a basis for the column space today? Row reduction, right we need to do row reduction to see which of these columns will will choose as the basis vectors. So I'm going to cheat a little bit until you okay. I already did that. The third column is the sum of the first two columns. So that column is certainly not a pivotal column ": [
            1928.1,
            1953.3,
            66
        ],
        "basis. So let's do an example. So here is a matrix and I would like to find an orthogonal basis or an orthodontist basis. In fact, I'm asked for here for the column space of this Matrix. Okay. So the first thing we need to do is produce a basis and then we'll use the gram Schmidt orthogonalization process to turn that basis into a new or so a normal ": [
            1905.7,
            1928.1,
            65
        ],
        "be in the Subspace. But there is an orthogonal projection straight down to a vector in the Subspace and there is a simple formula for it as we computed. You just take the duck parts of X with each of the basis vectors and take that Skiller X the basis vector and then add those up. In other words that is just the sum of the one-dimensional projections you project ": [
            180.1,
            203.7,
            6
        ],
        "bunch of people saying to is that correct? Can you tell by the fact that I'm stroking my beard that it's not correct? The column space of this Matrix is a Subspace of R2. Does the columns are in our to the null-space which is if you want to think about it look like we did last week the orthogonal complement of the row space. Is going to have the ": [
            668.8,
            692.7,
            23
        ],
        "column space. Does that make you to find those in the grammar Smith process? They were three? 2 root 2 + 3 / root 2 those worthy the length squares of the vectors and then to express 2 as a as a linear combination of V1 and V2 traditionally had this coefficient here 9 quarters and to express you three as an accommodation of V1 and V3 you had two ": [
            2822.2,
            2861.0,
            96
        ],
        "do is so we start with the given basis so you have to start with a basis. I'm going to produce a new bassist you want you to? That is orthogonal. What is Microsoft OneNote for now, we can normalize later. So, how do we do this? Well, there's a million ways to do this. But the procedure I'm going to show you the first step is to say, you ": [
            939.5,
            967.3,
            32
        ],
        "doing is an RN everything is concrete column vectors. If you have a Subspace and you have an orthonormal basis for okay, so a basis for that Subspace where all the vectors in the bases are orthogonal to each other and of unit length. Then you can use them to calculate the orthogonal projection into that Subspace that is to take any event or action or end. It may not ": [
            156.3,
            180.1,
            5
        ],
        "dotted with you on / length of you on Square x u 1 - V3. It with you two / length of you two squared times you two, so that's the case if we have three factors. So don't give me an orthogonal basis and then from there. I produce an orthodontist basis. You want hot you two hats and you Three Hats just by normalizing them you 1/2 Flex? ": [
            1777.5,
            1810.8,
            61
        ],
        "dreary Monday everyone Yeah, but it's going to be fun in here, right? Did someone say no, but thank you for your honesty. I'm going to have fun last week. This is the last week of classes. We are going to be moving on to section 6 point for today. And then finally section 7.1 on Wednesday, that will be the end of the new material Friday will be review ": [
            23.4,
            53.0,
            0
        ],
        "e3 and we subtract from it the projections of that Vector V3 onto each of the vectors you wanting you to that we already produced and we keep going and this procedure is guaranteed to produce an orthogonal set of vectors, but more importantly in each stage. We got exactly what I wrote here. We get this. Okay, that first one you want is in the span of V1 in ": [
            1653.7,
            1684.8,
            57
        ],
        "easier to computer. It's a V2 daughter with you one hat time. Do you want hat if we've already normalized it, but we still have to take that thing and normalize it just a little cumbersome to write down here. And so on like that, so if you want you could normalize as you go. Or you can just do the procedure that's written at the top and get an ": [
            1854.2,
            1884.3,
            63
        ],
        "example, we just have to compute that YouTube is equal to okay - 2101 Minus V2. U18. What's a V2. You one will you wanted to be ones with the. Parts of those two vectors, which we computed is equal to 1. Can we come shoot at that? Right here. / the length of you once we're discussing how we compute that links where that Victor is -1 - 110 ": [
            1228.2,
            1263.4,
            42
        ],
        "fact, how do you even know that a given Subspace has an orthodontist basis you needed in order to do this orthogonal projection? So for example here is a Subspace one of the kind that we've studied a million times. Now this no space just sanity check here. What size vectors are in this no space. This is a Subspace of what RNR and for what end? I hear a ": [
            632.8,
            668.8,
            22
        ],
        "first equation says x 1 - x 3 - 2 x 4 equals 0 in the second equation says that text 2 - x 3 + x 4 equals 0 and as usual I write this as x 3 x a vector which in this case is -1 - 110 plus x 4 times a vector which is -2 1 0 1 Okay, and that's my basis. There are my ": [
            814.6,
            844.1,
            28
        ],
        "force Vector it gets quite tedious, but it's tedious for you. It's not tedious for a computer. Computationally very easy for a computer. So that's the gram Schmidt process now in the last 12 minutes here. I'd like to tell you beyond the final piece of the puzzle for how this tell us how to computer Thug. No projections how to find orthonormal basis in general. I want to tell ": [
            2347.9,
            2374.2,
            80
        ],
        "going to be an orthogonal basis of nonzero vectors therefore and second. We're going to arrange it so that will either rate as we go to start the same way. As before you want is V1 and then you to the second orthogonal basis Vector is going to be a linear combination of V1 and V2. And then you three it's going to be a linear combination of V1 V2 ": [
            1372.3,
            1397.9,
            47
        ],
        "have an orthogonal basis. Okay, you wanted you to so that is the one that Regional director -1 - 110 and this new Vector here. And if you want you can go ahead and verify that those two are orthogonal we designed them to be orthogonal. Orthogonal basis for a two-dimensional Subspace. What if we have more vectors? What do we do if we have more than two extra? Well, ": [
            1291.5,
            1318.0,
            44
        ],
        "homework you going to be doing if you haven't already a bunch of examples of this and you just need to get used to how to put those coefficients into the upper triangular matrix. It's just what this one tells you is exactly riding. The things on the left as linear combinations of the things on the right wear for the first column on the left. You only need to ": [
            2898.6,
            2918.1,
            98
        ],
        "in the span of V1 and V2, which is equal to the Subspace to be so there we go. After that is in the Subspace V. It's been designed to be orthogonal to you want and therefore you want and you too will be an orthogonal set of two vectors in a two dimensional space and therefore will be a basis. That's how we do this. So in this specific ": [
            1202.5,
            1228.2,
            41
        ],
        "in this Matrix. The first two are visually linearly independent and the third one is not hard to check is also linearly independent. If we did row reduction, we would see that calms one two and four are the pivotal columns. So that's the basis that I'm going to start with. That's B1 B2. I'll call it even though it's the fourth Factor. So we started with a basis and ": [
            1953.3,
            1982.4,
            67
        ],
        "into the orthogonal complement of the one. Okay, and as we saw the formula for the ad is V2 - V2. You want / the length of you 1 squared as you want no notice that I'm using you one there. Okay, which is the same thing as if you want but it's important we use the you there specially when we get to the next V3 if we could ": [
            1445.2,
            1478.8,
            50
        ],
        "is it has to be linearly independent from the first well, in fact, we want to be orthogonal to the first so it'll be linearly independent but it also needs two together with the first one form a basis. Will it will do that as long as it's here and in the Subspace to be because if it's supposed to be a nonzero independent vectors for this two-dimensional Subspace, so ": [
            992.9,
            1016.7,
            34
        ],
        "know, what that first detector and I'll decide which one is first but slips to take the one that's labeled first. I'm just going to keep it. To the first Vector in the basis is -1 - 110. I'll keep that one. I know what I need to do. In the second step is to choose a second Vector here that is going to have two properties. One of them ": [
            967.3,
            992.9,
            33
        ],
        "lecture. Your final exam is on Saturday this coming Saturday at 11:30 a.m. In three rooms many of what you've already had midterms in all what you already had been turned in that's posted on the course web page. I wrote her that your seating assignment is already on Triton said it hasn't actually been made visible T. I thought it would be by the end of the day. Okay, ": [
            53.0,
            73.7,
            1
        ],
        "let me solve for V one there that's easy enough is equal to the length of you one time is it you want to have okay. Now what about you two hats in the procedure that was equal to one over the length of the vector that we find you two times that Vector you to which we produced as a V2 - your projection of V2 on. 4 V ": [
            2442.5,
            2475.0,
            84
        ],
        "much. I'm glad that I don't pay you guys for finding my errors because I would be broken now, but thank you for finding my errors for free. Soviet 30090 to 2 10009 dotted with 0220 + 0 + 18 that's why the other one is V3 daughter with you one. Which is 009 dotted with you on the same as V 1 1-2 to so I just get again ": [
            2194.5,
            2232.9,
            75
        ],
        "now we follow gram Schmidt. So you want is just going to equal? Which is 1 - 2 2. Well, we're going to need it. Anyway, so I'll go ahead and compute the length of that. Now the length of that squared is 1 squared + 2 squared + 2 squared which is 9 so the length of you one is three. So if I want I could normalize it ": [
            1982.4,
            2006.9,
            68
        ],
        "now, but normalize Vector you two had I divide that vector by two or two and I get one one of our route to 1 / root 2. In these kind of problems where you're normalizing you're often going to see things like that way you have radicals in the denominator really hard to avoid. Okay, and then finally three-dimensional space so we only have to do one more Vector ": [
            2125.0,
            2145.4,
            73
        ],
        "now. We've known each other for a whole quarter that no, I'm trying to pull you. So what I do wrong. what I did wrong was to not normalize the vector since it's not an orthodontist visit so we can either normalize those vectors and then use the normal vectors, but that's the same thing as dividing through each of those terms by the length of you once we're north ": [
            410.9,
            435.0,
            14
        ],
        "of that to normalize. So the length of you three squared is the sum of the squares of those numbers, which if you work it out comes out tonight, and so therefore the normalized u3, is that Vector user? I just computed divided by the square root of 9/2. So the length of you three is equal to 3 / root 2, so I have to multiply buy root 2/3 ": [
            2288.7,
            2318.5,
            78
        ],
        "of you to squirt. Okay. Okay, so let's just quickly run through those calculations. We need to calculate four numbers. I need to calculate the length of you 1 squared. Okay. Well you want is the Spectre to 5-1 so that's 2 squared + 5 squared + 1 squared which is 30. And the length of U2 squared is 2 squared + 1 squared + 1 squared which is fixed. ": [
            435.0,
            468.3,
            15
        ],
        "onto each basis vector and add up those one-dimensional projections. That's the orthogonal projection for that formula be true. It's important. He's been normalized vectors if they're not normalized. If you just have an orthogonal basis, then you additionally have to divide each of those terms by the length of the vector of you 1 squared because there's a length of you one inside The Delfonics on the lights of ": [
            203.7,
            226.5,
            7
        ],
        "onto you one daughter with you 1 / the length of you 1 squared times you want. So that's equal to. 104 minus the dot product of V2 and u1f to compute that in order to do this. Do I need a box over here? V2. Adieu one Right. Well that's equal to. 1 * 1 + 0 * -2 + 4 * 2 + 1 + 8 which is ": [
            2033.6,
            2070.5,
            70
        ],
        "original Subspace. Okay. Last flight the sum of these two things the sum of this thing and this thing is V2 by definition. But that second thing there is a one-dimensional projected we know how to compute those. So this is just V2 - V2 dotted with you 1 / you 1 squared times you want okay, that gives us a linear combination of V1 and V2. This thing is ": [
            1163.9,
            1202.5,
            40
        ],
        "orthogonal basis and then normalize at the end those two are equivalent in the level of difficulty and level of computational complexity. I always do the former. I'll always forget the lengths does I go just compute an orthogonal basis and then normal exit at the end. So that's what I recommend you do but it's actually flying to do either way. So that is how we find an orthogonal ": [
            1884.3,
            1905.7,
            64
        ],
        "orthogonal projections. We know how to compute already. We not a complete one dimension orthogonal projections. And moreover remember the definition of orthogonal projection the definition of orthogonal projection. Which is written right here. Is that if you want to project into Z you finding a vector y so that y - the original Vector is perpendicular to V. Purp you need to find a vector y so that y ": [
            1102.8,
            1135.1,
            38
        ],
        "pee Well, we start with VP. We subtract from it the top part of the VP with you 1 / you one light squared times you want. the top part of BP with you two divided by the length of you two squared times you too and all the way down the line. the price of BP with u p - 1 divided by the length of u p -1 ": [
            1602.3,
            1631.4,
            55
        ],
        "perpendicular to view on which I'm not calling you one? What is the set of vectors perpendicular to a V1 called? Just remind you there's a symbol that we use for I said upside down T. It's V1 Purp. All I need to do is take The orthogonal projection of V2 into V1 purple you want now, that's how I do this. Okay now hold on a sec. Hold on ": [
            1044.6,
            1079.4,
            36
        ],
        "previous slide hear. What we're doing is projecting in the projection onto you one. So that's what I wrote. Their. What we want to do is take the orthogonal projection into the ortho complement of V1 and you to text me right as you want and you too. V3 that's what we want. We want the new Vector to be orthogonal to both of the previous one so that all ": [
            1502.9,
            1542.7,
            52
        ],
        "produce a North enormo bassist. But again, we can do that quite easily once we have an orthogonal basis. So I'll just write it one more time, but if we're going to To do this. Let me write it as You want is V1? U2 is V2 - the projection so that is V2 dotted you 1/2 length of you 1 squared times you want. U3 is V3. Minus V3 ": [
            1742.9,
            1777.5,
            60
        ],
        "properties for these bases Beyond just being orthogonal. So let me reiterate what we did on the last slide. So we do the same thing here. We start this procedure just by selecting the first basis Vector V1. the second one we would like to just take V2. But beaches probably not orthogonal to view want so we have to do is make it or sogamoso V1 by projecting it ": [
            1419.8,
            1445.2,
            49
        ],
        "remember matrix multiplication nearly never commutes. This is going to be a new Matrix, but we'll see next time. It has the same eigenvalues as a and then you can repeat keep doing QR factorization and reversing and we'll discuss next day. Why doing that will get you quickly close to Matrix is eigenvalues are easy to compute. See you on Wednesday. ": [
            3024.2,
            3046.3,
            103
        ],
        "same number of entries as thin as the elements in the Rose as the rose. It's at some place of our for space consists of those extra which one you multiply 8 times actually get 0 to x 8 x x x has to have number of components equal to the number of columns. So this is a Subspace of R4. How do we find out? How do we find ": [
            692.7,
            715.7,
            24
        ],
        "so it's like squared is 1 squared + 1 squared + 1 squared which is 3 * the vector U1 which is the vector v ones that's -1 - 110 Okay, and what the heck that's actually compute that thing. So it's - 2 + 1/3 - 5/3 and then 1 + 1/3 which is 4/3 0-3, which is -1 third and the last component just 1-0. And now we ": [
            1263.4,
            1291.5,
            43
        ],
        "so same drill as though with the midterms. You have one more MyMathLab assignment. It's due this Thursday at 11:59 p.m. That you should be working on it now. After today's lecture you'll be able to do everything on it except for the last few questions already. And your Matlab quiz is scheduled for tomorrow Tuesday of this week in the usual time. When you have your section on Thursday ": [
            73.7,
            101.3,
            2
        ],
        "so we put all those things together and we just get that this Vector why this orthogonal projection is? 9/30 time is the vector u12 5 - 1 + 3 divided by 6. Time's the vector - 211 couple comments first If you're doing it this way where you haven't normalize the vectors, it's important that you use the vector you wanted you to they're not the normalized once I've ": [
            505.5,
            538.4,
            17
        ],
        "squared How do you pee? Okay, so we do this interactively we produce are you one just like before we just choose the first Spectre in Arles you to we get it by taking the second Vector in the original list and subtracting from it the projection of vector onto the first one that we already produced in the next step. We take the third one in the list of ": [
            1631.4,
            1653.7,
            56
        ],
        "take what's going on there anything that I want you to know, the only thing that's important. I'll tell you what those numbers are. I will write them in a moment thing. I want you to know is that just like when we did this procedure forward we got the us3 was in the span of V1 V2 and V3 and it didn't involve any of the further V's similarly ": [
            2610.3,
            2632.7,
            89
        ],
        "terms of device, but in the Gram stain procedure, I want to express the V's in terms of the use. So let's go back. Let's ride again one more time. So you one was equal to its you want normalized. So it is V1. Remember V1 was you want / the length of you one, which is the same as the length of V12? Let's leave it like that. So ": [
            2417.9,
            2442.5,
            83
        ],
        "the Matrix with those vectors V1 through VP as the columns in the basis you produce that new bases you want have you to have you. Which is an orthodontic basis? What this procedure with a garnishment procedure actually says is that in doing that you're writing the original Matrix X this Matrix here. with all zeros What you're doing is your writing the Vees? In terms of the use ": [
            2657.1,
            2706.3,
            91
        ],
        "the VIS? Well, I can't do that in a vacuum. It's like saying here's the reduced row Echelon form of a matrix find the original Matrix. You can't cuz lots of original matrices that Rob reduced to that one registration on form. But what I mean is if we backtrack through the calculations, we just made I want to express the V's in terms of the use the calculations using ": [
            2397.1,
            2417.9,
            82
        ],
        "the line between those two tip is longer than the one that straight down. Symmetrically what projection really means the most important reason for using your ex if you're constrained to be in the Subspace. Is that Vector why the projection so for example here I'm giving you an orthogonal basis for a Subspace. Okay. So I've the Subspace is the span of those two vectors. He wanted you to ": [
            308.7,
            341.5,
            11
        ],
        "the original basis Vector, but that's probably not going to work V3 is probably not orthogonal to V1 or V2. It's probably not much Tylenol 2 V1 or you to either but what we'll do is we'll make it orthogonal to them. By taking and subtracting from it. I'm sorry, but I but I broke it was not quite right. It's the -4 and 2 V 1 back to the ": [
            1478.8,
            1502.9,
            51
        ],
        "the span of V1 V2 and V3. Yes question. Because I made a mistake. Thank you. Yeah, it wouldn't make sense to have it X you because you haven't defined you peed yet. So that wouldn't be interested. That would be recursive. Thanks. Chris Wright, each stage of this procedure we use the stuff. We already did in the last step to produce. The next step should be no surprise ": [
            1684.8,
            1714.2,
            58
        ],
        "the vector -2 - 1/2 1/2 Okay, and I won't simplified any more than that. So there's the vector in question. So those three factors that I just computed. You want hot? you two hats you three had those three vectors form an orthodontic normal basis for the column space of a that's how you do it. That's how this procedure works. It's a little too if they were a ": [
            2318.5,
            2347.9,
            79
        ],
        "then after that we got was the 3-2 orthogonal projection of V 3 onto you one hat. Minus the orthogonal projection of V 3 onto you too hot. And now we can solve that for V3 just by multiplying through by the length of you too and adding to the other side we get that V 3 is equal to the length of you three times to use three hat ": [
            2504.0,
            2531.8,
            86
        ],
        "there's not enough projection. It's the vector in the Subspace V that is closest to the vector ex that you started with. Okay, so if I have a vector in space represented by my pencil over here Okay, then the vector in the table cuz tip is closest to the tip of this one. That's the one that you get by projecting it straight down any other point over here ": [
            283.3,
            308.7,
            10
        ],
        "three of those pictures are now orthogonal but like before we can write that as we take and we subtract from it the projection onto you one. And subtract from that the projection onto you, too. Hey, remember the orthogonal projection onto an orthonormal basis for an orthogonal basis is just gotten by projecting onto each basis Vector separately and adding up. I already have two orthogonal vectors you wanted ": [
            1542.7,
            1574.4,
            53
        ],
        "to find a basis, I would just the way I always do it is to go right from the definitions. They okay. This is the set of all x 1 x 2 x 3 x 4 where I use the reduced equations that the registration form gives me. I note that these two columns are free variables. So I'm going to put X3 and X4. the three variables in the ": [
            789.3,
            814.6,
            27
        ],
        "to you when we are doing no reduction The Next Step depends on the previous step, but we did okay. It's an interactive procedure. It's very fast for a computer to implement its again. It's like doing the Reversed phase of a reduction in order and the squared number of flops if you have any effects to order P-Square here. That's the gram Schmidt orthogonalization procedure. Now, we wanted to ": [
            1714.2,
            1742.9,
            59
        ],
        "to your I am going exactly back on my word telling you that I was normal as at the end of here on normalizing now, that's fine. So there's our first personal Vector then if we're going to find a second Vector you to Sorry that my tutu sometimes look like threes. Okay, so that's we take the vector v to in our lives and we subtract from it projection ": [
            2006.9,
            2033.6,
            69
        ],
        "two vectors and try to guess it will be orthogonal. We don't have to guess though. There's going to be a procedure. I'm going to show you right now or we can do exactly that. We're going to take linear combinations of these given basis vectors in order to produce a new basis of the Subspace that is orthogonal and the procedure works like this. So what we're going to ": [
            915.6,
            939.5,
            31
        ],
        "use the first column on the on the right for the Second Use the first two columns on the right the third one involves all three Okay. Now why would we care about that? Well in the last one minute I will just say what is it good for it's good for finding eigenvalues when Matlab compute eigenvalues when you put in a matrix a square Matrix in Matlab and ": [
            2918.1,
            2949.8,
            99
        ],
        "want to normalize it. I just calculated slang squared is 0 squared + 2 squared + 2 squared + 4 So therefore the length. Is the square root of 4, which is two? That's right. Squared to squared plus b squared is not for its equal to 2 * 4 or 8, which means that the length is to Route 2. And so I wipe I want I could compute ": [
            2098.6,
            2125.0,
            72
        ],
        "we can just enter 8 that procedure. So here is the process that is either a Ting exactly what we just did and it has a name is called the gram Schmidt orthogonalization procedure which takes longer to write than actually doing the procedure. Okay. So if you like so here's how it works. You haven't already found a basis for finding follow us. We know how to find a ": [
            1318.0,
            1345.3,
            45
        ],
        "we just proved which is it if you take an M by n Matrix, which is full rank tall and skinny if it's not Square because it has to have all the orphan with us have all linearly independent column. So you take a matrix with linearly independent columns. Gram Schmidt on that Matrix what grandma Schmidt actually does for you is it satirizes the Matrix in the form Q ": [
            2737.2,
            2762.5,
            93
        ],
        "when you do that you are factorising the Matrix you start with a matrix whose columns form a basis for your Subspace. Doing the gram Schmidt process would it actually does his factorize that Matrix as a matrix here whose columns are orthonormal time is another Matrix and Matrix over there will be a portray angular. That's what gram Schmidt actually does for you. So here is the theorem which ": [
            2706.3,
            2737.2,
            92
        ],
        "x r amusing to hear instead of you up until we use the letter and your textbook uses the letter q and so do Minnesota's Matlab. So I'm going to call you but that's the same Matrix you we've been talking about. It's the Matrix was columns are the orthodontist factors produced and what it works out to is it's just all the coefficients that you computed during the gram ": [
            2762.5,
            2787.0,
            94
        ],
        "you ask it to compute the eigenvalues send it immediately spits out a list if you put in a 4000 by 4,000 Matrix and ask it to spin out a list of the eigenvalues it will do it in two seconds on a not very fast computer now 4000 4000 Matrix the characteristic polynomial of that is a 4000 degree polynomial factoring such a polynomial even approximately is very difficult. ": [
            2949.8,
            2971.1,
            100
        ],
        "you do that? We already computed above which is 0 to 2. Again at this point If This Were A Course in arithmetic, I would want you to go through all the calculations to get there. Let me just give you the answer. If you want to run the calculation yourself later that all adds up to -2 - 1/2. And then finally, we would need to compute the length ": [
            2263.7,
            2288.7,
            77
        ],
        "you if you see a problem with that. Okay, so that's basically the story on orthogonal projection except. Part of the story in the introduction of the story at the top. I said given an orthogonal basis of an orthonormal basis you can do all this. So if I give you an Arsenal basis, you know what to do. But what if I don't give you an orthonormal basis, in ": [
            608.8,
            632.8,
            21
        ],
        "you it does something better as well. So it doesn't mean it's also important for a different reason. I just see why I want to answer this question, right? So we started with a basis V once review p How do we produce a new orthodontic basis you one's for you? Pee can I go backwards? So what if I had the use and I want to get back to ": [
            2374.2,
            2397.1,
            81
        ],
        "you one for the vector outside the door parts as well. And we saw last day that you can also consider that as a linear transformation. It has a matrix on that Matrix is you you transpose where you is the Matrix probably a tall skinny. Matrix is usually more rows and columns columns are the orthodontist. I suspected he wants for you p Okay. Now this is orthogonal projection. ": [
            226.5,
            252.0,
            8
        ],
        "you three which is V 3 - the projection of V 3 onto you one. Minus the projection of V 3 onto you, too. sohvi 3009 can't even calculate two more. Products here. V3 daughter with you, too. Is equal to okay. Well, these are easy cuz it's mostly zeros in but it's important that we use you too and not V2 here. Yes. It should indeed. Thank you very ": [
            2145.4,
            2194.5,
            74
        ],
        "you to I can compute that projections like this. Okay, and to belabor that point I'll write it as it's V3 - V3 dotted with you one over the length of you 1 squared x u 1 - V3. It with you two over at you to light squared times. You too. And then we keep going just like that. So when we get down to the last one you ": [
            1574.4,
            1602.3,
            54
        ]
    },
    "File Name": "Linear_Algebra___B00___Kemp__Todd_Aahron___Winter_2018-lecture_26.flac",
    "Full Transcript": "dreary Monday everyone  Yeah, but it's going to be fun in here, right?  Did someone say no, but thank you for your honesty. I'm going to have fun last week. This is the last week of classes.  We are going to be moving on to section 6 point for today.  And then finally section 7.1 on Wednesday, that will be the end of the new material Friday will be review lecture. Your final exam is on Saturday this coming Saturday at 11:30 a.m. In three rooms many of what you've already had midterms in all what you already had been turned in that's posted on the course web page. I wrote her that your seating assignment is already on Triton said it hasn't actually been made visible T. I thought it would be by the end of the day. Okay, so same drill as though with the midterms.  You have one more MyMathLab assignment. It's due this Thursday at 11:59 p.m. That you should be working on it now.  After today's lecture you'll be able to do everything on it except for the last few questions already.  And your Matlab quiz is scheduled for tomorrow Tuesday of this week in the usual time. When you have your section on Thursday a few people have posted on Piazza that you signed up for the makeup exam or that the conflict quiz time, which is today there were several times a day, but then now you don't have anymore and you can do it on your regular time. If that happens just email your ta your tea for your section is proctoring. You're not labquest. Okay, so just communicate with your ta  All right, and that's basically all the administer B. I wanted to go through so I'm going to get started now with today's course material. Let's begin by reviewing what we talked about last time. So we're now deep into orthogonal T or C normality orthonormal basis and last time we talked about orthogonal projection.  So if we have a Subspace V of RN and everything in the section we're doing is an RN everything is concrete column vectors.  If you have a Subspace and you have an orthonormal basis for okay, so a basis for that Subspace where all the vectors in the bases are orthogonal to each other and of unit length.  Then you can use them to calculate the orthogonal projection into that Subspace that is to take any event or action or end. It may not be in the Subspace. But there is an orthogonal projection straight down to a vector in the Subspace and there is a simple formula for it as we computed.  You just take the duck parts of X with each of the basis vectors and take that Skiller X the basis vector and then add those up.  In other words that is just the sum of the one-dimensional projections you project onto each basis vector and add up those one-dimensional projections. That's the orthogonal projection for that formula be true. It's important. He's been normalized vectors if they're not normalized. If you just have an orthogonal basis, then you additionally have to divide each of those terms by the length of the vector of you 1 squared because there's a length of you one inside The Delfonics on the lights of you one for the vector outside the door parts as well.  And we saw last day that you can also consider that as a linear transformation. It has a matrix on that Matrix is you you transpose where you is the Matrix probably a tall skinny. Matrix is usually more rows and columns columns are the orthodontist. I suspected he wants for you p  Okay. Now this is orthogonal projection. As I said, this is the straight down projection. So you take the vector ex and this why this projection why which work on projects of the it is the unique Vector in the Subspace V4, which that straight down wine y - x is perpendicular to be that's the definition.  And geometric see what that means because of Pythagoras Theorem geometrically what that means is that this Vector why there's not enough projection. It's the vector in the Subspace V that is closest to the vector ex that you started with. Okay, so if I have a vector in space represented by my pencil over here  Okay, then the vector in the table cuz tip is closest to the tip of this one. That's the one that you get by projecting it straight down any other point over here the line between those two tip is longer than the one that straight down.  Symmetrically what projection really means the most important reason for using your ex if you're constrained to be in the Subspace. Is that Vector why the projection so for example here I'm giving you an orthogonal basis for a Subspace. Okay. So I've the Subspace is the span of those two vectors. He wanted you to  and I'm telling you this is an orthogonal basis and that's actually easy to verify 2 * -2 - 4 + 5 - 1 is indeed 0 compute projection.  I would like to find the point. Why in this Subspace me that is closest to that back tracks. In other words. I just want to find the closest point y y is equal to the orthogonal projection.  I've asked interview.  Now what I have to do?  I can just use the formulas up there right? I take the vector X.  Stop product it with Y X a but with what you want and multiply that Skillet come to you want and take the vector ex and duct products with you to I'll fly by U2.  Is that right?  It's wrong because you can tell by my tone of voice by now. We've known each other for a whole quarter that no, I'm trying to pull you. So what I do wrong.  what I did wrong was to not normalize the vector since it's not an orthodontist visit so we can either  normalize those vectors and then use the normal vectors, but that's the same thing as dividing through each of those terms by the length of you once we're north of you to squirt.  Okay.  Okay, so let's just quickly run through those calculations. We need to calculate four numbers. I need to calculate the length of you 1 squared.  Okay. Well you want is the Spectre to 5-1 so that's 2 squared + 5 squared + 1 squared which is 30.  And the length of U2 squared is 2 squared + 1 squared + 1 squared which is fixed.  Another U2 remaining numbers. We need are the. Box soex dotted with you one.  well X is Vector 1 2 3 so I'll just write this then so that's 1 * 2 + 2 * 5 + 3 * -1 + 2 + 10 - 3 which is 9  and ex started with you, too.  is 1 * -2  + 2 * 1 + 3 * 1 which is three so we put all those things together and we just get that this Vector why this orthogonal projection is?  9/30 time is the vector u12 5 - 1 + 3 divided by 6.  Time's the vector - 211  couple comments first  If you're doing it this way where you haven't normalize the vectors, it's important that you use the vector you wanted you to they're not the normalized once I've already taken account of the length of you won in both places. So you're going to use that after you want to hear you wouldn't additionally divide that doctor by its length.  And then the other comments here is I'm not going to take this calculation any further. We could single Vector I'm not going to do that because this is not a course in arithmetic.  In fact, if you are on the final exam in this was the question you were asked if you want to go ahead and leave it in that form. That's fine. Did not only is it fine, but it's probably preferable because the greater will be able to recognize that better than the actual answer especially if you made a numerical answer the error in your answer if you made an America Lehrer and all you end up writing down as the finals record your grade is not going to have any idea where you went wrong. Whereas if you right back and that down there and you know, what if there was if I got eight instead of 6 by accident for the length of your greater will know where you went wrong and be able to.  And so that's my recommendation to you if you see a problem with that.  Okay, so that's basically the story on orthogonal projection except.  Part of the story in the introduction of the story at the top.  I said given an orthogonal basis of an orthonormal basis you can do all this.  So if I give you an Arsenal basis, you know what to do.  But what if I don't give you an orthonormal basis, in fact, how do you even know that a given Subspace has an orthodontist basis you needed in order to do this orthogonal projection?  So for example here is a Subspace one of the kind that we've studied a million times. Now this no space just sanity check here. What size vectors are in this no space.  This is a Subspace of what RNR and for what end?  I hear a bunch of people saying to  is that correct?  Can you tell by the fact that I'm stroking my beard that it's not correct?  The column space of this Matrix is a Subspace of R2. Does the columns are in our to the null-space which is if you want to think about it look like we did last week the orthogonal complement of the row space.  Is going to have the same number of entries as thin as the elements in the Rose as the rose.  It's at some place of our for space consists of those extra which one you multiply 8 times actually get 0 to x 8 x x x has to have number of components equal to the number of columns.  So this is a Subspace of R4. How do we find out? How do we find a basis for the Subspace?  Oh, wow, come on, guys. It's been 9 weeks already. We've been training. Let's do it again. How do we find the basis for the Subspace?  Write better know that by now for the final exam. So we're going to do row reduction on that Matrix. So first I'll subtract the first row from the second.  I got 0 - 2 - 2 - 2.  And then I'm going to Pivot the last row.  And now I'm going to subtract the second row from the first.  And I'm going to be in reduced row Echelon form when I do that. So I'm subtracting not that I make a mistake on that last one.  Yes could thank you, which means that this is -1 there now and I'm interested in reduced row Echelon form.  And so to find a basis, I would just the way I always do it is to go right from the definitions. They okay. This is the set of all x 1 x 2 x 3 x 4  where I use the reduced equations that the registration form gives me.  I note that these two columns are free variables.  So I'm going to put X3 and X4.  the three variables in the first equation says x 1 - x 3 - 2 x 4 equals 0 in the second equation says that text 2 - x 3 + x 4 equals 0 and as usual I write this as x 3 x a vector which in this case is -1 - 110  plus x 4 times a vector which is -2 1 0 1  Okay, and that's my basis. There are my basis vectors. Let me call them V1 and V2.  How do we know that the V1 V2?  Is a baker's cyst?  for this place  The Matrix today, okay, so I found a basis is it an orthogonal basis? Let's see. What's the dot product of those two vectors computed?  B1. It with v 2 is equal to -1 * -2 + -1 * 1 + 1 * 0  + 0 * 1  which is 2 - 1 which is what?  not zero  So this is not an orthogonal basis. Oh well.  Now what?  So I have no other procedure for finding a basis, right? I mean I could mess around with this thing is I know that if I take any two linearly independent vectors in they will form a basis so I can start taking linear combinations of those two vectors and try to guess it will be orthogonal.  We don't have to guess though. There's going to be a procedure. I'm going to show you right now or we can do exactly that. We're going to take linear combinations of these given basis vectors in order to produce a new basis of the Subspace that is orthogonal and the procedure works like this. So what we're going to do is so we start  with the given basis so you have to start with a basis.  I'm going to produce a new bassist you want you to?  That is orthogonal.  What is Microsoft OneNote for now, we can normalize later.  So, how do we do this? Well, there's a million ways to do this. But the procedure I'm going to show you the first step is to say, you know, what that first detector and I'll decide which one is first but slips to take the one that's labeled first.  I'm just going to keep it.  To the first Vector in the basis is -1 - 110. I'll keep that one.  I know what I need to do.  In the second step is to choose a second Vector here that is going to have two properties. One of them is it has to be linearly independent from the first well, in fact, we want to be orthogonal to the first so it'll be linearly independent but it also needs two together with the first one form a basis. Will it will do that as long as it's here and in the Subspace to be because if it's supposed to be a nonzero independent vectors for this two-dimensional Subspace, so all I need to do to be one  And then I'll have no basis now. I could set that up as a linear equation now, but actually here's exactly how we're going to do it.  This is the end result and it uses orthogonal projection in socks. Let's think about this what I want. I want to take a vector.  in the Subspace spanned by V1 and V2  What is perpendicular to view on which I'm not calling you one? What is the set of vectors perpendicular to a V1 called?  Just remind you there's a symbol that we use for I said upside down T. It's V1 Purp.  All I need to do is take  The orthogonal projection of V2 into V1 purple you want now, that's how I do this.  Okay now hold on a sec. Hold on a sec can hold on you're talking in circles, man, because the whole point of what we're trying to do here.  It's figure out how to find an orthogonal basis so that we can compute orthogonal projection. But now I'm telling you in order to find an orthogonal basis. We're going to use an orthogonal projection. How am I supposed to do that? Well valid point but there are some orthogonal projections. We know how to compute already. We not a complete one dimension orthogonal projections.  And moreover remember the definition of orthogonal projection the definition of orthogonal projection.  Which is written right here.  Is that if you want to project into Z you finding a vector y so that y - the original Vector is perpendicular to V.  Purp you need to find a vector y so that y - x is in the original Subspace. In other words are projections all about breaking down a vector as a sum of two things one in the Subspace in one perpendicular the Subspace that means forever orthogonally projecting into the OR thought into the orthogonal complement of a vector by definition we can just get this  By taking the vector V2 and subtract attracting from it the projection into the original Subspace.  Okay. Last flight the sum of these two things the sum of this thing and this thing is V2 by definition.  But that second thing there is a one-dimensional projected we know how to compute those. So this is just  V2 -  V2 dotted with you 1 / you 1 squared times you want  okay, that gives us  a linear combination of V1 and V2. This thing is in the span of V1 and V2, which is equal to the Subspace to be so there we go. After that is in the Subspace V. It's been designed to be orthogonal to you want and therefore you want and you too will be an orthogonal set of two vectors in a two dimensional space and therefore will be a basis.  That's how we do this. So in this specific example, we just have to compute that YouTube is equal to okay - 2101  Minus V2. U18. What's a V2. You one will you wanted to be ones with the. Parts of those two vectors, which we computed is equal to 1.  Can we come shoot at that?  Right here.  /  the length of you once we're discussing how we compute that links where that Victor is -1 - 110 so it's like squared is 1 squared + 1 squared + 1 squared which is 3  * the vector U1 which is the vector v ones that's -1 - 110  Okay, and what the heck that's actually compute that thing. So it's - 2 + 1/3 - 5/3 and then 1 + 1/3 which is 4/3 0-3, which is -1 third and the last component just 1-0.  And now we have an orthogonal basis. Okay, you wanted you to so that is the one that Regional director -1 - 110 and this new Vector here. And if you want you can go ahead and verify that those two are orthogonal we designed them to be orthogonal.  Orthogonal basis for a two-dimensional Subspace. What if we have more vectors? What do we do if we have more than two extra? Well, we can just enter 8 that procedure. So here is the process that is either a Ting exactly what we just did and it has a name is called the gram Schmidt orthogonalization procedure which takes longer to write than actually doing the procedure. Okay. So if you like so here's how it works. You haven't already found a basis for finding follow us. We know how to find a basis for column space of a matrix. We know how to find a basis you found some basis that basis is probably not an orthogonal basis.  Okay, but you got start with some basis. What we're going to do is we're going to produce a new basis. Every basis of a given space has the same number of elements.  I'm going to produce it with the following properties first. It's going to be an orthogonal basis of nonzero vectors therefore and second. We're going to arrange it so that will either rate as we go to start the same way. As before you want is V1 and then you to the second orthogonal basis Vector is going to be a linear combination of V1 and V2.  And then you three it's going to be a linear combination of V1 V2 and V3. And you for choosing to be a linear combination of V1 V2 V3 and V4. Okay, as we go through this procedure, we're going to find a new bassist where each new basis Vector is a linear combination of only the previous ones.  We don't have to reach further on in the bases and we'll see in a few minutes, but that's going to give us some nice properties for these bases Beyond just being orthogonal. So let me reiterate what we did on the last slide. So we do the same thing here. We start this procedure just by selecting the first basis Vector V1.  the second one  we would like to just take V2.  But beaches probably not orthogonal to view want so we have to do is make it or sogamoso V1 by projecting it into the orthogonal complement of the one.  Okay, and as we saw the formula for the ad is V2 - V2. You want / the length of you 1 squared as you want no notice that I'm using you one there. Okay, which is the same thing as if you want but it's important we use the you there specially when we get to the next V3 if we could the original basis Vector, but that's probably not going to work V3 is probably not orthogonal to V1 or V2. It's probably not much Tylenol 2 V1 or you to either but what we'll do is we'll make it orthogonal to them.  By taking and subtracting from it.  I'm sorry, but I but I broke it was not quite right. It's the -4 and 2 V 1 back to the previous slide hear. What we're doing is projecting in the projection onto you one. So that's what I wrote. Their. What we want to do is take the orthogonal projection into the ortho complement of V1 and you to text me right as you want and you too.  V3 that's what we want. We want the new Vector to be orthogonal to both of the previous one so that all three of those pictures are now orthogonal but like before we can write that as we take and we subtract from it the projection onto you one.  And subtract from that the projection onto you, too.  Hey, remember the orthogonal projection onto an orthonormal basis for an orthogonal basis is just gotten by projecting onto each basis Vector separately and adding up.  I already have two orthogonal vectors you wanted you to I can compute that projections like this.  Okay, and to belabor that point I'll write it as it's V3 - V3 dotted with you one over the length of you 1 squared x u 1 - V3. It with you two over at you to light squared times. You too.  And then we keep going just like that. So when we get down to the last one you pee  Well, we start with VP.  We subtract from it the top part of the VP with you 1 / you one light squared times you want.  the top part of BP with you two  divided by the length of you two squared times you too and all the way down the line.  the price of BP with u p - 1  divided by the length of u p -1 squared  How do you pee?  Okay, so we do this interactively we produce are you one just like before we just choose the first Spectre in Arles you to we get it by taking the second Vector in the original list and subtracting from it the projection of vector onto the first one that we already produced in the next step. We take the third one in the list of e3 and we subtract from it the projections of that Vector V3 onto each of the vectors you wanting you to that we already produced and we keep going and this procedure is guaranteed to produce an orthogonal set of vectors, but more importantly in each stage.  We got exactly what I wrote here. We get this. Okay, that first one you want is in the span of V1 in the span of V1 V2 and V3. Yes question.  Because I made a mistake. Thank you. Yeah, it wouldn't make sense to have it X you because you haven't defined you peed yet. So that wouldn't be interested. That would be recursive. Thanks.  Chris Wright, each stage of this procedure we use the stuff. We already did in the last step to produce. The next step should be no surprise to you when we are doing no reduction The Next Step depends on the previous step, but we did okay. It's an interactive procedure.  It's very fast for a computer to implement its again. It's like doing the Reversed phase of a reduction in order and the squared number of flops if you have any effects to order P-Square here.  That's the gram Schmidt orthogonalization procedure. Now, we wanted to produce a North enormo bassist. But again, we can do that quite easily once we have an orthogonal basis. So I'll just write it one more time, but if we're going to  To do this. Let me write it as  You want is V1?  U2 is V2 - the projection so that is V2 dotted you 1/2 length of you 1 squared times you want.  U3 is V3.  Minus V3 dotted with you on / length of you on Square x u 1 - V3. It with you two / length of you two squared times you two, so that's the case if we have three factors.  So don't give me an orthogonal basis and then from there.  I produce an orthodontist basis.  You want hot you two hats and you Three Hats just by normalizing them you 1/2 Flex?  YouTube / it's links.  Okay, so if we wanted to we could do the normalization as we go let me write down what that would look like. So then we would say well the first step is to take you 1/2 is equal to V1 divided by its length.  and then you two hat is equal to  V2 - Okay. Well if we've already normalized then the projection is slightly easier to computer. It's a V2 daughter with you one hat time. Do you want hat if we've already normalized it, but we still have to take that thing and normalize it just a little cumbersome to write down here.  And so on like that, so if you want you could normalize as you go.  Or you can just do the procedure that's written at the top and get an orthogonal basis and then normalize at the end those two are equivalent in the level of difficulty and level of computational complexity. I always do the former. I'll always forget the lengths does I go just compute an orthogonal basis and then normal exit at the end. So that's what I recommend you do but it's actually flying to do either way.  So that is how we find an orthogonal basis. So let's do an example. So here is a matrix and I would like to find an orthogonal basis or an orthodontist basis. In fact, I'm asked for here for the column space of this Matrix.  Okay. So the first thing we need to do is produce a basis and then we'll use the gram Schmidt orthogonalization process to turn that basis into a new or so a normal basis. So how do we find a basis for the column space today?  Row reduction, right we need to do row reduction to see which of these columns will will choose as the basis vectors. So I'm going to cheat a little bit until you okay. I already did that. The third column is the sum of the first two columns. So that column is certainly not a pivotal column in this Matrix. The first two are visually linearly independent and the third one is not hard to check is also linearly independent. If we did row reduction, we would see that calms one two and four are the pivotal columns. So that's the basis that I'm going to start with. That's B1 B2.  I'll call it even though it's the fourth Factor.  So we started with a basis and now we follow gram Schmidt. So you want is just going to equal?  Which is 1 - 2 2.  Well, we're going to need it. Anyway, so I'll go ahead and compute the length of that. Now the length of that squared is 1 squared + 2 squared + 2 squared which is 9 so the length of you one is three.  So if I want I could normalize it to your I am going exactly back on my word telling you that I was normal as at the end of here on normalizing now, that's fine.  So there's our first personal Vector then if we're going to find a second Vector you to  Sorry that my tutu sometimes look like threes. Okay, so that's we take the vector v to in our lives and we subtract from it projection onto you one daughter with you 1 / the length of you 1 squared times you want.  So that's equal to.  104  minus the dot product of V2 and u1f to compute that in order to do this.  Do I need a box over here?  V2. Adieu one  Right. Well that's equal to.  1 * 1 + 0 * -2 + 4 * 2 + 1 + 8 which is 9  So I got minus 1/9 or 30 - 9 / the length of you one square, which is also 9.  Time is the vector you on the on normalize Vector you want one- to two?  9 over 9 as one so this becomes easy. So this is 1 - 1 which is 0 0 - -2 which is 2 + 4 - 2 which is two and if I want to normalize it.  I just calculated slang squared is 0 squared + 2 squared + 2 squared + 4  So therefore the length.  Is the square root of 4, which is two?  That's right. Squared to squared plus b squared is not for its equal to 2 * 4 or 8, which means that the length is to Route 2.  And so I wipe I want I could compute now, but normalize Vector you two had I divide that vector by two or two and I get one one of our route to 1 / root 2.  In these kind of problems where you're normalizing you're often going to see things like that way you have radicals in the denominator really hard to avoid.  Okay, and then finally three-dimensional space so we only have to do one more Vector you three which is V 3 - the projection of V 3 onto you one.  Minus the projection of V 3 onto you, too.  sohvi 3009  can't even calculate two more. Products here.  V3 daughter with you, too.  Is equal to okay. Well, these are easy cuz it's mostly zeros in but it's important that we use you too and not V2 here. Yes.  It should indeed. Thank you very much.  I'm glad that I don't pay you guys for finding my errors because I would be broken now, but thank you for finding my errors for free.  Soviet 30090 to 2  10009 dotted with 0220 + 0 + 18 that's why the other one is V3 daughter with you one.  Which is 009 dotted with you on the same as V 1 1-2 to so I just get again 18. Okay, I get the first one is V3 sided with you one, which is 18 / the length of you wants word, which is 9 times the vector you one which is 1-2 2  And the second one is the top products V3 with you to which we calculated was also 18 divided by the length of you two squared which is 8.  Time's the vector you too. Can you do that? We already computed above which is 0 to 2.  Again at this point If This Were A Course in arithmetic, I would want you to go through all the calculations to get there. Let me just give you the answer. If you want to run the calculation yourself later that all adds up to -2 - 1/2.  And then finally, we would need to compute the length of that to normalize. So the length of you three squared is the sum of the squares of those numbers, which if you work it out comes out tonight, and so therefore the normalized u3, is that Vector user? I just computed divided by the square root of 9/2. So the length of you three is equal to 3 / root 2, so I have to multiply  buy root 2/3 the vector -2 - 1/2 1/2  Okay, and I won't simplified any more than that. So there's the vector in question. So those three factors that I just computed.  You want hot?  you two hats you three had those three vectors form an orthodontic normal basis for the column space of a  that's how you do it. That's how this procedure works.  It's a little too if they were a force Vector it gets quite tedious, but it's tedious for you. It's not tedious for a computer.  Computationally very easy for a computer. So that's the gram Schmidt process now in the last 12 minutes here. I'd like to tell you beyond the final piece of the puzzle for how this tell us how to computer Thug. No projections how to find orthonormal basis in general. I want to tell you it does something better as well. So it doesn't mean it's also important for a different reason. I just see why I want to answer this question, right? So we started with a basis V once review p  How do we produce a new orthodontic basis you one's for you? Pee can I go backwards?  So what if I had the use and I want to get back to the VIS? Well, I can't do that in a vacuum. It's like saying here's the reduced row Echelon form of a matrix find the original Matrix. You can't cuz lots of original matrices that Rob reduced to that one registration on form. But what I mean is if we backtrack through the calculations, we just made I want to express the V's in terms of the use the calculations using terms of device, but in the Gram stain procedure, I want to express the V's in terms of the use. So let's go back. Let's ride again one more time. So you one was equal to its you want normalized.  So it is V1. Remember V1 was you want / the length of you one, which is the same as the length of V12? Let's leave it like that. So let me solve for V one there that's easy enough is equal to  the length of you one  time is it you want to have okay.  Now what about you two hats in the procedure that was equal to one over the length of the vector that we find you two times that Vector you to which we produced as a V2 - your projection of V2 on.  4 V 2. Okay, that's a simple linear equation. And that is that says V2 is equal to the length of you two times you two hats plus the top part of V2 with you 1/2 x you want hat?  Unless I'll get you three.  I said, let me get myself a little more space here.  You straight down here. Okay, so that was we normalized the vector that we get and then after that we got was the 3-2 orthogonal projection of V 3 onto you one hat.  Minus the orthogonal projection of V 3 onto you too hot.  And now we can solve that for V3 just by multiplying through by the length of you too and adding to the other side we get that V 3 is equal to the length of you three times to use three hat + V3 dotted with you one hat times.  You want hot + V3 daughter with you too hot. I miss you, too hat.  So what I'm doing here is on solving for the V's in terms of the use that we already produced. No, it's kind of implicit because it involves the length of the number here. Let me call that R11.  These numbers down here. Let me call them R22 and r21.  And these numbers here. Let me call them r33 r31 and R32.  So when we do that what we have produced is we see that I've written it as some number rkk times UK hot.  + r. K k - 1 x UK - 1 Hat  + r. K k - 2 U K - 2 hat and so on Down the Line.  That might look pretty take what's going on there anything that I want you to know, the only thing that's important. I'll tell you what those numbers are. I will write them in a moment thing. I want you to know is that just like when we did this procedure forward we got the us3 was in the span of V1 V2 and V3 and it didn't involve any of the further V's similarly 3 depends only on Hue one that you two had a new 3 hat. It doesn't depend on you for you 5 until we get this same triangular structure.  So let me write that the Triangular structure. It really is a parent when you write things like this. If I take the same collection of vector equation is here and write it as an equation for the Matrix. So here's the Matrix with those vectors V1 through VP as the columns in the basis you produce that new bases you want have you to have you. Which is an orthodontic basis?  What this procedure with a garnishment procedure actually says is that in doing that you're writing the original Matrix X this Matrix here.  with all zeros  What you're doing is your writing the Vees?  In terms of the use when you do that you are factorising the Matrix you start with a matrix whose columns form a basis for your Subspace.  Doing the gram Schmidt process would it actually does his factorize that Matrix as a matrix here whose columns are orthonormal time is another Matrix and Matrix over there will be a portray angular.  That's what gram Schmidt actually does for you.  So here is the theorem which we just proved which is it if you take an M by n Matrix, which is full rank tall and skinny if it's not Square because it has to have all the orphan with us have all linearly independent column. So you take a matrix with linearly independent columns.  Gram Schmidt on that Matrix what grandma Schmidt actually does for you is it satirizes the Matrix in the form Q x r amusing to hear instead of you up until we use the letter and your textbook uses the letter q and so do Minnesota's Matlab. So I'm going to call you but that's the same Matrix you we've been talking about. It's the Matrix was columns are the orthodontist factors produced and what it works out to is it's just all the coefficients that you computed during the gram Schmidt process organized in the right way. So if you backtrack to slides to this calculation here, it's all of these coefficients.  that one  that one  those two  that one and that one that come in just need to keep track of which one goes where?  So in that example, I'm just going to copy those numbers over what we got there.  Play we started with that basis for the column space. Does that make you to find those in the grammar Smith process? They were three?  2 root 2 + 3 / root 2 those worthy the length squares of the vectors and then to express 2 as a as a linear combination of V1 and V2 traditionally had this coefficient here 9 quarters and to express you three as an accommodation of V1 and V3 you had two and one is so here is my Matrix are so kieu is ortha normal columns.  an R  is upper triangular.  What's the QR factorization of a matrix you need to know for this course in for the final exam how to find the QR factorization of a matrix, but it's exactly the same thing as doing gram Schmidt orthogonalization. Guess what? That's the QR decomposition of a matrix.  On your homework you going to be doing if you haven't already a bunch of examples of this and you just need to get used to how to put those coefficients into the upper triangular matrix. It's just what this one tells you is exactly riding. The things on the left as linear combinations of the things on the right wear for the first column on the left. You only need to use the first column on the on the right for the Second Use the first two columns on the right the third one involves all three  Okay. Now why would we care about that? Well in the last one minute I will just say what is it good for it's good for  finding  eigenvalues  when Matlab compute eigenvalues when you put in a matrix a square Matrix in Matlab  and you ask it to compute the eigenvalues send it immediately spits out a list if you put in a 4000 by 4,000 Matrix and ask it to spin out a list of the eigenvalues it will do it in two seconds on a not very fast computer now 4000 4000 Matrix the characteristic polynomial of that is a 4000 degree polynomial factoring such a polynomial even approximately is very difficult. That's not how that live doesn't what MetLife does instead. Is this Matt live computes the QR factorization of your Matrix, which is a very simple algorithm know you might say hey, I know what's going on here that are Matrix is upper triangular.  But caution the QR factorization. The are doesn't have the same eigenvalues as the original Matrix. If it did you would be easy to compute eigenvalues and it's not  what knot does instead and I'll just finish with this is to say we take a and factorise that is QR and then it takes the new Matrix, which it'll call A2 and writes those two in the other order.  In the QR factorization if you start with a square Matrix both q and R will be square. So you can write them in the other order remember matrix multiplication nearly never commutes. This is going to be a new Matrix, but we'll see next time. It has the same eigenvalues as a and then you can repeat keep doing QR factorization and reversing and we'll discuss next day. Why doing that will get you quickly close to Matrix is eigenvalues are easy to compute. See you on Wednesday. "
}