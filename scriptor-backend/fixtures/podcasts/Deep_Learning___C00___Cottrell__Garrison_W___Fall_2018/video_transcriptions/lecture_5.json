{
    "Blurbs": {
        "And so how do we fix that? Turn the ideas on how to fix that? Yeah. Okay, so I think what you're saying use the product of the two inputs. Or the square or something. Oh, okay. So you're saying use a quadratic function instead of a so maybe something it would go like this. Okay, that's cool. But I don't know if there's a learning algorithm for that. I ": [
            379.4,
            436.0,
            5
        ],
        "By doing what will you add Epsilon or sorry you add Epsilon to the weight. So remember we're trying to go. Downhill in the air so we have some error function in this is w i j and this is Jay. Okay, I'm supposed I'm right here. This is the value of w i j right now if I add a little bit to that and compute the air I'll ": [
            2888.7,
            2920.7,
            71
        ],
        "I change this weight, but I have to visit all the other weights in the network in order to get the output. So it's going to be a border W Squared where W is the number weights. So that's W squared. Backprop is linear in W. You visit each wait once when you do this and you do it for all the weights at once, so that's really really efficient. ": [
            2983.5,
            3014.9,
            74
        ],
        "I compare the output to the teacher and I get T minus y and I send that back here send that back here and then I've now computed the gradient for all the weights I've computed. Have computed the partial of Jay with suspected wi Jay. Okay, and So, why is this cool? Why is this really really good in terms of efficiency? How many people know how to numerically ": [
            2805.3,
            2848.8,
            69
        ],
        "I'll talk more about that as time goes on. So depending on where you start you're going to get different answers. So if you Dude, so this particular one it had two big weights to here and this guy learned or and this guy learned and and then or just turned on the output every time but and just when it came on it at a big negative way to ": [
            3137.7,
            3166.9,
            78
        ],
        "I'm going to plug in here. Okay. All right. Ready? Anyway, I call that the books that killed Frank rosenblatt. Because he died in a mysterious boating accident on his birthday and Chesapeake Bay is out sailing and he kind of disappeared over the side. And so there's only one date here. So he was pretty depressed. I assume this is Ithaca you was at Cornell. I was at Cornell, ": [
            929.0,
            976.7,
            18
        ],
        "In any if there's any input on it turns it off. So what's what's that function called? Nor okay. There we go. So by a combination of nand and nor it solves the problem. Okay, so this is a score except for this guy and that's when this guy comes on and turns the output off. Okay, so it's a lot like the or anansa Lucian but negated. Okay, so ": [
            3964.4,
            4009.6,
            95
        ],
        "Just about everything has an edge and that from that you can get like outlines of things in and recognize their shape. Okay, so okay. This was great. And you know, if you can just come up with great features and that's what what computer vision people did for decades was spend a weekend thinking about what might be a good feature and then put a linear classifier on top ": [
            732.9,
            767.1,
            13
        ],
        "Okay, any questions about that? questions Okay, yeah. First step of backpropagation. Yeah. I don't mean that it is linear in the weights to learn some function. That would be really amazing. Okay, so what is backprop learned? So you start out with some random Network? And by the way, now we have to use random weights to start out cuz if we make them all 0 it doesn't really ": [
            3017.7,
            3063.9,
            75
        ],
        "Okay. So let's say J is the objective function we want to figure out what's the derivative of Jay with respect to this weight in the network. And as usual we're going to start while I don't know if this is usual. I don't know if I did this before but we'll use the chain rule to do this. so we'll try and figure out the derivative of the air ": [
            1454.7,
            1483.0,
            32
        ],
        "PDP group meeting to present our results and we're kind of confused cuz here's the air. And here's the training time. And the air goes down. But then it starts to go up again. And it wasn't that our learning rate was too high or anyting and rummelhart to said there's a bug. So we started our code for like an hour or two. And finally we realized that we ": [
            2648.7,
            2684.7,
            65
        ],
        "So this will be wrong in some way and you've got a Delta there and you've got a activation here. So even though the way to zero you can change it. right, and now I start to get some action gear and we had some inputs here and we can change those and but the problem is when you do that all the hidden unit compute the same function. Yeah. ": [
            3096.5,
            3133.0,
            77
        ],
        "Who does it now, but we did it. So one Kotor is a neural network that takes an input in and and in this case an image in so it's a bunch of pixel values. And they were connected to Hidden units. and and then they were connected to the output which was again in image and what were training it to do is just to reproduce its input on ": [
            2460.6,
            2506.7,
            60
        ],
        "a cover page Okay. so Today we're going to talk about backpropagation. Today, we're going to talk about backpropagation. Can y'all hear me? Just working. Hello. Hello. Hello. Hello. Okay. Okay. Okay. Check check. Check my call. Mike volume is all the way up. Okay. Anyway, we're going to talk about perceptron. Sorry backpropagation today and I need the lights. Okay, my adapter for my laptop is over and see ": [
            100.9,
            204.2,
            0
        ],
        "activation function so they end up being t- why which we like Okay. But for hidden units we have to take into account every unit that Jason's output to so J is in unit. It send output directly to the output unit sword could send output to the next hidden layer up. And so the air is going to change as its input changes and we can again use the ": [
            1701.2,
            1733.7,
            39
        ],
        "activation function to that and get your final set of outputs. So this is the activation function of the outputs. It could be softmax. It could be linear. It could be the logistic Etc. Okay any questions so far? That's forward propagation. Yeah. Yeah, I don't know. What? Doesn't change it welcome to tinnitus. I just recently discovered. I have tinnitus I still here and there's some buzzing in one ": [
            1256.0,
            1312.4,
            27
        ],
        "actually the activation. So it's G 8 G of A J Wright. So this is just going to be the slope of the Hidden unit at that point. This is really giave Jay and so the derivative that was spec to attend put is G Prime of AJ. And there's the rub because it means we can't get rid of the slope for the hidden units and for years many ": [
            2025.0,
            2055.9,
            48
        ],
        "add a unit in between the input and the output? So you could have you know, here's the output. and we want Wait for more units. It's always one. That's -1. And here's X1. And here's X2. Thank you. Okay, and In between you have another unit. and what this guy does is or we already know how to do that and what this guy does is and we already ": [
            812.2,
            855.6,
            15
        ],
        "air propagate the are backwards through the network and the algorithm uses the chain rule calculus to go downhill in the air measure with stick to the weight switch what we've been doing right gradient descent and the hidden units have to then learn features that solve the problem. So first, let's talk about forward propagation just to make sure we're on the same page for that. So we start ": [
            1125.3,
            1155.9,
            23
        ],
        "all the other weights in the network fixed while I do that. Okay, so I have to do it for one way and that and I have to run everything all my patterns through the network to figure out the true grade in. And then I pick another wait and do it again pick another wait and do it again. It's really going to be expensive because not only do ": [
            2956.9,
            2983.5,
            73
        ],
        "all with any three of those on any two of those on will be it. Have a carry out of that place. And it just said case if there is a carry out of that place it turns off that guy and turns on that guy. How many layers is this network? It has two hidden layers. This one is before that one. Although there skip connections here. It's This ": [
            4429.2,
            4464.3,
            107
        ],
        "and output activation function and this is Delta if Jay is a hidden unit. And there is no Delta if Jay is an input unit. You don't change the input units okay for you, could you could backtrack propagate all the way to the inputs and try and change them? And it will see that later that that's been done. So we have a recursive definition of delta. We know ": [
            2359.8,
            2395.0,
            57
        ],
        "and put Delta K. That's Delta. Dudududu dudududu. Okay, and then I've got this guy. And now I'm going to use the chain rule on that guy. Like how does his input change as my output changes burnt times. How does my output change as my input changes? I just played the chain roll again. And what's this guy? Sorry. Delta no, no. The Thirst is this is This is ": [
            1979.7,
            2025.0,
            47
        ],
        "and that's going or ZJ. Wjk ZJ, and that's going to be w i j. Do do do do okay. So it's just like when you end up with just the input on that line, except it's the opposite you get the wait cuz you're taking the derivative with respect to the input on that line. Okay, so you got that? because every summon that every turn in the sun ": [
            2163.6,
            2201.7,
            52
        ],
        "and you gate what you're looking at so that only some things get through the network see but anyway, we get all these these and this is the there's still a bias term for the outputs. And then we propagate those forward and we get a awaited some for the output and now it's 0 to M because we have some number of hitting you and then you apply the ": [
            1220.0,
            1256.0,
            26
        ],
        "anything they could do. Do they can learn to do? Which is a pretty strong guarantee and there was great and the way they work is that you prevent you randomize the order of the pattern Supercenter pattern you update the way it's so it's supervised learning you're giving a target for every output for every input. It's slow because when you train on some patterns that might screw up ": [
            246.3,
            299.1,
            2
        ],
        "approximate a gradient or derivative anybody you do you do? Okay. What are you do so I have this input I give you the input and Philip. I got an output. And then I can compute the air. And suppose in the middle of year. I've got a weight w i j and I want to check is my gradient, right? You can do a numerical approximation to the derivative. ": [
            2848.8,
            2887.8,
            70
        ],
        "balanced. And that's the that's kind of the story about why they have to be in this ratio. You have to avoid combinations of the other ones set my Mistakenly turn on or off a head unit. Okay, so if it's 1 suppose, those are all zeros and this is 10. Well, it's got a big negative up to hear 1 x - 12 0 x + 12 to that ": [
            4197.3,
            4232.9,
            101
        ],
        "be from here to hear etcetera. Okay. and now we have to take into account the hidden units and the reason here why we can't Have the perceptron activation function here is because the way we're going to figure out how to propagate the error backwards is we're going to use the chain rule calculus. And you can't apply that to a binary threshold unit cuz it's not continuous and ": [
            1346.8,
            1387.1,
            29
        ],
        "besides. Any gradient of that would be zero. Okay. So let's talk a little bit about notation zi is going to mean here. It's going to mean either the output of hidden unit where this is hidden units taking the weighted sum of the 10 puts flying the non-linearity and then we've got the output and we multiply that times this way to add into this guy's input for z ": [
            1387.1,
            1423.2,
            30
        ],
        "but I was majoring in You know demonstrations and stuff like that. Okay, so if that's okay. There we go. Okay, and so and I wasn't into neural Nets or anything then accept as far as reading Isaac Asimov and science fiction, okay. So then in 1985, so 1973 Paul, we're both discovered it again or 74 in his PhD thesis and he showed it to Steve grossberg who I ": [
            976.7,
            1029.6,
            19
        ],
        "by having a big negative to wait and that likes to be off in the absence of any other input and it turns on the middle guy. Okay. And now what? So if it turns on the middle guy, but there's a one here. There should be a carry out of there. Not you know and tell. This guy says oh there's a carry into that place. And so with ": [
            4385.2,
            4429.2,
            106
        ],
        "chain rule where we sum up all of the ways that the air changes as the input to those guys were connected to changes. X how their input changes with respecting my input Okay, is that make sense? I'm going to sum up all the ways the guys are bug me that I'm connected to how the air changes with respected them their net input and how their net input ": [
            1733.7,
            1772.1,
            40
        ],
        "changes is my net input changes. Okay. This is just the chain rule. It's this weird-looking chain rule though, cuz we've got this some and I have to admit that I stared at this for a really long time back in 1985 or so before you were born. And I I kind of get it now. 33 years later, okay. So, you know how to changing my how changing my ": [
            1772.1,
            1809.9,
            41
        ],
        "choose some input output pattern you percent the input. Put. That's the sound of the activation makes when it hits the output you give it a teaching signal And you propagate the air backwards through the network and that's why it's called backprop. And then you change the connection strengths according to the air. So here's the picture you give it an input funk. You got an output compute the ": [
            1093.1,
            1125.3,
            22
        ],
        "computing? or nor Right, it's either or it's neither nor nor know it is nor is Computing door. Okay, very right. So why is it Computing nor this thing likes to be on in the absence of any input sonor means that it's going to be on 400. 401 this comes up and turns that down but it's not enough. It's still on 1-0. This comes up and turns it ": [
            3872.0,
            3922.2,
            93
        ],
        "end up with this and remember when we were doing the output. This is T minus why the difference between what you did and what you should have done. So so far, it's completely consistent with what we did before, okay. And so we know for the outputs that that comes out to that, okay. any questions that's assuming that we have the right objective function and the right output ": [
            1663.4,
            1701.2,
            38
        ],
        "functions. But if you do this so that the hidden you in an Interlinear in the output unit 3 linear, which is what we should have done. Then this essentially does principal component analysis. If you know what that is, I'll tell you what that is later, but Because it's trying to optimize the same thing as PCA as the square there. Yeah. see Yeah, I even programmed in bcpl ": [
            2729.0,
            2765.2,
            67
        ],
        "get that number. So that's adding Epsilon If I subtract Epsilon from that. I can compute then I compute the error. And then what's so that's the rise I can subtract that from this from that. That's the Run. It stood out to Epsilon until I'm Computing linear approximation to the slope. Okay, and if Epsilon is small enough, it's going to be pretty linear. But I have to hold ": [
            2920.7,
            2956.9,
            72
        ],
        "going on here? the weights are in The Sims 3 2 - 6 2 + 12 - 3 + 6 - 12 - 12 + 6 - 3 + 3 - 6 + 12 What what's going on here? So Yeah sort of. But okay. So so this would be symmetric if it was one one zero zero one one, right? When if it was one one zero zero one ": [
            4052.8,
            4097.6,
            97
        ],
        "guy's going to be off. But this guy gets a positive and a negative 12. So this guy I'll turn on so this guy turns on when this is one zero this guy turns on when it's 01. So again, it's an asymmetry detector. But if it's symmetric all of these cancel each other if it's one if it was zero one zero zero one zero. Let's get to -6 ": [
            4232.9,
            4267.0,
            102
        ],
        "hadn't put in the slope term for the hidden units. So backprop. It's kind of her best to programming yours. Okay. It's just kind of cool. Okay. So that's a little anecdote. I like to tell you. It's the difference between the input and the output. It said yeah. Yeah, this is T. This is why. And you some that up over all the outputs and we is logistic activation ": [
            2684.7,
            2729.0,
            66
        ],
        "he use that so A and D. Is the answer okay? So this isn't working. Okay, that's not working. Okay. So why is this wonderful? It's wonderful because it learns internal representations instead of having to on the weekend sit around and think of features. It learns the features. It also learns internal representations and it learns internal representations. It's like real estate location location location. This is the big ": [
            3718.2,
            3770.4,
            89
        ],
        "here. Does work very well? Okay, then and you so I have three I have three by 3 pixels here. And I multiply 0 times that and 1 times that and two times that and 1 times that they form the inner product of this with this. I got a big signal and now it's detected a visual Edge in the image and that's very useful for recognizing images cuz ": [
            697.6,
            731.5,
            12
        ],
        "his hair and put change its okay. So we're going to have to some my contribution to the error is based on how I affect all these guys above me. Okay, any questions about that? Yeah. Well, I contribute to the air or somehow. I'm going to contribute to the air by changing this guy. Who may be an output? Right. And so that's going to change the air or ": [
            1871.3,
            1907.1,
            44
        ],
        "i could be an input like it's the input layer in this is the first hidden unit layer. So these could be one hitting unit layer to the output. It could be one hitting unit layer to the next in unit layer. You going to multiple layers and that's how we get deep Networks. Okay, honey, so many questions about that. That's just notation Z is going to be ambiguous. ": [
            1423.2,
            1454.7,
            31
        ],
        "in the absence of any other input and I should say that these are logistic functions all of them. So anything like for an above and the input pretty much turns is all the way on and anyone any input -4 turns it all the way off. Okay. So what Boolean function is this network computing without to Hidden unit. So if you didn't have this, what's the outer part ": [
            3840.1,
            3872.0,
            92
        ],
        "input changes the error have to know how they are this what I just said X how their input changes as my input changes. So again, this part should be fairly clear because it's the chain rule but the sum is a little weird cuz you're thinking about what everybody you're connected to. So that's the idea you should have in your mind. Here's some hidden unit. And it's connected ": [
            1809.9,
            1842.9,
            42
        ],
        "installed bcpl and and there was the predecessor to see it had one day at a time word. Yeah, okay. Alright, so what can backprop learn? Okay, just just to be clear it was so if you're writing your back propped program, which we're going to make you actually Implement backdrop yourselves or not going to let you use some package. I taken and put front by get an outfit. ": [
            2765.2,
            2805.3,
            68
        ],
        "is 0 except when I Coast Jay, okay, and I've already said that Steve Prime of Hey Jay, and for some reason and this derivation, I didn't make the put it there. I don't know why I guess just to make it make it look more complicated. Okay, and congratulations on having made it this far in the class. There's not going to be any more homework sitters hard as ": [
            2201.7,
            2230.0,
            53
        ],
        "is kind of the first hidden layer. That's the second admire, but their direct also directly connected to the internet this network. Now what I didn't tell you I guess yet is that backprop has local Minima it can get stuck. And not learn something. And although if you have enough hidden units are a lot of ways around the block and hyperspace and most local minimum are okay, but ": [
            4464.3,
            4496.0,
            108
        ],
        "it back to Earth and suppose for every picture. I have to shoot a rocket off. So it's really expensive if I just tell her that but these weights are these rights are which are usually kind of the inverse of these weights. Ezra Millard said, you didn't even try and reproduce What it sees. Which is what I do if I was a hidden unit says rental heart and ": [
            2577.1,
            2607.1,
            63
        ],
        "it can only do linearly separable problems. So if your problem is let's make this. positive and this positive and this negative and this negative Concerta works, so there's no way to put a line here that separates these guys from these guys you can't do it. All right, but okay. So extra is kind of the smallest hard problem. It says it's like the hydrogen atom of machine learning. ": [
            328.0,
            379.4,
            4
        ],
        "it could be another hidden unit that's connected to an output and it's going to change the air as I change this and this and this and so The way I affect the error has to wait as to sum up the way all these guys affect the air. Cuz I affect all of them. Okay. Okay, fine. If your if you have the only dumb question is the one ": [
            1907.1,
            1943.9,
            45
        ],
        "it's going to filter the image and not be quite as sharp as the original image, but that's an that's an auto in Kotor. Okay. Any questions about one another and coder is Hell, yeah. Why? You know, they're used all over the place. It turns out but I mean suppose you wanted suppose. I'm up on Mars and I'm taking pictures of the Martians and I want to send ": [
            2544.2,
            2577.1,
            62
        ],
        "it's the tistical a significantly better, but it is better. Okay, I'm going to close it out going going going gone. Okay, what's the answer? She is gradient I sent. It's not gradient descent. You have to read the answers. before you answer Okay. Okay. Okay. Don't go I'll post on me. Yeah, and it changes the activations to go downhill know it changes the weights to go downhill in ": [
            3429.8,
            3495.6,
            83
        ],
        "just get to + 6 they balance this gives a plus fix. This goes to -6 they balance. So you can convince yourself. I hope that that solves the problem. Okay. Yeah. Yeah. Sorry. know your their sigmoid Sorry, I should have said that. But for the next example, it's a good idea to think of them as perceptrons, but they have to be they have to be smooth so ": [
            4267.0,
            4309.8,
            103
        ],
        "know how to do that. And then it has a big negative way to the output. So this does or right along and then just when we get to the line in the truth table where we have to go off and comes on and turns off the output. Okay. all right, so that's called a hidden unit and Minsky and pappert said well, if you had hidden units then ": [
            855.6,
            887.7,
            16
        ],
        "learn anything. Depends on some things but you make it all zero if it's 10 H, then then the tan 8080 and nothing happens if you use the logistic logistic is .5 at 0. So at least you get some activation here which gives you some output here while doesn't give you any outfit there because if the weights are zero, but at least it's an input on that line. ": [
            3063.9,
            3096.5,
            76
        ],
        "like a perceptron. or softmax Okay, so but then this as far as As far as perceptrons go though in 1969 Minsky and papert. Minsky just died recently. By the way, he's a professor at MIT came out with a book called perceptrons and it showed you know, there were a lot of things like X or that you couldn't compute one other approach to solving xor. Is you could ": [
            767.1,
            812.2,
            14
        ],
        "long didn't read. Okay, the opposite of forward propagation learnt internal representations. It's a form of gradient descent. It changes the activations to go downhill in the parameters. What's the what's the best answer here? I can imagine you might pick. Some other answer, but what's the best answer? Okay, 73 of you have answered said all of you. going going going gone. Okay. So 68% of you got the ": [
            3268.8,
            3322.6,
            81
        ],
        "mean they're sort of is but yeah, so that's one way is there something we could do to perceptrons to fix it? Yeah. I'll wait at some more percent. Okay. So if this is X1 and this is X2, we could have a third feature which is x 1 x X-2. and then if we set it up, so Oh, really? All you need is to have the the byas ": [
            436.0,
            490.3,
            6
        ],
        "my d'jais. And now we're going to Define delta J to B- of the derivative J with respect to AJ. and so the derivative of Jay with respect to w i j is this and this I just defined as Delta. Okay, the negative of this is is Delta. Yeah, okay. and gradient descent says we're going to do this. We're going to go downhill. Which means we're going to ": [
            1617.5,
            1663.4,
            37
        ],
        "nonlinear. Okay. Another example is I'd say we have some data like this. in a circle and we have a we want to separate these guys from these guys. What's a feature I could use their? Yeah, so so if you made the but I wanted to be linearly separable so I can if this is 0-0 I can use in this is excellent in this is why I can ": [
            563.8,
            608.1,
            9
        ],
        "of my ears. Okay anyway. Okay, so backprop learning. I'm going to drive back wrap for you. I'm going to do it on the slides cuz I always make mistakes when I do it on the board, but it's still gradient descent. Okay. So for any now, we have you no pairs of things just like we did for softmax, but it might be from here to hear. It might ": [
            1312.4,
            1346.8,
            28
        ],
        "of us used the logistic for hitting units and if that got really too active or to inactive it would be very slow to learn. Okay, then people noticed a these bipolar units like 10 H, which goes from -1 to 1 looks just like the sigmoid, but it goes from -1 to 1 when that's inactive you. At least you're putting in an input on the line above you ": [
            2055.9,
            2087.0,
            49
        ],
        "off at night off enough. But if both of them come on, it turns off the output. So what is it? It's it's nand not nor it's neither or nor nor its name and that's hard to say. Okay. So that's nand. Okay. Okay, what is this guy doing? It only turns on 400. It's got a hot. It's got a bias 2.2 is enough to turn on this guy. ": [
            3922.2,
            3963.3,
            94
        ],
        "on connected to you multiply them times the weight. So I'm running the weights backwards summing them up and then I put in the slope turn. So we can't get rid of that. Except if you make a mistake in your program. Okay, so Paul Monroe and I back in the day with our 1986 or 87 autoencoder for images. You know, it's it's kind of old nobody noticed though. ": [
            2425.2,
            2460.6,
            59
        ],
        "one of the Hidden units. So but since this is this Delta Zach guy, okay, so that's how we compute Delta. Okay. So the original schema for gradient descent is this and that leads to this? Okay, so it's just that there's still learning right? But this is exactly what we had before. So this is T minus 5 days and output unit sending the right combination of objective function ": [
            2317.8,
            2359.8,
            56
        ],
        "one what would happen? Well, if this was on and that was on this puts in a -6 this puts in a plus-6, they cancel each other out and this hitting you likes to be off in the absence of any other input. This guy likes to be on so it's detecting asymmetries. It's going to say it's symmetric unless told otherwise So if it doesn't get any input from ": [
            4097.6,
            4128.2,
            98
        ],
        "ones place up there. Or if there's a one over here, we want to win in the ones place over here. Right. So these are solid + 1 lines, but if they're both on then what we have a carry. And this guy only is an aunt of these two so it took Carrie detector. What is it? Do it turns off the guy that they're trying to turn on ": [
            4356.5,
            4385.2,
            105
        ],
        "other patterns, it wasn't so slow for you guys, but we saw it when I did or in class that it training on 00 would set things up great 400, but then you turn on 01 it couldn't do 0/0 anymore. She had to turn on 0-0 and then you do one zero and now can't 200 anymore. So I'm turning on some patterns will interfere with other patterns. And ": [
            299.1,
            328.0,
            3
        ],
        "output propagated backwards in the weights are changed, but it's not the best right? We don't start and this is the second one is actually how Russell and norvig had brought back prop in volume 1 of artificial intelligence a modern approach. But again backprop is resilient to programming yours. So if you compute the Deltas at the output and then changed the weights and do the output and then ": [
            3653.8,
            3689.5,
            87
        ],
        "output. So it's the output of the previous layer times to wake you some all that up take the derivative that was suspected wi J. And again since it's a partial derivative, all of these terms are zero because she is fixed. This is fixed when we are doing partial derivatives, except when I equals k, So I move the derivative inside cuz the derivative the same as some of ": [
            1517.2,
            1549.8,
            34
        ],
        "point of Bachrach. Okay, this is almost your Mantra and I quite it's a fishing and showed you that and the cool thing is that generalize has it turns out to recurrent networks, which is what our brains are. Mary Kay and we'll see how that works. Later. So in the next K slides work is some small integer because we've only got 7 minutes. I'm going to look at ": [
            3770.4,
            3804.9,
            90
        ],
        "propagate the Deltas back. Well, you're probably getting the Deltas. Four different weights than you computed them and you're getting the wrong answer at the hidden units, but because you usually change them slowly it's still pretty much works. So it's another backprop is a resilient to programming errors. We know it doesn't use the slope of the Hidden units that does use the slope of the app, which does ": [
            3689.5,
            3718.2,
            88
        ],
        "right answer. Nobody said to be thankful e this time. Okay. So turn to your neighbor. You have a 7 in 10 chance that your neighbor knows the answer. unless all the smart people sit together and Why this clicker question is really funny apparently. Okay, you settled down. Let's vote again. Try again. Okay, okay. Okay. Keep going. Okay, we went up 4% preds progress. I don't know if ": [
            3322.6,
            3429.8,
            82
        ],
        "so it's output weights are probably going to mirror its inputs. Wait, so whatever pattern it's responding to its going to try and put that pattern add that pattern into the pixels that its output depending on how active it is right. There are clicker questions coming. Okay. Get get your clippers out. Okay, so what palm and Rowan I did was return this thing. And we go to the ": [
            2607.1,
            2648.7,
            64
        ],
        "starts with the output their propagated backwards in the waiter changed starts at the output the weights are changed in the Deltas are propagated backwards uses the slope of the output unit uses the slope of the Hidden units A & D Okay. k we're we're we're going good so far. Mercy looking good hey many of you are not answering the be sure to catch question good. Okay, I ": [
            3549.9,
            3607.7,
            85
        ],
        "still have his tech report and then that when he called it learning logic and patented it and then Ian laocoon discovered it in his PhD thesis and Dave rumelhart and Ron Williams and Jeff Hinton discovered it here at UCSD. We're hitting in Williams for postdocs and Dave Roma heart is faculty in the psych department at that time and it works a lot like perceptron. So you randomly ": [
            1057.5,
            1093.1,
            21
        ],
        "that isn't asked. So if you have a question right now, there's probably doesn't other people in the class have the same question. I just have to find the person brave enough to ask it okay? No questions. Okay. It's all perfectly clear. Good glad to hear it. So moving right along. Okay. Guess what? We did find. This is Delta. Right? So I put a minus sign out there ": [
            1943.9,
            1979.7,
            46
        ],
        "that last one. That is written homework. Yeah. Why? Etsy That's so we were talking about w i j. Sorry WJ here. So we're trying to find the derivative of suspected Jay and did I screw up? I know there's Jay. Okay, it's the same guy. Okay, so This derivative now. I did it finally is mine SG Prime of AJ. So it's the slope of that hidden unit. * ": [
            2230.0,
            2288.9,
            54
        ],
        "that there was a carry so half the time that guy learns to be the carry out of the first place and there are originally there are connections everywhere here. So for example, this guy isn't connected to that that just means it that way to learn to be zero. Okay, but initially it was connected to everything and it could learn to carry out of that place and then ": [
            4547.3,
            4576.6,
            110
        ],
        "that's going to contribute to the error eventually, but it's not in putting a zero to the next layer up. Okay, and since Jay is independent of K. We're sending over K. And this just involves Jay. I can pull that out. And this is again just going to be the slope term. Okay. Okay, okay. Okay, and then the definition of this guy is is the sum of the ": [
            2087.0,
            2123.3,
            50
        ],
        "that's one representation learned by backprop. Not the features you think of first first off for a here's an interesting one. That's a little mind-boggling. So this is the input. This is a hidden unit now to Hidden unit and this is the output. So this is Computing symmetry of its input. So 11000 is not symmetrical. Is it symmetric around this? This is not symmetric. So, what's what what's ": [
            4009.6,
            4052.8,
            96
        ],
        "the derivatives. And that's just see I so that's the input on that line just like it was for perceptrons just like it was for logistic regression just like it was for softmax rejection regression. Okay, so going back. This is the input on that line. Just being put on that line. Okay? And we got this to be just being put on that line. And now we have to ": [
            1549.8,
            1585.5,
            35
        ],
        "the error. Okay. It is kind of the opposite of forward propagation, but in five you thought that but the real answer. Is that it learns internal representations or features like and and or the example, I just gave ya. backprop learns I'm sorry. Okay. Okay, it learns internal representations. Okay, just one more and then you can leave if you don't like this anymore. Okay, okay. Computing the Deltas ": [
            3495.6,
            3549.9,
            84
        ],
        "the last 30 years or so of computer vision up till 2012 was trying to come up with features like this that will not quite like this that could solve a problem like one kind of feature of an image. might be her sorry 000 - 2 - 1 - 1 so if you place this over a part of an image where it's dark over here. And white over ": [
            643.8,
            697.6,
            11
        ],
        "the output of that hidden unit. Okay, and this could be the logistic. It could be a rello. It could be 10 H. People are even starting to put soft Max in the middle of networks for MidFirst didn't make a lot of sense, but you can kind of use it as an attention signal like what you're attending to and you multiply the output of that X other outputs ": [
            1189.3,
            1220.0,
            25
        ],
        "the output. That's why it's called an auto encoder because even though it's supervised learning. Right, I'm using backprop. The input is the teacher. So it's really either self supervised learning or unsupervised learning and it's called in and coder because usually you have fewer units here than you have inputs. And so you're finding a compressed representation of the image and then you're blowing it up again. And so ": [
            2506.7,
            2544.2,
            61
        ],
        "the skies host. He can't be the carry out of the middle place cuz he doesn't have enough information. Okay, it's 6:16. I have a plane at 7:40. So I'm going to save the good stuff for next week. Thank you. ": [
            4576.6,
            4601.4,
            111
        ],
        "the sum of the Deltas have the guys I'm connected to so that we were trying to figure out the Delta 4 heading unit J. I take the Deltas of everybody. I'm connected to I run the network backwards multiplying the Deltas X the weights. So again, this is like an inner product of the Deltas X the weights, but it's going backwards to one to one of the guys ": [
            2288.9,
            2317.8,
            55
        ],
        "these guys, which have big negative nines to this, so if either one of those comes on and it turns off the output, so these this network is a kind of asymmetry detector. And so if the if things are symmetric, these guys are going to balance and they're going to balance down here as well. But if they're not symmetric. Well, this is -3 + 6, so it's going ": [
            4128.2,
            4159.8,
            99
        ],
        "think I told the story at the beginning right grossberg looked at it and said I did that I did that I didn't do that but it's not interesting. I did that I did that and then not interesting thing was backprop. So we're both kind of put it down for a while and then a 1985 three groups independently discovered it David Parker and undergrad at MIT and I ": [
            1029.6,
            1057.5,
            20
        ],
        "think that's almost everybody. Anybody else when our way in okay going going going gone. The last minute went from 81% to 79% So now you have to okay. So how many people here have written down the right answer on a test erased it and then written down the wrong answer. Everybody you're lying. Okay. Okay, what's the answer glass? Thank you. Okay, this is right. You started the ": [
            3607.7,
            3653.8,
            86
        ],
        "this problem is soft is small enough that half the time it has a local minimum. Can you guess why? Time stop. Okay. Okay. Well what if this guy out of the place? It's too late then for this guy to figure out if there's a carry out of the middle place. Because it can't find out that there's a carry because it's Upstream of that guy. doesn't find out ": [
            4496.0,
            4547.3,
            109
        ],
        "to all these other units above them. And so it's contributing its output ZJ X that CJ X whatever that way it is that way to teach a time so that way is going to change their input their weighted some of their inputs. Right and then so how does that change as my input changes? How does this change as my input changes? And then how does he change ": [
            1842.9,
            1871.3,
            43
        ],
        "to start up iclicker. Okay, start new session. Okay. Are you ready? Okay. Okay. Well, send me you already voted. You haven't even seen the question yet. That's amazing. How did you know? sorry, everybody thinks that my answers are alright already a Broke, I can't seem to Advanced. There we go. Okay. Have advanced the sides now. Okay, so the tldr of back rapid Rhino or TL DR too ": [
            3201.2,
            3268.8,
            80
        ],
        "to turn that one on. they can't be like all threes because if they are all threes instead of -3 and 6, then these would balance have so you have to have these guys get bigger and bigger in order to you know, suppose these guys somehow. Outweighed this guy. Will they can't because 6 + 3 is 9 at cannot wear that guy. So it's got to be perfectly ": [
            4159.8,
            4197.3,
            100
        ],
        "turn off the output. So that's one possible solution to ax or but in fact starting with a bunch of different initial random weight so you can get a bunch of solutions text or and some of them will surprise you right? It's not like what you would have thought of as a solution next door. Okay, we have clicker questions. You're ready clickers at the ready. Let's see have ": [
            3166.9,
            3201.2,
            79
        ],
        "use like x squared plus y squared is as an input or as two inputs and then I've got by squaring them. All the Mindless ones are going to be over here and all the plus ones are going to be over here. And then we can separate them. Okay. So yeah, so that's a good idea. So and that was where things stood for quite a while and then ": [
            608.1,
            643.8,
            10
        ],
        "various representation sit back prop has learned in problems that solved and again your mind right now back propped learns representations in the service of the tasks. Hey remember that that's your mantra. Okay. So here's a version of xor it's one of those ones. You wouldn't have thought of. The number in in the unit is the bias. So think of that is what the unit likes to do ": [
            3804.9,
            3840.1,
            91
        ],
        "we have like + 5 or something like that. Okay, so that turns out into a linearly separable problem. So we came up with a new feature which is just the product of the two input features to make it be able to do a xor and that's a common strategy is to try and come up with features and Ferg's amp. Rosenblatt himself assume did nonlinear pre-processing. Well, that's ": [
            529.0,
            563.8,
            8
        ],
        "weighted sum of the inputs W, you know the input to that guy and now it's going to happen. What's the derivative of that going to be? W i j yeah, so these are all going to be zero. Right, except when k equals J cuz we're thinking all the other things are constant. And so we're going to have just one term of this big some that's wi Jay-Z ": [
            2123.3,
            2163.6,
            51
        ],
        "what it is through the output units once we've computer dit for the output units, then we can compute it for the hidden layer below that once we've computed them for that we can computers for the heading layer below that Etc. So this the idea they have in mind I compute some Delta's for everybody hears me and trying to compute my Delta. I compute the Deltas of everybody ": [
            2395.0,
            2425.2,
            58
        ],
        "with respect to the input to the unit wi J's connected to so just said wait from I to Jay. So this the input to Jay and then this and this is the weighted some of the inputs. So this is the part that it always turns into the input on that line. So just recalling that this is the weighted sum of the inputs to unit J X the ": [
            1483.0,
            1517.2,
            33
        ],
        "with some inputs like feminist pickled pixel values. You're going to be doing mnist for your next assignment. And then at the hidden units you compute the weighted sum of the inputs. This is from Ida J and from 0 to D. And that gives you the net input or a sub Jay to this hidden unit Jay, okay. And then you apply the activation function do that to get ": [
            1155.9,
            1189.3,
            24
        ],
        "worry about this guy. And if you remember the Delta roll it had that input on that line there and this was the difference between what you did and what you should have done and there's so what we're going to end up doing is defining this or the negative of it as Delta. Okay. So just like before we have a term that the input on the line for ": [
            1585.5,
            1617.5,
            36
        ],
        "x0 which is equal to 1 W 0 if you have that piece of negative number. Then in the absence of other input it'll be off right and so actually leave these a d r o. Her sorry, sorry we want it on 400. So and then you have these B say -2 or -2 and then this is only going to be one when both are one and now ": [
            490.3,
            529.0,
            7
        ],
        "you can take derivatives. Okay, here it is useful to think of these is perceptrons. and Again, this the solid lines all that this is doing a binary addition. And so we're adding 0 and 1 2 1 1 and the output is 100. Okay, is that right? I think it is. Okay. How does this work? Okay. Well if there's a one here then we wanted one in the ": [
            4309.8,
            4356.5,
            104
        ],
        "you could compute any Boolean function but there's no learning algorithm for hitting units and we don't think one will ever be discovered and and that was the year that Bryson and who discovered back prop which nobody knew about it, except Bryson and toe and their colleagues because They weren't in our field. So anyway, and it was a generalization of the Robin's Monroe procedure from the 50s. And ": [
            887.7,
            929.0,
            17
        ],
        "you see it's on its way here as soon as my student goes and gets it brings it over. He borrowed it today. I didn't get it back. I'm if anybody now that there are more people here and I'll ask one more time. Does anybody have a USB C to VGA adapter? No Okay, so Okay, so I'm sure we saw perceptrons already and they were pretty cool because ": [
            204.2,
            246.3,
            1
        ]
    },
    "File Name": "Deep_Learning___C00___Cottrell__Garrison_W___Fall_2018-lecture_5.flac",
    "Full Transcript": "a cover page Okay.  so  Today we're going to talk about backpropagation.  Today, we're going to talk about backpropagation. Can y'all hear me?  Just working.  Hello. Hello.  Hello. Hello.  Okay. Okay. Okay. Check check. Check my call. Mike volume is all the way up.  Okay. Anyway, we're going to talk about perceptron. Sorry backpropagation today and I need the lights.  Okay, my adapter for my laptop is over and see you see it's on its way here as soon as my student goes and gets it brings it over. He borrowed it today. I didn't get it back.  I'm if anybody now that there are more people here and I'll ask one more time. Does anybody have a USB C to VGA adapter?  No Okay, so  Okay, so  I'm sure we saw perceptrons already and they were pretty cool because anything they could do.  Do they can learn to do?  Which is a pretty strong guarantee and there was great and the way they work is that you prevent you randomize the order of the pattern Supercenter pattern you update the way it's so it's supervised learning you're giving a target for every output for every input. It's slow because when you train on some patterns that might screw up other patterns, it wasn't so slow for you guys, but we saw it when I did or in class that it training on 00 would set things up great 400, but then you turn on 01 it couldn't do 0/0 anymore. She had to turn on 0-0 and then you do one zero and now can't 200 anymore. So I'm turning on some patterns will interfere with other patterns.  And it can only do linearly separable problems. So if your problem is let's make this.  positive and this positive and  this negative and  this negative  Concerta works, so there's no way to put a line here that separates these guys from these guys you can't do it.  All right, but okay. So extra is kind of the smallest hard problem. It says it's like the hydrogen atom of machine learning. And so how do we fix that? Turn the ideas on how to fix that? Yeah.  Okay, so I think what you're saying use the product of the two inputs.  Or the square or something.  Oh, okay. So you're saying use a quadratic function instead of a so maybe something it would go like this.  Okay, that's cool. But I don't know if there's a learning algorithm for that. I mean they're sort of is but yeah, so that's one way is there something we could do to perceptrons to fix it? Yeah.  I'll wait at some more percent.  Okay. So if this is X1 and this is X2, we could have a third feature which is x 1 x X-2.  and then if we set it up, so  Oh, really? All you need is to have the  the byas x0 which is equal to 1 W 0 if you have that piece of negative number.  Then in the absence of other input it'll be off right and so actually leave these a d r o.  Her sorry, sorry we want it on 400. So and then you have these B say -2 or -2 and then this is only going to be one when both are one and now we have like + 5 or something like that.  Okay, so that turns out into a linearly separable problem. So we came up with a new feature which is just the product of the two input features to make it be able to do a xor and that's a common strategy is to try and come up with features and Ferg's amp. Rosenblatt himself assume did nonlinear pre-processing. Well, that's nonlinear. Okay. Another example is  I'd say we have some data like this.  in a circle and we have a  we want to separate these guys from these guys.  What's a feature I could use their?  Yeah, so so if you made the but I wanted to be linearly separable so I can if this is 0-0 I can use in this is excellent in this is why I can use like x squared plus y squared is as an input or as two inputs and then I've got by squaring them. All the Mindless ones are going to be over here and all the plus ones are going to be over here.  And then we can separate them.  Okay. So yeah, so that's a good idea. So and that was where things stood for quite a while and then the last 30 years or so of computer vision up till 2012 was trying to come up with features like this that will not quite like this that could solve a problem like one kind of feature of an image.  might be  her sorry 000 - 2 - 1 - 1 so if you place this over a part of an image where it's dark over here.  And white over here.  Does work very well? Okay, then and you so I have three I have three by 3 pixels here.  And I multiply 0 times that and 1 times that and two times that and 1 times that they form the inner product of this with this. I got a big signal and now it's detected a visual Edge in the image and that's very useful for recognizing images cuz  Just about everything has an edge and that from that you can get like outlines of things in and recognize their shape.  Okay, so okay. This was great. And you know, if you can just come up with great features and that's what what computer vision people did for decades was spend a weekend thinking about what might be a good feature and then put a linear classifier on top like a perceptron.  or softmax  Okay, so  but then this as far as  As far as perceptrons go though in 1969 Minsky and papert. Minsky just died recently. By the way, he's a professor at MIT came out with a book called perceptrons and it showed you know, there were a lot of things like X or that you couldn't compute one other approach to solving xor.  Is you could add a unit in between the input and the output?  So you could have you know, here's the output.  and we want  Wait for more units. It's always one. That's -1. And here's X1. And here's X2. Thank you. Okay, and  In between you have another unit.  and what this guy does is or we already know how to do that and what this guy does is and  we already know how to do that. And then it has a big negative way to the output. So this does or right along and then just when we get to the line in the truth table where we have to go off and comes on and turns off the output.  Okay.  all right, so that's called a hidden unit and Minsky and pappert said well, if you had hidden units then you could compute any Boolean function but there's no learning algorithm for hitting units and we don't think one will ever be discovered and and that was the year that Bryson and who discovered back prop which nobody knew about it, except Bryson and toe and their colleagues because  They weren't in our field.  So anyway, and it was a generalization of the Robin's Monroe procedure from the 50s.  And I'm going to plug in here.  Okay.  All right.  Ready?  Anyway, I call that the books that killed Frank rosenblatt.  Because he died in a mysterious boating accident on his birthday and Chesapeake Bay is out sailing and he kind of disappeared over the side. And so there's only one date here. So he was pretty depressed. I assume this is Ithaca you was at Cornell. I was at Cornell, but I was majoring in  You know demonstrations and stuff like that.  Okay, so  if that's okay.  There we go. Okay, and so and I wasn't into neural Nets or anything then accept as far as reading Isaac Asimov and science fiction, okay.  So then in 1985, so 1973 Paul, we're both discovered it again or 74 in his PhD thesis and he showed it to Steve grossberg who I think I told the story at the beginning right grossberg looked at it and said I did that I did that I didn't do that but it's not interesting. I did that I did that and then not interesting thing was backprop. So we're both kind of put it down for a while and then a 1985 three groups independently discovered it David Parker and undergrad at MIT and I still have his tech report and then that when he called it learning logic and patented it and then  Ian laocoon discovered it in his PhD thesis and Dave rumelhart and Ron Williams and Jeff Hinton discovered it here at UCSD. We're hitting in Williams for postdocs and Dave Roma heart is faculty in the psych department at that time and it works a lot like perceptron. So you randomly choose some input output pattern you percent the input. Put. That's the sound of the activation makes when it hits the output you give it a teaching signal  And you propagate the air backwards through the network and that's why it's called backprop. And then you change the connection strengths according to the air.  So here's the picture you give it an input funk. You got an output compute the air propagate the are backwards through the network and the algorithm uses the chain rule calculus to go downhill in the air measure with stick to the weight switch what we've been doing right gradient descent and the hidden units have to then learn features that solve the problem.  So first, let's talk about forward propagation just to make sure we're on the same page for that.  So we start with some inputs like feminist pickled pixel values. You're going to be doing mnist for your next assignment.  And then at the hidden units you compute the weighted sum of the inputs. This is from Ida J and from 0 to D. And that gives you the net input or a sub Jay to this hidden unit Jay, okay.  And then you apply the activation function do that to get the output of that hidden unit.  Okay, and this could be the logistic. It could be a rello. It could be 10 H. People are even starting to put soft Max in the middle of networks for MidFirst didn't make a lot of sense, but you can kind of use it as an attention signal like what you're attending to and you multiply the output of that X other outputs and you gate what you're looking at so that only some things get through the network see but anyway, we get all these these and this is the there's still a bias term for the outputs.  And then we propagate those forward and we get a awaited some for the output and now it's 0 to M because we have some number of hitting you and then you apply the activation function to that and get your final set of outputs. So this is the activation function of the outputs. It could be softmax. It could be linear. It could be the logistic Etc. Okay any questions so far? That's forward propagation. Yeah.  Yeah, I don't know.  What?  Doesn't change it welcome to tinnitus. I just recently discovered. I have tinnitus I still here and there's some buzzing in one of my ears. Okay anyway.  Okay, so backprop learning. I'm going to drive back wrap for you. I'm going to do it on the slides cuz I always make mistakes when I do it on the board, but it's still gradient descent. Okay. So for any now, we have you no pairs of things just like we did for softmax, but it might be from here to hear. It might be from here to hear etcetera.  Okay.  and now we have to take into account the hidden units and the reason here why we can't  Have the perceptron activation function here is because the way we're going to figure out how to propagate the error backwards is we're going to use the chain rule calculus.  And you can't apply that to a binary threshold unit cuz it's not continuous and besides.  Any gradient of that would be zero.  Okay. So let's talk a little bit about notation zi is going to mean here. It's going to mean either the output of hidden unit where this is hidden units taking the weighted sum of the 10 puts flying the non-linearity and then we've got the output and we multiply that times this way to add into this guy's input for z i could be an input like it's the input layer in this is the first hidden unit layer. So these could be one hitting unit layer to the output. It could be one hitting unit layer to the next in unit layer. You going to multiple layers and that's how we get deep Networks.  Okay, honey, so many questions about that. That's just notation Z is going to be ambiguous. Okay. So let's say J is the objective function we want to figure out what's the derivative of Jay with respect to this weight in the network. And as usual we're going to start while I don't know if this is usual. I don't know if I did this before but we'll use the chain rule to do this.  so we'll try and figure out the derivative of the air with respect to the input to the unit wi J's connected to  so just said wait from I to Jay. So this the input to Jay and then this and this is the weighted some of the inputs. So this is the part that it always turns into the input on that line.  So just recalling that this is the weighted sum of the inputs to unit J X the output. So it's the output of the previous layer times to wake you some all that up take the derivative that was suspected wi J. And again since it's a partial derivative, all of these terms are zero because she is fixed. This is fixed when we are doing partial derivatives, except when I equals k,  So I move the derivative inside cuz the derivative the same as some of the derivatives.  And that's just see I so that's the input on that line just like it was for perceptrons just like it was for logistic regression just like it was for softmax rejection regression. Okay, so going back.  This is the input on that line. Just being put on that line. Okay?  And we got this to be just being put on that line. And now we have to worry about this guy.  And if you remember the Delta roll it had that input on that line there and this was the difference between what you did and what you should have done and there's so what we're going to end up doing is defining this or the negative of it as Delta.  Okay.  So just like before we have a term that the input on the line for my d'jais.  And now we're going to Define delta J to B- of the derivative J with respect to AJ.  and so  the derivative of Jay with respect to w i j is this and this I just defined as Delta. Okay, the negative of this is is Delta.  Yeah, okay.  and  gradient descent says we're going to do this. We're going to go downhill.  Which means we're going to end up with this and remember when we were doing the output. This is T minus why the difference between what you did and what you should have done.  So so far, it's completely consistent with what we did before, okay.  And so we know for the outputs that that comes out to that, okay.  any questions  that's assuming that we have the right objective function and  the right output activation function so they end up being t- why which we like  Okay.  But for hidden units we have to take into account every unit that Jason's output to so J is in unit. It send output directly to the output unit sword could send output to the next hidden layer up. And so the air is going to change as its input changes and we can again use the chain rule where we sum up all of the ways that the air changes as the input to those guys were connected to changes.  X how their input changes with respecting my input  Okay, is that make sense? I'm going to sum up all the ways the guys are bug me that I'm connected to how the air changes with respected them their net input and how their net input changes is my net input changes. Okay. This is just the chain rule. It's this weird-looking chain rule though, cuz we've got this some and I have to admit that I stared at this for a really long time back in 1985 or so before you were born.  And I I kind of get it now.  33 years later, okay.  So, you know how to changing my how changing my input changes the error have to know how they are this what I just said X how their input changes as my input changes. So again, this part should be fairly clear because it's the chain rule but the sum is a little weird cuz you're thinking about what everybody you're connected to.  So that's the idea you should have in your mind. Here's some hidden unit.  And it's connected to all these other units above them. And so it's contributing its output ZJ X that CJ X whatever that way it is that way to teach a time so that way is going to change their input their weighted some of their inputs.  Right and then so how does that change as my input changes? How does this change as my input changes? And then how does he change his hair and put change its okay. So we're going to have to some my contribution to the error is based on how I affect all these guys above me.  Okay, any questions about that?  Yeah.  Well, I contribute to the air or somehow. I'm going to contribute to the air by changing this guy.  Who may be an output?  Right. And so that's going to change the air or it could be another hidden unit that's connected to an output and it's going to change the air as I change this and this and this and so  The way I affect the error has to wait as to sum up the way all these guys affect the air.  Cuz I affect all of them.  Okay.  Okay, fine.  If your if you have the only dumb question is the one that isn't asked.  So if you have a question right now, there's probably doesn't other people in the class have the same question.  I just have to find the person brave enough to ask it okay?  No questions. Okay. It's all perfectly clear. Good glad to hear it. So moving right along.  Okay. Guess what? We did find. This is Delta. Right? So I put a minus sign out there and put Delta K. That's Delta.  Dudududu dudududu. Okay, and then I've got this guy.  And now I'm going to use the chain rule on that guy. Like how does his input change as my output changes burnt times. How does my output change as my input changes? I just played the chain roll again. And what's this guy?  Sorry.  Delta no, no.  The Thirst is this is  This is actually the activation. So it's G 8 G of A J Wright.  So this is just going to be the slope of the Hidden unit at that point.  This is really giave Jay and so the derivative that was spec to attend put is G Prime of AJ.  And there's the rub because it means we can't get rid of the slope for the hidden units and for years many of us used the logistic for hitting units and if that got really too active or to inactive it would be very slow to learn.  Okay, then people noticed a these bipolar units like 10 H, which goes from -1 to 1 looks just like the sigmoid, but it goes from -1 to 1 when that's inactive you.  At least you're putting in an input on the line above you that's going to contribute to the error eventually, but it's not in putting a zero to the next layer up.  Okay, and since Jay is independent of K. We're sending over K. And this just involves Jay. I can pull that out.  And this is again just going to be the slope term.  Okay.  Okay, okay.  Okay, and then the definition of this guy is is the sum of the weighted sum of the inputs W, you know the input to that guy and now it's going to happen.  What's the derivative of that going to be?  W i j yeah, so these are all going to be zero.  Right, except when k equals J cuz we're thinking all the other things are constant.  And so we're going to have just one term of this big some that's wi Jay-Z and that's going or ZJ.  Wjk ZJ, and that's going to be w i j.  Do do do do okay.  So it's just like when you end up with just the input on that line, except it's the opposite you get the wait cuz you're taking the derivative with respect to the input on that line.  Okay, so you got that?  because every summon that every turn in the sun is 0 except when I Coast Jay, okay, and I've already said that Steve Prime of  Hey Jay, and for some reason and this derivation, I didn't make the put it there. I don't know why I guess just to make it make it look more complicated.  Okay, and congratulations on having made it this far in the class. There's not going to be any more homework sitters hard as that last one.  That is written homework. Yeah.  Why?  Etsy  That's so we were talking about w i j.  Sorry WJ here.  So we're trying to find the derivative of suspected Jay and did I screw up?  I know there's Jay. Okay, it's the same guy.  Okay, so  This derivative now. I did it finally is mine SG Prime of AJ. So it's the slope of that hidden unit.  * the sum of the Deltas  have the guys I'm connected to so that we were trying to figure out the Delta 4 heading unit J. I take the Deltas of everybody. I'm connected to I run the network backwards multiplying the Deltas X the weights. So again, this is like an inner product of the Deltas X the weights, but it's going backwards to one to one of the guys one of the Hidden units.  So but since this is this Delta Zach guy, okay, so that's how we compute Delta.  Okay.  So the original schema for gradient descent is this and that leads to this? Okay, so it's just that there's still learning right? But this is exactly what we had before.  So this is T minus 5 days and output unit sending the right combination of objective function and output activation function and this is Delta if Jay is a hidden unit.  And there is no Delta if Jay is an input unit. You don't change the input units okay for you, could you could backtrack propagate all the way to the inputs and try and change them?  And it will see that later that that's been done. So we have a recursive definition of delta. We know what it is through the output units once we've computer dit for the output units, then we can compute it for the hidden layer below that  once we've computed them for that we can computers for the heading layer below that Etc.  So this the idea they have in mind I compute some Delta's for everybody hears me and trying to compute my Delta. I compute the Deltas of everybody on connected to you multiply them times the weight. So I'm running the weights backwards summing them up and then I put in the slope turn.  So we can't get rid of that.  Except if you make a mistake in your program.  Okay, so Paul Monroe and I back in the day with our 1986 or 87 autoencoder for images.  You know, it's it's kind of old nobody noticed though. Who does it now, but we did it. So one Kotor is a neural network that takes an input in and and in this case an image in so it's a bunch of pixel values.  And they were connected to Hidden units.  and and then they were connected to the output which was  again in image  and what were training it to do is just to reproduce its input on the output. That's why it's called an auto encoder because even though it's supervised learning.  Right, I'm using backprop. The input is the teacher. So it's really either self supervised learning or unsupervised learning and it's called in and coder because usually you have fewer units here than you have inputs. And so you're finding a compressed representation of the image and then you're blowing it up again. And so it's going to filter the image and not be quite as sharp as the original image, but that's an that's an auto in Kotor. Okay. Any questions about one another and coder is  Hell, yeah.  Why?  You know, they're used all over the place. It turns out but I mean suppose you wanted suppose. I'm up on Mars and I'm taking pictures of the Martians and I want to send it back to Earth and suppose for every picture. I have to shoot a rocket off. So it's really expensive if I just tell her that but these weights are these rights are which are usually kind of the inverse of these weights. Ezra Millard said, you didn't even try and reproduce What it sees.  Which is what I do if I was a hidden unit says rental heart and so it's output weights are probably going to mirror its inputs. Wait, so whatever pattern it's responding to its going to try and put that pattern add that pattern into the pixels that its output depending on how active it is right. There are clicker questions coming.  Okay.  Get get your clippers out. Okay, so what palm and Rowan I did was return this thing.  And we go to the PDP group meeting to present our results and we're kind of confused cuz here's the air.  And here's the training time.  And the air goes down.  But then it starts to go up again.  And it wasn't that our learning rate was too high or anyting and rummelhart to said there's a bug.  So we started our code for like an hour or two. And finally we realized that we hadn't put in the slope term for the hidden units. So backprop. It's kind of her best to programming yours. Okay. It's just kind of cool.  Okay.  So that's a little anecdote. I like to tell you.  It's the difference between the input and the output.  It said yeah. Yeah, this is T.  This is why.  And you some that up over all the outputs and we is logistic activation functions. But if you do this so that the hidden you in an Interlinear in the output unit 3 linear, which is what we should have done. Then this essentially does principal component analysis. If you know what that is, I'll tell you what that is later, but  Because it's trying to optimize the same thing as PCA as the square there. Yeah.  see  Yeah, I even programmed in bcpl installed bcpl and and there was the predecessor to see it had one day at a time word.  Yeah, okay. Alright, so what can backprop learn?  Okay, just just to be clear it was so if you're writing your back propped program, which we're going to make you actually Implement backdrop yourselves or not going to let you use some package. I taken and put front by get an outfit. I compare the output to the teacher and I get T minus y and I send that back here send that back here and then  I've now computed the gradient for all the weights I've computed.  Have computed the partial of Jay with suspected wi Jay. Okay, and  So, why is this cool? Why is this really really good in terms of efficiency?  How many people know how to numerically approximate a gradient or derivative anybody you do you do? Okay. What are you do so  I have this input I give you the input and Philip. I got an output.  And then I can compute the air.  And suppose in the middle of year. I've got a weight w i j and I want to check is my gradient, right? You can do a numerical approximation to the derivative.  By doing what will you add Epsilon or sorry you add Epsilon to the weight. So remember we're trying to go.  Downhill in the air so we have some error function in this is w i j and this is Jay.  Okay, I'm supposed I'm right here. This is the value of w i j right now if I add a little bit to that and compute the air I'll get that number. So that's adding Epsilon If I subtract Epsilon from that.  I can compute then I compute the error.  And then what's so that's the rise I can subtract that from this from that. That's the Run.  It stood out to Epsilon until I'm Computing linear approximation to the slope.  Okay, and if Epsilon is small enough, it's going to be pretty linear.  But I have to hold all the other weights in the network fixed while I do that.  Okay, so I have to do it for one way and that and I have to run everything all my patterns through the network to figure out the true grade in.  And then I pick another wait and do it again pick another wait and do it again. It's really going to be expensive because not only do I change this weight, but I have to visit all the other weights in the network in order to get the output. So it's going to be a border W Squared where W is the number weights.  So that's W squared.  Backprop is linear in W. You visit each wait once when you do this and you do it for all the weights at once, so that's really really efficient.  Okay, any questions about that?  questions  Okay, yeah.  First step of backpropagation. Yeah.  I don't mean that it is linear in the weights to learn some function. That would be really amazing.  Okay, so what is backprop learned? So you start out with some random Network?  And by the way, now we have to use random weights to start out cuz if we make them all 0 it doesn't really learn anything.  Depends on some things but you make it all zero if it's 10 H, then then the tan 8080 and nothing happens if you use the logistic logistic is .5 at 0. So at least you get some activation here which gives you some output here while doesn't give you any outfit there because if the weights are zero, but at least it's an input on that line. So this will be wrong in some way and you've got a Delta there and you've got a activation here. So even though the way to zero you can change it.  right, and now I start to get  some action gear and we had some inputs here and we can change those and but the problem is when you do that all the hidden unit compute the same function.  Yeah.  I'll talk more about that as time goes on.  So depending on where you start you're going to get different answers. So if you  Dude, so this particular one it had two big weights to here and this guy learned or and this guy learned and and then or just turned on the output every time but and just when it came on it at a big negative way to turn off the output. So that's one possible solution to ax or but in fact starting with a bunch of different initial random weight so you can get a bunch of solutions text or and some of them will surprise you right? It's not like what you would have thought of as a solution next door.  Okay, we have clicker questions. You're ready clickers at the ready.  Let's see have to start up iclicker.  Okay, start new session.  Okay.  Are you ready?  Okay.  Okay.  Well, send me you already voted. You haven't even seen the question yet. That's amazing. How did you know?  sorry, everybody thinks that my answers are alright already a  Broke, I can't seem to Advanced. There we go.  Okay.  Have advanced the sides now.  Okay, so the tldr of back rapid Rhino or TL DR  too long didn't read. Okay, the opposite of forward propagation learnt internal representations. It's a form of gradient descent.  It changes the activations to go downhill in the parameters.  What's the what's the best answer here? I can imagine you might pick.  Some other answer, but what's the best answer?  Okay, 73 of you have answered said all of you.  going  going  going  gone. Okay. So 68% of you got the right answer. Nobody said to be thankful e this time. Okay. So turn to your neighbor. You have a 7 in 10 chance that your neighbor knows the answer.  unless all the smart people sit together and  Why this clicker question is really funny apparently.  Okay, you settled down. Let's vote again. Try again.  Okay, okay.  Okay.  Keep going.  Okay, we went up 4%  preds progress. I don't know if it's the tistical a significantly better, but it is better.  Okay, I'm going to close it out going going going gone. Okay, what's the answer?  She is gradient I sent.  It's not gradient descent.  You have to read the answers.  before you answer  Okay.  Okay. Okay. Don't go I'll post on me. Yeah, and it changes the activations to go downhill know it changes the weights to go downhill in the error.  Okay.  It is kind of the opposite of forward propagation, but in five you thought that but the real answer.  Is that it learns internal representations or features like and and or the example, I just gave ya.  backprop learns  I'm sorry. Okay. Okay, it learns internal representations. Okay, just one more and then you can leave if you don't like this anymore. Okay, okay.  Computing the Deltas starts with the output their propagated backwards in the waiter changed starts at the output the weights are changed in the Deltas are propagated backwards uses the slope of the output unit uses the slope of the Hidden units A & D  Okay.  k  we're we're we're going good so far.  Mercy looking good  hey many of you are not answering the  be sure to catch question good.  Okay, I think that's almost everybody.  Anybody else when our way in okay going going going gone. The last minute went from 81% to 79% So now you have to okay. So how many people here have written down the right answer on a test erased it and then written down the wrong answer.  Everybody you're lying. Okay.  Okay, what's the answer glass?  Thank you. Okay, this is right. You started the output propagated backwards in the weights are changed, but it's not the best right?  We don't start and this is the second one is actually how Russell and norvig had brought back prop in volume 1 of artificial intelligence a modern approach. But again backprop is resilient to programming yours. So if you compute the Deltas at the output and then changed the weights and do the output and then propagate the Deltas back. Well, you're probably getting the Deltas.  Four different weights than you computed them and you're getting the wrong answer at the hidden units, but because you usually change them slowly it's still pretty much works. So it's another backprop is a resilient to programming errors. We know it doesn't use the slope of the Hidden units that does use the slope of the app, which does he use that so A and D.  Is the answer okay?  So this isn't working. Okay, that's not working.  Okay.  So why is this wonderful?  It's wonderful because it learns internal representations instead of having to on the weekend sit around and think of features.  It learns the features. It also learns internal representations and it learns internal representations. It's like real estate location location location. This is the big point of Bachrach. Okay, this is almost your Mantra and I quite it's a fishing and showed you that and the cool thing is that generalize has it turns out to recurrent networks, which is what our brains are.  Mary Kay  and we'll see how that works. Later.  So in the next K slides work is some small integer because we've only got 7 minutes. I'm going to look at various representation sit back prop has learned in problems that solved and again your mind right now back propped learns representations in the service of the tasks.  Hey remember that that's your mantra.  Okay. So here's a version of xor it's one of those ones. You wouldn't have thought of.  The number in in the unit is the bias. So think of that is what the unit likes to do in the absence of any other input and I should say that these are logistic functions all of them. So anything like for an above and the input pretty much turns is all the way on and anyone any input -4 turns it all the way off.  Okay. So what Boolean function is this network computing without to Hidden unit. So if you didn't have this, what's the outer part computing?  or  nor  Right, it's either or it's neither nor nor know it is nor is Computing door. Okay, very right. So why is it Computing nor this thing likes to be on in the absence of any input sonor means that it's going to be on 400.  401  this comes up and turns that down but  it's not enough. It's still on 1-0. This comes up and turns it off at night off enough. But if both of them come on, it turns off the output. So what is it? It's it's nand not nor it's neither or nor nor its name and that's hard to say.  Okay.  So that's nand.  Okay.  Okay, what is this guy doing?  It only turns on 400. It's got a hot. It's got a bias 2.2 is enough to turn on this guy.  In any if there's any input on it turns it off.  So what's what's that function called?  Nor okay. There we go. So by a combination of nand and nor it solves the problem.  Okay, so this is a score except for this guy and that's when this guy comes on and turns the output off.  Okay, so it's a lot like the or anansa Lucian but negated.  Okay, so that's one representation learned by backprop.  Not the features you think of first first off for a  here's an interesting one. That's a little mind-boggling.  So this is the input.  This is a hidden unit now to Hidden unit and this is the output. So this is Computing symmetry of its input.  So 11000 is not symmetrical. Is it symmetric around this? This is not symmetric.  So, what's what what's going on here?  the weights are in The Sims 3 2 - 6 2 + 12 - 3 + 6 - 12  - 12 + 6 - 3 + 3 - 6 + 12  What what's going on here? So  Yeah sort of. But okay. So so this would be symmetric if it was one one zero zero one one, right?  When if it was one one zero zero one one what would happen? Well, if this was on and that was on this puts in a -6 this puts in a plus-6, they cancel each other out and this hitting you likes to be off in the absence of any other input. This guy likes to be on so it's detecting asymmetries. It's going to say it's symmetric unless told otherwise  So if it doesn't get any input from these guys, which have big negative nines to this, so if either one of those comes on and it turns off the output, so these this network is a kind of asymmetry detector.  And so if the if things are symmetric, these guys are going to balance and they're going to balance down here as well. But if they're not symmetric.  Well, this is -3 + 6, so it's going to turn that one on.  they can't be like all threes because if they are all threes instead of -3 and 6, then these would balance have  so you have to have these guys get bigger and bigger in order to you know, suppose these guys somehow.  Outweighed this guy. Will they can't because 6 + 3 is 9 at cannot wear that guy. So it's got to be perfectly balanced. And that's the that's kind of the story about why they have to be in this ratio. You have to avoid combinations of the other ones set my  Mistakenly turn on or off a head unit.  Okay, so if it's 1 suppose, those are all zeros and this is 10. Well, it's got a big negative up to hear 1 x - 12 0 x + 12 to that guy's going to be off.  But this guy gets a positive and a negative 12. So this guy I'll turn on so this guy turns on when this is one zero this guy turns on when it's 01.  So again, it's an asymmetry detector.  But if it's symmetric all of these cancel each other if it's one if it was zero one zero zero one zero.  Let's get to -6 just get to + 6 they balance this gives a plus fix. This goes to -6 they balance.  So you can convince yourself. I hope that that solves the problem.  Okay.  Yeah.  Yeah.  Sorry.  know your their sigmoid  Sorry, I should have said that.  But for the next example, it's a good idea to think of them as perceptrons, but they have to be they have to be smooth so you can take derivatives.  Okay, here it is useful to think of these is perceptrons.  and  Again, this the solid lines all that this is doing a binary addition.  And so we're adding 0 and 1 2 1 1 and the output is 100. Okay, is that right?  I think it is. Okay. How does this work? Okay. Well if there's a one here then we wanted one in the ones place up there.  Or if there's a one over here, we want to win in the ones place over here.  Right. So these are solid + 1 lines, but if they're both on then what we have a carry.  And this guy only is an aunt of these two so it took Carrie detector. What is it? Do it turns off the guy that they're trying to turn on by having a big negative to wait and that likes to be off in the absence of any other input and it turns on the middle guy.  Okay.  And now what?  So if it turns on the middle guy, but there's a one here.  There should be a carry out of there.  Not you know and tell.  This guy says oh there's a carry into that place.  And so with all with any three of those on any two of those on will be it. Have a carry out of that place.  And it just said case if there is a carry out of that place it turns off that guy and turns on that guy.  How many layers is this network?  It has two hidden layers. This one is before that one. Although there skip connections here. It's  This is kind of the first hidden layer. That's the second admire, but their direct also directly connected to the internet this network.  Now what I didn't tell you I guess yet is that backprop has local Minima it can get stuck.  And not learn something.  And although if you have enough hidden units are a lot of ways around the block and hyperspace and most local minimum are okay, but this problem is soft is small enough that half the time it has a local minimum.  Can you guess why?  Time stop. Okay. Okay. Well what if this guy out of the place?  It's too late then for this guy to figure out if there's a carry out of the middle place.  Because it can't find out that there's a carry because it's Upstream of that guy.  doesn't find out that there was a carry so  half the time that guy learns to be the carry out of the first place and there are originally there are connections everywhere here. So for example, this guy isn't connected to that that just means it that way to learn to be zero.  Okay, but initially it was connected to everything and it could learn to carry out of that place and then the skies host. He can't be the carry out of the middle place cuz he doesn't have enough information.  Okay, it's 6:16. I have a plane at 7:40. So I'm going to save the good stuff for next week. Thank you. "
}