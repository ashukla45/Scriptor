{
    "Blurbs": {
        "1 and 1/4 to Binary classification problems. So we're giving a large set of input output examples. So we've got a bunch of pictures of handwritten A's and B's and a label that this scenario has to be. There are goal is to develop the classifier that assigns those class labels to new examples with minimal mistakes. I mean the reason we do I mean why if we have the ": [
            3151.8,
            3181.4,
            83
        ],
        "146 programming assignments to grade should be so much fun. And then the second assignment. I think we start to work in pairs by the end. You can work in groups of four. Okay, so that's the story. If you don't know anyone in the class Piazza has a find a partner option or feature that you can use to try and find people to work with. Okay. do to ": [
            1853.8,
            1891.7,
            49
        ],
        "8 a.m. So but 5 p.m. You have all day to get here. So get here on time and you know, maybe sleep here. I don't know but I often start the class with clicker questions. So and they're usually about last the last lecture and then during the lecture. I will throw in some clicker questions for To check your understanding and if I use a standard technique, which ": [
            1752.1,
            1789.4,
            46
        ],
        "90s and early 90s and then in 2013 or 2012 Jeff hinton's group published a paper where they used a very deep neural network one with many layers the process and it did 10% better than the next best computer vision program on the imagenet visual recognition challenge Evans net is a data set of 1.2 million images and 1000 categories in their job is to learn to classify things ": [
            1100.9,
            1146.9,
            27
        ],
        "Goldilocks tries to three bowls of soup. And when's too hot when it's too cold and one is just right and just sit and eat the soup. There are three chairs ones two little ones too big in one's just right. So this is the kind of Goldilocks probably want the just write polynomial. So we're overfitting. So what this is is the root mean squared error. So it's the ": [
            4238.2,
            4267.6,
            114
        ],
        "I hear we're going to use a line. So first order polynomial and that that's a little better but you know turns out this was actually a third order polynomial that it was generated from Soph. We fit it with a third order polynomial will get a pretty good fit. Okay, and that looks a lot more like the function. So better for using ninth order polynomial we can fit ": [
            4141.4,
            4169.9,
            111
        ],
        "I hope this course uses Clickers. Did you know that? Okay, go to clicker clickers will be worth about 5% of your grade. I'll drop the two lowest. I will throughout the term. I will be off and asking clicker questions at the beginning of class. You have no excuse for not getting here at 5 p.m. There were plenty of students who complain last year about getting here at ": [
            1718.5,
            1752.1,
            45
        ],
        "I open a eyes goal is to you know, do hey, I forgot I would as opposed to AI for evil. And yet they came out with the DOTA bot on just the week weekend before it's guy. I was I went to his car for the first time in decades last year cuz we had papers and The day in the week before it's guy was a GI, which ": [
            1558.7,
            1595.6,
            40
        ],
        "I'm going to go so I will be giving and it's during last week of class. So I'll be giving the last couple of lectures remotely from Montreal. I like over Skype or hang out or something like that. And I hope you will forgive me for that. But you know, I could sell this ticket I suppose for a lot of money, but I really want to go to ": [
            664.4,
            692.3,
            15
        ],
        "Julian McAuley teaches the data science course and set data mining and Etc and that's hugely popular. And you went to and talks. I came here from UCLA where he was in computer science, and he came here to join the cogsci department. That's how much she wanted to leave UCLA and come to UCSD is doing that the wrong way but anyway, so these are people that Teach good ": [
            2860.8,
            2898.4,
            76
        ],
        "Oh great. It's a great book, but then I read some of it. I mean the PCA explanation goes on for like a page of of equations with no insight. Anyway, but hopefully they'll get the revised it as time goes on. Okay programming don't put post your code on GitHub that that's we don't want other people to get your code and don't use code from GitHub. Okay, please ": [
            2049.0,
            2089.0,
            54
        ],
        "Okay Lang. Let's get started. Can you hear me? Can you hear me? check check check check check Check, okay. Seems to be working now. I think you know hear me. Okay, so this is Sia c190 still I hope someday it'll be CSE 1:53 today. When I want to do is go over the syllabus and give a brief introduction to pattern recognition. So Let's go see the syllabus ": [
            38.6,
            107.1,
            0
        ],
        "Okay. So my slides will be available online and I usually post the two versions one in PowerPoint and one in PDF so you don't have to buy PowerPoint. Okay. So this is the the book I really like is the 1995 book. There wasn't deep learning then but he does a really great job of introducing what he does introduced and I learned a lot. From that book and ": [
            1963.4,
            1999.4,
            52
        ],
        "Okay. So, let's see. Let's make this a little bigger. Orin bigoness is one of my students said so there is a th so take a few weeks before we get into deep learning as you probably have heard in the news and everywhere else deep learning has caused a revolution in computer vision. It's also Cause 2 Revolution and reinforcement learning. We cracked go last year go as an ": [
            247.9,
            290.3,
            4
        ],
        "Okay. What about this? stay Stay stay stay. So the only difference between those two or somebody inserted a gap artificially in the middle of and that's the difference between saying stay. So this is kind of a hard pattern recognition problem. But one of my Chinese grad student said that this was part of a test in China after you'd learned English yet at the beginning of the course. ": [
            3079.9,
            3117.1,
            81
        ],
        "Pitbull but just a little bit that's what makes him growl when you come in and then the rest he wags his tail after that soon. Okay, any last minute questions? Okay. Wow, that's slow. Okay. There are a lot of other courses on campus that have related content. I strongly recommend any course taught by Lawrence saw their son. Jerry does Gupta. They're both wonderful professors really good teachers ": [
            2809.1,
            2860.8,
            75
        ],
        "Russell norvig describe reinforcement learning as you're given you play a game where you're not told what the rules are you make about a hundred moves in your opponent says you lose and that's that's reinforcement learning in a nutshell. It's a little more complicated than that, but that's basically what it is. Okay, so we will talk a little bit about all three kinds of learning but mostly will ": [
            595.0,
            628.1,
            13
        ],
        "Singularity because it's it's in one domain the domain of go and and now that the systems are getting even better than the ones that beat us or beat the champions of go and there's nothing we don't have to be afraid of it. It doesn't even Doesn't even know it's playing go. It doesn't want to play go. It's just playing go and it doesn't know at all what ": [
            369.3,
            409.9,
            7
        ],
        "So maybe all the males are over here and all the females are over there Etc that allows you to put down a linear classifier. So in one day we have a threshold and 2D we have a line and in 3D, we have a plane and after that we have a hyperplane and your intuitions about things in in high Dimensions go to hell after 5. So after five ": [
            3633.8,
            3669.4,
            97
        ],
        "So sum of squared are the other thing about squared error is really it's euclidean distance, right? If you just took the square root you get the eclipse in distance. So you're just trying to minimize the distance between your estimate and the data. And so squared error is the sum over all the examples of the output of your system which in this case going to be a polynomial ": [
            4042.3,
            4068.8,
            108
        ],
        "Some of the year that we're going to overshoot. Sometimes you're going to undershoot. So some of the errors are going to be positive. Some of them are going to be negative. The nice thing about squaring the Earth is now they're all positive and we can try and minimize that and it especially penalizes figures, right? So here's an example of regression. We're giving this input. We're trying to ": [
            3941.5,
            3973.1,
            105
        ],
        "Some optional this reading is all optional. There's lots of courses out there and neural networks and deep learning on the web some of them lecture better than I do. So you might want to look at those. And then the third week will get to backpropagation learning which is it was invented in the 50s by Robinson Monroe. It's a special case of their procedure. But the version that ": [
            901.5,
            935.7,
            21
        ],
        "That's just based on one number and we're going to pick a a threshold here somewhere like here where we get to minimize. The number of errors were going to make okay. So that's that's a class fire and I'll make you know, this many mistakes and eat anybody that's over the threshold in the wrong category or under the threshold in the moron category will make mistakes. So they're ": [
            3384.7,
            3418.2,
            90
        ],
        "They couldn't hear the difference between these two, but by the end they had to to pass I guess. Okay, so here's a simpler example detecting age from bees. So here's to a handwritten a n a n a b and so we have to take in those pixels. Those are very simple images and the output is a class label either class 1 or 2. Sometimes we'll use - ": [
            3117.1,
            3151.8,
            82
        ],
        "Vector calculus and some probability and statistics. Here's a couple of chapters to read with your really poorly written in the Deep learning book, which is available online or you could read a good book Chris Bishops 95 neural networks for pattern recognition books this lecture today that I'm going to give his based on his first chapter in that book. So we're going to do machine learning gradient descent. ": [
            859.8,
            898.9,
            20
        ],
        "a I could try and sneak and pull the plug out of the robot. But you know if they're super smart they're going to think of that too. So you have to have some way to make them not too worried about being turned off and he gave us a lecture about a solution to the off switch problem. So there's a begun to be some work on beneficial a ": [
            1528.2,
            1558.7,
            39
        ],
        "a PhD, you know that they just stimulus-response stimulus-response. So it had no idea what it was doing that it was doing it very well and and it beat the DOTA Champion within a couple weeks regular people were beating it and discovered its weaknesses, but eventually it's not going to have those weaknesses so we should be worried. I'm worried but But like I said, I probably be dead ": [
            1656.8,
            1688.8,
            43
        ],
        "a a surface in a multidimensional space. So if you take all these pictures of faces, for example, there's a gazillion different possible images, right but the faces The Story Goes lie on some lower dimensional manifold. Where is he move along? So here's 3D we could have a surface in 3D and as you move along that surface, you get different faces and hopefully there's some organization to it. ": [
            3602.3,
            3633.8,
            96
        ],
        "a few broken all of your arms or something equally bad. Okay, don't cheat working in pairs and trios en cuadros and the Machine on the programming problems and sometimes required but if there's a written part of the homework, you should do that individually. So you should all individually turn in the written part of the homework. You can talk to each other about the problem, but you have ": [
            2257.7,
            2291.8,
            60
        ],
        "all of the lectures for the class except maybe this one are already up on Piazza from last year. So they your actual lectures will vary but if you want to preview what's going on in the class that day the lectures are already available and then I will usually post them after class for the actual luxury received because I'm always updating them at the last minute like today. ": [
            1927.5,
            1961.5,
            51
        ],
        "ancient Chinese game on a 19 by 19 board that we didn't expect to be solved in the sense of beating a human until about 10 years from now, but It actually was cracked last year and now the computer programs that play go or are fighting it out for Supremacy. And the most recent version is was based on no human knowledge at all alphago zero and it was ": [
            290.3,
            328.7,
            5
        ],
        "and I hope that we'll be able to have office hours every day of the week so you can always get help and we hope you will use Piazza to ask questions. And unlike most professors. I actually put my cell phone number on the in the service so you can text me if you're not getting your questions answered on Piazza and I'll give give the Tas holy hell. ": [
            212.2,
            246.9,
            3
        ],
        "and T. These are Target's T4 Target so we can take the derivative Saturday equal to 0 and with a polynomial we get a closed form solution. So here is trying to fit this data with oh, actually I lied this isn't our function approximator. This is the actual underlying function that this theater was taken from so we just took this polynomial and added some gaussian noise with 0 ": [
            4068.8,
            4105.5,
            109
        ],
        "and if it's still bad then we talk about it. Okay. All right. So 5% for that evenly distributed between performance and participation. So if you push the button you get half your points if you push the button get the right answer you get the other half, so that's that's Clickers. Okay, and then there for programming assignments, the first one will be on an individual basis will have ": [
            1815.2,
            1853.8,
            48
        ],
        "and that aliens were manufacturing everything that I see and when I there's nothing really outside that door, but when I open it, they quickly like created a new reality for me out there. That's when I process the other is that the world is real some people think that we're all in a simulation and that's more likely than not I don't believe that but anyway, which is a ": [
            4643.3,
            4674.6,
            125
        ],
        "and you would like me to not bring Wally. You can tell me privately because if you tell me publicly you'll be very unpopular. Okay, so that's alright. That's Wally. Okay, anything else? He has not a lab. He's a lab imposter. Although my wife says he never said he was a lamb and he's got some German Shepherd some golden retrievers some Mastiff and some Staffordshire Bull Terrier, that's ": [
            2769.3,
            2809.1,
            74
        ],
        "answers to all these pictures do we want to do this the answers we want to generalize the new examples. So, you know, if even though it's supervised learning we wanted to learn a general rule so that given examples from the same distribution does a good job. So when problem with this is is high dimensionality. How many dimensions are there in a picture? 2 as many as you ": [
            3181.4,
            3217.7,
            84
        ],
        "are the training set. The green line is the output of the polynomial where we've learned. We we not just we don't just have X we have built-in features. We have x squared we have access to the end etcetera and what we're trying to do is fit these dummies. These weights these multipliers on the different features, so it's linear in that. and so we need some lost function. ": [
            4009.4,
            4042.3,
            107
        ],
        "attic. You're probably going to have a bigger house. If you have a bigger salary, right and now given a new example of some salary. We're going to go up here to this line and say oh you're how sizes around that okay, that's linear regression. So now the output is not a category or probability of being in some category. It's a real number or continuous value or continuous ": [
            3781.8,
            3812.6,
            101
        ],
        "be talkin about supervised learning. Sore after taking this class you should be able to understand about half of a nips paper nips has the neural information processing systems conference. It's the premier machine learning conference this year the registrations for nips sold out in under 11 minutes. I was considered one of the good reviewers and so I got a ticket and they gave me a free registration. So ": [
            628.1,
            664.4,
            14
        ],
        "because it's commonly used in neural-net land. Okay. Hey any questions about that? Okay. So best way to deal with overfitting is to get more data or make more data and that's actually kind of subfield. Now of machine learning is how to take your training set and make it bigger. So if you're given a bunch of pictures of people and you're trying to recognize her face as you ": [
            4567.8,
            4602.8,
            123
        ],
        "before that happened. So okay prereqs. You should know some linear algebra some Vector calculus and probability and you should have good programming skills as what we want you to do your first couple of programming assignments in hopefully, we'll be able to do it in such a way that we can use an auto greater with your programs. We don't we haven't figured that out yet. But we will ": [
            1688.8,
            1718.5,
            44
        ],
        "better than Berkeley, but but they were second-best in the UC system and There's a lot of research going on here and you should try and get involved. Especially if you want to go to grad school. So there are 199. Swear. You do research with a professor and its nowadays. You have to look like somebody who is looking for a job 10 years ago, you know having a ": [
            1428.1,
            1463.9,
            36
        ],
        "by MIT undergraduate David Parker who called it learning logic and patented it and it was also discovered by Young laocoon and his PhD thesis in France. So it was kind of the Zeitgeist at that point and they were at a nature paper about it. That is rummelhart Hinton and Williams did I'll be posting chapter 8 if it's not already on your website, which is the chapter in ": [
            996.5,
            1032.0,
            24
        ],
        "by the end of this class. Hopefully by the third week and and so it was really cool. Everybody was very excited cuz here was this thing that really solved a bunch of problems, but Couldn't solve really hard problems because there were relatively shallow networks back then with one or two layers of features that were learned. And so other techniques became more popular in the in the mid ": [
            1064.9,
            1100.9,
            26
        ],
        "category that you're training it on unsupervised. So you tell them that work exactly what you wanted to do. Every time unsupervised learning is where you're trying to learn something about the statistics of the data clustering is an example of that and you can use neural networks to clustered data. Just learning the probability distribution over the data is an example. We don't we'll do some of that with ": [
            528.1,
            563.9,
            11
        ],
        "conference or something. Let us know in advance and we'll try and accommodate you by either giving you the exam before you leave or after you get back. Okay. And again, we're going to answer your questions on Piazza and the timely manner. I hope if and there are lots of online resources for these these things where they you know, I think there's it's really good to to see ": [
            2426.6,
            2458.9,
            65
        ],
        "could take each one and rotated a little bitter and big in it or in smaller it and and now you've multiplied your data by a large number. So that's manufacturing more data. Another way no is to use Occam's razor. So William of ockham a long time ago said the simplest if we're given a choice between two hypotheses like it could be that you guys aren't really here ": [
            4602.8,
            4643.3,
            124
        ],
        "courses, okay. So today I'm just going to give a really sort of fast overview of pattern recognition. This is all based on the first chapter of Bishop. So here's a standard example. Mnist. This is what Jeff Hinton tends to try out his new ideas on all the time at dataset of handwritten digits are 60,000 examples in the training Sutton 10,000 and the test set And some of ": [
            2898.4,
            2939.9,
            77
        ],
        "deep learning models. And you can see where we're having a steady pace of programming assignments due and programming assignments handed out and then we talked about recurrent Network's most rural networks. Network. So they taken an input they get an output. That's the sound the activation makes as it hits the output layer, but if you want something to think or to process things over time you need recurrence ": [
            1323.9,
            1360.4,
            33
        ],
        "deep learning or at least I'll tell you about it. And then the Third Kind of learning is reinforcement learning, which is mostly how we learn. We try stuff out and see if it works or not in reinforcement learning you are not told what to do at every step but you have a sequence of steps. Usually that you have to do and then you just get feedback. So ": [
            563.9,
            593.3,
            12
        ],
        "dimensions, for example, you all know where the Galaxy and distribution is Right looks like this into DA got a bump. I am 3D together kind of sphere after 5D a gaussian distribution. All of the density is in the thin layer and it's in a shell around the center. There's practically nothing at the average. Okay, it's all in the thin shell and if you take if you were ": [
            3669.4,
            3705.8,
            98
        ],
        "do all 26 letters, we need a lot more features and what you want then is some sort of function that gives you the probability that something's in a category and so here's a function we have if we had 26 letters we have 26 of these functions the taken the input and have some parameters or weights here for each function and its output is some probability of being ": [
            3484.9,
            3522.1,
            93
        ],
        "do to do Yeah, there has been studies now is that people who have their laptops open during class get lower grades than people that don't. The people are actually have a a writing down stuff. So I encourage you to not have open laptops during class. You'll get a better grade. It's very distracting and you were like trying to type while you're trying to watching stuff like that ": [
            1891.7,
            1927.5,
            50
        ],
        "doing that is called cross-validation. In fact, we could take 10% Outfitter polynomial take a different 10% outfit are polynomial and see how and do that 10 times get an average of the error on those held at examples and then that's our estimate of how well we're going to do a new examples. We've never seen before. So that's called crotch validation and it's something you should know about ": [
            4533.1,
            4567.8,
            122
        ],
        "down before we had his one day. We had a single number the threshold hurts to day. We want to find a line that separates these two categories with the minimum number of mistakes will still make a couple mistakes here, but will make fewer mistakes. And and so this is called a linear classifier or a just use the line is your decision boundary. But if we have to ": [
            3445.7,
            3484.9,
            92
        ],
        "everything. his on this Piazza page So we have a Piazza page for CSE 190. There is no separate page for the chorus. This is it just the Piazza page so you can go there and I assume most of you are familiar with Piazza and you can sign yourselves up if you're not there already or soon anyway. So this syllabus is there. Okay. MC sir full page view ": [
            107.1,
            164.1,
            1
        ],
        "exactly fitting every point and now but now we're going to generalize poorly. So this is the kind of Goldilocks problem in the interest of diversity and inclusion explain what Goldilocks has to those you don't have the same cultural background Goldilocks and the Three Bears. The Three Bears are out having a picnic or something or they ran away. I don't know because they're soup on the table and ": [
            4206.6,
            4238.2,
            113
        ],
        "few papers would get you an assistant professor job now having a few papers might get you into grad school. So having papers while you're still an undergrad is something you want to aim for. Okay, and I usually finish up with a lecture on ethics of AI. This has become an important area as we worry about the singularity. If I asked a robot to make me a cup ": [
            1463.9,
            1498.1,
            37
        ],
        "for Okay, this is really annoying. We're going to use pytorch for two of the programming assignments. It's at one of the deep learning platforms. So and we would like you to do your assignments in in Python and numpy for the first couple and then pytorch for the second couple. So today we're giving a kind of an overview of the course fuck you Okay. Yes. Well anyway, I ": [
            723.8,
            778.7,
            17
        ],
        "getting One either. I can't remember now. It's either 1.2 million or $800,000 a year from openai. So you should be interested in this if you care at all about money. Okay? So this is the kind of stuff that you will help you get a job at Google and so hopefully you'll keep up your interest over the quarter to spite my monotone. Okay, so so we're going to ": [
            1208.2,
            1250.1,
            30
        ],
        "going to find the part of the space where all the A's are in kind of drop box around it. Right that would be a very high dimensional box and so classifiers have a problem with this dimensionality. So usually what we try and do is extract features. From the images that are smaller than the dimension of the original images. So when idea is we could use the ratio ": [
            3290.3,
            3321.8,
            87
        ],
        "good most of you. Okay, how about logistic regression good softmax in multinomial regression. Okay. Sorry, you're going to learn some stuff then good. How about gradient descent? Okay, good. Alright, you guys help the other guys who knows great again, raise your hands and everybody else. Look at who's got their hands raised. Ask them for help. Okay. Yeah for this course, you should know some linear algebra and ": [
            817.4,
            859.8,
            19
        ],
        "have pixels, most people think pictures are two-dimensional. That's not true. You have H pictures of point in a very high dimensional space. So think of one axis is the value on Pixel one. Another access is the value on Pixel 2 to pixel 3, and if you have a 6 megapixel camera, you've got six million dimensions of an image. So this is really high dimensional. So Even a ": [
            3217.7,
            3253.5,
            85
        ],
        "hope you are not too offended by my use of the English language. But anyway, I'm going to pass a programming assignment 1. This weekend so we get started right away. It's going to involve some written homework and some programming. And in this first week zero and one today and next week, we'll talk about linear regression logistic regression and multinomial regression. Who knows what linear regression is already ": [
            778.7,
            817.4,
            18
        ],
        "in that category and then we just finds the one that has the maximum output and will decide that that's the category. So we use an ARG Max ARG Max is a funny function that says, okay overall the Jays what's the maximum value here for return J, which is the index of the maximum value. So that's called Hard Knocks. So feature extraction is a way to deal with ": [
            3522.1,
            3560.4,
            94
        ],
        "in the back and the woman at the shelter. So the women at the shelter had the name 55 puppies and the day and Wally and his sister Wilma and his brother window or in the W's He is 12 years old. So is that old man? He's in his late 80s early 90s, you can open the door to the computer science building by nose ring the handicap button ": [
            2711.4,
            2738.8,
            72
        ],
        "into those thousand categories, and they didn't 10% better than the best. Computer vision algorithm before that and it was a real eye-opener for the computer vision community and everybody's switch to using deep learning after that. So there were a few tricks that a few things that made this possible and we'll learn some of those like first of all a large amount of data was available to train ": [
            1146.9,
            1176.7,
            28
        ],
        "is a kind of function approximation problem. So in the real world, usually the data were trying to fit has some underlying regularity and some noise. He could be measurement noise or actual noise. Like when you're trying to get a phone number in a bar. It could be gaussian pixel noise doesn't matter whether it's auditory or visual. We still call it noise. And so you're trying to approximate ": [
            3849.5,
            3887.7,
            103
        ],
        "is artificial general intelligence. So a disco is very narrow intelligence. But an AI that walks talks crawls in his belly like a reptile and can adapt to new situations is some more general intelligence like us, so I went to a GI because I figured if anybody was going to create the Terminator or Commander Data would be them and it was so weak of in computable math and ": [
            1595.6,
            1630.0,
            41
        ],
        "is if 80% of you don't get the question, right then I let you talk to your neighbor for a couple of minutes and discuss the question and then we try again and the second one counts if that happens and so this a little bit of peer instruction there. So you get to talk to your neighbors and and figure out what you what you didn't understand. I hope ": [
            1789.4,
            1815.2,
            47
        ],
        "it's available. You can Google it and find it online. It's also in the Piazza page in PDF format. Also probably you can also get 50 cognitive science. One to computer science salary, that's the right way to do that. Okay, so, okay and then the Deep learning book came out last year. It's all online. Unfortunately in Goodfellow has no idea how to explain things to people. Everybody said. ": [
            1999.4,
            2049.0,
            53
        ],
        "it's doing. But when they figure out what they're up to then then we have to worry. Okay, so we're going to be talking about pattern recognition mostly by the end of the course. I'll talk a little bit about hello. Oh, I know what I need to do. I need the score again. Somewhere in this hello pack for a trip. I'm getting on a red-eye tonight. Still have ": [
            409.9,
            452.2,
            8
        ],
        "know, it linear regression is important. And you try and draw a line through them such that those points are is close to line as possible. So we want the line to be in such a location that it's possible. And then what we're going to do is say maybe we want to learn some relationship like this is salary. And this is house size. Unless you're in it this ": [
            3748.2,
            3781.8,
            100
        ],
        "learn about backdrop and why rummelhart Hitman Williams were wrong in the way that they derived it. They use the wrong objective function and injective function is something you're trying to minimize like squared error, and they use squared error when they should have been using cross entropy. So we'll learn about those and we'll actually drive squared are in Cross entropy from Maximum likelihood. Which is pretty spooky. And ": [
            1250.1,
            1286.5,
            31
        ],
        "like you're being harassed or discriminated against please come and tell us and we will deal with it. Okay. If you're a student with a disability, there's usually one or two and every class. Please go to the office for students with disabilities and get your accommodation paper the so-called AFA and give me a copy and we'll make arrangements for you. You don't have to announce in class that ": [
            2545.5,
            2583.8,
            68
        ],
        "little image of 256 by 256 with 8-bit pixels the inputs are then of dimensionality 65k and there are approximately 10 to the hundred fifty-eight thousand different possible images. For all the different pixel values most of them will be garbage but there will be a small number that correspond days and bees will small and so that's called The Curse of dimensionality, you know, you could say, okay. I'm ": [
            3253.5,
            3290.3,
            86
        ],
        "match these up with these Target values and with a neural that we can do a nonlinear curve fitting. So not just linear regression, but nonlinear regression. But a common example of this is to use a polynomial and we're not going to use any polynomials have to worry about that. But it's a great way to illustrate some of the issues. So here's a training set. The blue circles ": [
            3973.1,
            4009.4,
            106
        ],
        "mean right now we have neural Nets with millions of parameters. And so they going to Tender overfit then what you need to do something about that and that this is just a motivating example. We're not going to be fitting any polynomials here. Did I answer your question? How would you know? Well there's a good answer to that? Say this is my training set I take about 10% ": [
            4460.5,
            4501.0,
            120
        ],
        "mean to it so that the green line is actually the truth. Okay, and so the red line now is our function approximator and if we use a03 order polynomial we get a line doesn't fit very well. If we use our first order polynomial. Well, it's not just a line at the constant value. We just turned approximated probably with its average using its average will minimize squared error. ": [
            4105.5,
            4140.1,
            110
        ],
        "midnight until midnight the next day you can turn it in for 90% credit after that midnight until midnight of the next day. You can be turn in for 50% credit and after that have you got bupkis so we give you more flexible? Were really great people you're going to really love me at the end. No, I don't know. Yeah, so the only exceptions to that rule be ": [
            2224.6,
            2257.7,
            59
        ],
        "multiple ways of explaining things and there's a lot of people on the Weber do really good job of explaining things in different ways. Okay. Yeah and Right, and if you are not getting your answers on Piazza fast enough, let me know. Text me I have 22,000 unread emails emailing me is an hour unless you also text me and say please read my email. Okay. We are committed ": [
            2458.9,
            2504.8,
            66
        ],
        "night. How do you diss? Yeah, I mean if if the data was generated by a 10th or 11th order polynomial instead of a third order polynomial then the 9th order polynomial under fit. Right. So this is just a manufactured example. We we just made it up for training purposes of you. And so it's just giving you an intuition that if your neuron that is too powerful. I ": [
            4419.2,
            4460.5,
            119
        ],
        "nips for once. It's been a few years. Anyway, after taking this course you should be able to at least understand some of what the words are in a nips conference favor. You should understand how backpropagation Learning Works know what a convolutional neural network is Noah long short term memory is which is a really weird name and have a working knowledge of pytorch. We're going to use pytorch ": [
            692.3,
            723.8,
            16
        ],
        "of coffee. It doesn't want to be turned off because it can't make a cup of coffee if it's dead. And so it's going to resist being turned off that's called the off switch problem and Stuart Russell, one of the co-authors of the best AI book we have Russell and norvig gave it talkit. Did Sky last year in Skies the international joint conference on a ion provably beneficial ": [
            1498.1,
            1528.2,
            38
        ],
        "of height to width of these as a feature. Okay. So the bees are going to have a higher ratio of height to width in the A's are so freaking some out like put a bounding box around these days we can figure that out. So here's a hypothetical distribution of values from the training set. So let's see. This is category a here in category B wear. These are ": [
            3321.8,
            3353.5,
            88
        ],
        "of these examples and I don't use them to train. Then I see how well it does on those 10 examples. It wasn't trained on those 10 examples are going to stand in for new data that we've never seen before the model is not seen those before until we get an estimate that way of how well it'll generalized to new data. So that's called a holdout set and ": [
            4501.0,
            4533.1,
            121
        ],
        "or jumping up on it. And after awhile one day I would get him to do that. And after while one day just did it himself to get outside quickly. So he'll do it on his own going out not so much coming in. Muncie Please don't feed him but he is friendly, you know, you can pet him all you want. He doesn't fight if you're scared of dogs ": [
            2738.8,
            2769.3,
            73
        ],
        "our ninth order polynomial to it fit the data so it starts to look better. Okay, but if we have a hundred. Examples now or 9th order polynomial is almost just right. Okay. So getting more data is your first defense. So we can get more data or we can manufacture more data. So it's ya. How did how did I what? order polynomials, right and then and then good ": [
            4361.2,
            4419.2,
            118
        ],
        "penalized. Okay, so I'm trying to make that smaller. and so now or function our objective function is the squared error as well as minimizing the weights got one more minute. So here's our ninth order polynomial was still 10 data points, but we've added regularisation to it. And that made the weight smaller if we make it too big if we make the regularisation too big. I'll just flatten ": [
            4748.6,
            4789.8,
            128
        ],
        "phds. So actually if you want to work in the really good stuff. You should get a PhD and And you laugh but you know there been there's a lot of research going on at UCSD. We we be we're better than UCLA have been for quite a while but it takes a while for reputation to catch up with reality. So you were lucky to be here. We're not ": [
            1398.6,
            1428.1,
            35
        ],
        "players and and yeah, so but mostly we're going to talk about What we call supervised learning so there's generally machine learning is broken up into three categories supervised learning where you give the network a bunch of input output examples. And you say you know, this is that this is that this is that and now you hope that it'll generalized to new examples of the category that you ": [
            495.3,
            528.1,
            10
        ],
        "problem of taking our to we have now we have two strong a model. It can perfectly fit the data and we want to make it as simple as possible. Well, when way to help is to just get more data and that's one solution. We use a lot of neural Nets we try and find more data and now we can take if we get 15 examples and fit ": [
            4333.8,
            4361.2,
            117
        ],
        "simpler. I pasta swell no aliens. So to apply this to a model when way to think about simpler means that the parameters are fewer or smaller and so we can Apply this to a model where we not just try and minimize the error. Our new objective function is Jay it minimizes the error but it also has this term which corresponds to how complicated the model is and ": [
            4674.6,
            4711.9,
            126
        ],
        "so that's when we'll be talking about objective functions and appropriate activation functions for each one. Pa2 will be due in October 23rd and Pa 3lb handed out and then we'll have a midterm this brackets my birthday by the way presidents are not required. Or accepted okay, and then we'll talk about some more tricks and finally some convolutional networks Richard the kinds of networks are used in these ": [
            1286.5,
            1323.9,
            32
        ],
        "speeding ones that were trained on some human knowledge first. So we've reached what I would call a micro Singularity. So the singularity is when the ai's get smarter than us and then start the design themselves to be even smarter. Luckily. I'll be dead when that happens, but this is something that You will have to deal with in your lifetimes. I imagine. So I call this a micro ": [
            328.7,
            369.3,
            6
        ],
        "square root of the of the squared error and the the average of it. This is how well you're doing on the training set. The Blue Line. The red line is how well you do on the test set and everything goes along pretty well until you get a ninth order polynomial which perfectly fits the data in the air gets really big. So that's overfitting. So each one of ": [
            4267.6,
            4298.6,
            115
        ],
        "still going to be a lot of mistakes. So second idea is to come up with another feature called X2. I don't know what that feature is, but maybe poking us or something and has a big line where the B is. Now, we've got two features and then you can think of that as a two-dimensional feature space. Okay, and what we want to do is put a line ": [
            3418.2,
            3445.7,
            91
        ],
        "sure about that set right anybody now. I think they do it but Anyway, you can ask questions on Piazza if you know, so that's a synchronous or you can come to our office hours and talk to us about things and hopefully we won't actually just give you the answer but will help you come to it. If you have to miss an exam like you're going to a ": [
            2397.0,
            2426.6,
            64
        ],
        "taking 253 instead of 190, I'd be making you prove that this week. So be glad you're not into 53. Okay. So, you know one one goal of say unsupervised learning is to find that manifold to learn a lower dimensional representation of the data that then allows you to classify it more easily. Okay. I'm so those are classification problems. And that's in contrast to regression problems. Most you ": [
            3705.8,
            3748.2,
            99
        ],
        "that chapter 4, I didn't do that. That's not interesting chapter 5. I did that chapter 6 so chapter 4 was backpropagation. So Paul we're both dropped it for a while. yeah, and then it was rediscovered in 1985 here at UCSD in the psychology department by Dave rummelhart, Jeff Hinton and Ron Williams and they really fig knew what they had at that point. And it was also discovered ": [
            966.1,
            996.5,
            23
        ],
        "that is you need activation like your brain to propagate around the network and and make inferences. Then we'll squeeze and some reinforcement learning and some recent papers. This is a hard course to teach because sometimes I'm teaching things that happened a couple of weeks ago Google deepmind has three floors to the floors have a hundred phds each. There's a third floor ready to take in another hundred ": [
            1360.4,
            1398.6,
            34
        ],
        "the data exactly but you know, we'll make some really big mistakes that go off the page here in between the values. So it doesn't generalize. Well, this one generalized as well. This one is called underfitting. Your model is too simple to fit the data. This is just right and this is overfitting. So that's called over that. We've learned too much about the data. the such that were ": [
            4169.9,
            4206.6,
            112
        ],
        "the like a kind of histogram. So in each box of correspondences some value of the height versus the with NSF gets bigger. We getting more and more bees. But there's some overlap we're going to make some mistakes and that's always going to happen. And so if you look at the distribution is a histogram like yours the A's and here's the bees then we have a class fire. ": [
            3353.5,
            3384.7,
            89
        ],
        "the neural networks. Gpus made training possible it took Helia subscriber 2 weeks on his see if he used to train the network that he used for that and some tricks about how to do gradient descent when you're going through a lot of variables and those are the main main things that made it possible. So now everybody and their brother is doing deep learning Aaliyah subscriber, I think ": [
            1176.7,
            1208.2,
            29
        ],
        "the whole thing. And so this is without any regularization. This is ": [
            4789.8,
            4799.7,
            129
        ],
        "them are actually ambiguous and somebody asked the post office one day. Will what do you do? If you can't read the ZIP code they look at the city. Okay, sorry facial expression identity. This is work. I've done this is JJ from the pictures of facial affect dataset here. He's being what hangry disgusted that's neutral. surprise happy happy sweat scared Yeah, so JJ is the guys pictures are ": [
            2939.9,
            2994.5,
            78
        ],
        "there Okay. Alright. So this course covers neural networks. We start out with shallow and that works. We do linear talk briefly about linear regression and perceptrons and logistic regression and multinomial regression. And if you don't know what any of those terms means that's good because you're wearing something today. Mostly I'm just going to be introducing the course we've got five ta and a tutor and Jenny hammer ": [
            164.1,
            212.2,
            2
        ],
        "these corresponds to a model of the data has been trained V of parameters with different orders of the polynomial. And these are the coefficients of the polynomial for w0 W1 W2 etcetera and notice when we get to 9 we get these crazy numbers write something you can just look at those and say it something's wrong. So, how can we deal with this? This is called regularisation the ": [
            4298.6,
            4333.8,
            116
        ],
        "this curse of dimensionality. It reduces the size of the training set because now we don't just taken an image which is all these pixels and is high-dimensional now. We just taken two numbers X1 and X2 and train the system on that. The idea is that this data actually lies on some lower dimensional manifold of the ambient space. So, who knows what a manifold is. It's it's like ": [
            3560.4,
            3602.3,
            95
        ],
        "this data and there's going to be some watch some difference between what you want and what you get like squared error. So why do we use Square there anybody now? Yep, if we didn't use so trying to punish higher-value. So the if the error is squared it gets really big really quickly. Not differentiable Park. Yeah, so it's differentiable. That's nice. So if if we just used error ": [
            3887.7,
            3940.1,
            104
        ],
        "this is Wally Wally. He is a hurricane Katrina rescue dog, which means she was one of the puppies that were born after the hurricane when the dogs got out and do with dogs 2 there were a lot of puppies and they ship them all over the country. So Wally came on the love train from Southern Mississippi. The love train was actually a tractor-trailer truck with 55 cages ": [
            2679.1,
            2711.4,
            71
        ],
        "to dig through all my stuff here and find my charger which is not coming up. somewhere in here It is. Okay. I think it's just stopping cuz it's trying to save battery. Okay. So anyway. Where was I? Yeah, by the end of the course will start talking about reinforcement learning witches. Where are some of these go players and DOTA players and things like that are Atari game ": [
            452.2,
            495.3,
            9
        ],
        "to diversity and inclusion and we want to make you feel more included here. If you don't feel that way and we'll do what we can to help. It's important their communities clubs on campus for all kinds of people those people can those clubs can make you could give you a community that become part of okay. So we're going to be inclusive. Yeah, if you if you feel ": [
            2504.8,
            2545.5,
            67
        ],
        "to follow the Gilligan's Island role that Pat Diamond coined in 1986. Actually, he he said he got it from somebody that Mighty but the rule is you can't take any written notes during discussing the problems and you have to watch an hour of some insipid TV show. Before you write anything down. So Gilligan's Island being the canonical insipid TV show guests orders Housewives of New Jersey and ": [
            2291.8,
            2330.9,
            61
        ],
        "up to Facebook apparently has some number of billions of faces that they train their systems on this dataset had I think a hundred and ninety two faces. I don't have Let's see if I can get this to work. I don't know. Let's see if this works. Nope playing but it's not coming in. Okay. Let's see what happens. Okay. Say anywhere that at all. What's it say say? ": [
            3031.2,
            3079.9,
            80
        ],
        "used in all the psychology studies cuz almost everybody agrees on his facial expressions. But you know suppose you have to recognize this guy as JJ. So all of those expressions are noise with respect identity. Likewise if you have to recognize Expressions, the identity is noise. She don't you want to recognize the same expression across the bunch of people. So that's another kind of categorization problem. Now, we're ": [
            2994.5,
            3031.2,
            79
        ],
        "values because you could have multiple regression. But both of those are examples of function approximation. What a neural net is is a function approximator at least a feed-forward one you give it an input folk you got an output what happens in between is f the function so you take Epi vaccine yet wise and output and in classification, we want the probability of a class membership often which ": [
            3812.6,
            3849.5,
            102
        ],
        "vaporware. And so I stop being worried for a while and then that weekend adult about came out in this thing was like You know killing people right and left and it had no idea what it was doing, but it was certainly getting rewarded for doing it and you know, just like the the go player, you know, all these are all like amoebas and just very amoebas with ": [
            1630.0,
            1656.8,
            42
        ],
        "we can use an auto grader with jupyter notebook. So that's as to the particular version of python. Yeah, 10.0 is great. I just made that up. So yeah, well we'll get back to you on that one. Okay other questions. Okay. Just checking there's one ta and here and Mal. Are you there? You are remember that question? Okay. This is an MO is the new ta wave yeah, ": [
            2630.1,
            2679.1,
            70
        ],
        "we have some Lambda on that sand multiplier. It says how much we care about that. And that's called regularization. So this is a function that Dave remmler came up with if you're trying to minimize the dysfunction, I won't go into that right now. Typically what we do is try and minimize. So we're trying to make the wait small and if they get really big they get really ": [
            4711.9,
            4748.6,
            127
        ],
        "we we so I want you to really understand how it works. I guess I'm kind of old school that way. But that's how it goes. Okay search for teammates feature. You'll have to find partners for the last few assignments and that's going to happen in the Work World 2 so you should get used to it. I said most of that. Okay. Yeah numpy is really good because ": [
            2120.8,
            2158.8,
            56
        ],
        "we will know and love is was discovered in 1969 by Bryson and hell, but nobody noticed because they weren't in neural networks. And then it was discovered in 1974. And Paul we're of us is thesis and he showed his thesis to Steve grossberg and Anna classic Steve grossberg story. Steve looked at chapter ones that I did that chapter 2 I did that check for 3, I did ": [
            935.7,
            966.1,
            22
        ],
        "whatever anyway And I obviously sharing during exams is prohibited. It's going to be very hard to have people sit every other seat in this room. So but hopefully we'll scare enough of you. We will be every other seat by the time we get to the midterm. I don't know anyway. Okay, so no cheating. Usually what we do is pass out two versions of the exam so that ": [
            2330.9,
            2364.2,
            62
        ],
        "which it was designed to be used and your code will run much much slower than it should okay. Grading policy. We're going to use gradescope and I think we have a license now for so that we can use the auto grader and gradescope. We'll see how all that works out programming assignments are due at midnight on the date do that is 11:59 p.m. Plus 1 minute after ": [
            2189.8,
            2224.6,
            58
        ],
        "which they drive backpropagation and they nature is kind of the Rolling Stone of Science. And so it was a big splash and and neural nets for really hot for a long time for at least 10 years or so. And the cool thing that neural Nets that backpropagation does is it learns features in the service of the task? And I hope you all will understand what that means ": [
            1032.0,
            1064.9,
            25
        ],
        "write your programs yourself. Hi, there are online quora answers to the question. Do I have to know math to do deep learning in the answer is not really anymore because of these platform. She just Turn on something and say I want to but I want you to understand it. So I want you to program it. So II programming assignment is actually programming up back prop yourself. Okay, ": [
            2089.0,
            2120.8,
            55
        ],
        "you can't look at your neighbor. Okay, and you should follow the UCSD send Integrity of scholarship policy. So, you know, we want you to get a good education and we don't want you to cheat yourself out of getting a good education. We are here for you. We will give you all the help we can at office hours. I think there's tutoring at the idea Center. I'm not ": [
            2364.2,
            2397.0,
            63
        ],
        "you should never in like Matlab or numpy have for Loops to multiply a matrix times a vector. There's a lot of Matrix X vector or Matrix X matrix multiplication. You can just call the function that does that in in numpy and it's much faster than doing it for a loop. If you're doing it for a loop for that. Then you're not using numpy in the way in ": [
            2160.7,
            2189.8,
            57
        ],
        "you're disabled. Just tell me privately. If you're having trouble getting the food housing or financial resources, there's things on campus such as the Triton food pantry that give out free food. So, okay. That's it any questions. Yeah. Yeah, let me check. Let me get back to you on the jupyter notebooks question. I don't know that's a great question. Super notebooks are wonderful. I don't know how easily ": [
            2583.8,
            2630.1,
            69
        ]
    },
    "File Name": "Deep_Learning___C00___Cottrell__Garrison_W___Fall_2018-lecture_1.flac",
    "Full Transcript": "Okay Lang.  Let's get started.  Can you hear me?  Can you hear me?  check check check  check check  Check, okay.  Seems to be working now. I think you know hear me. Okay, so this is Sia c190 still I hope someday it'll be CSE 1:53 today. When I want to do is go over the syllabus and give a brief introduction to pattern recognition.  So  Let's go see the syllabus everything.  his  on this Piazza page  So we have a Piazza page for CSE 190. There is no separate page for the chorus. This is it just the Piazza page so you can go there and I assume most of you are familiar with Piazza and you can sign yourselves up if you're not there already or soon anyway.  So this syllabus is there.  Okay.  MC sir  full page view there  Okay. Alright.  So this course covers neural networks. We start out with shallow and that works. We do linear talk briefly about linear regression and perceptrons and logistic regression and multinomial regression. And if you don't know what any of those terms means that's good because you're wearing something today. Mostly I'm just going to be introducing the course we've got five ta and a tutor and Jenny hammer and I hope that we'll be able to have office hours every day of the week so you can always get help and we hope you will use Piazza to ask questions.  And unlike most professors. I actually put my cell phone number on the in the service so you can text me if you're not getting your questions answered on Piazza and I'll give give the Tas holy hell.  Okay.  So, let's see. Let's make this a little bigger.  Orin bigoness is one of my students said so there is a th so take a few weeks before we get into deep learning as you probably have heard in the news and everywhere else deep learning has caused a revolution in computer vision. It's also Cause 2 Revolution and reinforcement learning. We cracked go last year go as an ancient Chinese game on a 19 by 19 board that we didn't expect to be solved in the sense of beating a human until about 10 years from now, but  It actually was cracked last year and now the computer programs that play go or are fighting it out for Supremacy. And the most recent version is was based on no human knowledge at all alphago zero and it was speeding ones that were trained on some human knowledge first. So we've reached what I would call a micro Singularity. So the singularity is when the ai's get smarter than us and then start the design themselves to be even smarter. Luckily. I'll be dead when that happens, but this is something that  You will have to deal with in your lifetimes. I imagine.  So I call this a micro Singularity because it's it's in one domain the domain of go and and now that the systems are getting even better than the ones that beat us or beat the champions of go and there's nothing we don't have to be afraid of it. It doesn't even  Doesn't even know it's playing go. It doesn't want to play go. It's just playing go and it doesn't know at all what it's doing. But when they figure out what they're up to then then we have to worry. Okay, so we're going to be talking about pattern recognition mostly by the end of the course. I'll talk a little bit about hello.  Oh, I know what I need to do.  I need the score again.  Somewhere in this hello pack for a trip. I'm getting on a red-eye tonight.  Still have to dig through all my stuff here and find my charger which is not coming up.  somewhere in here  It is. Okay. I think it's just stopping cuz it's trying to save battery.  Okay. So anyway.  Where was I?  Yeah, by the end of the course will start talking about reinforcement learning witches.  Where are some of these go players and DOTA players and things like that are Atari game players and  and yeah, so but mostly we're going to talk about  What we call supervised learning so there's generally machine learning is broken up into three categories supervised learning where you give the network a bunch of input output examples. And you say you know, this is that this is that this is that and now you hope that it'll generalized to new examples of the category that you category that you're training it on unsupervised. So you tell them that work exactly what you wanted to do. Every time unsupervised learning is where you're trying to learn something about the statistics of the data clustering is an example of that and you can use neural networks to clustered data.  Just learning the probability distribution over the data is an example. We don't we'll do some of that with deep learning or at least I'll tell you about it. And then the Third Kind of learning is reinforcement learning, which is mostly how we learn. We try stuff out and see if it works or not in reinforcement learning you are not told what to do at every step but you have a sequence of steps. Usually that you have to do and then you just get feedback. So  Russell norvig describe reinforcement learning as you're given you play a game where you're not told what the rules are you make about a hundred moves in your opponent says you lose and that's that's reinforcement learning in a nutshell. It's a little more complicated than that, but that's basically what it is. Okay, so we will talk a little bit about all three kinds of learning but mostly will be talkin about supervised learning.  Sore after taking this class you should be able to understand about half of a nips paper nips has the neural information processing systems conference. It's the premier machine learning conference this year the registrations for nips sold out in under 11 minutes. I was considered one of the good reviewers and so I got a ticket and they gave me a free registration. So I'm going to go so I will be giving and it's during last week of class. So I'll be giving the last couple of lectures remotely from Montreal.  I like over Skype or hang out or something like that. And I hope you will forgive me for that. But you know, I could sell this ticket I suppose for a lot of money, but I really want to go to nips for once. It's been a few years. Anyway, after taking this course you should be able to at least understand some of what the words are in a nips conference favor. You should understand how backpropagation Learning Works know what a convolutional neural network is Noah long short term memory is which is a really weird name and have a working knowledge of pytorch. We're going to use pytorch for  Okay, this is really annoying. We're going to use pytorch for two of the programming assignments. It's at one of the deep learning platforms.  So and we would like you to do your assignments in in Python and numpy for the first couple and then pytorch for the second couple.  So today we're giving a kind of an overview of the course fuck you  Okay.  Yes. Well anyway, I hope you are not too offended by my use of the English language. But anyway, I'm going to pass a programming assignment 1.  This weekend so we get started right away. It's going to involve some written homework and some programming.  And in this first week zero and one today and next week, we'll talk about linear regression logistic regression and multinomial regression. Who knows what linear regression is already good most of you. Okay, how about logistic regression good softmax in multinomial regression. Okay. Sorry, you're going to learn some stuff then good. How about gradient descent?  Okay, good. Alright, you guys help the other guys who knows great again, raise your hands and everybody else. Look at who's got their hands raised.  Ask them for help. Okay.  Yeah for this course, you should know some linear algebra and Vector calculus and some probability and statistics. Here's a couple of chapters to read with your really poorly written in the Deep learning book, which is available online or you could read a good book Chris Bishops 95 neural networks for pattern recognition books this lecture today that I'm going to give his based on his first chapter in that book.  So we're going to do machine learning gradient descent.  Some optional this reading is all optional. There's lots of courses out there and neural networks and deep learning on the web some of them lecture better than I do. So you might want to look at those.  And then the third week will get to backpropagation learning which is it was invented in the 50s by Robinson Monroe. It's a special case of their procedure. But the version that we will know and love is was discovered in 1969 by Bryson and hell, but nobody noticed because they weren't in neural networks. And then it was discovered in 1974. And Paul we're of us is thesis and he showed his thesis to Steve grossberg and Anna classic Steve grossberg story. Steve looked at chapter ones that I did that chapter 2 I did that check for 3, I did that chapter 4, I didn't do that. That's not interesting chapter 5. I did that chapter 6 so chapter 4 was backpropagation. So Paul we're both dropped it for a while.  yeah, and then it was rediscovered in 1985 here at UCSD in the psychology department by Dave rummelhart, Jeff Hinton and Ron Williams and  they really fig knew what they had at that point. And it was also discovered by MIT undergraduate David Parker who called it learning logic and patented it and it was also discovered by Young laocoon and his PhD thesis in France. So it was kind of the Zeitgeist at that point and they were at a nature paper about it. That is rummelhart Hinton and Williams did I'll be posting chapter 8 if it's not already on your website, which is the chapter in which they drive backpropagation and they nature is kind of the Rolling Stone of Science. And so it was a big splash and and neural nets for really hot for a long time for at least 10 years or so.  And the cool thing that neural Nets that backpropagation does is it learns features in the service of the task? And I hope you all will understand what that means by the end of this class. Hopefully by the third week and and so it was really cool. Everybody was very excited cuz here was this thing that really solved a bunch of problems, but  Couldn't solve really hard problems because there were relatively shallow networks back then with one or two layers of features that were learned.  And so other techniques became more popular in the in the mid 90s and early 90s and then in 2013 or 2012 Jeff hinton's group published a paper where they used a very deep neural network one with many layers the process and it did 10% better than the next best computer vision program on the imagenet visual recognition challenge Evans net is a data set of 1.2 million images and 1000 categories in their job is to learn to classify things into those thousand categories, and they didn't 10% better than the best.  Computer vision algorithm before that and it was a real eye-opener for the computer vision community and everybody's switch to using deep learning after that. So there were a few tricks that a few things that made this possible and we'll learn some of those like first of all a large amount of data was available to train the neural networks.  Gpus made training possible it took Helia subscriber 2 weeks on his see if he used to train the network that he used for that and some tricks about how to do gradient descent when you're going through a lot of variables and those are the main main things that made it possible. So now everybody and their brother is doing deep learning Aaliyah subscriber, I think getting  One either. I can't remember now. It's either 1.2 million or $800,000 a year from openai. So you should be interested in this if you care at all about money. Okay? So this is the kind of stuff that you will help you get a job at Google and so hopefully you'll keep up your interest over the quarter to spite my monotone.  Okay, so so we're going to learn about backdrop and why rummelhart Hitman Williams were wrong in the way that they derived it. They use the wrong objective function and injective function is something you're trying to minimize like squared error, and they use squared error when they should have been using cross entropy. So we'll learn about those and we'll actually drive squared are in Cross entropy from Maximum likelihood.  Which is pretty spooky.  And so that's when we'll be talking about objective functions and appropriate activation functions for each one.  Pa2 will be due in October 23rd and Pa 3lb handed out and then we'll have a midterm this brackets my birthday by the way presidents are not required.  Or accepted okay, and then we'll talk about some more tricks and finally some convolutional networks Richard the kinds of networks are used in these deep learning models.  And you can see where we're having a steady pace of programming assignments due and programming assignments handed out and then we talked about recurrent Network's most rural networks. Network. So they taken an input they get an output. That's the sound the activation makes as it hits the output layer, but if you want something to think or to process things over time you need recurrence that is you need activation like your brain to propagate around the network and and make inferences.  Then we'll squeeze and some reinforcement learning and some recent papers. This is a hard course to teach because sometimes I'm teaching things that happened a couple of weeks ago Google deepmind has three floors to the floors have a hundred phds each. There's a third floor ready to take in another hundred phds. So actually if you want to work in the really good stuff. You should get a PhD and  And you laugh but you know there been there's a lot of research going on at UCSD. We we be we're better than UCLA have been for quite a while but it takes a while for reputation to catch up with reality. So you were lucky to be here. We're not better than Berkeley, but but they were second-best in the UC system and  There's a lot of research going on here and you should try and get involved. Especially if you want to go to grad school.  So there are 199. Swear. You do research with a professor and its nowadays. You have to look like somebody who is looking for a job 10 years ago, you know having a few papers would get you an assistant professor job now having a few papers might get you into grad school.  So having papers while you're still an undergrad is something you want to aim for.  Okay, and I usually finish up with a lecture on ethics of AI. This has become an important area as we worry about the singularity. If I asked a robot to make me a cup of coffee. It doesn't want to be turned off because it can't make a cup of coffee if it's dead. And so it's going to resist being turned off that's called the off switch problem and  Stuart Russell, one of the co-authors of the best AI book we have Russell and norvig gave it talkit. Did Sky last year in Skies the international joint conference on a ion provably beneficial a I could try and sneak and pull the plug out of the robot. But you know if they're super smart they're going to think of that too.  So you have to have some way to make them not too worried about being turned off and he gave us a lecture about a solution to the off switch problem.  So there's a begun to be some work on beneficial a I open a eyes goal is to you know, do hey, I forgot I would as opposed to AI for evil.  And yet they came out with the DOTA bot on just the week weekend before it's guy. I was I went to his car for the first time in decades last year cuz we had papers and  The day in the week before it's guy was a GI, which is artificial general intelligence. So a disco is very narrow intelligence. But an AI that walks talks crawls in his belly like a reptile and can adapt to new situations is some more general intelligence like us, so I went to a GI because I figured if anybody was going to create the Terminator or Commander Data would be them and it was so weak of in computable math and vaporware. And so I stop being worried for a while and then that weekend adult about came out in this thing was like  You know killing people right and left and it had no idea what it was doing, but it was certainly getting rewarded for doing it and you know, just like the the go player, you know, all these are all like amoebas and just very amoebas with a PhD, you know that they just stimulus-response stimulus-response. So it had no idea what it was doing that it was doing it very well and and it beat the DOTA Champion within a couple weeks regular people were beating it and discovered its weaknesses, but eventually it's not going to have those weaknesses so we should be worried. I'm worried but  But like I said, I probably be dead before that happened. So okay prereqs. You should know some linear algebra some Vector calculus and probability and you should have good programming skills as what we want you to do your first couple of programming assignments in hopefully, we'll be able to do it in such a way that we can use an auto greater with your programs.  We don't we haven't figured that out yet. But we will I hope this course uses Clickers.  Did you know that?  Okay, go to clicker clickers will be worth about 5% of your grade. I'll drop the two lowest. I will throughout the term. I will be off and asking clicker questions at the beginning of class. You have no excuse for not getting here at 5 p.m. There were plenty of students who complain last year about getting here at 8 a.m. So but 5 p.m. You have all day to get here. So get here on time and you know, maybe sleep here. I don't know but I often start the class with clicker questions. So and they're usually about last the last lecture and then during the lecture. I will throw in some clicker questions for  To check your understanding and if I use a standard technique, which is if 80% of you don't get the question, right then I let you talk to your neighbor for a couple of minutes and discuss the question and then we try again and the second one counts if that happens and so this a little bit of peer instruction there. So you get to talk to your neighbors and and figure out what you what you didn't understand. I hope and if it's still bad then we talk about it. Okay.  All right. So 5% for that evenly distributed between performance and participation. So if you push the button you get half your points if you push the button get the right answer you get the other half, so that's that's Clickers.  Okay, and then there for programming assignments, the first one will be on an individual basis will have 146 programming assignments to grade should be so much fun. And then the second assignment. I think we start to work in pairs by the end. You can work in groups of four. Okay, so that's the story.  If you don't know anyone in the class Piazza has a find a partner option or feature that you can use to try and find people to work with.  Okay.  do to do to do  Yeah, there has been studies now is that people who have their laptops open during class get lower grades than people that don't.  The people are actually have a a writing down stuff.  So I encourage you to not have open laptops during class. You'll get a better grade. It's very distracting and you were like trying to type while you're trying to watching stuff like that all of the lectures for the class except maybe this one are already up on Piazza from last year. So they your actual lectures will vary but if you want to preview what's going on in the class that day the lectures are already available and then I will usually post them after class for the actual luxury received because I'm always updating them at the last minute like today.  Okay.  So my slides will be available online and I usually post the two versions one in PowerPoint and one in PDF so you don't have to buy PowerPoint. Okay. So this is the the book I really like is the 1995 book. There wasn't deep learning then but he does a really great job of introducing what he does introduced and I learned a lot.  From that book and it's available. You can Google it and find it online. It's also in the Piazza page in PDF format. Also probably you can also get 50 cognitive science.  One to computer science salary, that's the right way to do that.  Okay, so, okay and then the Deep learning book came out last year. It's all online. Unfortunately in Goodfellow has no idea how to explain things to people. Everybody said. Oh great. It's a great book, but then I read some of it.  I mean the PCA explanation goes on for like a page of of equations with no insight.  Anyway, but hopefully they'll get the revised it as time goes on.  Okay programming don't put post your code on GitHub that that's we don't want other people to get your code and don't use code from GitHub. Okay, please write your programs yourself. Hi, there are online quora answers to the question. Do I have to know math to do deep learning in the answer is not really anymore because of these platform. She just  Turn on something and say I want to but I want you to understand it. So I want you to program it. So II programming assignment is actually programming up back prop yourself. Okay, we we so I want you to really understand how it works. I guess I'm kind of old school that way.  But that's how it goes. Okay search for teammates feature. You'll have to find partners for the last few assignments and that's going to happen in the Work World 2 so you should get used to it.  I said most of that. Okay. Yeah numpy is really good because  you should never in like Matlab or numpy have for Loops to multiply a matrix times a vector. There's a lot of Matrix X vector or Matrix X matrix multiplication. You can just call the function that does that in in numpy and it's much faster than doing it for a loop. If you're doing it for a loop for that. Then you're not using numpy in the way in which it was designed to be used and your code will run much much slower than it should okay.  Grading policy. We're going to use gradescope and I think we have a license now for so that we can use the auto grader and gradescope. We'll see how all that works out programming assignments are due at midnight on the date do that is 11:59 p.m. Plus 1 minute after midnight until midnight the next day you can turn it in for 90% credit after that midnight until midnight of the next day. You can be turn in for 50% credit and after that have you got bupkis so we give you more flexible?  Were really great people you're going to really love me at the end. No, I don't know.  Yeah, so the only exceptions to that rule be a few broken all of your arms or something equally bad. Okay, don't cheat working in pairs and trios en cuadros and the Machine on the programming problems and sometimes required but if there's a written part of the homework, you should do that individually. So you should all individually turn in the written part of the homework. You can talk to each other about the problem, but you have to follow the Gilligan's Island role that Pat Diamond coined in 1986. Actually, he he said he got it from somebody that Mighty but the rule is you can't take any written notes during discussing the problems and you have to watch an hour of some insipid TV show.  Before you write anything down. So Gilligan's Island being the canonical insipid TV show guests orders Housewives of New Jersey and whatever anyway  And I obviously sharing during exams is prohibited. It's going to be very hard to have people sit every other seat in this room. So but hopefully we'll scare enough of you. We will be every other seat by the time we get to the midterm. I don't know anyway.  Okay, so no cheating. Usually what we do is pass out two versions of the exam so that you can't look at your neighbor.  Okay, and you should follow the UCSD send Integrity of scholarship policy. So, you know, we want you to get a good education and we don't want you to cheat yourself out of getting a good education. We are here for you. We will give you all the help we can at office hours. I think there's tutoring at the idea Center. I'm not sure about that set right anybody now.  I think they do it but  Anyway, you can ask questions on Piazza if you know, so that's a synchronous or you can come to our office hours and talk to us about things and hopefully we won't actually just give you the answer but will help you come to it. If you have to miss an exam like you're going to a conference or something. Let us know in advance and we'll try and accommodate you by either giving you the exam before you leave or after you get back.  Okay. And again, we're going to answer your questions on Piazza and the timely manner. I hope if and there are lots of online resources for these these things where they you know, I think there's it's really good to to see multiple ways of explaining things and there's a lot of people on the Weber do really good job of explaining things in different ways.  Okay. Yeah and  Right, and if you are not getting your answers on Piazza fast enough, let me know.  Text me I have 22,000 unread emails emailing me is an hour unless you also text me and say please read my email.  Okay.  We are committed to diversity and inclusion and we want to make you feel more included here. If you don't feel that way and we'll do what we can to help. It's important their communities clubs on campus for all kinds of people those people can those clubs can make you could give you a  community that become part of  okay.  So we're going to be inclusive.  Yeah, if you if you feel like you're being harassed or discriminated against please come and tell us and we will deal with it.  Okay.  If you're a student with a disability, there's usually one or two and every class. Please go to the office for students with disabilities and get your accommodation paper the so-called AFA and give me a copy and we'll make arrangements for you.  You don't have to announce in class that you're disabled. Just tell me privately.  If you're having trouble getting the food housing or financial resources, there's things on campus such as the Triton food pantry that give out free food. So, okay. That's it any questions.  Yeah.  Yeah, let me check. Let me get back to you on the jupyter notebooks question. I don't know that's a great question. Super notebooks are wonderful. I don't know how easily we can use an auto grader with jupyter notebook. So that's as to the particular version of python. Yeah, 10.0 is great. I just made that up.  So yeah, well we'll get back to you on that one. Okay other questions.  Okay. Just checking there's one ta and here and Mal. Are you there? You are remember that question? Okay. This is an MO is the new ta  wave  yeah, this is Wally Wally. He is a hurricane Katrina rescue dog, which means she was one of the puppies that were born after the hurricane when the dogs got out and do with dogs 2 there were a lot of puppies and they ship them all over the country. So Wally came on the love train from Southern Mississippi. The love train was actually a tractor-trailer truck with 55 cages in the back and the woman at the shelter. So the women at the shelter had the name 55 puppies and the day and Wally and his sister Wilma and his brother window or in the W's  He is 12 years old. So is that old man? He's in his late 80s early 90s, you can open the door to the computer science building by nose ring the handicap button or jumping up on it. And after awhile one day I would get him to do that. And after while one day just did it himself to get outside quickly. So he'll do it on his own going out not so much coming in.  Muncie  Please don't feed him but he is friendly, you know, you can pet him all you want. He doesn't fight if you're scared of dogs and you would like me to not bring Wally. You can tell me privately because if you tell me publicly you'll be very unpopular. Okay, so that's alright. That's Wally.  Okay, anything else?  He has not a lab. He's a lab imposter. Although my wife says he never said he was a lamb and he's got some German Shepherd some golden retrievers some Mastiff and some  Staffordshire Bull Terrier, that's Pitbull but just a little bit that's what makes him growl when you come in and then the rest he wags his tail after that soon.  Okay, any last minute questions?  Okay.  Wow, that's slow. Okay.  There are a lot of other courses on campus that have related content. I strongly recommend any course taught by Lawrence saw their son. Jerry does Gupta. They're both wonderful professors really good teachers Julian McAuley teaches the data science course and set data mining and Etc and that's hugely popular.  And you went to and talks. I came here from UCLA where he was in computer science, and he came here to join the cogsci department. That's how much she wanted to leave UCLA and come to UCSD is doing that the wrong way but  anyway, so these are people that  Teach good courses, okay.  So today I'm just going to give a really sort of fast overview of pattern recognition. This is all based on the first chapter of Bishop. So here's a standard example. Mnist. This is what Jeff Hinton tends to try out his new ideas on all the time at dataset of handwritten digits are 60,000 examples in the training Sutton 10,000 and the test set  And some of them are actually ambiguous and somebody asked the post office one day. Will what do you do? If you can't read the ZIP code they look at the city.  Okay, sorry facial expression identity. This is work. I've done this is JJ from the pictures of facial affect dataset here. He's being what  hangry  disgusted that's neutral.  surprise  happy happy  sweat scared  Yeah, so JJ is the guys pictures are used in all the psychology studies cuz almost everybody agrees on his facial expressions. But you know suppose you have to recognize this guy as JJ. So all of those expressions are noise with respect identity.  Likewise if you have to recognize Expressions, the identity is noise. She don't you want to recognize the same expression across the bunch of people. So that's another kind of categorization problem. Now, we're up to Facebook apparently has some number of billions of faces that they train their systems on this dataset had I think a hundred and ninety two faces. I don't have  Let's see if I can get this to work. I don't know.  Let's see if this works.  Nope playing but it's not coming in.  Okay.  Let's see what happens.  Okay.  Say anywhere that at all. What's it say say? Okay. What about this?  stay  Stay stay stay. So the only difference between those two or somebody inserted a gap artificially in the middle of  and that's the difference between saying stay. So this is kind of a hard pattern recognition problem. But one of my Chinese grad student said that this was part of a test in China after you'd learned English yet at the beginning of the course. They couldn't hear the difference between these two, but by the end they had to to pass I guess.  Okay, so here's a simpler example detecting age from bees. So here's to a handwritten a n a n a b and so we have to take in those pixels. Those are very simple images and the output is a class label either class 1 or 2. Sometimes we'll use - 1 and 1/4 to Binary classification problems. So we're giving a large set of input output examples. So we've got a bunch of pictures of handwritten A's and B's and a label that this scenario has to be.  There are goal is to develop the classifier that assigns those class labels to new examples with minimal mistakes. I mean the reason we do I mean why if we have the answers to all these pictures do we want to do this the answers we want to generalize the new examples. So, you know, if even though it's supervised learning we wanted to learn a general rule so that given examples from the same distribution does a good job.  So when problem with this is is high dimensionality. How many dimensions are there in a picture?  2  as many as you have pixels, most people think pictures are two-dimensional. That's not true.  You have H pictures of point in a very high dimensional space. So think of one axis is the value on Pixel one. Another access is the value on Pixel 2 to pixel 3, and if you have a 6 megapixel camera, you've got six million dimensions of an image. So this is really high dimensional. So  Even a little image of 256 by 256 with 8-bit pixels the inputs are then of dimensionality 65k and there are approximately 10 to the hundred fifty-eight thousand different possible images.  For all the different pixel values most of them will be garbage but there will be a small number that correspond days and bees will small and so that's called The Curse of dimensionality, you know, you could say, okay. I'm going to find the part of the space where all the A's are in kind of drop box around it. Right that would be a very high dimensional box and so classifiers have a problem with this dimensionality. So usually what we try and do is extract features.  From the images that are smaller than the dimension of the original images. So when idea is we could use the ratio of height to width of these as a feature. Okay. So the bees are going to have a higher ratio of height to width in the A's are so freaking some out like put a bounding box around these days we can figure that out.  So here's a hypothetical distribution of values from the training set. So let's see. This is category a here in category B wear. These are the like a kind of histogram. So in each box of correspondences some value of the height versus the with NSF gets bigger. We getting more and more bees.  But there's some overlap we're going to make some mistakes and that's always going to happen. And so if you look at the distribution is a histogram like yours the A's and here's the bees then we have a class fire. That's just based on one number and we're going to pick a a threshold here somewhere like here where we get to minimize. The number of errors were going to make okay. So that's that's a class fire and I'll make you know, this many mistakes and eat anybody that's over the threshold in the wrong category or under the threshold in the moron category will make mistakes.  So they're still going to be a lot of mistakes. So second idea is to come up with another feature called X2. I don't know what that feature is, but maybe poking us or something and has a big line where the B is. Now, we've got two features and then you can think of that as a two-dimensional feature space. Okay, and what we want to do is put a line down before we had his one day. We had a single number the threshold hurts to day. We want to find a line that separates these two categories with the minimum number of mistakes will still make a couple mistakes here, but will make fewer mistakes.  And and so this is called a linear classifier or a just use the line is your decision boundary.  But if we have to do all 26 letters, we need a lot more features and what you want then is some sort of function that gives you the probability that something's in a category and so here's a function we have if we had 26 letters we have 26 of these functions the taken the input and have some parameters or weights here for each function and its output is some probability of being in that category and then we just finds the one that has the maximum output and will decide that that's the category.  So we use an ARG Max ARG Max is a funny function that says, okay overall the Jays what's the maximum value here for return J, which is the index of the maximum value.  So that's called Hard Knocks.  So feature extraction is a way to deal with this curse of dimensionality. It reduces the size of the training set because now we don't just taken an image which is all these pixels and is high-dimensional now. We just taken two numbers X1 and X2 and train the system on that.  The idea is that this data actually lies on some lower dimensional manifold of the ambient space. So, who knows what a manifold is.  It's it's like a a surface in a multidimensional space. So if you take all these pictures of faces, for example, there's a gazillion different possible images, right but the faces The Story Goes lie on some lower dimensional manifold. Where is he move along?  So here's 3D we could have a surface in 3D and as you move along that surface, you get different faces and hopefully there's some organization to it. So maybe all the males are over here and all the females are over there Etc that allows you to put down a linear classifier. So in one day we have a threshold and 2D we have a line and in 3D, we have a plane and after that we have a hyperplane and your intuitions about things in in high Dimensions go to hell after 5. So after five dimensions, for example, you all know where the Galaxy and distribution is Right looks like this into DA got a bump.  I am 3D together kind of sphere after 5D a gaussian distribution. All of the density is in the thin layer and it's in a shell around the center. There's practically nothing at the average.  Okay, it's all in the thin shell and if you take if you were taking 253 instead of 190, I'd be making you prove that this week.  So be glad you're not into 53. Okay. So, you know one one goal of say unsupervised learning is to find that manifold to learn a lower dimensional representation of the data that then allows you to classify it more easily.  Okay.  I'm so those are classification problems. And that's in contrast to regression problems. Most you know, it linear regression is important.  And you try and draw a line through them such that those points are is close to line as possible. So we want the line to be in such a location that it's possible. And then what we're going to do is say maybe we want to learn some relationship like this is salary.  And this is house size.  Unless you're in it this attic. You're probably going to have a bigger house. If you have a bigger salary, right and now given a new example of some salary. We're going to go up here to this line and say oh you're how sizes around that okay, that's linear regression. So now the output is not a category or probability of being in some category. It's a real number or continuous value or continuous values because you could have multiple regression.  But both of those are examples of function approximation. What a neural net is is a function approximator at least a feed-forward one you give it an input folk you got an output what happens in between is f the function so you take Epi vaccine yet wise and output and in classification, we want the probability of a class membership often which is a kind of function approximation problem.  So in the real world, usually the data were trying to fit has some underlying regularity and some noise. He could be measurement noise or actual noise. Like when you're trying to get a phone number in a bar. It could be gaussian pixel noise doesn't matter whether it's auditory or visual. We still call it noise. And so you're trying to approximate this data and there's going to be some watch some difference between what you want and what you get like squared error. So why do we use Square there anybody now?  Yep, if we didn't use so trying to punish higher-value. So the if the error is squared it gets really big really quickly.  Not differentiable Park. Yeah, so it's differentiable. That's nice. So if if we just used error  Some of the year that we're going to overshoot. Sometimes you're going to undershoot. So some of the errors are going to be positive. Some of them are going to be negative.  The nice thing about squaring the Earth is now they're all positive and we can try and minimize that and it especially penalizes figures, right?  So here's an example of regression. We're giving this input.  We're trying to match these up with these Target values and with a neural that we can do a nonlinear curve fitting. So not just linear regression, but nonlinear regression.  But a common example of this is to use a polynomial and we're not going to use any polynomials have to worry about that. But it's a great way to illustrate some of the issues. So here's a training set. The blue circles are the training set. The green line is the output of the polynomial where we've learned. We we not just we don't just have X we have built-in features. We have x squared we have access to the end etcetera and what we're trying to do is fit these dummies.  These weights these multipliers on the different features, so it's linear in that.  and  so we need some lost function. So sum of squared are the other thing about squared error is really it's euclidean distance, right? If you just took the square root you get the eclipse in distance. So you're just trying to minimize the distance between your estimate and the data.  And so squared error is the sum over all the examples of the output of your system which in this case going to be a polynomial and T. These are Target's T4 Target so we can take the derivative Saturday equal to 0 and with a polynomial we get a closed form solution.  So here is trying to fit this data with oh, actually I lied this isn't our function approximator. This is the actual underlying function that this theater was taken from so we just took this polynomial and added some gaussian noise with 0 mean to it so that the green line is actually the truth.  Okay, and so the red line now is our function approximator and if we use a03 order polynomial we get a line doesn't fit very well. If we use our first order polynomial. Well, it's not just a line at the constant value. We just turned approximated probably with its average using its average will minimize squared error.  I hear we're going to use a line. So first order polynomial and that that's a little better but you know turns out this was actually a third order polynomial that it was generated from Soph. We fit it with a third order polynomial will get a pretty good fit.  Okay, and that looks a lot more like the function.  So better for using ninth order polynomial we can fit the data exactly but you know, we'll make some really big mistakes that go off the page here in between the values. So it doesn't generalize. Well, this one generalized as well. This one is called underfitting. Your model is too simple to fit the data. This is just right and this is overfitting. So that's called over that. We've learned too much about the data.  the such that were exactly fitting every point and now  but now we're going to generalize poorly. So this is the kind of Goldilocks problem in the interest of diversity and inclusion explain what Goldilocks has to those you don't have the same cultural background Goldilocks and the Three Bears. The Three Bears are out having a picnic or something or they ran away. I don't know because they're soup on the table and Goldilocks tries to three bowls of soup. And when's too hot when it's too cold and one is just right and just sit and eat the soup. There are three chairs ones two little ones too big in one's just right. So this is the kind of Goldilocks probably want the just write polynomial.  So we're overfitting. So what this is is the root mean squared error. So it's the square root of the of the squared error and the the average of it. This is how well you're doing on the training set. The Blue Line. The red line is how well you do on the test set and everything goes along pretty well until you get a ninth order polynomial which perfectly fits the data in the air gets really big.  So that's overfitting. So each one of these corresponds to a model of the data has been trained V of parameters with different orders of the polynomial.  And these are the coefficients of the polynomial for w0 W1 W2 etcetera and notice when we get to 9 we get these crazy numbers write something you can just look at those and say it something's wrong.  So, how can we deal with this? This is called regularisation the problem of taking our to we have now we have two strong a model. It can perfectly fit the data and we want to make it as simple as possible. Well, when way to help is to just get more data and that's one solution. We use a lot of neural Nets we try and find more data and now we can take if we get 15 examples and fit our ninth order polynomial to it fit the data so it starts to look better.  Okay, but if we have a hundred.  Examples now or 9th order polynomial is almost just right.  Okay. So getting more data is your first defense.  So we can get more data or we can manufacture more data. So it's ya.  How did how did I what?  order polynomials, right and then and then  good night.  How do you diss?  Yeah, I mean if if the data was generated by a 10th or 11th order polynomial instead of a third order polynomial then the 9th order polynomial under fit.  Right. So this is just a manufactured example.  We we just made it up for training purposes of you. And so it's just giving you an intuition that if your neuron that is too powerful. I mean right now we have neural Nets with millions of parameters. And so they going to Tender overfit then what you need to do something about that and that this is just a motivating example. We're not going to be fitting any polynomials here.  Did I answer your question?  How would you know? Well there's a good answer to that?  Say this is my training set I take about 10% of these examples and I don't use them to train.  Then I see how well it does on those 10 examples. It wasn't trained on those 10 examples are going to stand in for new data that we've never seen before the model is not seen those before until we get an estimate that way of how well it'll generalized to new data.  So that's called a holdout set and doing that is called cross-validation. In fact, we could take 10% Outfitter polynomial take a different 10% outfit are polynomial and see how and do that 10 times get an average of the error on those held at examples and then that's our estimate of how well we're going to do a new examples. We've never seen before.  So that's called crotch validation and it's something you should know about because it's commonly used in neural-net land.  Okay.  Hey any questions about that?  Okay. So best way to deal with overfitting is to get more data or make more data and that's actually kind of subfield. Now of machine learning is how to take your training set and make it bigger. So if you're given a bunch of pictures of people and you're trying to recognize her face as you could take each one and rotated a little bitter and big in it or in smaller it and and now you've multiplied your data by a large number. So that's manufacturing more data.  Another way no is to use Occam's razor. So William of ockham a long time ago said the simplest if we're given a choice between two hypotheses like it could be that you guys aren't really here and that aliens were manufacturing everything that I see and when I there's nothing really outside that door, but when I open it, they quickly like created a new reality for me out there. That's when I process the other is that the world is real some people think that we're all in a simulation and that's more likely than not  I don't believe that but anyway, which is a simpler. I pasta swell no aliens. So to apply this to a model when way  to think about simpler means that the parameters are fewer or smaller and so we can  Apply this to a model where we not just try and minimize the error. Our new objective function is Jay it minimizes the error but it also has this term which corresponds to how complicated the model is and we have some Lambda on that sand multiplier. It says how much we care about that.  And that's called regularization. So this is a function that Dave remmler came up with if you're trying to minimize the dysfunction, I won't go into that right now. Typically what we do is try and minimize. So we're trying to make the wait small and if they get really big they get really penalized. Okay, so I'm trying to make that smaller.  and  so now or function our objective function is the squared error as well as minimizing the weights got one more minute.  So here's our ninth order polynomial was still 10 data points, but we've added regularisation to it. And that made the weight smaller if we make it too big if we make the regularisation too big. I'll just flatten the whole thing.  And so this is without any regularization. This is "
}