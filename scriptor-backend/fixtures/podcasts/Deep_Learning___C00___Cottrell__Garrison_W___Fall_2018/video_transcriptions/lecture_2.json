{
    "Blurbs": {
        "2 years that four of those two. 0 0 0 0 1 1 1 0 1 1 1 1 Okay, and now I'm going to make a little table x 1 x 2. Out why the output teeth at Target? W0 W1 W2 and I'm going to train this thing in real time. You're going to help me. We're going to start out with the weights being 000 Sol. Let ": [
            2287.8,
            2341.8,
            16
        ],
        "And I can even have the learning rate be that small and it's still. Still gets there eventually. But you know, but you don't want your learning rate to be too big to start. So what if we just set the learning rate to 1 generate some samples. You know, that's that's too big. Let's try to make it even worse goes off into never-never land. So this used to ": [
            4249.5,
            4294.8,
            66
        ],
        "I'm going to randomly pick another example. I'll pick just one and give it one zero. his input, this is our new network - 1 Okay - 101. Yep. Okay, so I'm going to give it now 1-0. So 1 * 002. What's the output? 0 okay and teacher says I should be one. So we're word on we need to raise things. So I'm going to give it in ": [
            2580.9,
            2624.8,
            23
        ],
        "It's a good construction, but it's not very practical turns out you can do parody with ended parody with and hidden units that are linear number. So that's what perceptrons are and again the goal was to make this new really inspired machine that could categorize inputs, but learn to do it from examples. And so In the there's a story. We don't I don't know if it's apocryphal or ": [
            1973.2,
            2009.9,
            7
        ],
        "No, I'm pretty sure it did not. Probably win the th get to it. I don't have it. ": [
            4514.0,
            4531.5,
            72
        ],
        "Okay. And you guys are going to see that mirror images. Okay, so it's supervised learning. It's Eric correction learning. We only punish it. We never praised it. So it's not very psychologically plausible and the patterns were presented randomly. Haha. I presented him that way so it would converge in class time. And it's really slow because learning on some patterns screws up learning and other patterns and turns ": [
            2890.9,
            2931.1,
            32
        ],
        "Thank you. Okay, if you're just joining us. I now have a microphone. Okay, and now is 0110 * 1201 * 1 is one one is greater than or equal to one. So it's on and the rest are like that. Okay, how about a set of weights and a threshold that do hand? Yeah. 1 1 2 Yeah, so now this won't fire until we got both one's one ": [
            1710.5,
            1758.4,
            0
        ],
        "a line in 2D. So why is x equals zero then you get that and I usually drive slope-intercept form here, but you guys are supposed to do it. So I'm not Tough noogies. Okay. So I took that out cuz your homework so for 2D it's a line. Why is the weight and the separating line perpendicular to each other? That's what this diagram is showing take two points ": [
            3485.3,
            3519.2,
            48
        ],
        "a linearly it classifies linearly separable things. No way factor determines the orientation of the battery and the bias determines where along that weighed Vector the separating plane is Hey. She's supposed to think about that. Okay? So anything's a perceptron can compute its can learn to compute thingy of it is a linear discriminant makes you makes it clear what you should do with multiple categories. Pick the category ": [
            3927.6,
            3969.4,
            60
        ],
        "a little simpler than going Mumble Mumble Mumble wife K is bigger than everything else arginmax. If you haven't seen it before it just means the DJ that makes this maximum. overall the from 1 to 10 until it Returns the J that makes maximum and we'll call it k Okay. And we couldn't have done that if we'd stuck with the output of zero or one. Okay. So this ": [
            3696.6,
            3730.1,
            54
        ],
        "a raise this this was an active input. This was not so we leave it alone. Okay, so this is our new network. It's starting to look. Nfm. Lawyer Okay. Now I'm going to randomly pick another example of pick this one and now if I give it to 00 The output is what? One the teacher says I've told you three times now and you should be in that ": [
            2624.8,
            2669.1,
            24
        ],
        "a very long time even if you have a GPU. Okay, so, okay, so it's 6:12. I guess I don't really. Oh, I know what I can do. No, I better not do that. Okay. then next time This is fall 17. So it's probably not going to change very much. We're going to talk about logistic regression and multinomial regression. And the basic idea is that instead of the ": [
            4335.0,
            4381.4,
            68
        ],
        "and it turns out that this picture is in fact the way it works, so What can we say about the point here? fast well, but what can we say about the wise? They're all equal. So why I Y kyj are all the same is that point? Okay, and what about that point? Yeah why I am. So this is think of this was input space. And it's 2D ": [
            3798.7,
            3854.9,
            57
        ],
        "and start over. I think the simulation I was running before send me a set the the boundaries. That's better. Okay. So here's two categories. Now, these are clearly not linearly separable because there's some red dots among the blue dots where you can do in cases like that. There's something called the pocket algorithm where he keeps the best weight so far in your pocket, or you can just ": [
            4145.9,
            4212.4,
            64
        ],
        "are the same role. So if the output is one should be zero lower the weight. I put his one and should be 0-1. We're going to lower the weights. and so 01 that becomes one and we're just adding. right 3 - wise one if the Opera is wondering should be zero. I want to lower the weights. So I'll put his lunch should be zero I get a ": [
            3087.7,
            3122.6,
            38
        ],
        "are. What categories and you can even put out .5 it's totally confused. And in fact, we do this, right this will be the probability. The conditional probability of category 1 given the input. All right, so that's what we're going to do next time. Let's go a little early. Yeah, yeah. We're on of we have to fly from Chile Chile point and then the next time we can ": [
            4417.0,
            4469.4,
            70
        ],
        "be a not coming back. Zach not going to come back. And now I can you know, make the perceptron stop wiggling around by just making learning rate smaller and smaller. Okay, so that's that's the kind of intuition you stand for what's going on? And what happens when you make the learning rate too big or too small? Okay to make it. to too small It's going to take ": [
            4294.8,
            4335.0,
            67
        ],
        "be on I want to read switch to active inputs. That means I'm going to change the the bias back to zero. WW2 was active so raised that way and now this is our new network. Okay. Now I'm going to randomly pick another example. I'll pick this one again. given zero Zero's input what's the output? 1 that's supposed to be zero. So 0 * 000 * 1 is ": [
            2502.4,
            2549.1,
            21
        ],
        "can of perceptron compute? Let's do one more a little change here. I'm just writing Y is a function of X. It's just the inner product of the weights bigger than 0 okay ball W in bold extra vectors, so that should seem okay. Now wear this is equal to zero is where the decision changes from. category 1 to Category 2 or vice versa So you can set why ": [
            3251.2,
            3295.5,
            42
        ],
        "compared to the Target and if they don't match it changes the weights and threshold, so I'll get closer to producing the target next time. So first go to training set your design Matrix and the targets. But first let's make a little transformation. So everything will be a little simpler to make it a more modern version. There's the activation rule. If we subtract 800 from both sides now, ": [
            2147.2,
            2184.3,
            12
        ],
        "discriminant. a two-class discriminant function is one where we decide the taxes in category 1 if Why is X is greater than or equal to 0 L sits in Category 2? So now instead of the output being 1 or 0 we're making a decision about what category it's in just by using the one or zero and but that makes generalizing this idea the multiple categories simpler. So here's ": [
            3625.4,
            3661.6,
            52
        ],
        "do it after I need a Geico. We're at over a Tuesday again again until he Yeah. Way to say that we regularly shoes the same point water is not the same thing because you go through all the training points in any way before you change the weights it only matters if it's online learning learning on a pattern at a time and then you wanted to be randomized. ": [
            4469.4,
            4510.4,
            71
        ],
        "don't know why it's doing that. But that means the learning rate is too high. It's changing the weights too much for each example. So if we slow it down. I can't even see this is so weird doesn't usually do this. I don't know why this is like this. Oh, I bet it might have something to do with what I was doing before. Okay. Let's quit adalat Matlab ": [
            4101.6,
            4145.9,
            63
        ],
        "down here zero Zero's there. So for 2 to eat at salon. So what it does is everything over here. It's off everything up here at Saint so it can only solve linearly separable problems. That's that's what it can do. Okay, so and now I have to go back and get the truck. Okay, so for example here's a You know zero one one one is 10000 if you ": [
            3325.3,
            3383.0,
            44
        ],
        "happened? There should be some other stuff, too. Okay. so here's a little demo of a perceptron like thing Oh, that's interesting and let's try again. Why is it not doing? 6 + 20 6 Okay, so this is not when you're really separable. It's not even very clear. But this is let's let's make this. 6 and 6 and usually doesn't run those off the page like that. I ": [
            4032.9,
            4101.6,
            62
        ],
        "have to deal with it. Yeah. yasso active just means that this is one in in the Boolean case. Inactive means at 0 so if it's active it's going to make something change if it's not active it won't so this is just the rule for 1 weight and we're going to do this by the same road all the ways, but I should say I can say here I ": [
            3020.7,
            3054.4,
            36
        ],
        "have to prove this to its the length minus? W0 over the length of w? And in doing this you have to assume that the weight Vector is pointing that way because otherwise he get a negative distance, but the answer is - W 0 / the length of w. Okay, and that's what a part of your homework. Okay, so we can think of a perceptron as a linear ": [
            3593.0,
            3625.4,
            51
        ],
        "hope things will generalize generalizing for a Boolean function doesn't really make sense. But for you know, if we train this thing to be a Gary detector, we would want it to generalize the new pictures of me. so I'm going to get a lot of exercise. Okay, so how would I train it to be a Gary detector? So I put an image which is just a table of ": [
            2744.1,
            2777.9,
            27
        ],
        "how bright the image was. Didn't didn't generalize very well to dim pictures with tanks in them or bright pictures without tanks. So rosenblatt discovered this learning rule called the perceptron convergence procedure and it's called that because it's stop straining after a while. It's guaranteed to learn anything computable by a 2-ton people would call this two layers. Some people would call it one layer. Let's call a two ": [
            2040.0,
            2073.6,
            9
        ],
        "in a high dimensional space where every Dimension corresponds to a pixel you're finding some separating hyperplane in that space that puts the happy guys on one side and the sad guys on the other. Okay. and I'm going to abuse our notation don't report me, but I'm going to make W here just the vector without the bias and there's a couple of points here to make. So it's ": [
            3443.7,
            3485.3,
            47
        ],
        "in the same will be true of WW1. fourth row third row and then the last row says W 1 * 1 and + W 2 * 1 has to be less than the threshold. So the threshold is positive the two weights or bigger than or equal to that. So they're positive and they have to be bigger than equal to that and then there's some has to be ": [
            1831.6,
            1864.7,
            3
        ],
        "in this case. And at this point why I is bigger than y J & Y K and it turns out that every region is convex meaning. That if any two points are in this region, then the point between them is in this region. And so it look it really does look like this. There's no like funny holes in it or anything. and you can prove that I'm ": [
            3854.9,
            3890.9,
            58
        ],
        "inner product is zero, so they're perpendicular. So the weight the weight doctor determines where the line is or if it's a plane where the plane is. and then The distance to the origin that is where you set your threshold kind of thing is El going that again. We're WW1 through WD. And you have to prove this in your homework. So here's one kind of proof. Do you ": [
            3552.5,
            3593.0,
            50
        ],
        "is what it looks like. You just have your inputs with your bias. And your outputs and each output has its own set of weights. So each one is like a perceptron now, except you're taking the weighted sum of the input and using that to decide which one of these is maximum. You're not outputting a 0 or 1. You're just out putting numbers and you're just picking the ": [
            3730.1,
            3765.0,
            55
        ],
        "is x equal to 0 and that is the decision boundary. Where are the output changes from 0 to 1 and that now has the simple geometric interpretation. Why is x equal 0 is a D minus one dimensional hyperplane in 2D dimensional space? So here's a two dimensional space. This is input space. So here's two inputs, you know one ones up here. They're ones over here one zeroes ": [
            3295.5,
            3325.3,
            43
        ],
        "is yet, but basically leave it mostly as an exercise for the reader, but So here's this and here's the X1 and X2. If I can find weights and a threshold, then I'll make that fire 400 which I can then I can supposed to be off. I can never mind. This one here 400 South got a kind of unit that only fires 400 that I have another unit ": [
            1898.4,
            1939.4,
            5
        ],
        "it just has a very simple interpretation. Sometimes I'm going to include w0 on the weights and sometimes I'm not that's just to confuse you. Okay. So here's the learning rule if the output is one but it should be zero. What I should do is lower weights to active input. So that the next time I won't be on in that situation and here I stay active inputs. I ": [
            2219.0,
            2251.2,
            14
        ],
        "largest one. Okay. So this is now our decision X is assigned to class c k if it's the biggest one. Now if you're trying to If you're looking at a boundary between CI and CJ, that's where those two or equal. And that's going to be aligned. So here is our knrj, this is this is where are Kay and RJ are equal. and here's where there's another one ": [
            3765.0,
            3798.7,
            56
        ],
        "layers today. This is the wonderful guarantee anything it can compute it can learn to compute. That's a very strong guarantee. The problem was there were a lot of things that couldn't compute but rosenblatt did assume nonlinear pre-processing. So he assumed some functions ahead of time that did some nonlinear manipulations of the data. And so that actually turns out to be like a support Vector machine. Okay. If ": [
            2073.6,
            2113.0,
            10
        ],
        "less than that not possible. Okay, so you can't compute actually Mexican passport said now if you had hidden units you could compute any Boolean function. There is no learning real for networks with hidden units and we don't think one will ever be discovered. and by that they meant anyone that has the same kind of guarantee that a perceptron has and I haven't told you what that guarantee ": [
            1864.7,
            1898.4,
            4
        ],
        "make it one. So that becomes -1 actually these don't change because the inputs they're not active inputs there 00. But the bias always has an active input. So the bias is often the first thing learned by a neuron that okay. Now I'm going to randomly pick another example of pick that one. I will give it $0. And this way it is now - 1 that's my new ": [
            2416.0,
            2451.8,
            19
        ],
        "make the learning rate smaller and smaller until it stops. It'll still make mistakes, but at least So that's the that's the separating plane. And it's going to keep doing this as long as I do this until because the guys near the boundaries are changing the weights too much by may make the learning rate a little smaller. It slows down. I make it a little smaller. It's smoother. ": [
            4212.4,
            4248.0,
            65
        ],
        "make the learning roll that. so this is the Delta rule call the Delta rule because you're taking the old way and you're adding to it some learning rate times the difference between what what you did and what you should have done. That's the teacher minus the output times the input on that line. And so if this is one and this is zero, then you're adding that input ": [
            2960.2,
            2993.6,
            34
        ],
        "me see. So I guess I'll do it down here. Here is why here is WW1 equals W 2 equals here is a unit that's always one. This is one. This is ex-2 and this is W 0 Okay, so we're starting out with all these being 0. Okay, so I'm going to randomly pick an example of pick this one and give it to your zero. So what's the ": [
            2341.8,
            2379.6,
            17
        ],
        "mean that the input is A1 if the output of zero and it should be big. Should be one then raise the waist active inputs including the bias. So This is a very simple learning roll. So we're going to choose or and we're going to stop here for a quick demo on the board because he have to do this in your homework. So Deus Ex 1 years at ": [
            2251.2,
            2287.8,
            15
        ],
        "might as well that it doesn't matter. This doesn't need to be 0 or 1 it can be any real number. And then how big it is makes a difference and how much it's going to change the weights. so, you know thinking about this is just doing Boolean functions is kind of Not really. You don't need to think of it that way. So let's get ourselves at these ": [
            3054.4,
            3087.7,
            37
        ],
        "minus one. So are there should be an X1 here? Oh, yeah, this is assuming this input is 1 What affects is inactive I-80 then the weight doesn't change. Okay. and No way. That's that's an old slide. I should take that side out. What side is that? 27 remove slide 27. What about the bias Retreat? W0 is a Wade from a unit that's always a constant one. So ": [
            3122.6,
            3170.5,
            39
        ],
        "network after that one. Tray example. Okay pics 01 and what's my output? How many people vote for zero? Not even the guy who said it, okay. But the target is one because 0-1000 is 1 * 001 is not greater than or equal to 0 so we're off. Okay, so now I'm going to so this is okay. So what are we do I was off and I should ": [
            2451.8,
            2502.4,
            20
        ],
        "not going to make you do it, but if you think about Any point exits between these two? It has a form like that. It's some fraction that one plus some other fraction of that one and You can go from there to figure out sit. That one is still in our k. Okay. So a perceptrons a single layer Network the Opera 01 and it's a classifier and it's ": [
            3890.9,
            3927.6,
            59
        ],
        "not. But basically rosenblatt, of course, he took pictures of Tanks. Some of the pictures had tanks in them. Some of them didn't you wanted the perceptron to tell you where there was a tank in the picture and it did that great. The problem was it turned out all the pictures with tanks were taken in bright sunlight in the ones with outward and it was just figuring out ": [
            2009.9,
            2040.0,
            8
        ],
        "numbers. And you know, there's me. and I have a wait for every pixel at this is what you guys are actually doing here and I Every time now one thing it's too late. Well. You can kind of see what I did hear is I just added and subtracted the input vector. To the weights. That's all it is. If you're off and you should be on you add ": [
            2777.9,
            2817.5,
            28
        ],
        "on this line x a and x b say, so they're on the line Y of x equals 0 so why the excavator equal 0 & y mx + b equal 0 and obviously then why have excessive a - 5x vs Stihl 0 so wtxl A- WTSP equal zero So w t x x a minus x p which is all segments of the line equal zero so their ": [
            3519.2,
            3552.5,
            49
        ],
        "out this can explain u-shaped learning in the past tense of English, but we're not going to talk about that cuz it's not cognitive science. Okay, you can talk to me later. If you want to know what that mean. Okay. So again now let's make it simple for computer science this you know English verbal stuff is is great. But how do I program that? Instead. I'm going to ": [
            2931.1,
            2960.2,
            33
        ],
        "output being this. Bang bang kind of thing. There's going to be some nonlinear function here. So instead of Instead of a function that looks like this. We're going to have a function that looks like this. And then we can say something about how confident we are. You know, this one is just the same as this one, but now we can have a measure of how confident we ": [
            4381.4,
            4417.0,
            69
        ],
        "output? Okay, this is to raise your acetylcholine its 1. 0 is greater than equal to zero. Okay, so the output is 1 the target is zero. And so the system says it's still your turn. Okay, so we were and we should be off so we want to lower the weights in the bias. How much should we lower them by that's called The Learning rate and let's just ": [
            2379.6,
            2416.0,
            18
        ],
        "picture of you and it fires a banging on the head and subtract that image from the weights. And so what do you should end up with is a ghostly if you pot the weights as an image, which you can do because there's as many ways as pixels. You should see your kind of ghostly looking picture of me, which is really the difference between me and everyone else. ": [
            2868.4,
            2890.1,
            31
        ],
        "plus one is two two is greater than equal to 2 so that fires. Okay, how about actor? No, no, you guys are cheating. You've seen perceptrons before? Okay, so you can't find a store and that's one of the men ski and peppered pointed out in their book perceptrons. and so it's really easy to prove so you can even do it in class time. So if I needed ": [
            1758.4,
            1799.5,
            1
        ],
        "situation. So what do I write here? How about here? How about here? Okay, the bias is the opposite of threshold. So Stevens one. That was one one one. That was the threshold down 2-1 1-1, so this and now if I give it one one. Wow, it generalize this is company before and it will be on for that impact. It'll be the right answer for everyone and when ": [
            2669.1,
            2707.1,
            25
        ],
        "that only fire. 401 and I want that to be on and and that's how it goes. So you have a hidden unit for every line in the truth table that only turns on for that particular input pattern and then you have my sweater plus one depending on what What the target is? So that requires an exponential number of hidden units and so that's not very that's it. ": [
            1939.4,
            1973.2,
            6
        ],
        "that's right, we don't do anything. So the weights of wandered around and wait space and found a place where it fits the data. Okay, and so we're done that's it's converged. That's why it's called the perceptron convergence theorem. so this was supervised learning. We gave it a set of input output examples told it what to do in every situation and the reason we do this as we ": [
            2707.1,
            2744.1,
            26
        ],
        "the Opera is one should be zero lower the weights and lower the bias. This leads to subtracting one from the bias or subtracting learning right from the bias. What if we get it right? Well if we get it right then TN, why are the same so this is zero and nothing happens. Okay. So one line of code to rule them all. One line to bind them in ": [
            3170.5,
            3204.4,
            40
        ],
        "the darkness has not need to be binary in that case is the weight in proportion to its size. OKC demo, the demo is not going to show you the demo just yet. Okay, still need that at these slides a little bit. I made a bunch of changes today. Okay can learn to compute. Trained on a function that it can compute it. It will always converge. So what ": [
            3204.4,
            3251.2,
            41
        ],
        "the idea suppose we have 10 digits just to pick a random example, then we'd have 10 discriminant function. So wife can a Vex you'll have a set of weights. For that particular category for the 10 categories. And now we make decisions ex's assigned to class c k if K is the one that makes this biggest. Okay, I like to use the yard Max notation just cuz it's ": [
            3661.6,
            3696.6,
            53
        ],
        "them point in opposite directions. So it'll tend to be off when it sees something that's supposed to be off for so if I train this thing to be a Gary detector, I'll have to wait for every pic. So I'll show you a picture of me and if it doesn't bring it on the head and I had that picture me to the weights and if I showed a ": [
            2844.6,
            2868.4,
            30
        ],
        "there's no way to put a line down here that separates this in this from this and this. everybody always kind of goes like that for a while, but So that's why it can't that's another way of showing why it can't do X or so when you're doing happy vs. Sad and image space and image. Remember we said an image is not 2D an image of the point ": [
            3412.5,
            3443.7,
            46
        ],
        "those weights those inputs your weights. Why because if you're supposed to be on adding that pattern to your weights that input Vector is going to make your input and your weight Vector line up and you're going to get a positive inner product and it's going to fire. If I'm on and I should be off I subtract the input Vector from the weights and that's going to make ": [
            2817.5,
            2844.6,
            29
        ],
        "to do the first row, that means the W 1 * 0 + W 2 * 0 would have to be less than the threshold. So that means the threshold is positive. How to get the second row I need W 1 * 0 + W 2 * 1 is greater than or equal to the threshold. That means W2 has to be greater than or equal to the threshold ": [
            1799.5,
            1831.6,
            2
        ],
        "to the weights some fraction of it. If this is your own this is one so you were on and you should be off your subtracting that input from the wake. That again. This is called the Delta rule because learning is based on the delta or difference between what you did and what you should have done. And what follows I'm going to assume Alphas one, so I don't ": [
            2993.6,
            3020.7,
            35
        ],
        "want to do or we'd to set the boundary somewhere like that, and we can easily separate the good guys from the bad guys. if we want to do and we just set the boundary there and now it does and but now it's kind of easy to see intuitively why you can't do excellent cuz he's supposed to be on here on here off here and off here. And ": [
            3383.0,
            3412.5,
            45
        ],
        "with the strongest evidence. And then here's a quicker question. The guy on the left is Jeff Hinton Frank. Rosenblatt Frank. Rosenblatt air fryer Frozen glass CNC If it's B&C, cuz those are the same thing. Okay. Alright, so we've got 14 minutes left. Let me just give up a little demo. I guess not this demo. Let's see. Where am I looking for desktop? Okay. Pray for it. What ": [
            3969.4,
            4032.9,
            61
        ],
        "you can think of the threshold as being 0. and then if you changed they might have stayed and the w0 that is a bias weight. Now you can have the bias weight be awake for me. You know, it's always one. And then you get a simpler equation. So you just tack one onto the front of your design Matrix to come of ones and then you have something ": [
            2184.3,
            2219.0,
            13
        ],
        "you know what that is, if you don't know what that is, don't worry. So it's supervised learning. There's a set of input patterns called the design Matrix, which just makes you sound smart at conferences set of desired outputs of the targets or the teaching signal the network is presented with the inputs. A shin makes when it hits the output that's a technical term and the output is ": [
            2113.0,
            2147.2,
            11
        ],
        "zero a 1000 is greater than or equal to 0 that's one feature says it's still your turn. Okay, so we were on and we should be in a lower everything make that minus one. Now the next time it's e00. I'll be okay these word active inputs. So they stay the same we had zero and zero coming in. They're not active in. The only one is this. Okay. ": [
            2549.1,
            2580.9,
            22
        ]
    },
    "File Name": "Deep_Learning___C00___Cottrell__Garrison_W___Fall_2018-lecture_2.flac",
    "Full Transcript": "Thank you.  Okay, if you're just joining us.  I now have a microphone.  Okay, and now is 0110 * 1201 * 1 is one one is greater than or equal to one. So it's on and the rest are like that. Okay, how about a set of weights and a threshold that do hand?  Yeah.  1 1 2  Yeah, so now this won't fire until we got both one's one plus one is two two is greater than equal to 2 so that fires.  Okay, how about actor?  No, no, you guys are cheating. You've seen perceptrons before? Okay, so you can't find a store and that's one of the men ski and peppered pointed out in their book perceptrons.  and  so it's really easy to prove so you can even do it in class time. So if I needed to do the first row, that means the W 1 * 0 + W 2 * 0  would have to be less than the threshold.  So that means the threshold is positive.  How to get the second row I need W 1 * 0 + W 2 * 1 is greater than or equal to the threshold. That means W2 has to be greater than or equal to the threshold in the same will be true of WW1.  fourth row third row and then the last row says W 1 * 1 and + W 2 * 1  has to be less than the threshold.  So the threshold is positive the two weights or bigger than or equal to that. So they're positive and they have to be bigger than equal to that and then there's some has to be less than that not possible.  Okay, so you can't compute actually Mexican passport said now if you had hidden units you could compute any Boolean function. There is no learning real for networks with hidden units and we don't think one will ever be discovered.  and by that they meant anyone that has the same kind of guarantee that a perceptron has and I haven't told you what that guarantee is yet, but basically leave it mostly as an exercise for the reader, but  So here's this and here's the X1 and X2.  If I can find weights and a threshold, then I'll make that fire 400 which I can then I can supposed to be off. I can never mind. This one here 400 South got a kind of unit that only fires 400 that I have another unit that only fire.  401 and I want that to be on  and and that's how it goes. So you have a hidden unit for every line in the truth table that only turns on for that particular input pattern and then you have my sweater plus one depending on what  What the target is?  So that requires an exponential number of hidden units and so that's not very that's it. It's a good construction, but it's not very practical turns out you can do parody with ended parody with and hidden units that are linear number.  So that's what perceptrons are and again the goal was to make this new really inspired machine that could categorize inputs, but learn to do it from examples. And so  In the there's a story. We don't I don't know if it's apocryphal or not. But basically rosenblatt, of course, he took pictures of Tanks. Some of the pictures had tanks in them. Some of them didn't you wanted the perceptron to tell you where there was a tank in the picture and it did that great. The problem was it turned out all the pictures with tanks were taken in bright sunlight in the ones with outward and it was just figuring out how bright the image was.  Didn't didn't generalize very well to dim pictures with tanks in them or bright pictures without tanks.  So rosenblatt discovered this learning rule called the perceptron convergence procedure and it's called that because it's stop straining after a while. It's guaranteed to learn anything computable by a 2-ton people would call this two layers. Some people would call it one layer. Let's call a two layers today. This is the wonderful guarantee anything it can compute it can learn to compute. That's a very strong guarantee. The problem was there were a lot of things that couldn't compute but rosenblatt did assume nonlinear pre-processing.  So he assumed some functions ahead of time that did some nonlinear manipulations of the data. And so that actually turns out to be like a support Vector machine.  Okay.  If you know what that is, if you don't know what that is, don't worry.  So it's supervised learning. There's a set of input patterns called the design Matrix, which just makes you sound smart at conferences set of desired outputs of the targets or the teaching signal the network is presented with the inputs. A shin makes when it hits the output that's a technical term and the output is compared to the Target and if they don't match it changes the weights and threshold, so I'll get closer to producing the target next time.  So first go to training set your design Matrix and the targets.  But first let's make a little transformation. So everything will be a little simpler to make it a more modern version. There's the activation rule.  If we subtract 800 from both sides now, you can think of the threshold as being 0.  and then if you changed  they might have stayed and the w0 that is a bias weight. Now you can have the bias weight be awake for me. You know, it's always one.  And then you get a simpler equation. So you just tack one onto the front of your design Matrix to come of ones and then you have something it just has a very simple interpretation.  Sometimes I'm going to include w0 on the weights and sometimes I'm not that's just to confuse you.  Okay. So here's the learning rule if the output is one but it should be zero. What I should do is lower weights to active input. So that the next time I won't be on in that situation and here I stay active inputs. I mean that the input is A1 if the output of zero and it should be big. Should be one then raise the waist active inputs including the bias. So  This is a very simple learning roll. So we're going to choose or and we're going to stop here for a quick demo on the board because he have to do this in your homework.  So  Deus Ex 1 years at 2 years that four of those two.  0 0 0 0 1 1 1 0 1 1 1 1  Okay, and now I'm going to make a little table x 1 x 2.  Out why the output teeth at Target?  W0 W1 W2  and I'm going to train this thing in real time. You're going to help me.  We're going to start out with the weights being 000 Sol.  Let me see. So I guess I'll do it down here.  Here is why here is WW1 equals W 2 equals here is a unit that's always one. This is one. This is ex-2 and this is W 0  Okay, so we're starting out with all these being 0.  Okay, so I'm going to randomly pick an example of pick this one and give it to your zero. So what's the output?  Okay, this is to raise your acetylcholine its 1.  0 is greater than equal to zero.  Okay, so the output is 1 the target is zero. And so the system says it's still your turn. Okay, so we were  and we should be off so we want to lower the weights in the bias. How much should we lower them by that's called The Learning rate and let's just make it one. So that becomes -1 actually these don't change because the inputs they're not active inputs there 00.  But the bias always has an active input.  So the bias is often the first thing learned by a neuron that  okay.  Now I'm going to randomly pick another example of pick that one.  I will give it $0.  And this way it is now - 1 that's my new network after that one. Tray example. Okay pics 01 and what's my output?  How many people vote for zero?  Not even the guy who said it, okay.  But the target is one because 0-1000 is 1 * 001 is not greater than or equal to 0 so we're off.  Okay, so now I'm going to so this is okay. So what are we do I was off and I should be on I want to read switch to active inputs. That means I'm going to change the the bias back to zero.  WW2 was active so raised that way and now this is our new network.  Okay. Now I'm going to randomly pick another example. I'll pick this one again.  given zero Zero's input  what's the output?  1  that's supposed to be zero. So 0 * 000 * 1 is zero a 1000 is greater than or equal to 0 that's one feature says it's still your turn. Okay, so we were on and we should be in a lower everything make that minus one. Now the next time it's e00. I'll be okay these word active inputs. So they stay the same we had zero and zero coming in. They're not active in. The only one is this. Okay. I'm going to randomly pick another example. I'll pick just one and give it one zero.  his input, this is our new network - 1  Okay - 101. Yep. Okay, so I'm going to give it now 1-0. So 1 * 002. What's the output?  0 okay and teacher says I should be one.  So we're word on we need to raise things. So I'm going to give it in a raise this this was an active input.  This was not so we leave it alone.  Okay, so this is our new network. It's starting to look.  Nfm. Lawyer  Okay. Now I'm going to randomly pick another example of pick this one and now if I give it to 00  The output is what?  One the teacher says I've told you three times now and you should be in that situation. So what do I write here?  How about here?  How about here?  Okay, the bias is the opposite of threshold. So Stevens one. That was one one one. That was the threshold down 2-1 1-1, so this and now if I give it one one.  Wow, it generalize this is company before and it will be on for that impact. It'll be the right answer for everyone and when that's right, we don't do anything. So the weights of wandered around and wait space and found a place where it fits the data. Okay, and so we're done that's it's converged. That's why it's called the perceptron convergence theorem.  so  this was supervised learning.  We gave it a set of input output examples told it what to do in every situation and the reason we do this as we hope things will generalize generalizing for a Boolean function doesn't really make sense. But for you know, if we train this thing to be a Gary detector, we would want it to generalize the new pictures of me.  so  I'm going to get a lot of exercise. Okay, so how would I train it to be a Gary detector? So I put an image which is just a table of numbers.  And you know, there's me.  and I have a wait for every pixel at this is what you guys are actually doing here and I  Every time now one thing it's too late. Well.  You can kind of see what I did hear is I just added and subtracted the input vector.  To the weights. That's all it is.  If you're off and you should be on you add those weights those inputs your weights. Why because if you're supposed to be on adding that pattern to your weights that input Vector is going to make your input and your weight Vector line up and you're going to get a positive inner product and it's going to fire.  If I'm on and I should be off I subtract the input Vector from the weights and that's going to make them point in opposite directions. So it'll tend to be off when it sees something that's supposed to be off for so if I train this thing to be a Gary detector, I'll have to wait for every pic. So I'll show you a picture of me and if it doesn't bring it on the head and I had that picture me to the weights and if I showed a picture of you and it fires a banging on the head and subtract that image from the weights. And so what do you should end up with is a ghostly if you pot the weights as an image, which you can do because there's as many ways as pixels. You should see your kind of ghostly looking picture of me, which is really the difference between me and everyone else.  Okay.  And you guys are going to see that mirror images. Okay, so it's supervised learning. It's Eric correction learning. We only punish it. We never praised it. So it's not very psychologically plausible and the patterns were presented randomly. Haha. I presented him that way so it would converge in class time.  And it's really slow because learning on some patterns screws up learning and other patterns and turns out this can explain u-shaped learning in the past tense of English, but we're not going to talk about that cuz it's not cognitive science.  Okay, you can talk to me later. If you want to know what that mean. Okay. So again now let's make it simple for computer science this you know English verbal stuff is is great. But how do I program that?  Instead. I'm going to make the learning roll that.  so  this is the Delta rule call the Delta rule because you're taking the old way and you're adding to it some learning rate times the difference between what what you did and what you should have done. That's the teacher minus the output times the input on that line. And so if this is one and this is zero, then you're adding that input to the weights some fraction of it. If this is your own this is one so you were on and you should be off your subtracting that input from the wake.  That again. This is called the Delta rule because learning is based on the delta or difference between what you did and what you should have done.  And what follows I'm going to assume Alphas one, so I don't have to deal with it.  Yeah.  yasso  active just means that this is one in in the Boolean case.  Inactive means at 0 so if it's active it's going to make something change if it's not active it won't so this is just the rule for 1 weight and we're going to do this by the same road all the ways, but I should say I can say here I might as well that it doesn't matter. This doesn't need to be 0 or 1 it can be any real number.  And then how big it is makes a difference and how much it's going to change the weights.  so, you know thinking about this is just doing Boolean functions is kind of  Not really. You don't need to think of it that way.  So let's get ourselves at these are the same role. So if the output is one should be zero lower the weight. I put his one and should be 0-1. We're going to lower the weights.  and so  01 that becomes one and we're just adding.  right  3 - wise one  if  the Opera is wondering should be zero. I want to lower the weights. So I'll put his lunch should be zero I get a minus one.  So are there should be an X1 here? Oh, yeah, this is assuming this input is 1  What affects is inactive I-80 then the weight doesn't change.  Okay.  and  No way. That's that's an old slide. I should take that side out. What side is that? 27 remove slide 27. What about the bias Retreat? W0 is a Wade from a unit that's always a constant one. So the Opera is one should be zero lower the weights and lower the bias.  This leads to subtracting one from the bias or subtracting learning right from the bias.  What if we get it right? Well if we get it right then TN, why are the same so this is zero and nothing happens.  Okay.  So one line of code to rule them all.  One line to bind them in the darkness has not need to be binary in that case is the weight in proportion to its size.  OKC demo, the demo is not going to show you the demo just yet.  Okay, still need that at these slides a little bit. I made a bunch of changes today. Okay can learn to compute.  Trained on a function that it can compute it. It will always converge.  So what can of perceptron compute?  Let's do one more a little change here. I'm just writing Y is a function of X. It's just the inner product of the weights bigger than 0  okay ball W in bold extra vectors, so that should seem okay.  Now wear this is equal to zero is where the decision changes from.  category 1 to Category 2 or vice versa  So you can set why is x equal to 0 and that is the decision boundary. Where are the output changes from 0 to 1 and that now has the simple geometric interpretation.  Why is x equal 0 is a D minus one dimensional hyperplane in 2D dimensional space? So  here's a two dimensional space. This is input space. So here's two inputs, you know one ones up here. They're ones over here one zeroes down here zero Zero's there.  So for 2 to eat at salon.  So what it does is everything over here. It's off everything up here at Saint so it can only solve linearly separable problems.  That's that's what it can do.  Okay, so  and now I have to go back and get the truck.  Okay, so  for example  here's a  You know zero one one one is 10000 if you want to do or we'd to set the boundary somewhere like that, and we can easily separate the good guys from the bad guys.  if we want to do and  we just set the boundary there and now it does and  but now it's kind of easy to see intuitively why you can't do excellent cuz he's supposed to be on here on here off here and off here. And there's no way to put a line down here that separates this in this from this and this.  everybody always kind of goes like that for a while, but  So that's why it can't that's another way of showing why it can't do X or so when you're doing happy vs. Sad and image space and image.  Remember we said an image is not 2D an image of the point in a high dimensional space where every Dimension corresponds to a pixel you're finding some separating hyperplane in that space that puts the happy guys on one side and the sad guys on the other.  Okay.  and  I'm going to abuse our notation don't report me, but I'm going to make W here just the vector without the bias and there's a couple of points here to make.  So it's a line in 2D. So why is x equals zero then you get that and I usually drive slope-intercept form here, but you guys are supposed to do it. So I'm not  Tough noogies. Okay. So I took that out cuz your homework so for 2D it's a line. Why is the weight and the separating line perpendicular to each other? That's what this diagram is showing take two points on this line x a and x b say, so they're on the line Y of x equals 0 so why the excavator equal 0 & y mx + b equal 0 and obviously then why have excessive a - 5x vs Stihl 0  so wtxl A- WTSP equal zero  So w t x x a minus x p which is all segments of the line equal zero so their inner product is zero, so they're perpendicular.  So the weight the weight doctor determines where the line is or if it's a plane where the plane is.  and then  The distance to the origin that is where you set your threshold kind of thing is El going that again. We're WW1 through WD.  And you have to prove this in your homework.  So here's one kind of proof. Do you have to prove this to its the length minus? W0 over the length of w?  And in doing this you have to assume that the weight Vector is pointing that way because otherwise he get a negative distance, but the answer is - W 0 / the length of w.  Okay, and that's what a part of your homework.  Okay, so we can think of a perceptron as a linear discriminant.  a two-class discriminant function is one where we decide the taxes in category 1 if  Why is X is greater than or equal to 0 L sits in Category 2?  So now instead of the output being 1 or 0 we're making a decision about what category it's in just by using the one or zero and but that makes generalizing this idea the multiple categories simpler.  So here's the idea suppose we have 10 digits just to pick a random example, then we'd have 10 discriminant function. So wife can a Vex you'll have a set of weights.  For that particular category for the 10 categories. And now we make decisions ex's assigned to class c k if K is the one that makes this biggest.  Okay, I like to use the yard Max notation just cuz it's a little simpler than going Mumble Mumble Mumble wife K is bigger than everything else arginmax. If you haven't seen it before it just means the DJ that makes this maximum.  overall the from 1 to 10 until it Returns the J that makes maximum and we'll call it k  Okay.  And we couldn't have done that if we'd stuck with the output of zero or one.  Okay. So this is what it looks like. You just have your inputs with your bias.  And your outputs and each output has its own set of weights.  So each one is like a perceptron now, except you're taking the weighted sum of the input and using that to decide which one of these is maximum. You're not outputting a 0 or 1. You're just out putting numbers and you're just picking the largest one.  Okay. So this is now our decision X is assigned to class c k if it's the biggest one.  Now if you're trying to If you're looking at a boundary between CI and CJ, that's where those two or equal.  And that's going to be aligned.  So here is our knrj, this is this is where are Kay and RJ are equal.  and here's where there's another one and it turns out that this picture is in fact the way it works, so  What can we say about the point here?  fast  well, but what can we say about the wise?  They're all equal. So why I Y kyj are all the same is that point?  Okay, and what about that point?  Yeah why I am. So this is think of this was input space.  And it's 2D in this case. And at this point why I is bigger than y J & Y K and it turns out that every region is convex meaning.  That if any two points are in this region, then the point between them is in this region.  And so it look it really does look like this. There's no like funny holes in it or anything.  and you can prove that I'm not going to make you do it, but if you think about  Any point exits between these two? It has a form like that. It's some fraction that one plus some other fraction of that one and  You can go from there to figure out sit. That one is still in our k.  Okay.  So a perceptrons a single layer Network the Opera 01 and it's a classifier and it's a linearly it classifies linearly separable things. No way factor determines the orientation of the battery and the bias determines where along that weighed Vector the separating plane is  Hey.  She's supposed to think about that. Okay?  So anything's a perceptron can compute its can learn to compute thingy of it is a linear discriminant makes you makes it clear what you should do with multiple categories.  Pick the category with the strongest evidence.  And then here's a quicker question. The guy on the left is Jeff Hinton Frank. Rosenblatt Frank. Rosenblatt air fryer Frozen glass CNC  If it's B&C, cuz those are the same thing.  Okay. Alright, so we've got 14 minutes left.  Let me just give up a little demo. I guess not this demo.  Let's see.  Where am I looking for desktop?  Okay.  Pray for it. What happened? There should be some other stuff, too.  Okay.  so here's a little demo of a perceptron like  thing  Oh, that's interesting and let's try again.  Why is it not doing?  6 + 20  6  Okay, so  this is not when you're really separable. It's not even very clear. But this is let's let's make this.  6 and 6 and usually doesn't run those off the page like that. I don't know why it's doing that. But that means the learning rate is too high.  It's changing the weights too much for each example.  So if we slow it down.  I can't even see this is so weird doesn't usually do this. I don't know why this is like this.  Oh, I bet it might have something to do with what I was doing before.  Okay.  Let's quit adalat Matlab and start over.  I think the simulation I was running before send me a set the the boundaries.  That's better. Okay. So here's two categories. Now, these are clearly not linearly separable because there's some red dots among the blue dots where you can do in cases like that. There's something called the pocket algorithm where he keeps the best weight so far in your pocket, or you can just make the learning rate smaller and smaller until it stops. It'll still make mistakes, but at least  So that's the that's the separating plane.  And it's going to keep doing this as long as I do this until because the guys near the boundaries are changing the weights too much by may make the learning rate a little smaller. It slows down. I make it a little smaller.  It's smoother.  And I can even have the learning rate be that small and it's still.  Still gets there eventually.  But you know, but you don't want your learning rate to be too big to start. So what if we just set the learning rate to 1 generate some samples.  You know, that's that's too big. Let's try to make it even worse goes off into never-never land. So this used to be a not coming back.  Zach not going to come back.  And now I can you know, make the perceptron stop wiggling around by just making learning rate smaller and smaller.  Okay, so that's that's the kind of intuition you stand for what's going on? And what happens when you make the learning rate too big or too small? Okay to make it.  to too small  It's going to take a very long time even if you have a GPU.  Okay, so, okay, so it's 6:12. I guess I don't really.  Oh, I know what I can do. No, I better not do that. Okay.  then next time  This is fall 17. So it's probably not going to change very much.  We're going to talk about logistic regression and multinomial regression.  And the basic idea is that instead of the output being this.  Bang bang kind of thing. There's going to be some nonlinear function here. So instead of  Instead of a function that looks like this. We're going to have a function that looks like this.  And then we can say something about how confident we are.  You know, this one is just the same as this one, but now we can have a measure of how confident we are. What categories and you can even put out .5 it's totally confused. And in fact, we do this, right this will be the probability.  The conditional probability of category 1 given the input. All right, so that's what we're going to do next time. Let's go a little early.  Yeah, yeah.  We're on of we have to fly from Chile Chile point and then the next time we can do it after I need a Geico. We're at over a Tuesday again again until he  Yeah.  Way to say that we regularly shoes the same point water is not the same thing because you go through all the training points in any way before you change the weights it only matters if it's online learning learning on a pattern at a time and then you wanted to be randomized.  No, I'm pretty sure it did not.  Probably win the th get to it.  I don't have it. "
}