{
    "Blurbs": {
        "Direction with a little bit of that and it got a little bit that way that I make the correction. That's my new way chain. Yeah, yeah, we're still doing momentum. It's just like when do we add in the gradient? So it's really just a slight change in the code. Yeah, well. These days we just use the Adam Optimizer and forget about it, which probably wondering if that ": [
            4614.4,
            4659.7,
            107
        ],
        "Fannin. If we set the standard deviation of the weights to be won over the fan in square root. You got the standard deviation of AJ is the square root of the sum of 1 / M square root squared which is just one over end and there's end of them. So the sum of 1 over n m x as one. So the standard deviation is one. So this ": [
            3208.8,
            3242.1,
            69
        ],
        "I jump across this funnel. I get a different I get an opposite sign gradient. so the intuition behind the momentum method is I go in the direction to the gradient, but then my next one which is going to be that way. I add in some of this time averaging the gradient is a go and now I go that way the gradients that way but it's been pushing ": [
            3808.7,
            3842.8,
            86
        ],
        "I minimizing squared error is like minimizing distance does first principal component is that is the access that's close to install the data? and if the Martians all had like size 13 shoes, let's say so they're all out here and I could send just two numbers back this axis and x axis and the center would be a 13 for the shoe size and I get all the data ": [
            1486.2,
            1524.1,
            24
        ],
        "I use different characters. Alpha is the momentum rate. So the weight change is equal to the current average she take okay. So how fast should be less than 1 cuz you're taking some fraction of how much you've been moving and this has to this should be small Epsilon here should be small. So usually the momentum rate is about 2.9 or something like that in this can be ": [
            4044.5,
            4082.8,
            92
        ],
        "Is that what you're asking? Yeah. Well suppose I have two inputs coming in. So now they're not positive anymore. Once you know, they very positively and negatively but one of them is going like this. And the other one is going like this. Then if this is important that I need to wait to get really small and if this is important, I need to waste to get really ": [
            2078.3,
            2110.2,
            38
        ],
        "It's 25% chance. What is the forecast for tomorrow? Okay. Are we ready to try again? Okay last time 16 if you got it, right. Let's try this time. Okay this time 14 of You Got a Friend. Care instruction fails again. Okay. Wow. alrighty Maybe the questions to ambiguous. All right. I'm going to close it out and throw away this question later. Nobody's getting it. Okay. Unit J. ": [
            822.7,
            992.0,
            11
        ],
        "It's it's more going to be more idiosyncratic. And so the the direction of the grating is going to change more. So this is Jeff's advice. These are Jeff slides that I've made small changes to So He suggests starting at the beginning of Learning. There's going to be big gradients because you're beginning to learn and so start with a small momentum and once they've disappeared in the weights ": [
            4252.7,
            4290.5,
            97
        ],
        "Louvre transform and variance. And that's what PCA does. Stop here. That's PCA. But it turns out that the eigenvalue on the eigenvector is the variance along that Dimension and if you divided by the square root of the variance then You squish the data down. So it's all equally variant. So it's like Z scoring. The difference between PCA and see scoring is PCA finds the directions of Maximum ": [
            1730.4,
            1772.2,
            30
        ],
        "Okay, Rose Looking Good there for a while. I guess the people are pretty sure answer first. Answer is naughty. No, it's not. Come on there, either. He okay. Okay, we're hovering around 68 70% I'm going to close it out going going going gone. Just spend a couple minutes talking to your neighbor about this. You have a 71% chance that your neighbor has the right answer. Okay. Rahab ": [
            320.7,
            462.0,
            3
        ],
        "Okay, so that's momentum, but now forget everything I said because there's a better version of momentum called nesteroff momentum. So what the momentum does is say? Okay. I'm going this way. I want to go a lot that way. But before I do that, I'm going to figure out which way the slope is from here and add that to it. Hey, that seems a little weird because I ": [
            4370.4,
            4405.5,
            100
        ],
        "Okay. Let's go. Let's get started. Okay. So last time we started talking about tricks of the trade and we talked about stochastic gradient descent vs. Batch. shuffling the examples and how wonderful that was and then perform PCA of the input. But I serve it a little bit of introductory information here. So the Earth's surface of a linear neuron. Hi with a horizontal axis for each weight and ": [
            156.4,
            208.9,
            0
        ],
        "Okay. Why why is it so stay with the unit J Okay, so What happens for this guy? It takes the Deltas propagated back from everybody else. It's some some up and multiplies * the slope and that's the Delta of this guy. Okay, so it's associated with the unit. Okay. See you in a day. Okay. All right. So what happened? So I'm going to ask again. What do ": [
            999.3,
            1046.1,
            12
        ],
        "So and you can differentiate this so you can learn these guys. Do gradient descent on these parameters? Okay. However, there has been some discussion on the web and I don't know if it's resolved yet. But some people have found that if you fly batch normalization to the input to the layer instead of the output of the layer it works better. Who knows. So we're still in empirical ": [
            3565.1,
            3605.7,
            79
        ],
        "So if I move the access hear the variance of the data on this axis has highest and the variance on this axis is much smaller until I can neglect a taxes if I want to if I am willing to tolerate that much are and PCA for a linear system provably minimizes the squared error for the number of numbers that you sent back. So for example, how could ": [
            1452.2,
            1486.2,
            23
        ],
        "So we want them to be very small. And if so, we want smaller incoming weights if the fan is big and vice versa. So do what I say. okay, but Okay, that's all great that then what happens we learn? retrieves the weight so things will no longer be zero mean and unit standard deviation fact, we don't really want that because eventually we want to learn online your ": [
            3316.2,
            3349.8,
            72
        ],
        "That's not 2/3 x 2/3 x So if ex has unit variance than the output will have unit variance turns out so the output is now formatted for the next layer up. So again, you get zero in you get zero out. If he turns out if you put one in you got one out and if you put minus one and you got minus one out that seems like ": [
            2626.1,
            2655.9,
            52
        ],
        "W R mean 0 so this is me and zero that's means your own. This is the mean squared. So it's 0 sqrt of 0. And X has unit standard deviation. So the variance of X is 1 Right, cuz we made it that way. And so the variance of the weighted sum of the inputs is the variance of w in this case. So the variance of the standard ": [
            3139.7,
            3173.6,
            67
        ],
        "Where in which we want to travel with large distance? And even for most popular nuts, the earth's surface is off and locally quadratic. So the same issues apply. Okay, so that's that's the beginning. Now the question is what's wrong with all positive influence. What happens if all the incoming weights the one hidden unit say these are the inputs acts to this hidden unit. I call it Jay ": [
            674.1,
            711.8,
            8
        ],
        "a good property to have and it's mostly linear in that range. So and the second derivative is maximum at x equal one. So the slope is changing fastest there, which is going to change the Deltas. And so one good thing about this is for example, if you're doing regression, it's everything's in the linear range to start out with and it can learn the linear part of whatever ": [
            2655.9,
            2692.3,
            53
        ],
        "a physicist. So here's the gradient. That's what we've been Computing and we take the running average of the previous one and we want to go downhill in the air so we subtract and that's our new weight change. sohvi, here is the Is how we change the weight? Survivor becomes big Delta W which is how you change the weight. Epsilon rho. That's a learning rate. Yeah, sorry. Yeah, ": [
            3998.4,
            4044.5,
            91
        ],
        "a quadratic. That's the reason. Okay, if you multiply this out. Okay any questions? For the 13th of you who thought it was 10 of you thought it was C. And three of you thought it was hey. Okay. Alrighty. Okay good. So again this preliminary kind of data. Before we get to the normal stuff the PCA stuff. So let's think about the case when the the error surface ": [
            585.0,
            638.1,
            6
        ],
        "adiposity. You can look that up. So adiposity is fatness. So, you know, this guy over here is a little tall. Where is wait wait now, he's about right this guy is short and fat. Yeah. Okay, so this is height and that's weight. So this is how heavy you are for your weight. Yeah her size and she said yeah. Oh. Yeah, when you do this, so this data ": [
            1814.7,
            1861.4,
            32
        ],
        "ago. Maybe before your parents were born. so actually, somebody else showed that I started empirically they said Pierre Baldi showed it, theoretically Okay, so if I'm doing Auto encoding that means what I'm doing is I'm taking some input. I'm running it through a narrow channel of hidden units and trying to reproduce it again on the output. Okay, and because everybody is linear, my objective function is squared ": [
            2222.3,
            2260.9,
            42
        ],
        "all my variables are independent, then I just need a linear number of numbers because I just multiply them together, right? So this is why you want to learn independent features of the world. And I don't know if anybody's proved it or shown it or empirically tested it but generally deep networks will learn relatively independent features. Yeah. I'm sorry. What? Oh, so why is this a good idea? ": [
            2027.5,
            2078.3,
            37
        ],
        "and all the excise are positive what's going to happen? In order to answer this. I'm going to ask you another clicker question. So during learning Delta J is associated with the input on the line. Wjk the activation function of y j the unit J the weight JK, which is it. High entropy answers here You know what a high entropy distribution looks like right. splat there's a little ": [
            711.8,
            771.2,
            9
        ],
        "are stuck in one of these Ravines then crank the momentum up. And you can learn at a rate that would that would oscillate without the momentum. You don't know. music in fact recent more recent analysis has suggested that a lot of times were in a saddle point, which means you know, what a horse saddle looks like it's like this answer your like right here on top of ": [
            4290.5,
            4327.6,
            98
        ],
        "assuming that we were using logistic unit. So they had an activation even though the input wasn't was 0 what happens if they're tan H. The input is 0 the output of zero the weights never change nothing happens. Okay, can you have no weights? Be very lightweight Network. Okay. So again, we want the remember I said 0 and 0 at 1 and went up - 1 in - ": [
            3004.0,
            3038.6,
            63
        ],
        "average in all these guys going the same way. Yeah. if if you've been getting opposite signs So are you going to stop doing that? And then I'm going to start doing that. Yeah, yeah. So here's the equations, right? So you take the hidden calls it the velocity cuz it's really the velocity not the momentum if you're a physicist and I think he's start out life. Maybe he's ": [
            3950.5,
            3998.4,
            90
        ],
        "be the case. Which we're going to talk about Xavier was the first name of one of Yoshua bengio students. Okay, so we want to initialize the weights also so that these properties are achieved at the next level up. So I said the great thing about this funny 10 inches is 0 and 0 out 1 in one out -1 in minus one out, but That is if the ": [
            2729.9,
            2763.3,
            55
        ],
        "because that would make this one positive in that one- Okay, so you can only change. In this direction or that direction roughly, right? Okay. So that's bad. That means the the weights. Can't move an arbitrary directions specially if they want to get to that green. Okay. So this is why it's a good idea to have. positive and negative inputs So why PCA of the inputs? So I ": [
            1211.1,
            1255.8,
            17
        ],
        "before I did that to it was highly correlated. Write the taller you are the heavier you are. Right. When I do this, I've got a new access size and adiposity and those aren't correlated with each other. Sawadee correlates the data which is pretty close to Independence. It's not Independence, but you know, it's like using variance for entropy. It's close enough. So like x and x squared are ": [
            1861.4,
            1905.3,
            33
        ],
        "big to have the same effect. So the weights are going to have to take a long time for the weights to this one to get really small in the waist of this one to get really big. So if I scale them if I whiten them so that each one varies about the same amount then they're all equal in how much they affect the hidden units are connected ": [
            2110.2,
            2140.5,
            39
        ],
        "can start to change my weight. So the Deltas are the heavens are all going to be the same. But the inputs are going to be different. Right. So I'm going to diesel how I'll have the same Delta but the inputs are different and the weight changes according to the input X adult x to the delta. so the waves to these guys will be the same the weights ": [
            2931.3,
            2964.8,
            61
        ],
        "chance to undo that. Okay, cuz maybe that's not a great thing. Okay, so maybe it needs to be nuns he's scored. So, okay. So you take every variable like this might be the activation of the third head unit in the first layer over the mini batch you compute the mean of that and divided by the standard deviation and add a little something to avoid / 0 just ": [
            3458.6,
            3489.8,
            76
        ],
        "coming into the unit depend on the input on those lines and dealt with the inputs are all positive Delta's either going to be positive or negative. They're all going to change positively or I'll change negatively. So where can the weights go? They could go this way or this way or this way or that way or that way or that way or that way. They can't go that ": [
            1145.9,
            1177.3,
            15
        ],
        "could be positive or negative. So Delta is the same sign for all of these weights. It's the one guy that you know, most place times those guys and changes these wait. So that means that either all the weights go up or all the way to go down. Okay. So, all right. That makes sense. Because Delta is associated with the unit and the weight change for you guys ": [
            1108.7,
            1145.9,
            14
        ],
        "deviation of a j is the square root of the variance of w. which is the square root of the sum of the squares of the W's because these are all 0 mean right? So it's approximately just the sum of the squares. Okay, so we figured that out. So suppose the number of inputs to unit J is M. So we have M inputs coming in call that the ": [
            3173.6,
            3208.8,
            68
        ],
        "do the same thing the next layer up in the next layer up in the next layer up. so it normalizes all the activation sin the network the inputs in the chitin unit throughout the network on a per-unit basis overreach mini batch. So this is the cliff notes version z-scores every variable. At every layer of the network over the mini batch. So you run the mini batch into ": [
            3389.7,
            3426.1,
            74
        ],
        "don't don't do much. Queso with last week when I show you those eigenfaces those were principal components of images. Yeah. Relatively. So yeah. backprop custoza Okay. Okay change the sigmoid, okay. So now we've got these nicely formatted data. We've got it zero mean unit standard deviation for all the variables. That would be for your problem. It would be like taking every pixel through all the images. And ": [
            2442.8,
            2502.4,
            48
        ],
        "each one of these guys has a weight Vector that corresponds to one of these directions. Right, but it won't perfectly be it'll just span the space. And then they won't have one guys doing all the work. They're not going to sound principal components analysis. There's an ordering to the principal components. The first principal component is the one that captures most of the variance and it's basically just ": [
            2334.3,
            2376.4,
            45
        ],
        "error. So I'm trying to minimize the squared error or what is PCA do it provably minimizes the square there and what happens is that these guys don't end up being the principal components, but they span the principal Subspace. So there's no one guy hero becomes principal component one and another guy that becomes principal component to what happens is they kind of equally because of the randomness of ": [
            2260.9,
            2292.6,
            43
        ],
        "factor to it. Okay, so that seems better, right? Yeah, what? Directions to make the big jump is the momentum term. It's the weighted sum of all your previous gradients. And then I do that and now I take these two and average them together this the momentum. This is the correction. And that's going to tell down a little bit from the green doctor because I've been following this ": [
            4572.3,
            4614.4,
            106
        ],
        "features. So then enter batch normalization with just came out like I don't know where to go at this point. Batch normalization uses all these ideas, but dynamically as the network is being trained. So after the after the activation of a of a hidden unit layer has been computed through forward propagation. Then batch normalization makes them all 0 mean in unit standard deviation and then it's going to ": [
            3349.8,
            3389.7,
            73
        ],
        "five four variables. I need 15 with number. So if I wanted to represent the probability distribution of the world. In my brain, which I would like to do because that will tell me what to expect right then. I would need in my features of the world are not independent. Then I would need a brain the size of Manhattan in order to represent that probability distribution. But if ": [
            1994.1,
            2027.5,
            36
        ],
        "five. For this guy and they're all going to change the same amount. And for this guy they're all going to change the same amount. Okay. and but what happens here I get The black the green and the red I get the black and green the red I get the black the green the red. I'm going to get the same Delta at the hidden Lair and then I ": [
            2903.1,
            2931.3,
            60
        ],
        "gradient the momentum term and then you measure the gradient where you end up. And make a correction. So it's better to correct a mistake after you've made it to see idea instead of before e So here is a picture of the nest Rock method. I take a big jump because I'm using the momentum term which is a big Vector to make this big job. Then I compute ": [
            4504.3,
            4539.3,
            104
        ],
        "gradients and what do I mean by that? So I mean by consistent? Okay this way the gradient is shallow, but it's consistent keeps going that way this way. The gradient is high cuz it's a steep slope. But if I jump across there then it's going to be pointing in the opposite direction. So I'm going to go like this. Right and so those gradients have different signs when ": [
            3778.3,
            3808.7,
            85
        ],
        "has I think so. Your next assignment is going to be using a package. Yay. Does the gradient for you? No more back prop. And so it's God's, you know, one from column A to from column B and I like switch on nesteroff momentum switch on the Adam Optimizer. You don't have to worry about any of this stuff. Okay. in the remaining two minutes adaptive learning rates. So ": [
            4659.7,
            4697.2,
            108
        ],
        "has not died down, but I think you've had enough time. Let's try again. Who? shoots up with the right answer are still 11 of you twelve and go 13 where did 80% come on Stick there? Okay. Okay going. going going John okay. I think we convinced maybe six of you. So what's the answer class? Yeah come why is it be? So when you're on some Spirit Air? ": [
            462.0,
            531.0,
            4
        ],
        "have different deltas. So the weights are going to change because there's actually errors up here and there is activations down here. So even though the weights are zero. You got a Delta up there and you have an input here so that the weights changed but they're all going to change into this guy depending on what his Delta is cuz all the temperature the same they're all point ": [
            2871.9,
            2903.1,
            59
        ],
        "have hidden units last time nevermind. This time you have hidden units. So you got like nine hundred inputs to the hidden units. So that's a fan of 900 and then you got 50 hidden unit. So the fan into the outputs is 50. Vs900 that's least an order of magnitude different. So you want the gradients are the learning rate should be different for those cuz he make a ": [
            4728.3,
            4754.3,
            110
        ],
        "here will be the same cuz this input is the same in the input in the Deltas are the same and the weights to this guys are all the same. So in the end. Every hidden unit computes exactly the same teacher. Like having more than one hidden unit. Hey. So that's that's fodder great fodder for a midterm question. Okay. Okay. So any questions about that, so this was ": [
            2964.8,
            3004.0,
            62
        ],
        "if I could we move the Oxys rotate these actually so the center is in the center of the data that the mean of the data and the other access is this way. This is called principal component 1 this is principal component to what principal components does. It's a linear technique and it linear algebra technique. These guys turn out to be the eigenvectors of the covariance Matrix of ": [
            1342.7,
            1377.2,
            20
        ],
        "in case this gets really small and that's a z scoring the variable. Okay, then. Then what are you do you take that nicely z-score thing and you multiply it by this parameter and add that parameter. So these are learned by backpropagation as well. And this is specific to this particular hidden unit as is this and if you start them out around 1 and 0 then you get ": [
            3489.8,
            3524.9,
            77
        ],
        "information with two numbers. Because the third number is redundant. but if you want to get rid of If you want to reduce the dimensionality I could. I could take a hit for this much error, and I'll be relatively happy. So what that does? Another way of thinking about what that does. Is it it Dean noises the data to so you get rid of high frequency things that ": [
            2406.0,
            2442.8,
            47
        ],
        "is like this. Okay, which way is straight downhill. It's that way. But where do you want to go that way? So unless the ellipse is a circle then all directions Point towards where you want to go. So it'd be great if we could make it more of a circle. So, it's big in the Direction Where We want to go a small distance. It's small in the direction. ": [
            638.1,
            672.9,
            7
        ],
        "is the Earth's surface of a linear Neuron a quadratic? Because quadratics are good approximation to most pervs. Is it because of some spray tear it has to be quite erratic or is it because cross entropy are leads to quadratic or is it because linear narancia squared inputs? hand I seem okay. I've started it now. I seem to have misplaced my clicker again and Murrieta Advanced ER okay ": [
            269.2,
            312.1,
            2
        ],
        "it gives you an idea of the effect of momentum. Yeah. Is the alphas constant usually at? Well, Jeff, I'll give you some different advice in a moment. Yeah. What time do you update your? Yeah, how many batches? Yeah, if you have small many batches then your radiant is going to be less consistent, right? Cuz a small mini batch is a bad sample of all your data, right? ": [
            4205.2,
            4252.7,
            96
        ],
        "it is. You're learning first and then is the weights grow. It'll get into the nonlinear regime and start to take care of things that require nonlinear representations. Okay. so I noticed that brother units don't have all these properties and you need a different analysis for those then people have done that. That's why you use Xavier initialization that tells you how to initialize the weights to have this ": [
            2692.3,
            2729.9,
            54
        ],
        "it's going to go hoops. Okay, and if we go too far then what happens we we end up like increasing the air if we have too high of a learning rate. Okay, so that's why it's good to lower. The learning rate generally is Hugo. So what we want to do is move quickly and directions was small that consistent gradients. And slowly and directions with big but inconsistent ": [
            3743.7,
            3778.3,
            84
        ],
        "know I'm going to go that far but I'm going to compute the slope up here and add that to it. So I'll go that way a little bit but down there is going to be different than it is up here probably. Write him in a different part of the space. So I'm adding and taking this big Vector, which is the gradient. And I'm adding a little Vector ": [
            4405.5,
            4430.4,
            101
        ],
        "learning right wolf kind of get to that but Okay, so just to reminder we've got this. quadratic Bowl that's usually a pretty good approximation and the steepest descent here doesn't go to their which is where we want to go to the bottom of the bowl. The slope is going to be that way. Here is a salt will be good in here. It'll be good. But over here, ": [
            3711.1,
            3743.7,
            83
        ],
        "like .01 or whatever. And so you're taking a big chunk of the way, you've been moving and a little chunk of which way is downhill from here. Okay. And so this is the sum fraction of the previous weight change. Minus the current gradient cuz we're going to keep going downhill in the gradient. That's why it's negative. Okay. So if the air surface is a tilted plane. one ": [
            4082.8,
            4125.7,
            93
        ],
        "like 80 numbers instead of $90,000. Why because the number of principal components is the maximum. We're sorry. the minimum of the dimensionality and the number of points 4/8 - 1 so I imagine I have this huge and pixels but I only have two images. So here is pixel 1 years pixel 2 if I only have two images in that high dimensional space. I can only have one ": [
            1598.0,
            1645.6,
            27
        ],
        "making it zero mean in unit standard deviation. Which is different than I think what I told you to do, which was take the images and make them 0 mean in the unit standard deviation. So they're slightly different ideas. Okay. So now we get this nicely formatted data snow 0 mean it's units are deviation their positive and negative and then we put it into the sigmoid and then ": [
            2502.4,
            2532.9,
            49
        ],
        "me this way. So I'll go a little bit that way and in the end. Those oscillations will average out and I'll head straight down. If I keep track of my previous gradients and average them in. The momentum keeps it going in the in the previous Direction. So instead of changing the way to cording to the gradient you change the way to cording to the average roughly of ": [
            3842.8,
            3879.2,
            87
        ],
        "method Starts Here, we know we're going to add that big momentum term in and it computes the gradient right here and adds it to that seems wrong instead. We should go there and then make a correction. That makes sense. You're not shaking your head. No this time. Okay. heliostats Guyver thought of this I'm so you first make a big jump in the direction of the previously accumulated ": [
            4464.7,
            4504.3,
            103
        ],
        "mode with deep learning. We have some theories about deep learning and surprisingly one of them just assumes the whole network is linear. You know, maybe you can figure you can get something out of assuming the whole network is linear because then you can just multiply all the weights together. You have a single layer Network. Spell but it turns out if it keeps on coming keep getting more ": [
            3605.7,
            3637.5,
            80
        ],
        "nevermind. That's that's what their names look like. And this it this Martian has this height and that way and I have to every time I want to The Story Goes every time I want to send a data back to Earth. I have to shoot off a little rocket. And so I would like to send just one number instead of two numbers. Height and weight are highly correlated. ": [
            1308.5,
            1341.0,
            19
        ],
        "of the weight changes. And use a second, so that's one thing. And you can use to separate adaptive learning rates for every parameter. So for every weight in your network, you can have a learning rate. Dudududu and then you can slowly adjust those things the learning rates using the consistency of the gradient for that. Okay. So let's and then Jeff Hinton had this rmsprop idea. that normalizes ": [
            3674.4,
            3711.1,
            82
        ],
        "one vertical axis for the error. There are four a linear and around with squared error. It's a quadratic fall. so vertical cross-sections of parabolas horizontal cross-sections are ellipses and fur multi layer neural networks. The earth's surface is much more complicated. But locally the quadratic ball is usually a pretty good approximation. So here's a quicker question. so this is in the case of of sum squared error. Why ": [
            208.9,
            269.2,
            1
        ],
        "our initial learning writing. Again. Jeff uses Epsilon gij is the Adaptive guy every weight. ": [
            4790.0,
            4799.7,
            112
        ],
        "peek at the right answer but most of it's flat. Okay. Meaning you guys have no idea must be okay Bad Teacher could be associated with what when you compute it. Okay. I'm going to give you some time to talk about it. Hey talk about it. You have a 34% chance that the person next to you has the right answer. I'm sorry. Let me let me correct that. ": [
            771.2,
            822.7,
            10
        ],
        "perfectly reconstructed cuz there's no variance in their shoe size. So this PCA is it does two things or several things it whiten centers the data, so the mean is here to put and now if you look at what the projections are the new coordinates of these guys, they're all positive and negative. And you have some measure of importance or the amount of information along each axis. It's ": [
            1524.1,
            1561.4,
            25
        ],
        "point of this is that after normalizing the inputs by combining a particular weight initialization with the right sigmoid, we get normalized inputs the next layer up so 0 and 0 out standard deviation 1 in standard deviation one out. And it makes some intuitive sense. Get a room Wally has big fan in small change incoming weights can make it over shoot, right? It has a lot of weight. ": [
            3274.8,
            3316.2,
            71
        ],
        "principal component. If I have three pixels, then I'll have two principal components. So if I have 80 images I can only have 79 principal component and I'll convert all these images to 79 numbers preserving all of the information. Yeah, I just can't dry 91000 dimensions. Maybe you can anyway. So this was really great back in the day when we had data sets for the 80 images. Now, ": [
            1645.6,
            1693.8,
            28
        ],
        "real data that has correlations in it and you plot The eigenvalues which tell you what the variances. They usually go like this until you can cut it off where you can explain 95% of the variance with you know, this many eigenvectors say maintains most of the information. All right. Can you train? Is a side note linear autoencoder essentially does PCA and I showed that a long time ": [
            2178.4,
            2222.3,
            41
        ],
        "results out of that surprisingly. Who knew? And they seem to apply to real deep networks with I mean if he is relo your kind of linear, right? Okay. Okay, so now some slides from Jeff hinton's course. About mini Bachelor ending ways to speed it up in the first ones going to be momentum. So instead of using the gradient to change the way to keep a running average ": [
            3637.5,
            3674.4,
            81
        ],
        "running average does a gradient so if I started here and it's a positive gradient and then I end up over here. It's negative to positive and a negative is going to be smaller than both of them. So I'm going to going to add the last going this direction. If I keep a running average of the gradient going this way. It's going to get bigger and bigger say ": [
            3922.9,
            3950.5,
            89
        ],
        "says using funny can h0me and younes standard Aviation then puts then we should initialize the weights to be one over the square root of the Fanning. So assuming the inputs have been normalized. The unit 0 many units are eviation. The sigmoid is funny can age then the way should be drawn from a distribution with zero mean and standard deviation one of her skirt event. Okay. So the ": [
            3242.1,
            3274.8,
            70
        ],
        "small change to a large amount of Weights is going to change your weight did some of your inputs much more than that. Same Small Change 250 weights. All right, so that implies maybe we should have adaptive learning rates. And then the remaining minute. Rick we start with a Global Learning right and then we have a little multiplier on that called the gain. That's this guy. So here's ": [
            4754.3,
            4790.0,
            111
        ],
        "symmetric round 0 and so what you should see is if you haven't use 10 h that's symmetric around 0 and now you'll send positive and negative inputs up to the next layer. Okay, but yeah, I'm Lagoon did some work and he came out with it came up with an even better idea which I call funny can H because it's so funny that it's 1.715 910h two-thirds X. ": [
            2587.9,
            2626.1,
            51
        ],
        "the data. Okay, and you don't have to know that it's just fun to say so now if I send back to Earth the coordinates of this new coordinate system. I could just send them the coordinate along this axis. And here's xyzzy. He's now his coordinate on that new axis. It's right there. And so this is the height of xyzzy. And this is the weight of x y ": [
            1377.2,
            1416.0,
            21
        ],
        "the first hidden lair. Then you Z score those so this hitting unit has a bunch of different values over the input in the mini batch you z-score that naked 0 mean in unit standard deviation. Then you propagate it up do the same thing again, etc, etc. And then well suppose that's not a good idea. Hit ABS on a little bit of stuff to give the network a ": [
            3426.1,
            3458.6,
            75
        ],
        "the gradient overtime. It's an exponentially decaying average of the gradient. So that dance the oscillations from going this way and because I have a lot of little guys pointing me this way after while they add up and I go even faster in that direction, so that does exactly what I want. What's happening? Okay. That makes sense. No, okay. Not at all. Yeah. I keep track of a ": [
            3879.2,
            3922.9,
            88
        ],
        "the gradient there. So in the end my weight changes is some of those too. Now I take a big jump which is like the average of those two. So it's shifted a little bit to the down. Compute the gradient there. That's my new gradient. On the other hand what momentum standard momentum does is it figures out which way the gradient is here and then add set big ": [
            4539.3,
            4572.3,
            105
        ],
        "the gradients are could be very small and it really layers and there's going to be a different fan into different units depending on you know, like in your guys last assignment you had like a huge fan in from the pixels and then a tiny fan in Well, I'm at smaller fan in like 50 to the outputs, right? Oh wait. No. That was the last time he didn't ": [
            4697.2,
            4728.3,
            109
        ],
        "the line through the space that's closest to all the data. And then the next Direction you find because it's a rotation of the axis. You're basically finding something that's at right angles to this and it's going to capture the next most variance. And if there is a third dimension here and it's very tiny variants. Like they also have size 13 shoes. Then you get all of the ": [
            2376.4,
            2406.0,
            46
        ],
        "the saddle on you. Good morning, the less you care. Yeah. Yeah, exactly. You got it. He's got it asked him if you have a question. Right when the when the learning slows down. Would it be that towards the end of my hair less about the previous average because we've arrived at the correct rating. Yeah, but we want to go faster now that we're on this shallow thing. ": [
            4327.6,
            4368.9,
            99
        ],
        "the variance variances Poor Man's information and not you know, it's like a cheap way to measure entropy. It's not quite entropy but close enough for government work. And so I can I can reduce the dimensionality of the data also. So in your programming assignment last one you gave it all the images, but if I did PCA of those images You could have turned out given the network ": [
            1561.4,
            1598.0,
            26
        ],
        "the weight of nationalization needs to be coordinated with the input normalization and the choice of sigmoid. So we achieved zero mean and unit standard deviation with the inputs using PCA. We also want that to be true of the outputs of the first hidden layer. And so AJ is the product of the inputs x 2 weight. So assuming the inputs in the weights are independently given in the ": [
            3067.7,
            3101.1,
            65
        ],
        "them to small or there won't be a gradient because we're propagating the Deltas back through these very small weights and if they're too large. The 10h will get hit to the rails in the slope will be near zero and will have very small gradients. But let's quick ask why the weights can't start at 0. Okay. So let's assume that we have logistic kidney in a tear for ": [
            2797.3,
            2832.9,
            57
        ],
        "this demonstration. Okay, if all the way it started zero then these guys are going to have an activation of .5 if there is a logistic cert NH. They're just going to have activation zero and nothing happens in 25 out. Okay, and the outputs if they were Logistics lb point five. Also and the outputs will have but they're going to have different targets. Right, so they're going to ": [
            2832.9,
            2871.9,
            58
        ],
        "to Okay. So, you know, I wouldn't if I was in Iran, I wouldn't want to be the first principal component. Going like that a lot. And this one's like Wall-E. Doesn't do much. right So, okay. And so I can throw away like this Dimension and I'll get a small amount of error. And so that's why dimensionality reduction and if you look at the if you take some ": [
            2140.5,
            2178.4,
            40
        ],
        "to it, which is the gradient right here. So this the running average of the gradient. This is my gradient at this point. So I'm making a little correction and then I'm taking a big jump wouldn't it be better? To go that way and then compute which way is downhill from there to make a little correction. That's the difference between regular momentum and nesteroff momentum. So the standard ": [
            4430.4,
            4464.7,
            102
        ],
        "training and up relatively equally having similar variabilities. But they're for that number of numbers. They represent as much information as possible in a squared error sense, which is what PCA is trying to do with linear projections again. You're projecting the data under the saxis. That's how you got to coordinate inner product is practically projection. You just have to divide by the length of one of the vectors ": [
            2292.6,
            2334.3,
            44
        ],
        "turn some Knights T minus y quantity squared, but why is in for a linear neuron w i x 20 squared so this is going to have a t squared term A- Duty some wi, there's going to be a lot of w, so, this is constant. That's constant. It's W's are the variables you're going to have a lot of W Squared terms. Which is going to make it ": [
            540.2,
            585.0,
            5
        ],
        "uncorrelated, but they're not independent. What we would like is to have independent variables to represent the world. If we could because A lot of pixels out there in the world are highly correlated with one another all these pixels are pretty correlated with one another I'll get to you and so if I but if you know if you have two variables to Binary variables, how many numbers do ": [
            1905.3,
            1940.8,
            34
        ],
        "usually give a little. demonstration here of what PCA does so suppose I'm up on Mars. I'm the Martian. And I'm meeting all these martians up there. And I'm recording their height and their way and it turns out that height and weight are highly correlated with one another. Bright, so this is the height and weight of xxyzz. Why? The Martians with mainly going to Mars spring vowels. Okay, ": [
            1255.8,
            1308.5,
            18
        ],
        "variability first. And then you see score. And you can't use he's going to reduce dimensionality, but you can use PCA for that. Okay. Yep. You throw away some of the axes so back here when I have the Martians and I threw away this axis. So, you know where this data? I've got basically to axies this one you could call size. Right and this one you could call ": [
            1772.2,
            1814.7,
            31
        ],
        "waiter initialize randomly. So they're going to be independent of the inputs and here he needs expectation. Send the variance of AJ, which is the variance of x x w if you just you know variance is X W Squared. This is just a and identity and look this up on the web. Okay, its variants of actions very very squared squared. Okay, which turns into this because X and ": [
            3101.1,
            3139.7,
            66
        ],
        "way to think about this is what if the What if the gradient is always the same? And then you can kind of apply this formula and saw. So it turns out if the gradient is always the same. You can solve this and get this and what it does is this is at Infinity time steps the velocity or momentum. Comes out to be one over 1 minus alpha ": [
            4125.7,
            4161.4,
            94
        ],
        "way. Because so you can only change in this quadrant or that quadrant direction, right? You have to tack for those he that no other sales are essentially going up when try to get here. And so you're not allowed to change. This way because that would be changing this way in the negative Direction and that one in a positive direction and you're not allowed to go this way ": [
            1177.3,
            1211.1,
            16
        ],
        "we've got data sets with a gazillion images. And so the dimensionality maybe the the guy that you use instead, okay. alright, so This is a picture you should have in your mind. Here's the data and instead of moving the accies. I'm going to move the data. So same idea the first I put the origin at the center of the data. And PCA is also called the Kernan ": [
            1693.8,
            1730.4,
            29
        ],
        "weighted sum of the inputs, you know, that's nice if the weighted sum of the inputs is zero mean and unit standard deviation, but I didn't do that when I did was make the input 0 mean and standard deviation not the weighted sum of the infinite. and so we need initializing to make these properties true and we need them to be random not zero. And we don't want ": [
            2763.3,
            2797.3,
            56
        ],
        "went out. We want the weighted some of the inputs to be in the linear range of the sigmoid because the gradients will be big cuz it's where the slope is biggest. And the network and learn whatever linear part of the mapping that it needs right away. So we want the AJ's to be 0 mean in unit standard deviation with the recommended sigmoid the funny 10 h. So ": [
            3038.6,
            3067.7,
            64
        ],
        "what happens I put it into the logistic heading unit. Now what? It's all positive. Okay, so that's bad. right and okay, so we need a different activation function. So, you know back in the day. We all used to logistic hidden units and we suffered through. But you don't have to do that. I've suffered for your for you. Okay. So bipolar sigmoid would be better. Okay one that ": [
            2532.9,
            2587.9,
            50
        ],
        "x this this is 9. So this is 10. So you're essentially speeding up your learning by a factor of 10. Okay, it's a recurrent. It's a recurrence relation, which maybe you didn't 21. so Just saw that recurrence relation. That's assuming that. This thing is always the same. So you're on a tilted plane? Okay. So that's a kind of analysis you can do. It's not exactly correct, but ": [
            4161.4,
            4205.2,
            95
        ],
        "you have to have to fill in the joint probability table? We have two binary variables. How many numbers do you need for the joint probability table? 45 what? What did you say? Yes are variable one variable to true-false true-false. War No. 3 They send the one. I only need three numbers then I can fill in the fourth. What if I have three variables I need seven numbers ": [
            1940.8,
            1994.1,
            35
        ],
        "you think? What happens when? They're all positive inputs. Remember, that's the way change rule. Say that again. What will always be positive? Okay, so the way change rule for w i j so here's Iwi day, okay all those guys. X the input on that line in Delta J. So what's going to happen there all all the way to going to change which way? or negatively cuz Delta ": [
            1046.1,
            1108.7,
            13
        ],
        "z z y. But if I projecting onto this access, I'll get a slightly different answer. For his height and his weight and I get a small amount of error here. And a small amount of error here and if I'm willing to tolerate that I can just send one number for everybody. So what principal component analysis does is it finds the directions of highest variance in the data? ": [
            1416.0,
            1452.2,
            22
        ],
        "z-score and variables. But their learnable so you can replace you can change it to not be z-score it if you want to in fact. This transformation a lot. If you figure it out, you can actually invert the scoring completely if you learn the right gamma and beta. Okay. And the cool thing is well, look at this. You can actually these are smooth functions. You can differentiate these. ": [
            3524.9,
            3564.0,
            78
        ]
    },
    "File Name": "Deep_Learning___C00___Cottrell__Garrison_W___Fall_2018-lecture_7.flac",
    "Full Transcript": "Okay.  Let's go. Let's get started.  Okay. So last time we started talking about tricks of the trade and  we talked about stochastic gradient descent vs. Batch.  shuffling the examples  and how wonderful that was and then perform PCA of the input.  But I serve it a little bit of introductory information here.  So the Earth's surface of a linear neuron.  Hi with a horizontal axis for each weight and one vertical axis for the error.  There are four a linear and around with squared error. It's a quadratic fall.  so vertical cross-sections of parabolas  horizontal cross-sections are ellipses  and  fur multi layer neural networks. The earth's surface is much more complicated. But locally the quadratic ball is usually a pretty good approximation.  So here's a quicker question.  so  this is in the case of of sum squared error.  Why is the Earth's surface of a linear Neuron a quadratic?  Because quadratics are good approximation to most pervs. Is it because of some spray tear it has to be quite erratic or is it because cross entropy are leads to quadratic or is it because linear narancia squared inputs?  hand  I seem okay. I've started it now.  I seem to have misplaced my clicker again and  Murrieta Advanced ER okay  Okay, Rose Looking Good there for a while.  I guess the  people are pretty sure answer first.  Answer is naughty. No, it's not.  Come on there, either. He okay.  Okay, we're hovering around 68 70% I'm going to close it out going going going gone. Just spend a couple minutes talking to your neighbor about this. You have a 71% chance that your neighbor has the right answer.  Okay.  Rahab has not died down, but I think you've had enough time. Let's try again.  Who?  shoots up with the right answer  are still  11 of you twelve and go  13  where did 80% come on Stick there?  Okay.  Okay going.  going going  John okay. I think we convinced maybe six of you. So what's the answer class?  Yeah come why is it be?  So when you're on some Spirit Air?  turn some  Knights  T minus y quantity squared, but why is in for a linear neuron w i x  20 squared so this is going to have a t squared term A- Duty some wi, there's going to be a lot of w, so, this is constant. That's constant. It's W's are the variables you're going to have a lot of W Squared terms.  Which is going to make it a quadratic.  That's the reason.  Okay, if you multiply this out.  Okay any questions?  For the 13th of you who thought it was 10 of you thought it was C.  And three of you thought it was hey.  Okay.  Alrighty. Okay good.  So again this preliminary kind of data.  Before we get to the normal stuff the PCA stuff. So let's think about the case when the the error surface is like this. Okay, which way is straight downhill. It's that way.  But where do you want to go that way? So unless the ellipse is a circle then all directions Point towards where you want to go.  So it'd be great if we could make it more of a circle.  So, it's big in the Direction Where We want to go a small distance. It's small in the direction.  Where in which we want to travel with large distance?  And even for most popular nuts, the earth's surface is off and locally quadratic. So the same issues apply.  Okay, so that's that's the beginning.  Now the question is what's wrong with all positive influence.  What happens if all the incoming weights the one hidden unit say these are the inputs acts to this hidden unit.  I call it Jay and all the excise are positive what's going to happen?  In order to answer this. I'm going to ask you another clicker question.  So during learning Delta J is associated with the input on the line. Wjk the activation function of y j the unit J the weight JK, which is it.  High entropy answers here  You know what a high entropy distribution looks like right.  splat  there's a little peek at the right answer but most of it's flat. Okay.  Meaning you guys have no idea must be okay Bad Teacher could be associated with what when you compute it. Okay. I'm going to give you some time to talk about it.  Hey talk about it. You have a 34% chance that the person next to you has the right answer.  I'm sorry. Let me let me correct that. It's 25% chance.  What is the forecast for tomorrow?  Okay.  Are we ready to try again?  Okay last time 16 if you got it, right. Let's try this time.  Okay this time 14 of You Got a Friend.  Care instruction fails again. Okay. Wow.  alrighty  Maybe the questions to ambiguous.  All right. I'm going to close it out and throw away this question later.  Nobody's getting it. Okay.  Unit J.  Okay.  Why why is it so stay with the unit J Okay, so  What happens for this guy? It takes the Deltas propagated back from everybody else. It's some some up and multiplies * the slope and that's the Delta of this guy.  Okay, so it's associated with the unit.  Okay. See you in a day.  Okay.  All right. So what happened? So I'm going to ask again. What do you think? What happens when?  They're all positive inputs.  Remember, that's the way change rule.  Say that again.  What will always be positive?  Okay, so the way change rule for w i j so here's  Iwi day, okay all those guys.  X  the input on that line in Delta J. So what's going to happen there all all the way to going to change which way?  or  negatively cuz Delta could be positive or negative.  So  Delta is the same sign for all of these weights. It's the one guy that you know, most place times those guys and changes these wait. So that means that either all the weights go up or all the way to go down.  Okay.  So, all right. That makes sense.  Because Delta is associated with the unit and the weight change for you guys coming into the unit depend on the input on those lines and dealt with the inputs are all positive Delta's either going to be positive or negative. They're all going to change positively or I'll change negatively.  So where can the weights go?  They could go this way or this way or this way or that way or that way or that way or that way.  They can't go that way.  Because so you can only change in this quadrant or that quadrant direction, right? You have to tack for those he that no other sales are essentially going up when try to get here.  And so you're not allowed to change.  This way because that would be changing this way in the negative Direction and that one in a positive direction and you're not allowed to go this way because that would make this one positive in that one- Okay, so you can only change.  In this direction or that direction roughly, right?  Okay.  So that's bad.  That means the the weights.  Can't move an arbitrary directions specially if they want to get to that green.  Okay.  So this is why it's a good idea to have.  positive and negative inputs  So why PCA of the inputs?  So I usually give a little.  demonstration here of what PCA does  so suppose  I'm up on Mars. I'm the Martian.  And I'm meeting all these martians up there.  And I'm recording their height and their way and it turns out that height and weight are highly correlated with one another.  Bright, so this is the height and weight of xxyzz. Why?  The Martians with mainly going to Mars spring vowels.  Okay, nevermind. That's that's what their names look like. And this it this Martian has this height and that way and I have to every time I want to The Story Goes every time I want to send a data back to Earth. I have to shoot off a little rocket.  And so I would like to send just one number instead of two numbers.  Height and weight are highly correlated.  if I could  we move the Oxys rotate these actually so the center is in the center of the data that the mean of the data and the other access is this way.  This is called principal component 1 this is principal component to what principal components does. It's a linear technique and it linear algebra technique.  These guys turn out to be the eigenvectors of the covariance Matrix of the data.  Okay, and you don't have to know that it's just fun to say so now if I send back to Earth the coordinates of this new coordinate system.  I could just send them the coordinate along this axis. And here's xyzzy. He's now his coordinate on that new axis. It's right there.  And so this is the height of xyzzy.  And this is the weight of x y z z y.  But if I projecting onto this access, I'll get a slightly different answer.  For his height and his weight and I get a small amount of error here.  And a small amount of error here and if I'm willing to tolerate that I can just send one number for everybody.  So what principal component analysis does is it finds the directions of highest variance in the data? So if I move the access hear the variance of the data on this axis has highest and the variance on this axis is much smaller until I can neglect a taxes if I want to if I am willing to tolerate that much are and PCA for a linear system provably minimizes the squared error for the number of numbers that you sent back.  So for example, how could I minimizing squared error is like minimizing distance does first principal component is that is the access that's close to install the data?  and if the Martians all had  like size 13 shoes, let's say so they're all out here and I could send just two numbers back this axis and x axis and the center would be a 13 for the shoe size and I get all the data perfectly reconstructed cuz there's no variance in their shoe size. So this PCA is it does two things or several things it whiten centers the data, so the mean is here to put and now if you look at what the projections are the new coordinates of these guys, they're all positive and negative.  And you have some measure of importance or the amount of information along each axis. It's the variance variances Poor Man's information and not you know, it's like a cheap way to measure entropy. It's not quite entropy but close enough for government work. And so I can I can reduce the dimensionality of the data also. So in your programming assignment last one you gave it all the images, but if I did PCA of those images  You could have turned out given the network like 80 numbers instead of $90,000. Why because the number of principal components is the maximum.  We're sorry.  the minimum of the dimensionality  and the number of points  4/8 - 1 so I imagine I have this huge and pixels but I only have two images. So here is pixel 1 years pixel 2 if I only have two images in that high dimensional space. I can only have one principal component.  If I have three pixels, then I'll have two principal components. So if I have 80 images  I can only have 79 principal component and I'll convert all these images to 79 numbers preserving all of the information.  Yeah, I just can't dry 91000 dimensions.  Maybe you can anyway.  So this was really great back in the day when we had data sets for the 80 images. Now, we've got data sets with a gazillion images. And so the dimensionality maybe the the guy that you  use instead, okay.  alright, so  This is a picture you should have in your mind. Here's the data and instead of moving the accies. I'm going to move the data. So same idea the first I put the origin at the center of the data.  And PCA is also called the Kernan Louvre transform and variance.  And that's what PCA does.  Stop here. That's PCA. But it turns out that the eigenvalue on the eigenvector is the variance along that Dimension and if you divided by the square root of the variance then  You squish the data down. So it's all equally variant. So it's like Z scoring.  The difference between PCA and see scoring is PCA finds the directions of Maximum variability first.  And then you see score.  And you can't use he's going to reduce dimensionality, but you can use PCA for that.  Okay.  Yep.  You throw away some of the axes so back here when I have the Martians and I threw away this axis.  So, you know where this data?  I've got basically to axies this one you could call size. Right and this one you could call adiposity.  You can look that up. So adiposity is fatness. So, you know, this guy over here is a little tall.  Where is wait wait now, he's about right this guy is short and fat. Yeah. Okay, so this is height and that's weight. So this is how heavy you are for your weight.  Yeah her size and she said yeah.  Oh.  Yeah, when you do this, so this data before I did that to it was highly correlated.  Write the taller you are the heavier you are.  Right. When I do this, I've got a new access size and adiposity and those aren't correlated with each other.  Sawadee correlates the data  which is pretty close to Independence. It's not Independence, but you know, it's like using variance for entropy. It's close enough. So  like  x and x squared are uncorrelated, but they're not independent.  What we would like is to have independent variables to represent the world. If we could because  A lot of pixels out there in the world are highly correlated with one another all these pixels are pretty correlated with one another I'll get to you and so if I but if you know if you have two variables to Binary variables, how many numbers do you have to have to fill in the joint probability table?  We have two binary variables. How many numbers do you need for the joint probability table?  45 what? What did you say?  Yes are variable one variable to true-false true-false.  War  No.  3  They send the one.  I only need three numbers then I can fill in the fourth.  What if I have three variables I need seven numbers five four variables. I need 15 with number. So if I wanted to represent the probability distribution of the world.  In my brain, which I would like to do because that will tell me what to expect right then.  I would need in my features of the world are not independent. Then I would need a brain the size of Manhattan in order to represent that probability distribution.  But if all my variables are independent, then I just need a linear number of numbers because I just multiply them together, right? So this is why you want to learn independent features of the world.  And I don't know if anybody's proved it or shown it or empirically tested it but generally deep networks will learn relatively independent features.  Yeah.  I'm sorry. What?  Oh, so why is this a good idea? Is that what you're asking? Yeah.  Well suppose I have two inputs coming in. So now they're not positive anymore. Once you know, they very positively and negatively but one of them is going like this.  And the other one is going like this.  Then if this is important that I need to wait to get really small and if this is important, I need to waste to get really big to have the same effect. So the weights are going to have to take a long time for the weights to this one to get really small in the waist of this one to get really big.  So if I scale them if I whiten them so that each one varies about the same amount then they're all equal in how much they affect the hidden units are connected to  Okay.  So, you know, I wouldn't if I was in Iran, I wouldn't want to be the first principal component.  Going like that a lot. And this one's like Wall-E.  Doesn't do much.  right  So, okay.  And so I can throw away like this Dimension and I'll get a small amount of error. And so that's why dimensionality reduction and if you look at the if you take some real data that has correlations in it and you plot  The eigenvalues which tell you what the variances.  They usually go like this until you can cut it off where you can explain 95% of the variance with you know, this many eigenvectors say maintains most of the information.  All right.  Can you train?  Is a side note linear autoencoder essentially does PCA and I showed that a long time ago.  Maybe before your parents were born.  so  actually, somebody else showed that I started empirically they said Pierre Baldi showed it, theoretically  Okay, so if I'm doing Auto encoding that means what I'm doing is I'm taking some input. I'm running it through a narrow channel of hidden units and trying to reproduce it again on the output.  Okay, and because everybody is linear, my objective function is squared error. So I'm trying to minimize the squared error or what is PCA do it provably minimizes the square there and what happens is that these guys don't end up being the principal components, but they span the principal Subspace. So there's no one guy hero becomes principal component one and another guy that becomes principal component to what happens is they kind of equally because of the randomness of training and up relatively equally having similar variabilities.  But they're for that number of numbers. They represent as much information as possible in a squared error sense, which is what PCA is trying to do with linear projections again.  You're projecting the data under the saxis. That's how you got to coordinate inner product is practically projection. You just have to divide by the length of one of the vectors each one of these guys has a weight Vector that corresponds to one of these directions.  Right, but it won't perfectly be it'll just span the space.  And then they won't have one guys doing all the work.  They're not going to sound principal components analysis. There's an ordering to the principal components. The first principal component is the one that captures most of the variance and it's basically just the line through the space that's closest to all the data.  And then the next Direction you find because it's a rotation of the axis.  You're basically finding something that's at right angles to this and it's going to capture the next most variance. And if there is a third dimension here and it's very tiny variants. Like they also have size 13 shoes. Then you get all of the information with two numbers.  Because the third number is redundant.  but if you want to get rid of  If you want to reduce the dimensionality I could.  I could take a hit for this much error, and I'll be relatively happy. So what that does?  Another way of thinking about what that does. Is it it Dean noises the data to so you get rid of high frequency things that don't don't do much.  Queso with last week when I show you those eigenfaces those were principal components of images.  Yeah.  Relatively. So yeah.  backprop custoza  Okay.  Okay change the sigmoid, okay.  So now we've got these nicely formatted data. We've got it zero mean unit standard deviation for all the variables.  That would be for your problem. It would be like taking every pixel through all the images.  And making it zero mean in unit standard deviation.  Which is different than I think what I told you to do, which was take the images and make them 0 mean in the unit standard deviation.  So they're slightly different ideas. Okay. So now we get this nicely formatted data snow 0 mean it's units are deviation their positive and negative and then we put it into the sigmoid and then what happens  I put it into the logistic heading unit.  Now what?  It's all positive.  Okay, so that's bad.  right  and  okay, so we need a different activation function. So, you know back in the day. We all used to logistic hidden units and we suffered through.  But you don't have to do that.  I've suffered for your for you.  Okay. So bipolar sigmoid would be better. Okay one that symmetric round 0 and so what you should see is if you haven't use 10 h  that's symmetric around 0 and now you'll send positive and negative inputs up to the next layer.  Okay, but yeah, I'm Lagoon did some work and he came out with it came up with an even better idea which I call funny can H because it's so funny that it's 1.715 910h two-thirds X. That's not 2/3 x 2/3 x  So if ex has unit variance than the output will have unit variance turns out so the output is now formatted for the next layer up. So again, you get zero in you get zero out.  If he turns out if you put one in you got one out and if you put minus one and you got minus one out that seems like a good property to have and it's mostly linear in that range. So and the second derivative is maximum at x equal one. So the slope is changing fastest there, which is going to change the Deltas.  And so one good thing about this is for example, if you're doing regression, it's everything's in the linear range to start out with and it can learn the linear part of whatever it is. You're learning first and then is the weights grow. It'll get into the nonlinear regime and start to take care of things that require nonlinear representations.  Okay.  so  I noticed that brother units don't have all these properties and you need a different analysis for those then people have done that. That's why you use Xavier initialization that tells you how to initialize the weights to have this be the case.  Which we're going to talk about Xavier was the first name of one of Yoshua bengio students.  Okay, so we want to initialize the weights also so that these properties are achieved at the next level up. So I said the great thing about this funny 10 inches is 0 and 0 out 1 in one out -1 in minus one out, but  That is if the weighted sum of the inputs, you know, that's nice if the weighted sum of the inputs is zero mean and unit standard deviation, but I didn't do that when I did was make the input 0 mean and standard deviation not the weighted sum of the infinite.  and  so we need initializing to make these properties true and we need them to be random not zero.  And we don't want them to small or there won't be a gradient because we're propagating the Deltas back through these very small weights and if they're too large.  The 10h will get hit to the rails in the slope will be near zero and will have very small gradients.  But let's quick ask why the weights can't start at 0.  Okay.  So let's assume that we have logistic kidney in a tear for this demonstration. Okay, if all the way it started zero then these guys are going to have an activation of .5 if there is a logistic cert NH. They're just going to have activation zero and nothing happens in 25 out.  Okay, and the outputs if they were Logistics lb point five.  Also and the outputs will have but they're going to have different targets.  Right, so they're going to have different deltas.  So  the weights are going to change because there's actually errors up here and there is activations down here. So even though the weights are zero.  You got a Delta up there and you have an input here so that the weights changed but they're all going to change into this guy depending on what his Delta is cuz all the temperature the same they're all point five.  For this guy and they're all going to change the same amount. And for this guy they're all going to change the same amount.  Okay.  and but what happens here I get  The black the green and the red I get the black and green the red I get the black the green the red. I'm going to get the same Delta at the hidden Lair and then I can start to change my weight.  So the Deltas are the heavens are all going to be the same.  But the inputs are going to be different.  Right. So I'm going to diesel how I'll have the same Delta but the inputs are different and the weight changes according to the input X adult x to the delta.  so  the waves to these guys will be the same the weights here will be the same cuz this input is the same in the input in the Deltas are the same and the weights to this guys are all the same.  So in the end.  Every hidden unit computes exactly the same teacher. Like having more than one hidden unit.  Hey.  So that's that's fodder great fodder for a midterm question.  Okay. Okay. So any questions about that, so this was assuming that we were using logistic unit. So they had an activation even though the input wasn't was 0 what happens if they're tan H. The input is 0 the output of zero the weights never change nothing happens.  Okay, can you have no weights?  Be very lightweight Network.  Okay.  So again, we want the remember I said 0 and 0 at 1 and went up - 1 in - went out. We want the weighted some of the inputs to be in the linear range of the sigmoid because the gradients will be big cuz it's where the slope is biggest.  And the network and learn whatever linear part of the mapping that it needs right away.  So we want the AJ's to be 0 mean in unit standard deviation with the recommended sigmoid the funny 10 h.  So the weight of nationalization needs to be coordinated with the input normalization and the choice of sigmoid.  So we achieved zero mean and unit standard deviation with the inputs using PCA. We also want that to be true of the outputs of the first hidden layer.  And so AJ is the product of the inputs x 2 weight.  So assuming the inputs in the weights are independently given in the waiter initialize randomly. So they're going to be independent of the inputs and here he needs expectation.  Send the variance of AJ, which is the variance of x x w if you just you know variance is X W Squared. This is just a  and identity and look this up on the web. Okay, its variants of actions very very squared squared.  Okay, which turns into this because  X and W R mean 0 so this is me and zero that's means your own. This is the mean squared.  So it's 0 sqrt of 0.  And X has unit standard deviation. So the variance of X is 1  Right, cuz we made it that way.  And so the variance of the weighted sum of the inputs is the variance of w in this case.  So the variance of the standard deviation of a j is the square root of the variance of w.  which is  the square root of the sum of the squares of the W's because these are all 0 mean right? So it's approximately just the sum of the squares.  Okay, so we figured that out.  So suppose the number of inputs to unit J is M. So we have M inputs coming in call that the Fannin.  If we set the standard deviation of the weights to be won over the fan in square root.  You got the standard deviation of AJ is the square root of the sum of 1 / M square root squared which is just one over end and there's end of them. So the sum of 1 over n m x as one.  So the standard deviation is one. So this says using funny can h0me and younes standard Aviation then puts then we should initialize the weights to be one over the square root of the Fanning.  So assuming the inputs have been normalized.  The unit 0 many units are eviation. The sigmoid is funny can age then the way should be drawn from a distribution with zero mean and standard deviation one of her skirt event.  Okay.  So the point of this is that after normalizing the inputs by combining a particular weight initialization with the right sigmoid, we get normalized inputs the next layer up so 0 and 0 out standard deviation 1 in standard deviation one out.  And it makes some intuitive sense.  Get a room Wally has big fan in small change incoming weights can make it over shoot, right? It has a lot of weight. So we want them to be very small.  And if so, we want smaller incoming weights if the fan is big and vice versa. So do what I say.  okay, but  Okay, that's all great that then what happens we learn?  retrieves the weight  so things will no longer be zero mean and unit standard deviation fact, we don't really want that because eventually we want to learn online your features.  So then enter batch normalization with just came out like I don't know where to go at this point.  Batch normalization uses all these ideas, but dynamically as the network is being trained.  So after the  after the activation of a of a hidden unit layer has been computed through forward propagation.  Then batch normalization makes them all 0 mean in unit standard deviation and then it's going to do the same thing the next layer up in the next layer up in the next layer up.  so  it normalizes all the activation sin the network the inputs in the chitin unit throughout the network on a per-unit basis overreach mini batch.  So this is the cliff notes version z-scores every variable.  At every layer of the network over the mini batch. So you run the mini batch into the first hidden lair.  Then you Z score those so this hitting unit has a bunch of different values over the input in the mini batch you z-score that naked 0 mean in unit standard deviation. Then you propagate it up do the same thing again, etc, etc.  And then well suppose that's not a good idea.  Hit ABS on a little bit of stuff to give the network a chance to undo that.  Okay, cuz maybe that's not a great thing. Okay, so maybe it needs to be nuns he's scored. So, okay. So you take every variable like this might be the activation of the third head unit in the first layer over the mini batch you compute the mean of that and divided by the standard deviation and add a little something to avoid / 0 just in case this gets really small and that's a z scoring the variable.  Okay, then.  Then what are you do you take that nicely z-score thing and you multiply it by this parameter and add that parameter. So these are learned by backpropagation as well.  And this is specific to this particular hidden unit as is this and if you start them out around 1 and 0 then you get z-score and variables.  But their learnable so you can replace you can change it to not be z-score it if you want to in fact.  This transformation a lot. If you figure it out, you can actually invert the scoring completely if you learn the right gamma and beta.  Okay.  And the cool thing is well, look at this. You can actually these are smooth functions. You can differentiate these.  So and you can differentiate this so you can learn these guys.  Do gradient descent on these parameters?  Okay.  However, there has been some discussion on the web and I don't know if it's resolved yet. But some people have found that if you fly batch normalization to the input to the layer instead of the output of the layer it works better. Who knows.  So we're still in empirical mode with deep learning. We have some theories about deep learning and surprisingly one of them just assumes the whole network is linear.  You know, maybe you can figure you can get something out of assuming the whole network is linear because then you can just multiply all the weights together. You have a single layer Network.  Spell but it turns out if it keeps on coming keep getting more results out of that surprisingly.  Who knew?  And they seem to apply to real deep networks with I mean if he is relo your kind of linear, right?  Okay.  Okay, so now some slides from Jeff hinton's course.  About mini Bachelor ending ways to speed it up in the first ones going to be momentum.  So  instead of using the gradient to change the way to keep a running average of the weight changes.  And use a second, so that's one thing.  And you can use to separate adaptive learning rates for every parameter. So for every weight in your network, you can have a learning rate.  Dudududu and then you can slowly adjust those things the learning rates using the consistency of the gradient for that. Okay. So let's  and then Jeff Hinton had this rmsprop idea.  that normalizes learning right wolf kind of get to that but  Okay, so just to reminder we've got this.  quadratic Bowl that's usually a pretty good approximation and  the steepest descent here doesn't go to their which is where we want to go to the bottom of the bowl. The slope is going to be that way.  Here is a salt will be good in here. It'll be good. But over here, it's going to go hoops.  Okay, and if we go too far  then what happens we we end up like increasing the air if we have too high of a learning rate.  Okay, so that's why it's good to lower. The learning rate generally is Hugo.  So what we want to do is move quickly and directions was small that consistent gradients.  And slowly and directions with big but inconsistent gradients and what do I mean by that? So I mean by consistent? Okay this way the gradient is shallow, but it's consistent keeps going that way this way.  The gradient is high cuz it's a steep slope. But if I jump across there then it's going to be pointing in the opposite direction. So I'm going to go like this. Right and so those gradients have different signs when I jump across this funnel. I get a different I get an opposite sign gradient.  so the intuition behind the momentum method is  I go in the direction to the gradient, but then my next one which is going to be that way.  I add in some of this time averaging the gradient is a go and now I go that way the gradients that way but it's been pushing me this way. So I'll go a little bit that way and in the end.  Those oscillations will average out and I'll head straight down.  If I keep track of my previous gradients and average them in.  The momentum keeps it going in the in the previous Direction. So instead of changing the way to cording to the gradient you change the way to cording to the average roughly of the gradient overtime. It's an exponentially decaying average of the gradient.  So that dance the oscillations from going this way and because I have a lot of little guys pointing me this way after while they add up and I go even faster in that direction, so that does exactly what I want.  What's happening? Okay.  That makes sense.  No, okay.  Not at all.  Yeah.  I keep track of a running average does a gradient so  if I started here and it's a positive gradient and then I end up over here. It's negative to positive and a negative is going to be smaller than both of them.  So I'm going to going to add the last going this direction. If I keep a running average of the gradient going this way. It's going to get bigger and bigger say average in all these guys going the same way.  Yeah.  if if you've been getting opposite signs  So are you going to stop doing that?  And then I'm going to start doing that.  Yeah, yeah.  So here's the equations, right? So  you take the  hidden calls it the velocity cuz it's really the velocity not the momentum if you're a physicist and I think he's start out life. Maybe he's a physicist. So here's the gradient. That's what we've been Computing and we take the running average of the previous one and we want to go downhill in the air so we subtract and that's our new weight change.  sohvi, here is the  Is how we change the weight?  Survivor becomes big Delta W which is how you change the weight.  Epsilon rho. That's a learning rate.  Yeah, sorry.  Yeah, I use different characters.  Alpha is the momentum rate.  So the weight change is equal to the current average she take okay. So how fast should be less than 1 cuz you're taking some fraction of how much you've been moving and this has to this should be small Epsilon here should be small. So usually the momentum rate is about 2.9 or something like that in this can be like .01 or whatever.  And so you're taking a big chunk of the way, you've been moving and a little chunk of which way is downhill from here.  Okay.  And so this is the sum fraction of the previous weight change.  Minus the current gradient cuz we're going to keep going downhill in the gradient. That's why it's negative.  Okay.  So if the air surface is a tilted plane.  one way to think about this is what if the  What if the gradient is always the same?  And then you can kind of apply this formula and saw.  So it turns out if the gradient is always the same.  You can solve this and get this and what it does is this is at Infinity time steps the velocity or momentum.  Comes out to be one over 1 minus alpha x this this is 9. So this is 10. So you're essentially speeding up your learning by a factor of 10.  Okay, it's a recurrent. It's a recurrence relation, which maybe you didn't 21.  so  Just saw that recurrence relation. That's assuming that.  This thing is always the same.  So you're on a tilted plane? Okay.  So that's a kind of analysis you can do. It's not exactly correct, but it gives you an idea of the effect of momentum. Yeah.  Is the alphas constant usually at?  Well, Jeff, I'll give you some different advice in a moment. Yeah.  What time do you update your?  Yeah, how many batches? Yeah, if you have small many batches then your radiant is going to be less consistent, right? Cuz a small mini batch is a bad sample of all your data, right? It's it's more going to be more idiosyncratic. And so the the direction of the grating is going to change more.  So this is Jeff's advice.  These are Jeff slides that I've made small changes to So He suggests starting at the beginning of Learning. There's going to be big gradients because you're beginning to learn and so start with a small momentum and once they've disappeared in the weights are stuck in one of these Ravines then crank the momentum up.  And you can learn at a rate that would that would oscillate without the momentum.  You don't know.  music  in fact recent more recent analysis has suggested that a lot of times were in a saddle point, which means you know, what a horse saddle looks like it's like this answer your like right here on top of the saddle on you.  Good morning, the less you care.  Yeah.  Yeah, exactly.  You got it. He's got it asked him if you have a question.  Right when the when the learning slows down.  Would it be that towards the end of my hair less about the previous average because we've arrived at the correct rating.  Yeah, but we want to go faster now that we're on this shallow thing.  Okay, so that's momentum, but now forget everything I said because there's a better version of momentum called nesteroff momentum.  So what the momentum does is say? Okay. I'm going this way. I want to go a lot that way. But before I do that, I'm going to figure out which way the slope is from here and add that to it.  Hey, that seems a little weird because I know I'm going to go that far but I'm going to compute the slope up here and add that to it. So I'll go that way a little bit but down there is going to be different than it is up here probably.  Write him in a different part of the space. So I'm adding and taking this big Vector, which is the gradient.  And I'm adding a little Vector to it, which is the gradient right here. So this the running average of the gradient. This is my gradient at this point. So I'm making a little correction and then I'm taking a big jump wouldn't it be better?  To go that way and then compute which way is downhill from there to make a little correction.  That's the difference between regular momentum and nesteroff momentum.  So the standard method Starts Here, we know we're going to add that big momentum term in and it computes the gradient right here and adds it to that seems wrong instead. We should go there and then make a correction.  That makes sense.  You're not shaking your head. No this time.  Okay.  heliostats Guyver  thought of this  I'm so you first make a big jump in the direction of the previously accumulated gradient the momentum term and then you measure the gradient where you end up.  And make a correction. So it's better to correct a mistake after you've made it to see idea instead of before e  So here is a picture of the nest Rock method.  I take a big jump because I'm using the momentum term which is a big Vector to make this big job. Then I compute the gradient there. So in the end my weight changes is some of those too.  Now I take a big jump which is like the average of those two. So it's shifted a little bit to the down.  Compute the gradient there. That's my new gradient.  On the other hand what momentum standard momentum does is it figures out which way the gradient is here and then add set big factor to it.  Okay, so that seems better, right?  Yeah, what?  Directions to make the big jump is the momentum term. It's the weighted sum of all your previous gradients.  And then I do that and now I take these two and average them together this the momentum. This is the correction.  And that's going to tell down a little bit from the green doctor because I've been following this Direction with a little bit of that and it got a little bit that way that I make the correction. That's my new way chain.  Yeah, yeah, we're still doing momentum. It's just like when do we add in the gradient?  So it's really just a slight change in the code.  Yeah, well.  These days we just use the Adam Optimizer and forget about it, which probably wondering if that has I think so. Your next assignment is going to be using a package.  Yay. Does the gradient for you? No more back prop. And so it's God's, you know, one from column A to from column B and I like switch on nesteroff momentum switch on the Adam Optimizer. You don't have to worry about any of this stuff.  Okay.  in the remaining two minutes  adaptive learning rates. So the gradients are could be very small and it really layers and there's going to be a different fan into different units depending on you know, like in your guys last assignment you had like a huge fan in from the pixels and then a tiny fan in  Well, I'm at smaller fan in like 50 to the outputs, right?  Oh wait. No. That was the last time he didn't have hidden units last time nevermind.  This time you have hidden units. So you got like nine hundred inputs to the hidden units. So that's a fan of 900 and then you got 50 hidden unit. So the fan into the outputs is 50.  Vs900 that's least an order of magnitude different.  So you want the gradients are the learning rate should be different for those cuz he make a small change to a large amount of Weights is going to change your weight did some of your inputs much more than that. Same Small Change 250 weights.  All right, so that implies maybe we should have adaptive learning rates.  And then the remaining minute.  Rick we start with a Global Learning right and then we have a little multiplier on that called the gain.  That's this guy. So here's our initial learning writing. Again. Jeff uses Epsilon gij is the Adaptive guy every weight. "
}