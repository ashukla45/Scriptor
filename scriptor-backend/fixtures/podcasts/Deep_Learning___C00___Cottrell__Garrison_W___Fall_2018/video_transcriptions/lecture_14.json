{
    "Blurbs": {
        "0 0 1 0 0 1. Okay number 2 is there. Okay, and so it's supposed to Short. It's giving some numbers. It sells it where in the sequence it is, roughly. and am I running over now? So So what they did was they tried to predict where the memory locations were by fitting a linear function of the priorities to be observed right locations, so they have no ": [
            3872.2,
            3912.9,
            89
        ],
        "1.001. So again a large betta sharpens the address. Going to find the closest thing. Huh? It's probably not correct. If it can't find it. I mean it has to learn to use this and so it's going to have to see if it needs something later. It's going to have to write it to memory. Right, so I'm going to have to learn to do that. This is mind-boggling ": [
            2656.5,
            2701.2,
            58
        ],
        "Age 3 of developed the model of other people that allows you to reason about what they know. And I'm trying to reason about what you know, and how would I say leads to what you know, and then I hope that when you take the test, you know what I'm asking on the test, right and this can go on. You know, I know that you know that I ": [
            4164.2,
            4190.5,
            97
        ],
        "Barger ranges are possible intend to use them. So this is you have to work this out on your own. But basically this is a shift if st001 that increments the address by 1 so. 010 become 0 0 1 that is this is location one location to location three location location location. And this is location 3 so it goes from location to location three and this is a ": [
            2923.0,
            2963.8,
            65
        ],
        "Computer gradient through I don't I have mentioned that this is possible before but this is I think the first time we've actually shown it. writing to the memory is inspired by dating from an lstm and they use gates to write to the memory and a write operation happens in 2 steps first. Erase the memories and then you add to them. so here's a racing memory element. I ": [
            1788.6,
            1827.1,
            38
        ],
        "E. What's wrong with e? Yeah. map to work Yeah, if it ask for a translation immediately and for example of German the verb doesn't come until the end and so if it was Germany in he wouldn't know what to generate for the verb for English right away. And so you need to I mean the story here is anyway. Except for attention is all you need that you ": [
            647.2,
            693.3,
            9
        ],
        "Even back in the late 90s. There was a guy Colorado Delta neural-net where it had a stacked and all it had to do was learn to use the stack. It had outputs that would control the stack. So this is the neural turing machine. It it's everything inside the stash box. Take some internal external input. It's got a controller and this they try to different things at least ": [
            1280.2,
            1316.3,
            25
        ],
        "How is it computed? Yeah. I don't know if it's enforced to do it that way, but I I think it learns to do that. Okay, so there's no overwrite operation in arithmetic. We could use an ad Vector. That's the new Vector minus the old Vector but that requires three steps and I believe this is an exercise for the reader. UK Oh, wait. Damn. Okay, I didn't leave ": [
            2147.2,
            2193.6,
            46
        ],
        "I mean, it's just not it's taking in a 20-minute sequence. Instead of a water was training on which was 10:00. And it looks like it's just got one of the bits a little Ron probably. Of course finder that which looks pretty dark to me. So that's probably supposed to be off and it's too little active. And then it turns yellow and then turns blue. Very cool. So ": [
            3793.6,
            3833.4,
            87
        ],
        "Okay gang. Let's get started going to start right off with some clicker questions. But before we do, Are people getting enough resources to finish the programming assignment? Yeah. Okay good. All right. Don't forget to release your pot if you're done with it. Okay. So today I'm going to talk about the neural turing machine and which is a structured neural network with the learn programs from examples and ": [
            66.7,
            119.8,
            0
        ],
        "Okay, so we'll see that in the second. Any other questions? Okay. And if W was a half a 1/2 0 then actually you get the average of the first two rows. So you could actually use it as a kind of way to compute something from the words. That's that's a weird usage of it. But remember the network has to learn to use this this is differentiable so ": [
            1706.0,
            1746.1,
            36
        ],
        "Okay. So this part of the model takes the content-based address of a navigating variable which switches between content-based addressing in the previous memory address Factor so we can incrementar decrements OC it takes the previous guy and the new guy that's just been content the soups either means content-based. And we're going to interpolate between those. The other thing you can do though. This guy is going to incrementar ": [
            2737.9,
            2774.1,
            60
        ],
        "Okay. alright, so the answer was e so any any questions? Okay, okay. Alright. Okay, so neural Nets have been shown to be touring equivalent. Assuming in fact, they're super touring if you assume the weights are infinite precision as Hava siegelmann shown. But can they learn programs? Well, yes, they can we know this at least for my own work that I told you about before where we trained ": [
            996.0,
            1043.3,
            17
        ],
        "That's roughly like 5 should be somewhere in the middle. So it's it's thrown them in order of priority right into these locations. And then they say we hypothesize that to test this refit Interlinear Network reads for memory and increasing or they're so I basically I was saying that I said, so it's got to take the priority and multiply it by some number to get where to go ": [
            3951.8,
            3990.3,
            91
        ],
        "U14 of you answered e Do you want to Ask her talk about. Why more Wyatt, isn't he? Okay. This is all perfectly clear. You'll get it right on the final. Okay, okay, okay. One more, which of the following were used for beer mind? After I open it up. What is spearmint you have to come to the last lecture? Where we talked about fear mind in Factory demonstrated ": [
            736.0,
            783.5,
            11
        ],
        "Yeah. All those up and you get the second row, okay. So that's reading so here's an example 3.14159. I might recognize that sequence of bits. Anyway suppose we want the second row if it was if the weight doctor was 010 and then again, and this is just not really. Awaiting the standard since it's a computed way then this would yield 5997. So w is the address, okay. ": [
            1618.8,
            1659.1,
            34
        ],
        "Year how to program Okay. Okay, don't you try again? run guys girls dogs Why don't you answer your clicker question yet? Okay, there's three more of you haven't answered yet. Pick an answer in the answer one more. Okay going going going gone. Okay got very similar responses. Okay, what's wrong with a That's a feed-forward net now. I try to keep up you guys like it that I ": [
            439.2,
            567.5,
            6
        ],
        "a hey, that's why I just realized that's why it's not really so cameras to this becomes that. Okay. And that I compute it so you might think wait if camera is 2.9 becomes .81. But wait, this becomes .01 that stay zero. So when you do this ratio thing it comes out that way. k So that's the addressing mechanism. Which is okay. So first you create a vector ": [
            3005.6,
            3052.9,
            67
        ],
        "a sequence of random bed vectors followed by a scalar representing the number and of copies to make and then it repeats the sequence in x so the lstm network Also learns this in fact, it learns it better than it learn the previous one. Lstm controller neural turing machine learns it pretty quick about the same level of accuracy. There's one blip here with the lstm controller. Okay, and ": [
            3562.9,
            3601.9,
            82
        ],
        "address based on similarity to existing memories. Then you can switch between content and location. You can increment or decrement the address and then you can sharpen it. And that's it. That's that's the so all of this is built in to the architecture. The network doesn't like choose to do this. It's got to do this. It's part of the built-in mechanisms, but the cool thing about this I ": [
            3052.9,
            3087.5,
            68
        ],
        "again, what these are showing is that red is one blue is zero. All of these has its really confused basically is somewhere between zero and one. Okay priority sort. you got a number this is priority 5 this is priority 7 What's priority 46321 etcetera and that what it's supposed to do is output whatever was one. Which was 1 0 0 2 0 1 0 0 1 1 ": [
            3833.4,
            3872.2,
            88
        ],
        "allows you to go through with the content memory or switches to the location-based one. And asks is a shift factor that can increment decrement for leave the address alone. Gamma is a game parameter on the soft Max address making it more binary. So what this does is if you have a wussy softmax output that's like point-to-point point 1.8.1. It makes it 010 or tries to Okay. So ": [
            2341.5,
            2381.9,
            51
        ],
        "and ask the two-year-old. Where does this person? He left the room going to think the candy is the note say, oh it's under their don't think it's under there. It's takes and takes about when they get to about Age 3, then they realize the guy doesn't know that it was moved while he was out of the room that's called theory of Mind in psychology. And so by ": [
            4138.3,
            4164.2,
            96
        ],
        "any did I go too far down you have to stand up to see the bottom. Okay? Yeah. I'm so there is some stir one. So it's positive and we want a peaky softmax. That is one. That's really high on one thing if we want to read one row. yeah, it's it's like a probability distribution and you could there's you could actually do some computations for this thing. ": [
            1661.5,
            1706.0,
            35
        ],
        "at what point do we call that consciousness? You know, this will probably happen in your lifetimes and you should be ready for it. Cuz that's when when they start being aware of themselves, that's when we should start worrying while we start wearing now and people are okay. So that's it for today or give you 10 minutes more to work on your programming assignment, okay? Have a great ": [
            4227.4,
            4326.0,
            99
        ],
        "based on my lecture that is Okay. What? Going. Oh, wait. I just wondering why everybody was answering that one? Okay. Okay. Sorry about that. During and there's two more of you at least that haven't answered yet picking answer any answer. Okay. Okay, why don't you turn your neighbor who has a 61% chance of having the right answer and talk about it for a few minutes? Happy New ": [
            366.8,
            439.2,
            5
        ],
        "be shifted left or right? And the tape is infinite Armada won't have an infinite memory. But that's the kind of thing we want to do. So can we build one of these things out of a neuron that we build that out of a neural net? And that may seem kind of crazy like these folks at MIT you built a tinkertoy turing machine that played tic-tac-toe. It's about ": [
            1211.1,
            1244.2,
            23
        ],
        "beer mind. Okay, what was different about beer mind? What to do to make sure that all the way through you knew the right you were doing the right thing. Okay. I'm going to close it out. Okay now. Your mind was an innovation over previous things. You could have said see which must have you did actually and say you'd get that wrong in the final here. He gave ": [
            783.5,
            843.3,
            12
        ],
        "but it does pretty well up to like 50 and it was only trained on once this so generalize has the longer sequences then it was trained on. Okay. Do do do do. And then this is the lstm network it was Trent. This is just a vanilla lstm Network trained on up to like 20. Does that really well, but give it a little bit longer and it screws ": [
            3367.6,
            3401.6,
            77
        ],
        "but it was constant. It wasn't like the output of the previous time step. Okay. All right, is that makes sense? Is that sound right? Cuz actually 47% of you said See which would have been the old way of doing it before Zach came along but the way Zack actually did it was this way. What's another example of something you might use this one for? How do you ": [
            878.2,
            929.4,
            14
        ],
        "circular convolution operation that you can try at home and make sure that it does what I just told you it does. and then the sharpening box does the show here's why is it anyway, but this is Wait, there's this act like this isn't actually a soft. Oh, it's not actually a softmax. I guess. That's why. Okay, it's not either this thing. It's this thing to a to ": [
            2963.8,
            3005.6,
            66
        ],
        "commands or process. He's addressing mechanism on the other hand is very complex. And that's the hardest part to understand one of the cool things about it though is that it allows the network to either use a Content addressable memory or a location memory. You guys know what content addressable memory is. Nope. Okay content addressable means you give a pattern a bits and the system returns all the ": [
            1386.0,
            1430.1,
            28
        ],
        "contents of locations that match that pattern of bits. Hopfield network is an example of a Content addressable memory. We haven't talked about it. It's the simplest kind of neural network, but it you can store patterns in it and you can give it part of the pattern internal complete the pattern like you can store actually every unit in the network could be a pixel a binary pixel and ": [
            1430.1,
            1462.3,
            29
        ],
        "day. feel free to Pat Wiley so for the country, so I read something online about like pretty similar to the I think it was actually a turing machine. ": [
            4326.0,
            4365.5,
            100
        ],
        "decrement the memory thing or leave it alone. You can do content-based addressing and then increment the address and so one of the programs they teach it is to store these pairs of vectors and it puts one vector in this location memory and then it's associate. It's associate in the next place and so it finds the location by content of dressing and then increments it to get to ": [
            2774.1,
            2806.3,
            61
        ],
        "does here, is it computes this Colonel K? Okay is is a function. It's cosine similarity. So it's Computing the angle between this factor in this Factor. Okay, so if cosine is maximum, it's one it can be minus one that can be zero etcetera. So K is just a function that computes the cosine between the ski vector and this memory element for a location. I okay. So there's ": [
            2424.0,
            2464.8,
            53
        ],
        "exponent, right? So it's easy to the this and so if this is like, you know 2 or something, it's going to make a really sharp if this was very small it would flatten. the distribution Yeah. Yeah. Sorry. And in one of the examples that it does pretty much computer exactly what it want. I didn't get slides ready in time for this had too many meetings today, but ": [
            2537.8,
            2601.3,
            56
        ],
        "going to be one of these for every This guy is the index for the memory like 1 2 and 3. And if K is most similar to what's an address 1 this will be high. And this will sharpen it and I'll come out with 1 0 0. If the key is closest to the second element, that one will be high and it'll come out 010 or close ": [
            2464.8,
            2496.5,
            54
        ],
        "guess is that all of them are differentiable, which means we can back propagate through them to change the parameters. Do do do do okay. I was asked to come up with something besides do-do-do-do, but I know I can't think bathing. Okay, so what can we do with it? So you can learn to copy you can learn repeated copy associative recall Dynamic engrams. Send Priority sort. They compare ": [
            3087.5,
            3123.6,
            69
        ],
        "has these they think these little things are no. Sorry. This isn't see ya. Okay. This is the lstm vanilla lstm network is screws up terribly might expect. Doesn't doesn't generalize like the lstm bntm does. This is what it does. So here's the input. And it's adding that to the memory. Here's the outputs and it's just going they looked at these these seem to be signaling go back ": [
            3643.8,
            3694.1,
            84
        ],
        "have to do this. It has to generate the E and it has to generate the W so I can do some other weird stuff instead. We'll see what what happened. So. But if he is zero or w0 nothing changes so for if for this is again a soft neck, so if it's 1 somewhere it's zero everywhere else or close enough for government work. And then the e ": [
            1868.3,
            1903.6,
            40
        ],
        "haven't seen the input to limiter receive the input Factor right It To The Head location increment by 1. And while return had to start location, so we've seen the input to limiter read from the head location admit output increment head location by 1. Okay, so this isn't written anywhere. This is just what the lstm controller is doing. So these this is like the internal state of the ": [
            3478.7,
            3510.9,
            80
        ],
        "here and then it gets that and then it gets that and then it gets that and then it gets out Colin and then there's a marker it says okay, repeat that back to me and then it supposed to have put the exact same thing. And this is the output but after the end of this not a it's not lined up in time. Ettore's train on like 20 ": [
            3271.0,
            3297.5,
            74
        ],
        "here's some more uncertainties but like it's still doing pretty well up to link 50 and then this is I don't know what a hundred and and the reason why it goes bad so early here is because this only comes out after this whole thing. So it takes in this whole thing and then start price to generate it and it it starts to screw up pretty early there, ": [
            3338.6,
            3367.6,
            76
        ],
        "idea but they're trying to like analyze it. So this is what they figured the right locations ought to be by fitting a linear function do what they were in another run. And then this is what they found it looks pretty clothes. And here's the Reid waiting. So what's happening here is it's kind of taking the priority and throwing that bit that sequence in a location in memory. ": [
            3912.9,
            3951.8,
            90
        ],
        "in memory. How does it do that? I don't know. They didn't know. Okay. So the neural turing machine is a recurrent neural network augmented by memory. The network is given these mechanisms there built-in and I can learn to use them. Because it's end-to-end differentiable. Okay, and so it learns algorithms from examples of desired input-output sequences, and there's been more work on this. They had another paper called ": [
            3990.3,
            4032.1,
            92
        ],
        "in put some sequence? And then after the end of the sequence you have to Output 1 2 3 4. So are used to call this the recorder problem and my students and I spent quite a while trying to get you know, just repeat after me. Basically, we spent quite a while trying to get an element that to do this and we could not get it to do ": [
            3158.3,
            3182.8,
            71
        ],
        "is a is usually a one so that you erase one row of the memory. by the network now it's computer in real time. W at time T So as its. thinking along its Computing these W's and this is Multiplied pointwise. So this just zeros out. If both of these are wanted zeros out that memory location location High. Right. So after this operation at time T minus one ": [
            1903.6,
            1957.0,
            41
        ],
        "it and notice that you don't have to know what's there to erase it? Okay can just erase it with this. Yeah, probably would wanted to be that way. But it's got to learn to do that. Yeah, I mean first you're going to race it. Then you're going to ride it. So you need this problem if you want it, if you're talking about the same guy. Let's see. ": [
            2110.8,
            2147.2,
            45
        ],
        "it as an exercise for the reader. Alright, okay now for the hard part, which is Computing the address. these are outputs of the controller all of these things and they've got built in operations that lead to an address in the end of the old address. We have the memory that goes into this and then we have these five things that are actually computed by the controller and ": [
            2193.6,
            2231.7,
            47
        ],
        "it seems about as practical but But a neural turing machine doesn't have to be so impractical. And the main idea is to add a structured memory to a neural controller that it can read and write from so we're going to give it a go gadget a memory. We're going to give it some structured. Things that it can learn to use to read and write from the memory. ": [
            1244.2,
            1279.1,
            24
        ],
        "it to actually develop a program and here is actually that's where it should be this time. So starting with this you map to that and then you work at the next one map to that etcetera. And this was my model. And we had a program that we are basically training it to do so would give it a two digits as an input. And out training with output ": [
            1043.3,
            1077.8,
            18
        ],
        "it. I want a Russian Stout and Imperial Stout grade 5 and then you would generate the sequence that character at a time, which is awesome. Except it. That didn't work very well for him for his ACT Lipton instead. What did he do? Yeah. Yeah, he gave it that context all the way through. He said Russian Stout, five Russian set five Russian staff fun. So it was here ": [
            843.3,
            878.2,
            13
        ],
        "know if they know the answer. I wrote one of them about one of these things and I said how does it do that? I don't know. Okay, so this is a little scary right? I mean you can't explain itself. Okay, how about priorities or any questions about this besides the ones I can't answer? Yeah. I guess it out with v. I, I don't know. It looks like ": [
            3749.7,
            3793.6,
            86
        ],
        "know you seen Princess Bride. Great movie. There's a thing where they have this guy is with another guy and they have two cops. One of them has poison in it and they switch them around and and they try and reason about how well you think that I think that you think hey, Auntie. yeah, so you should see Princess Bride if you haven't seen it. So and then ": [
            4190.5,
            4227.4,
            98
        ],
        "let's start with this guy. So with this these two guys do is it computes an address? By doing a softmax. And beta is like a temperature parameter here. If betta is Big it's going to make the soft Max sharper. Okay, it's the inverse of the temperature parameters bettas High we get a more discrete distribution with a 1 in one place and zeros everywhere else. So what it ": [
            2381.9,
            2424.0,
            52
        ],
        "like all men are mortal Socrates is a man therefore Socrates is Mortal. That's a inference that you can make logically. I'm doing some reasoning can these things be trained the reason at some point and then can they learn to reflect? So I have a model of you you have a model of me in your head. You have a model of yourself in your head. I remodeled my ": [
            4076.2,
            4108.5,
            94
        ],
        "logistic output in the network. So somewhere there's a logistic and it feeds into this process exactly that way. Okay. Oh and I added this to make it clear where this goes. And a mixture doesn't really make sense here. Okay, the convolutional shift box takes in the content-based or in the location-based address and then it can incremented by -1 0 or plus one based on the s factor. ": [
            2878.8,
            2921.3,
            64
        ],
        "memory b c and d choose the best answer. it's probably not a to view that are picking a okay. Going everybody I'm coming going. Going going gone. Okay. What's the answer? What's the answer? he so it does all three of these things the weight of one like the way this is like the skip Connections in a in a neural in the resnet architecture allows air to get ": [
            167.4,
            242.8,
            2
        ],
        "of just having a next to you have a next after a carry in that part of the state spaces allows it to remember to write an extra one add one to the result. Yeah, so you can explicitly training her on that Temple Mentor program. But that Network had to learn to remember things and its internal State space and when it had to remember an extra bet it ": [
            1142.1,
            1177.6,
            21
        ],
        "of these all of this these gadgets that they built into it are differentiable so it can be trained by backprop Through Time. Okay, so we're going to structure or neural network to act like a turning machine. Okay, and there were current one works better most of the time though not always. Okay, so the read and write heads are pretty simple. They're going to be pretty simple arithmetic ": [
            1352.1,
            1386.0,
            27
        ],
        "okay start generating now. This little bit says okay start reading now. And here's it's doing a bunch of ads and look at where it's putting it in memory. It's these are the the weight of the addresses. Okay. These are the addresses reading it back out. Okay. This is the program it learned. It's the same program. You would have written to do the same thing, right? Well, you ": [
            3442.5,
            3478.7,
            79
        ],
        "on the content match which basically sharpens it. And there's different. Camera is also gain parameter, but they use them in slightly different ways, which is kind of annoying. I don't know why they have do it differently in two cases. Sobeyda sharpens. The content match G is a switch between content and location-based addressing. So it goes in here and it's a it's a 01 Logistics gate that either ": [
            2304.1,
            2341.5,
            50
        ],
        "previous input, right A1. Okay, and and this thing this is an example of the kind of thing. It could do it would either output right next to right Terry next door right next door done depending on what the situation is with the digits and it had a kind of internal representation of States like a finite State machine. And the memory for a carry was appear by instead ": [
            1106.1,
            1142.1,
            20
        ],
        "propagated very far back. It allows networks to store. He's for a long. Of time and that uses gates to control the memory. But as you heard last time or time before we're just going to have participation points in clickers now after the KQ s thing, okay? Okay, it's not advancing. Okay, try again. Okay recurrent networks can be used to caption images translate from English to French understand ": [
            242.8,
            297.1,
            3
        ],
        "return the closest match. Okay. You can buy a Content addressable memory, but they're very expensive. I don't know if you can actually still by 1 probably NSA hasn't okay. So this memory has three words of like for okay. and we can index these Rose as one two and three and we can read from it using. the weighted sum of every this means every memory row essentially times ": [
            1539.9,
            1581.8,
            32
        ],
        "right and the lower digit of the sum and then if that's some was actually bigger than the radius is the output a carry and I don't care. What's this? Here means I don't care what's in the result field cuz I'm not paying attention to it and then ask for the next column of inputs. And if you come off the end and there was a carry on the ": [
            1077.8,
            1106.1,
            19
        ],
        "sequences up to like 20, this is 20. This is 30 think that's probably 40 or 50 and where it starts to go wrong. You can see that it's like getting unsure. So this is where the lstm controller. and it's starting to get a couple bits wrong and then it Let's see. So this should look what your seniors this should look just like that and it doesn't. And ": [
            3297.5,
            3338.6,
            75
        ],
        "son what I can think about what I might do in a certain situation that's called theory of mind takes at least two 2 years to learn. So if you take a a two-year-old and you hide some candy under something while somebody's in the room and then the person leaves the room and now I had the candy I take the candy out of here and put it there ": [
            4108.5,
            4138.3,
            95
        ],
        "speech and be a beacon sea. Is it? Okay. I guess I should have asked where there any questions about the last one. Okay. Hey going most of you've answered now going going. There's one more now going. Gone. Okay. The answer is He okay. This is not working. Okay. Which of the following would be used for language translation after training? Is it a b c d or e? ": [
            297.1,
            362.0,
            4
        ],
        "spell? Generating what? Yeah, so learning a language model. So not only did he give it Russian. Russian Stout 5e also gave it the character from the previous time step, which is a standard thing to do learning a language model. You have a stark symbol you generate the first letter or a distribution over the first letters a softmax you sample from it. So you're not doing a deterministic ": [
            929.4,
            965.3,
            15
        ],
        "steam controller. Repeat any questions about what this represents this is the input sequence which is external to the machine the output sequence generated by the machine. This is the internal State and then it starts its reads. And it just does exactly what you would have done. But sit in a sequence of locations and reads those how Okay. Wow, okay, good like that. repeat copy the Network's giving ": [
            3510.9,
            3562.9,
            81
        ],
        "taken the sequence and get a representation of the meaning of the whole sentence and then you generate the other language. Okay. So you need to find an embedding of all of the words in the sequence into a single memory Vector the hidden unit vector and then from that generate the output words. This is too fast, generally. Okay. So the answer is D. So there were 23% of ": [
            693.3,
            736.0,
            10
        ],
        "tell you about current things. So I'm going to talk about attention is all you need which is a paper that came out of nips last year and has 790 citations already and it's actually a feed-forward network that does language translation to full blue score points above the previous state-of-the-art. So It wasn't a terrible answer. It's actually feed-forward. It's not convolutional. It's just got a lot of attention ": [
            567.5,
            602.8,
            7
        ],
        "that's built into the the network, but you can back propagate through it all and learn the these things. Okay, this is called the key Vector for Content addressable memory. We want the memory element most similar to this Factor. So this would be a length for vector. And so that's that's the key. That's the thing. We're looking for the thing that's that content. Beta is a game parameter ": [
            2268.9,
            2304.1,
            49
        ],
        "the differential neural compute differentiable neural computer that added and ability to have structured data structures and it learned like a map of the London Tube system. so yeah, it got so that's That I am in the big questions now or now we got this thing. That's like a turing machine. Okay, how long before this kind of thing learns to networks that leads networks that can reason write ": [
            4032.1,
            4076.2,
            93
        ],
        "the feed-forward also learns it. Tell this is an example. So this is actually a bunch of bit patterns. Okay, and This is what it supposed to Output. And it learns to do this. and Red is wine and blue a zero. So these are our here. It was trained on like 20 sequences. So there's it's like 0 0 0 1 0 0 0. I think there's 10 bits ": [
            3218.7,
            3271.0,
            73
        ],
        "the normal kind. Okay. So this is the memory it's just a matrix of linear quote-unquote neurons. And you can think of each row here is a word and the memory and we're going to read and write a road a Time. Although the mechanism allows it to actually read from many locations at once and just some of them up. Yes. Yeah, so in this case, it's going to ": [
            1501.8,
            1539.9,
            31
        ],
        "the pear. So like if I say cow Railroad Bone elephant, you know I give you these pairs and then I asked you what goes with bone you have to come up with elephant. This is a standard kind of test. They do in Psychology experiments. I don't know. Why it's interest I do know why it's interesting but it seems weird right pair parrot Associates. It's called so they ": [
            2806.3,
            2841.1,
            62
        ],
        "the same weight Vector then going from here to here with 0 1 0 and a being 1 2 3 4 5 Now Rich a vector into memory. So why do we have to do a right in two steps? tongue. 10 seconds turn the right to vote. Yeah, basically there's there's no overwrite operation in arithmetic. Right, so you have to erase what's their first and then add to ": [
            2044.9,
            2110.8,
            44
        ],
        "then we get this. Okay, yeah. Yeah, W is going to Range Rover all the memory. It's a vector right? It's a softmax vector that's got as many elements as there are memory cells her memory Rose. Yeah, he know he is one one thing. So this is you know, he is typically going to be are unit vector or and but it could be something else but it learns ": [
            1957.0,
            1999.3,
            42
        ],
        "there's a one program that learns World learns associations between two vectors and it if you asked for this Factor it find the memory location by this. Okay, so suppose kfc3100. Okay, that's supposed to match point 8.1.1 and bettas one. So these are the cosines. Okay, that produces an address factor that looks like that. I I actually competed this but a bit is 10. You got .99 8.00 ": [
            2601.3,
            2656.5,
            57
        ],
        "there's all this stuff that goes on just to generate this one address. Okay, and I'm going to try and walk through it slowly. And there's a while one one part of this is the kind of switch which switches between content addressing and and location addressing. Show again, please have to be computed by the controller and it has to learn. To use them so you've got this structure ": [
            2231.7,
            2268.9,
            48
        ],
        "they teach it to do parody. Okay. Okay. So what the interpolation box does is it just as a weighted sum between the content-based address and the previous address that was there the one you computer on the last time step instead of G is one you got completely the content-based one and if G is 0 you got the previous address. So this gay dating guy is just a ": [
            2841.1,
            2878.8,
            63
        ],
        "they used to Nellis TM for the controller and they used to feed for Network for the controller and usually the Lstm was a little better than the feed-forward one, but not always depending on what the what the problem was and then it has an output and a memory. and it's got some built-in stuff that allows it to read from memory and a right to memory and all ": [
            1316.3,
            1352.1,
            26
        ],
        "thing you take that letter that you generated and put it back here generate the next probability distribution sample from it. Put that in here. So that's learning a language model. It's the same. That's the thing that I showed you slides from where I learned from Shakespeare. There was another one that learned from nips papers and another one that learn from Wikipedia what the meaning of life was. ": [
            965.3,
            995.0,
            16
        ],
        "this is again thousands of passes. This is the number of bits that are wrong. Okay. So the Network's tested for General a generalization first the size of the pattern to be copied or doubled and then the number of copies is doubled. Here it's repeating 10 repeating 20 times which is 10 times longer than it learned. Here it's got a link 20 sequence input and repeated ten times ": [
            3601.9,
            3643.8,
            83
        ],
        "this one is for the one of you who said they like the blue slides. Okay, but first some clicker questions. so the main idea between lstm units is The user weird acronym so I'll get confused. the create a mechanism so Eric and get propagated very far back in time to allow the network to store values for a long. Of time until needed to use gates to control ": [
            119.8,
            167.4,
            1
        ],
        "this to they they use a two experiments with the neural turing machine with a feed-forward controller and LS cam controller and then they have a vanilla lstm Network that is does not have all this stuff built in and train it on all these tasks, but we're just going to do these three. Cuz I didn't get time to add fight about this. So where copy does is he ": [
            3123.6,
            3158.3,
            70
        ],
        "this wait, so RT is the vector readout from the memory of time t WT is a softmax Column Vector of length 3 which is the address. So if the softmax this weight Vector is actually it's not a static weight. It's a computed wait and it's the soft Max like if it was 0100 times the first Row 1 times the second row and 0 times the third row. ": [
            1581.8,
            1618.8,
            33
        ],
        "this. It's a hard task for recurrent Network. In fact, this is the lstm network trying to learn this and it doesn't ever get the error very all the way down after even a thousand passes through and this is in Thousand. So this is a million. Passes through the learning and the Ellis neural turing machine with the lstm controllers learns it almost immediately. Spooky and the one with ": [
            3182.8,
            3218.7,
            72
        ],
        "to it anyway. Okay, so this is built into the architecture. Okay, this just happens the network has to learn to produce K and beta the controller has to learn to produce those. So it's got this built-in Gadget right for Content addressable memory and we could stop there. But we want to also give its location based memory if it wants it. Yeah. Yeah, so this is in the ": [
            2496.5,
            2537.8,
            55
        ],
        "to me that this thing works at all. I mean, but you'll see what it does. Okay says this make sense the beta sharpens the address. So these are actual cosine scores. I'm made those up, but this would be bad right but this would be good. Hey, probably, you know, it's probably just outputs a big betta all the time. You know cuz that's what it needs to do. ": [
            2701.2,
            2736.3,
            59
        ],
        "to the beginning address. So this is in the memory saying okay time to get its reading out probably from a location in memory the the start address. Yeah, I don't know so sexy. Yeah, I All this is is just saying go back to your asking. Where in the internal memory. Does it remember how many times that's what you're asking and I don't know the answer. I don't ": [
            3694.1,
            3749.7,
            85
        ],
        "to use it the way we expect it to. Was that two questions? Okay. so if W so, here is our memory again. If W is 010 and he is one one one one one. Then doing this yields this. Okay, so that's a racing yet. And now I have to write to it and writing is just here's the thing. I want to right there. And if I keep ": [
            1999.3,
            2044.9,
            43
        ],
        "took five thousand more epics and we told the model exactly what to do. On every step. So can it learn to program just by seeing input output examples? So there is our friend Alan. So this is just to remind you. What a turing machine is it has a control unit and it has a red head and a right head and it can ask for the tape to ": [
            1177.6,
            1211.1,
            22
        ],
        "units in it. Okay? Why? Okay. Nobody answered be good if UV in Searcy. Why would that not be the answer? What are you going to do with language translation? You don't just take one and put. You take a sequence of inputs, right? Okay. So this is not not the right answer because it doesn't take a sequence of words in what's okay. So now it's between D and ": [
            602.8,
            647.2,
            8
        ],
        "up pretty early on a lot longer. Yeah, it's just like I knew it's like nettalk. It's just going. Okay, so it sits on Can't do it. So you're altering machine with Alice Tim controller lstm Network. Okay. Now what's it doing? So here's the inputs and there's now it isn't sequence right here is the input coming in and then it gets this little bit right here, which says ": [
            3401.6,
            3442.5,
            78
        ],
        "we can differentiate this and computer gradient and train it. So you can even do a simple computation with the addressing mechanism, okay? Hey any questions, this is the simplest possible thing we have in this whole lecture. So if you don't get this. Let me know okay. So it's differentiable with respect to w&m. So we have this softmax in the middle of the network that we have to ": [
            1746.1,
            1787.5,
            37
        ],
        "you can store an image in it and then give it part of the image and through recurrent processing it completes the image. Okay, so here's the memory. So does everybody now know what a Content addressable memory is that you're finding patterns that match some pattern from the memory, okay. So you're looking for a particular pattern of pets and then it also allows just location addressing which is ": [
            1462.3,
            1501.8,
            30
        ],
        "you multiply that memory element in and again. These are so this is this is a half Rachel. We multiply the whole vector by a half. Right? And then this one by zero we some all that up. So it's it's a scalar times a vector. here e if he is 1 + W is 1 than 1 - 1 is 0 1 0 out that memory element. It doesn't ": [
            1828.6,
            1868.3,
            39
        ]
    },
    "File Name": "Deep_Learning___C00___Cottrell__Garrison_W___Fall_2018-lecture_14.flac",
    "Full Transcript": "Okay gang. Let's get started going to start right off with some clicker questions. But before we do,  Are people getting enough resources to finish the programming assignment?  Yeah. Okay good. All right. Don't forget to release your pot if you're done with it.  Okay. So today I'm going to talk about the neural turing machine and  which is a  structured neural network with the learn programs  from examples  and this one is for the  one of you who said they like the blue slides.  Okay, but first some clicker questions.  so  the main idea between lstm units is  The user weird acronym so I'll get confused.  the create a mechanism so Eric and get propagated very far back in time to allow the network to store values for a long. Of time until needed to use gates to control memory b c and d  choose the best answer.  it's probably not a  to view that are picking a okay.  Going everybody I'm coming going.  Going going gone. Okay. What's the answer?  What's the answer?  he  so it does all three of these things the weight of one like the way this is like the skip Connections in a in a neural in the resnet architecture allows air to get propagated very far back. It allows networks to store. He's for a long. Of time and that uses gates to control the memory.  But as you heard last time or time before we're just going to have participation points in clickers now after the KQ s thing, okay?  Okay, it's not advancing.  Okay, try again.  Okay recurrent networks can be used to caption images translate from English to French understand speech and be a beacon sea.  Is it?  Okay.  I guess I should have asked where there any questions about the last one.  Okay.  Hey going most of you've answered now going going.  There's one more now going.  Gone. Okay. The answer is  He okay.  This is not working.  Okay.  Which of the following would be used for language translation after training?  Is it a b c d or e?  based on my lecture that is  Okay.  What?  Going. Oh, wait. I just wondering why everybody was answering that one? Okay.  Okay.  Sorry about that.  During and there's two more of you at least that haven't answered yet picking answer any answer. Okay.  Okay, why don't you turn your neighbor who has a 61% chance of having the right answer and talk about it for a few minutes?  Happy New Year  how to program  Okay.  Okay, don't you try again?  run guys girls dogs  Why don't you answer your clicker question yet?  Okay, there's three more of you haven't answered yet.  Pick an answer in the answer one more.  Okay going going going gone. Okay got very similar responses. Okay, what's wrong with a  That's a feed-forward net now. I try to keep up you guys like it that I tell you about current things. So I'm going to talk about attention is all you need which is a paper that came out of nips last year and has 790 citations already and it's actually a feed-forward network that does language translation to full blue score points above the previous state-of-the-art. So  It wasn't a terrible answer. It's actually feed-forward. It's not convolutional. It's just got a lot of attention units in it. Okay? Why? Okay. Nobody answered be good if UV in Searcy.  Why would that not be the answer?  What are you going to do with language translation?  You don't just take one and put.  You take a sequence of inputs, right?  Okay.  So this is not not the right answer because it doesn't take a sequence of words in what's okay. So now it's between D and E.  What's wrong with e?  Yeah.  map to work  Yeah, if it ask for a translation immediately and for example of German the verb doesn't come until the end and so if it was Germany in he wouldn't know what to generate for the verb for English right away.  And so you need to I mean the story here is anyway.  Except for attention is all you need that you taken the sequence and get a representation of the meaning of the whole sentence and then you generate the other language. Okay. So you need to find an embedding of all of the words in the sequence into a single memory Vector the hidden unit vector and then from that generate the output words. This is too fast, generally.  Okay.  So the answer is D. So there were 23% of U14 of you answered e  Do you want to  Ask her talk about. Why more Wyatt, isn't he?  Okay.  This is all perfectly clear. You'll get it right on the final.  Okay, okay, okay.  One more, which of the following were used for beer mind?  After I open it up.  What is spearmint you have to come to the last lecture?  Where we talked about fear mind in Factory demonstrated beer mind.  Okay, what was different about beer mind?  What to do to make sure that all the way through you knew the right you were doing the right thing.  Okay. I'm going to close it out.  Okay now.  Your mind was an innovation over previous things.  You could have said see which must have you did actually and say you'd get that wrong in the final here. He gave it. I want a Russian Stout and Imperial Stout grade 5 and then you would generate the sequence that character at a time, which is awesome. Except it. That didn't work very well for him for his ACT Lipton instead. What did he do?  Yeah.  Yeah, he gave it that context all the way through. He said Russian Stout, five Russian set five Russian staff fun. So it was here but it was constant. It wasn't like the output of the previous time step. Okay.  All right, is that makes sense? Is that sound right? Cuz actually 47% of you said  See which would have been the old way of doing it before Zach came along but the way Zack actually did it was this way.  What's another example of something you might use this one for?  How do you spell?  Generating what?  Yeah, so learning a language model. So not only did he give it Russian.  Russian Stout 5e also gave it the character from the previous time step, which is a standard thing to do learning a language model. You have a stark symbol you generate the first letter or a distribution over the first letters a softmax you sample from it. So you're not doing a deterministic thing you take that letter that you generated and put it back here generate the next probability distribution sample from it. Put that in here. So that's learning a language model. It's the same. That's the thing that I showed you slides from where I learned from Shakespeare. There was another one that learned from nips papers and another one that learn from Wikipedia what the meaning of life was.  Okay.  alright, so  the answer was e so any any questions?  Okay, okay. Alright.  Okay, so neural Nets have been shown to be touring equivalent. Assuming in fact, they're super touring if you assume the weights are infinite precision as Hava siegelmann shown.  But can they learn programs? Well, yes, they can we know this at least for my own work that I told you about before where we trained it to actually develop a program and here is actually that's where it should be this time. So starting with this you map to that and then you work at the next one map to that etcetera. And this was my model.  And we had a program that we are basically training it to do so would give it a two digits as an input.  And out training with output right and the lower digit of the sum and then if that's some was actually bigger than the radius is the output a carry and I don't care. What's this? Here means I don't care what's in the result field cuz I'm not paying attention to it and then ask for the next column of inputs.  And if you come off the end and there was a carry on the previous input, right A1. Okay, and and this thing this is an example of the kind of thing. It could do it would either output right next to right Terry next door right next door done depending on what the situation is with the digits and it had a kind of internal representation of States like a finite State machine.  And the memory for a carry was appear by instead of just having a next to you have a next after a carry in that part of the state spaces allows it to remember to write an extra one add one to the result.  Yeah, so you can explicitly training her on that Temple Mentor program.  But that Network had to learn to remember things and its internal State space and when it had to remember an extra bet it took five thousand more epics and we told the model exactly what to do.  On every step. So can it learn to program just by seeing input output examples?  So there is our friend Alan. So this is just to remind you. What a turing machine is it has a control unit and it has a red head and a right head and it can ask for the tape to be shifted left or right?  And the tape is infinite Armada won't have an infinite memory.  But that's the kind of thing we want to do. So can we build one of these things out of a neuron that we build that out of a neural net?  And that may seem kind of crazy like these folks at MIT you built a tinkertoy turing machine that played tic-tac-toe. It's about it seems about as practical but  But a neural turing machine doesn't have to be so impractical.  And the main idea is to add a structured memory to a neural controller that it can read and write from so we're going to give it a go gadget a memory. We're going to give it some structured.  Things that it can learn to use to read and write from the memory.  Even back in the late 90s. There was a guy Colorado Delta neural-net where it had a stacked and all it had to do was learn to use the stack. It had outputs that would control the stack.  So this is the neural turing machine.  It it's everything inside the stash box. Take some internal external input. It's got a controller and this they try to different things at least they used to Nellis TM for the controller and they used to feed for Network for the controller and usually the  Lstm was a little better than the feed-forward one, but not always depending on what the what the problem was and then it has an output and a memory.  and it's got some built-in stuff that allows it to read from memory and a right to memory and  all of these all of this these gadgets that they built into it are differentiable so it can be trained by backprop Through Time.  Okay, so we're going to structure or neural network to act like a turning machine.  Okay, and there were current one works better most of the time though not always.  Okay, so the read and write heads are pretty simple. They're going to be pretty simple arithmetic commands or process. He's addressing mechanism on the other hand is very complex. And that's the hardest part to understand one of the cool things about it though is that it allows the network to either use a Content addressable memory or a location memory. You guys know what content addressable memory is.  Nope. Okay content addressable means you give a pattern a bits and the system returns all the contents of locations that match that pattern of bits.  Hopfield network is an example of a Content addressable memory. We haven't talked about it. It's the simplest kind of neural network, but it you can store patterns in it and you can give it part of the pattern internal complete the pattern like you can store actually every unit in the network could be a pixel a binary pixel and you can store an image in it and then give it part of the image and through recurrent processing it completes the image.  Okay, so here's the memory.  So does everybody now know what a Content addressable memory is that you're finding patterns that match some pattern from the memory, okay.  So you're looking for a particular pattern of pets and then it also allows just location addressing which is the normal kind. Okay. So this is the memory it's just a matrix of linear quote-unquote neurons.  And you can think of each row here is a word and the memory and we're going to read and write a road a Time.  Although the mechanism allows it to actually read from many locations at once and just some of them up. Yes.  Yeah, so in this case, it's going to return the closest match.  Okay.  You can buy a Content addressable memory, but they're very expensive. I don't know if you can actually still by 1 probably NSA hasn't okay. So this memory has three words of like for okay.  and  we can index these Rose as one two and three and we can read from it using.  the weighted sum of every this means every memory row essentially times this wait, so  RT is the vector readout from the memory of time t  WT is a softmax  Column Vector of length 3 which is the address. So if the softmax  this weight Vector is actually it's not a static weight. It's a computed wait and it's the soft Max like if it was 0100 times the first Row 1 times the second row and 0 times the third row. Yeah. All those up and you get the second row, okay.  So that's reading so here's an example 3.14159.  I might recognize that sequence of bits. Anyway suppose we want the second row if it was if the weight doctor was 010 and then again, and this is just not really.  Awaiting the standard since it's a computed way then this would yield 5997.  So w is the address, okay.  any  did I go too far down you have to stand up to see the bottom. Okay? Yeah.  I'm so there is some stir one. So it's positive and we want a peaky softmax. That is one. That's really high on one thing if we want to read one row.  yeah, it's it's like a probability distribution and you could  there's you could actually do some computations for this thing. Okay, so we'll see that in the second.  Any other questions?  Okay.  And if W was a half a 1/2 0 then actually you get the average of the first two rows.  So you could actually use it as a kind of way to compute something from the words. That's that's a weird usage of it. But remember the network has to learn to use this this is differentiable so we can differentiate this and computer gradient and train it.  So you can even do a simple computation with the addressing mechanism, okay?  Hey any questions, this is the simplest possible thing we have in this whole lecture. So if you don't get this.  Let me know okay.  So it's differentiable with respect to w&m. So we have this softmax in the middle of the network that we have to  Computer gradient through I don't I have mentioned that this is possible before but this is I think the first time we've actually shown it.  writing to the memory  is inspired by dating from an lstm and they use gates to write to the memory and a write operation happens in 2 steps first.  Erase the memories and then you add to them.  so  here's a racing memory element. I  you multiply that memory element in and again.  These are so this is this is a half Rachel. We multiply the whole vector by a half. Right? And then this one by zero we some all that up. So it's it's a scalar times a vector.  here  e if he is 1 + W is 1 than 1 - 1 is 0 1 0 out that memory element. It doesn't have to do this. It has to generate the E and it has to generate the W so I can do some other weird stuff instead.  We'll see what what happened. So.  But if he is zero or w0 nothing changes so for if for this is again a soft neck, so if it's 1 somewhere it's zero everywhere else or close enough for government work. And then the e is a is usually a one so that you erase one row of the memory.  by the network  now it's computer in real time.  W at time T So as its.  thinking along its Computing these W's  and this is Multiplied pointwise. So this just zeros out. If both of these are wanted zeros out that memory location location High.  Right. So after this operation at time T minus one then we get this.  Okay, yeah.  Yeah, W is going to Range Rover all the memory. It's a vector right? It's a softmax vector that's got as many elements as there are memory cells her memory Rose.  Yeah, he know he is one one thing. So this is  you know, he is typically going to be  are unit vector or and but it could be something else but it learns to use it the way we expect it to.  Was that two questions?  Okay.  so  if W so, here is our memory again.  If W is 010 and he is one one one one one.  Then doing this yields this.  Okay, so that's a racing yet.  And now I have to write to it and writing is just here's the thing. I want to right there.  And if I keep the same weight Vector then going from here to here with 0 1 0 and a being 1 2 3 4 5 Now Rich a vector into memory.  So why do we have to do a right in two steps?  tongue.  10 seconds  turn the right to vote.  Yeah, basically there's there's no overwrite operation in arithmetic.  Right, so you have to erase what's their first and then add to it and notice that you don't have to know what's there to erase it?  Okay can just erase it with this.  Yeah, probably would wanted to be that way.  But it's got to learn to do that.  Yeah, I mean first you're going to race it. Then you're going to ride it. So you need this problem if you want it, if you're talking about the same guy. Let's see. How is it computed?  Yeah.  I don't know if it's enforced to do it that way, but I I think it learns to do that.  Okay, so there's no overwrite operation in arithmetic.  We could use an ad Vector. That's the new Vector minus the old Vector but that requires three steps and I believe this is an exercise for the reader.  UK  Oh, wait. Damn. Okay, I didn't leave it as an exercise for the reader. Alright, okay now for the hard part, which is Computing the address.  these  are outputs of the controller all of these things and they've got built in operations that lead to an address in the end of the old address.  We have the memory that goes into this and then we have these five things that are actually computed by the controller and there's all this stuff that goes on just to generate this one address.  Okay, and I'm going to try and walk through it slowly.  And there's a while one one part of this is the kind of switch which switches between content addressing and and location addressing.  Show again, please have to be computed by the controller and it has to learn.  To use them so you've got this structure that's built into the the network, but you can back propagate through it all and learn the these things.  Okay, this is called the key Vector for Content addressable memory. We want the memory element most similar to this Factor.  So this would be a length for vector.  And so that's that's the key. That's the thing. We're looking for the thing that's that content.  Beta is a game parameter on the content match which basically sharpens it.  And there's different.  Camera is also gain parameter, but they use them in slightly different ways, which is kind of annoying. I don't know why they have do it differently in two cases. Sobeyda sharpens. The content match G is a switch between content and location-based addressing. So it goes in here and it's a it's a 01 Logistics gate that either allows you to go through with the content memory or switches to the location-based one.  And asks is a shift factor that can increment decrement for leave the address alone.  Gamma is a game parameter on the soft Max address making it more binary. So what this does is if you have a wussy softmax output that's like point-to-point point 1.8.1. It makes it 010 or tries to  Okay.  So let's start with this guy.  So with this these two guys do  is it computes an address?  By doing a softmax.  And beta is like a temperature parameter here.  If betta is Big it's going to make the soft Max sharper.  Okay, it's the inverse of the temperature parameters bettas High we get a more discrete distribution with a 1 in one place and zeros everywhere else.  So what it does here, is it computes this Colonel K? Okay is is a function. It's cosine similarity. So it's Computing the angle between this factor in this Factor.  Okay, so if cosine is maximum, it's one it can be minus one that can be zero etcetera. So K is just a function that computes the cosine between the ski vector and this memory element for a location. I okay. So there's going to be one of these for every  This guy is the index for the memory like 1 2 and 3.  And if K is most similar to what's an address 1 this will be high.  And this will sharpen it and I'll come out with 1 0 0.  If the key is closest to the second element, that one will be high and it'll come out 010 or close to it anyway.  Okay, so this is built into the architecture.  Okay, this just happens the network has to learn to produce K and beta the controller has to learn to produce those.  So it's got this built-in Gadget right for Content addressable memory and we could stop there. But we want to also give its location based memory if it wants it. Yeah.  Yeah, so this is in the exponent, right? So it's easy to the this and so if this is like, you know 2 or something, it's going to make a really sharp if this was very small it would flatten.  the distribution  Yeah.  Yeah.  Sorry.  And in one of the examples that it does pretty much computer exactly what it want. I didn't get slides ready in time for this had too many meetings today, but there's a one program that learns World learns associations between two vectors and it if you asked for this Factor it find the memory location by this.  Okay, so suppose kfc3100. Okay, that's supposed to match point 8.1.1 and bettas one.  So these are the cosines.  Okay, that produces an address factor that looks like that. I I actually competed this but a bit is 10.  You got .99 8.00 1.001. So again a large betta sharpens the address.  Going to find the closest thing.  Huh?  It's probably not correct. If it can't find it. I mean it has to learn to use this and so it's going to have to see if it needs something later. It's going to have to write it to memory.  Right, so I'm going to have to learn to do that.  This is mind-boggling to me that this thing works at all. I mean, but you'll see what it does. Okay says this make sense the beta sharpens the address.  So these are actual cosine scores. I'm made those up, but this would be bad right but this would be good.  Hey, probably, you know, it's probably just outputs a big betta all the time.  You know cuz that's what it needs to do.  Okay. So this part of the model takes the content-based address of a navigating variable which switches between content-based addressing in the previous memory address Factor so we can incrementar decrements OC it takes the previous guy and the new guy that's just been content the soups either means content-based.  And we're going to interpolate between those.  The other thing you can do though. This guy is going to incrementar decrement the memory thing or leave it alone. You can do content-based addressing and then increment the address and so one of the programs they teach it is to store these pairs of vectors and it puts one vector in this location memory and then it's associate. It's associate in the next place and so it finds the location by content of dressing and then increments it to get to the pear. So like if I say cow Railroad  Bone elephant, you know I give you these pairs and then I asked you what goes with bone you have to come up with elephant. This is a standard kind of test. They do in Psychology experiments. I don't know.  Why it's interest I do know why it's interesting but it seems weird right pair parrot Associates. It's called so they they teach it to do parody. Okay.  Okay. So what the interpolation box does is it just as a weighted sum between the content-based address and the previous address that was there the one you computer on the last time step instead of G is one you got completely the content-based one and if G is 0 you got the previous address. So this gay dating guy is just a logistic output in the network.  So somewhere there's a logistic and it feeds into this process exactly that way.  Okay.  Oh and I added this to make it clear where this goes.  And a mixture doesn't really make sense here.  Okay, the convolutional shift box takes in the content-based or in the location-based address and then it can incremented by -1 0 or plus one based on the s factor.  Barger ranges are possible intend to use them.  So this is you have to work this out on your own. But basically this is a shift if st001 that increments the address by 1 so.  010 become 0 0 1 that is this is location one location to location three location location location. And this is location 3 so it goes from location to location three and this is a circular convolution operation that you can try at home and make sure that it does what I just told you it does.  and then the sharpening box does the show here's why is it anyway, but this is  Wait, there's this act like this isn't actually a soft. Oh, it's not actually a softmax. I guess. That's why.  Okay, it's not either this thing. It's this thing to a to a hey, that's why I just realized that's why it's not really so cameras to this becomes that.  Okay.  And that I compute it so you might think wait if camera is 2.9 becomes .81.  But wait, this becomes .01 that stay zero. So when you do this ratio thing it comes out that way.  k  So that's the addressing mechanism.  Which is okay. So first you create a vector address based on similarity to existing memories.  Then you can switch between content and location.  You can increment or decrement the address and then you can sharpen it.  And that's it. That's that's the so all of this is built in to the architecture. The network doesn't like choose to do this. It's got to do this. It's part of the built-in mechanisms, but the cool thing about this I guess is that all of them are differentiable, which means we can back propagate through them to change the parameters.  Do do do do okay. I was asked to come up with something besides do-do-do-do, but I know I can't think bathing.  Okay, so what can we do with it? So you can learn to copy you can learn repeated copy associative recall Dynamic engrams. Send Priority sort.  They compare this to they they use a two experiments with the neural turing machine with a feed-forward controller and LS cam controller and then they have a vanilla lstm Network that is does not have all this stuff built in and train it on all these tasks, but we're just going to do these three.  Cuz I didn't get time to add fight about this.  So where copy does is he in put some sequence? And then after the end of the sequence you have to Output 1 2 3 4.  So are used to call this the recorder problem and my students and I spent quite a while trying to get you know, just repeat after me. Basically, we spent quite a while trying to get an element that to do this and we could not get it to do this. It's a hard task for recurrent Network. In fact, this is the lstm network trying to learn this and it doesn't ever get the error very all the way down after even a thousand passes through and this is in Thousand. So this is a million.  Passes through the learning and the Ellis neural turing machine with the lstm controllers learns it almost immediately.  Spooky and the one with the feed-forward also learns it.  Tell this is an example.  So this  is actually a bunch of bit patterns. Okay, and  This is what it supposed to Output.  And it learns to do this.  and  Red is wine and blue a zero. So these are our here. It was trained on like 20 sequences. So there's it's like 0 0 0 1 0 0 0. I think there's 10 bits here and then it gets that and then it gets that and then it gets that and then it gets out Colin and then there's a marker it says okay, repeat that back to me and then it supposed to have put the exact same thing.  And this is the output but after the end of this not a it's not lined up in time.  Ettore's train on like 20 sequences up to like 20, this is 20.  This is 30 think that's probably 40 or 50 and where it starts to go wrong. You can see that it's like getting unsure. So this is where the lstm controller.  and it's starting to get a couple bits wrong and then it  Let's see. So this should look what your seniors this should look just like that and it doesn't.  And here's some more uncertainties but like it's still doing pretty well up to link 50 and then this is I don't know what a hundred and and the reason why it goes bad so early here is because this only comes out after this whole thing. So it takes in this whole thing and then start price to generate it and it it starts to screw up pretty early there, but it does pretty well up to like 50 and it was only trained on once this so generalize has the longer sequences then it was trained on.  Okay.  Do do do do.  And then this is the lstm network it was Trent. This is just a vanilla lstm Network trained on up to like 20. Does that really well, but give it a little bit longer and it screws up pretty early on a lot longer. Yeah, it's just like I knew it's like nettalk. It's just going. Okay, so it sits on  Can't do it. So  you're altering machine with Alice Tim controller lstm Network.  Okay.  Now what's it doing? So here's the inputs and there's now it isn't sequence right here is the input coming in and then it gets this little bit right here, which says okay start generating now.  This little bit says okay start reading now.  And here's it's doing a bunch of ads and look at where it's putting it in memory. It's these are the the weight of the addresses.  Okay. These are the addresses reading it back out.  Okay.  This is the program it learned. It's the same program. You would have written to do the same thing, right?  Well, you haven't seen the input to limiter receive the input Factor right It To The Head location increment by 1.  And while return had to start location, so we've seen the input to limiter read from the head location admit output increment head location by 1.  Okay, so this isn't written anywhere. This is just what the lstm controller is doing. So these this is like the internal state of the steam controller.  Repeat any questions about what this represents this is the input sequence which is external to the machine the output sequence generated by the machine. This is the internal State and then it starts its reads.  And it just does exactly what you would have done. But sit in a sequence of locations and reads those how  Okay.  Wow, okay, good like that.  repeat copy the Network's giving a sequence of random bed vectors followed by a scalar representing the number and of copies to make and then it repeats the sequence in x  so the lstm network  Also learns this in fact, it learns it better than it learn the previous one.  Lstm controller neural turing machine learns it pretty quick about the same level of accuracy. There's one blip here with the lstm controller.  Okay, and this is again thousands of passes.  This is the number of bits that are wrong.  Okay.  So the Network's tested for General a generalization first the size of the pattern to be copied or doubled and then the number of copies is doubled.  Here it's repeating 10 repeating 20 times which is 10 times longer than it learned.  Here it's got a link 20 sequence input and repeated ten times has these they think these little things are no. Sorry. This isn't  see ya. Okay.  This is the lstm vanilla lstm network is screws up terribly might expect.  Doesn't doesn't generalize like the lstm bntm does.  This is what it does. So here's the input.  And it's adding that to the memory. Here's the outputs and it's just going they looked at these these seem to be signaling go back to the beginning address.  So this is in the memory saying okay time to get its reading out probably from a location in memory the the start address.  Yeah, I don't know so sexy.  Yeah, I  All this is is just saying go back to your asking. Where in the internal memory. Does it remember how many times that's what you're asking and I don't know the answer.  I don't know if they know the answer. I wrote one of them about one of these things and I said how does it do that? I don't know.  Okay, so this is a little scary right? I mean you can't explain itself.  Okay, how about priorities or any questions about this besides the ones I can't answer?  Yeah.  I guess it out with v. I, I don't know. It looks like I mean, it's just not it's taking in a 20-minute sequence.  Instead of a water was training on which was 10:00.  And it looks like it's just got one of the bits a little Ron probably.  Of course finder that which looks pretty dark to me. So  that's probably supposed to be off and it's too little active.  And then it turns yellow and then turns blue. Very cool.  So again, what these are showing is that red is one blue is zero. All of these has its really confused basically is somewhere between zero and one.  Okay priority sort.  you got a number this is priority 5 this is priority 7  What's priority 46321 etcetera and that what it's supposed to do is output whatever was one.  Which was 1 0 0 2 0 1 0 0 1 1 0 0 1 0 0 1. Okay number 2 is there.  Okay, and so it's supposed to Short.  It's giving some numbers. It sells it where in the sequence it is, roughly.  and  am I running over now?  So  So what they did was they tried to predict where the memory locations were by fitting a linear function of the priorities to be observed right locations, so they have no idea but they're trying to like analyze it.  So this is what they figured the right locations ought to be by fitting a linear function do what they were in another run.  And then this is what they found it looks pretty clothes.  And here's the Reid waiting.  So what's happening here is it's kind of taking the priority and throwing that bit that sequence in a location in memory. That's roughly like 5 should be somewhere in the middle. So it's it's thrown them in order of priority right into these locations.  And then they say we hypothesize that to test this refit Interlinear Network reads for memory and increasing or they're so I basically I was saying that I said, so it's got to take the priority and multiply it by some number to get where to go in memory. How does it do that? I don't know. They didn't know.  Okay. So the neural turing machine is a recurrent neural network augmented by memory. The network is given these mechanisms there built-in and I can learn to use them.  Because it's end-to-end differentiable.  Okay, and so it learns algorithms from examples of desired input-output sequences, and there's been more work on this.  They had another paper called the differential neural compute differentiable neural computer that added and ability to have structured data structures and it learned like a map of the London Tube system.  so yeah, it got so that's  That I am in the big questions now or now we got this thing. That's like a turing machine. Okay, how long before this kind of thing learns to networks that leads networks that can reason write like  all men are mortal Socrates is a man therefore Socrates is Mortal. That's a inference that you can make logically. I'm doing some reasoning can these things be trained the reason at some point and then can they learn to reflect? So I have a model of you you have a model of me in your head. You have a model of yourself in your head. I remodeled my son what I can think about what I might do in a certain situation that's called theory of mind takes at least two 2 years to learn.  So if you take a a two-year-old and you hide some candy under something while somebody's in the room and then the person leaves the room and now I had the candy I take the candy out of here and put it there and ask the two-year-old. Where does this person? He left the room going to think the candy is the note say, oh it's under their don't think it's under there. It's takes and takes about when they get to about Age 3, then they realize the guy doesn't know that it was moved while he was out of the room that's called theory of Mind in psychology.  And so by Age 3 of developed the model of other people that allows you to reason about what they know.  And I'm trying to reason about what you know, and how would I say leads to what you know, and then I hope that when you take the test, you know what I'm asking on the test, right and this can go on. You know, I know that you know that I know you seen Princess Bride.  Great movie. There's a thing where they have this guy is with another guy and they have two cops. One of them has poison in it and they switch them around and and they try and reason about how well you think that I think that you think  hey, Auntie.  yeah, so  you should see Princess Bride if you haven't seen it.  So and then at what point do we call that consciousness?  You know, this will probably happen in your lifetimes and you should be ready for it.  Cuz that's when when they start being aware of themselves, that's when we should start worrying while we start wearing now and people are okay. So that's it for today or give you 10 minutes more to work on your programming assignment, okay?  Have a great day.  feel free to Pat Wiley  so for the country, so I read something online about like pretty similar to the I think it was actually a turing machine. "
}