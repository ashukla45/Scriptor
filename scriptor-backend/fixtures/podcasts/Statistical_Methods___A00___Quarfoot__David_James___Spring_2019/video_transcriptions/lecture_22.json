{
    "Blurbs": {
        "I need to do to be nearly normal. Except it's okay. If they're a hot mess as long as there's enough of them. This isn't balanced, right they're always talking about to the same thing will turn out to be true here and Anova also the last one frustratingly. There's no nice way to fix if you don't have it except for Taylor polynomials and horrible transformations in another class. ": [
            839.2, 
            862.2, 
            32
        ], 
        "I've ever seen done with a Nova because it's a really cool result actually. So if you know anything about volcanology have you taken of volcanoes course here? Has anyone taken volcanoes? Apparently it's like one of the most popular courses at UCSD. I used to teach in the room right before the guy without volcanoes and was a crazy in there. He won't crazy for that class. Anyhow, so ": [
            1302.2, 
            1335.1, 
            49
        ], 
        "If you're not one of those people in your fine, like give me a tool I love using tools. When is a great day for you and you enjoy using things like a black box? We don't really truly understand. What's going on underneath. Okay, how about we do some Socrative? I wonder how you're going to do. Let's go see. so if doing an F test that is an ": [
            2092.8, 
            2147.3, 
            72
        ], 
        "Morning, welcome back. Hope you are emotionally recovering after the Game of Thrones finale. I won't spoil it. if you're still waiting to see okay. Today is a beautiful topic because it shows you the future of where you could go with all this. So we're going to talk about something known as Anova analysis of variance. And the right way to think about this straight off. Is it the ": [
            1.9, 
            39.4, 
            0
        ], 
        "Okay, so then there's the set up there. So you checking the payments through the two conditions. We always do that. You can see the normality and the Equus prednis quite easily in one of the following two things. You can make me a box plot. They had side-by-side grouping. Okay. Now in the box plot, the spread is just the height of this box. That's the IQR actually. So ": [
            862.2, 
            889.7, 
            33
        ], 
        "The other row will be the error residual row. Depending on what language they use here because of this Rose all about are ideas. So you can see Ms. Andy coming together 2.95 is the embassy. Okay, and you can even chat 45 / 3 is about 15. Is it ain't doing any magic here if you understand what those calculations are now over here the degree of Freedom call ": [
            1848.9, 
            1876.6, 
            63
        ], 
        "You just wrote it and it's as it normally is if this is really small you move to the alternative we've seen incredibly weird. Aprecio is what this number the fact that is so small this is what it means. Okay. Now if you want to think about what a p-value always means it's always the area under some curve. Based on your data or something more extreme. assuming the ": [
            1928.0, 
            1956.2, 
            66
        ], 
        "about the randomization? How do you feel about that? They are at why'd you say the randomly-selected? Where'd you get that? Hoping and dreaming in your soul. So you have to go back. I don't see anything about random. In fact, this was probably volunteer. I'm guessing. Okay, we don't know about this condition because we don't know how the data were collected. Volunteer bias is most likely what's going ": [
            2891.9, 
            2935.3, 
            89
        ], 
        "and between this description of what's going on in this table, you should be able to fill in everything that's going on here. So for example, you can get the DF column by just knowing how the study of structure there were five groups, right? So 5 - 1 is 4 There were 50739 people at the end. So if you subtract a from that you can get this value ": [
            2984.9, 
            3010.0, 
            92
        ], 
        "and get a good feel for what's going on. Okay. So what's up how you heard? All these things on the very left won't use the phrase one sample T Test? Okay, the one refers to the fact that you drew from one population. You were sampling from it. The teen tells you what distribution the analysis eventually happen. So, obviously this will be two sample T Test and over ": [
            351.5, 
            375.5, 
            13
        ], 
        "and to not choose to many. This is the randomization 10% condition. So the same thing is true here and that usually will give Independence both within all the different groups and across the different groups. Okay next to the data in each group have to be nearly normal. So if you took these dots from say the first group those all represent numbers, you can make a histogram out ": [
            733.6, 
            757.4, 
            28
        ], 
        "and you can always add Down The Columns to get the total idea. Now this sum of squares called me even talked much about okay. It's really just an intermediary it to get to the main idea which divides the sum of squares column by the degree of Freedom column. So if I subtract I can get this number and then if I divide the sum of squares by the ": [
            3010.0, 
            3032.9, 
            93
        ], 
        "anova how many groups? Are the researchers dataset? Hey, there we go. So this just seeing if you have learned this expression for the degree of Freedom related to the grouping idea K - 1 so you can back derive the number of groups that way. How many total data observations were in the data set? 20/20 Carrots are people breaking out the degree of Freedom related to the error ": [
            2147.3, 
            2221.8, 
            73
        ], 
        "are equal some at least one is different. Okay, next thing let's talk about inference conditions. There were a bunch of these. So if someone talked to me about any of these that you'd like to mention and whether you think it's met. Help us who checks conditions anyway. Let's be stopping garbage researchers. Never check anything. We have three conditions. Go ahead. Which one do you want? Okay, Sweeney ": [
            2764.2, 
            2823.9, 
            86
        ], 
        "are probably loving it Kristi noem eat. He's a little better 50739 women. Okay, so here is sort of a summary table of what's going on. So we're going to run in an over here. It is to see if different amounts of coffee leads to differing amounts of exercise on average. So I need you to set up temporary amateur some hypotheses for me. Try to do this in ": [
            2567.5, 
            2591.7, 
            83
        ], 
        "are terms that you've taken an average of okay. So this looks within each group and it says okay within each group. You'll see a lot of variability not all infielders. Look the same. Okay, and this Essa by is the standard deviation going on or the spread out and it's within a given group. Okay, so our infielders really spread out or catchers really spread out. And what you're ": [
            1217.8, 
            1242.5, 
            46
        ], 
        "average and how did the infielders differ from the overall in a sort of accumulating? This idea when you measure what's going on between the groups as they compare to the overall average. Okay, and then discenza buys how many people were in a certain group? So how many infielders were in your study how many outfielders so it's a weeding factor in case I'm very exciting to get a ": [
            1141.8, 
            1166.4, 
            43
        ], 
        "by default. So you need to tell it to shade or the right side, which is width D is doing. if a Nova suggest a move to the alternative Then we will have identified the mean that is different than the rest. So I said it a couple times already. The alternative is frustrating. It says something exciting is going on. That's all it says nothing more doesn't tell you ": [
            2332.7, 
            2373.1, 
            76
        ], 
        "can count that. There are six possible tests and that comes from the number for choose two of the four ones choose to that you'd like to compare and the two symbol tells you how many ways you could choose those two out of the group of four? Unfortunately The Chew symbol is nasty and get the big very fast. So if you have K different populations and you want ": [
            467.0, 
            489.1, 
            17
        ], 
        "comes from. One of the founders of Statistics did most of the important work in building the entire field between say 1920 and 1960. Now that's a statistic is a complicated object that we won't fully study. But I'll tell you how to use this. Basically now, it's going to be the ratio of two other quantities MSG taste good, doesn't it? And MSC whatever those are and you'll notice ": [
            265.4, 
            296.6, 
            10
        ], 
        "data to be independent within and across groups. It was a tough raise the first time you see it within means if here's a picture of our data. So we have three different populations. One, two, three, and we're measuring something quantitative about him. I would come whatever that is. Okay doesn't really matter in this problem. Okay. So all of your data points, you can see you fall into ": [
            686.5, 
            708.2, 
            26
        ], 
        "degrees of freedom, I'll get the mean square column, which is what we really care about. So we have to stop but I suggest you keep looking at these things and working on them. Have a great day. I'll see you later. ": [
            3032.9, 
            3049.2, 
            94
        ], 
        "difference should be which is almost always zero for us because of what the no looks like the null says they are two things are the same and then divide how to spread out things were and here we were just on a more complicated distribution. Now if your soul has good intuition here, it should tell you that we're about to make a big hot mess. I mean look ": [
            216.0, 
            240.3, 
            8
        ], 
        "different players when you group them based on where they play. If that idea differs in terms of their batting average. Okay. So here are the numbers that come out so sample standard deviation. Do these numbers look roughly the same? something emotional in your soul Okay, if you're worried, you can always tell me we are somewhat concerned about the equal spread condition. They don't have to choose a ": [
            972.2, 
            1003.0, 
            37
        ], 
        "differs from the group. This is a frustrating alternative because it doesn't tell you which one differs. Okay, so as you moved to more than two populations, you can continue the infrastructure, but maybe the alternative is less satisfying than it used to be. Okay, how do you test all these things? Well in one population? We said okay, let's study axe bar. That seems like a good way of ": [
            140.3, 
            168.9, 
            5
        ], 
        "don't / k / K - 1 we seen the sort of like weirdness before. That you don't really need to truly understand this at some point. You just have some emotional feeling that is trying to measure some idea that has to do with how groups differ. From the overall average. Okay down here the embassy this is the mean squared error. Or the sum of squares related to ": [
            1192.1, 
            1217.8, 
            45
        ], 
        "else might be totally fine. What are the other conditions? Go ahead. I'm sorry. 10 so 10% and randomization are trying to create the independence idea. How do you feel about I agree there more than 50,000 women in the US drink coffee. But we need more than 500,000 right when we need more than 50 thousand times 10. So I also think there's more than 500,000 Starbucks exists. What ": [
            2851.1, 
            2891.9, 
            88
        ], 
        "fair. You're going to flip them 10 times. And then you going to tell me what happens know most of you will get 5 out of 10 heads. Nothing exciting happened, but there's one person in the room. The lucky one that was just get 10 heads, right just like someone you know, it's like a one out of a hundred event or something, but we have a hundred people ": [
            562.9, 
            585.4, 
            21
        ], 
        "formula and minus K and back driving 110 was from that. collect salary info on a hundred 1012 200 people five different states What curve is the p-value found on for this study? His number should be subscripts here. But so proud of his so limited. Can't really show it nicely. Look at that 70% already. So here you're just using the you need to make sure you get the ": [
            2221.8, 
            2270.8, 
            74
        ], 
        "generalization of things you've already seen before? That's one thing you should always say like what are the limitations of my current tools and you'll see that all our testing around averages right now is limited to one or two Okay, so we've worked with populations for the rest of single one and we learned how to form hypotheses around these may be the average in your population is equal ": [
            39.4, 
            66.5, 
            1
        ], 
        "go up have a hump and then come back down. Is what these look like? So when you're calculating A P value for an anova all you do is draw the appropriate F distribution, and then you plop down your fstat or your value and then you just shaved to the right. Now one thing is strange about these. You're always going to shade to the right. And there is ": [
            2001.8, 
            2030.3, 
            69
        ], 
        "going to do is sort of again wait, he's and then averaged together everything going on. Now it's not obvious. We're doing any of these calculations is helpful. It's not obvious. Why taking their average gives you anything important at all. Right now it's just sort of we're analyzing the variation that can happen. And the variation here in MSC is all about variation sort of within a group in ": [
            1242.5, 
            1275.8, 
            47
        ], 
        "have exactly the same average, but the question is are they so far apart from each other that they must have come from populations that actually differ. So let's eat first thing. Can you define me some variables and write me some hypotheses? Let's start building the infrastructure around this a minute here to do that. Okay, three different populations. 1669 1780 1865 implies three different averages you can call ": [
            1435.1, 
            1627.9, 
            53
        ], 
        "have large sample sizes and we have huge sample sizes. So these numbers 6017 thousand at cetera. Now, how am I ever going to give you a problem like this on an exam? It mostly involves computer stuff, right? We're here so you can give her problem like this on an exam. You can give the print out and say there's missing pieces. Coffee stains are blurring things whatever. Okay ": [
            2956.0, 
            2984.9, 
            91
        ], 
        "here people just use the phrase a Nova for an analysis of variation for variance. Okay, as part of a huge framework where you study things by analyzing how they vary. Aunt in the variation of an object that you come to truly understand it. So slick name now and has lots of extensions. The first extension people usually learn this and Cova analysis of covariance. Then it can go ": [
            375.5, 
            406.5, 
            14
        ], 
        "here that the particular F distribution you do the analysis on suddenly has two indexing parameters. Where's the T distribution? Just had a single one in case I have to tell me two different things to figure out the curve is df1 and df2. If you take 181b which is what I teach right after this you learn what's going on Wheelie with a Nova and it takes about a ": [
            296.6, 
            326.6, 
            11
        ], 
        "here that we can figure out everything in life, basically. Okay, should we try some more? But try some more. Try smoking problem. No, no still coffee problems. I like the coffee one more than the smoking one. Okay. So here's an actual study. This is one of the freely accessible jumbo data sets on the internet that you can go get the women's health study. Okay, so they asked ": [
            2478.4, 
            2515.1, 
            80
        ], 
        "him. It's helping keep track of what kind of curve are eventually going to be on. Okay. Now the first one since this in the Years A grouping idea. This is known as DFG the degree of Freedom related to the group's. Okay. So it's the number of groups - 1 that's how you find this. So here we three different years. And so that's why they're getting to Now ": [
            1876.6, 
            1901.6, 
            64
        ], 
        "huge difference between two ideas if there weren't many people that went into making the difference. But if there's a huge difference in there's a million infielders that used to build that that's important. So this isn't a Sensei waited. Weird idea. So there's a square that has to do with groups. And then in the end you take an average across all of those different ideas. Okay, and you ": [
            1166.4, 
            1192.1, 
            44
        ], 
        "if all the IQR is look sort of similar now, that is an emotional judgment. I need you to just make emotional judgment sometimes. It's like I don't want to do that in a math class. Give me some rules. I can apply sorry. Now normality if the central line is roughly in the middle, it means that there's a nice balance between the two halves. No doesn't mean it's ": [
            889.7, 
            919.8, 
            34
        ], 
        "in relation to what you're trying to measure they're just a bunch of populations that happen to look identical across your quantitative variable that you're setting now things get strange here as you can see in the alternative. If you're claiming a bunch of things are all identical the opposite of that is that there's at least one person or one idea 1 average whatever it is, you're studying it ": [
            115.8, 
            140.3, 
            4
        ], 
        "infrastructure in case it is a rough thing to lose and finally the spread in each group have to be roughly equal. So you look at how spread out the data are in group one. That's a number using the standard deviation use the variance. Whatever you want to me roughly the same ideas and group. So this is like a very well controlled. Structured cookie pay the flower and ": [
            786.6, 
            814.5, 
            30
        ], 
        "is to satisfying something differs. What how many things differ I don't know but at least you have a starting point that you could then go off. And maybe run some tests never going to use this infrastructure. There are a lot of assumptions. This is one of the frustrating things about it cuz he's all the conditions. First of all, you can't have garbage data. So you need your ": [
            658.5, 
            686.5, 
            25
        ], 
        "it. Okay. So here's this analogy corruption present in the government, but you can't exactly identify where it is. Sometimes the totality of the experience flowing at you says excitement. If you can't pinpoint it for some reason maybe it's generated across the totality of lots of different comparisons are leading to that. But when you dig deep it's not visible. So this is a very counterintuitive results. In fact ": [
            2424.6, 
            2458.8, 
            78
        ], 
        "just a one-year change. Okay now. The calculations around an oval are horrible in complicated and they usually must be done with computer. Guess I'm not going to ask you to like. Find an F stat by hand or from the original data. This would be nightmarish. Okay, so let me show you what it looks like when we go put this into our especially if the structure correctly. Now ": [
            1670.9, 
            1699.4, 
            56
        ], 
        "logistical question? Now that your computer's you have no problem with telling the computer to do tons of work, right? So maybe it's not so bad anymore. Do we have technology to help us? The other question is is it bad to run? A lot of tests? This is sort of a theoretical question and it turns out that if you do something a lot of times this is a ": [
            519.6, 
            541.3, 
            19
        ], 
        "means are equal. So this is known as an Omnibus test Omni for everything. Now it's not the best kind of tests ever is everything equal in life. Seems like a silly question, right? You almost know the answer is no immediately, but there are some cases where you want to ask such a thing. Unfortunately the drawback of running an Omnibus test as we said earlier is the alternative ": [
            632.4, 
            658.5, 
            24
        ], 
        "no two-sided alternative. There is no symmetric shading both directions. There's none of that the hypothesis for an anova always look the same but no is all the means of the same. The alternative is something is different. And when you do the shading you always popped on your fstab and shake it to the right. So the infrastructure is built around that happening. How you feeling so far? Some ": [
            2030.3, 
            2063.4, 
            70
        ], 
        "normal. But that usually the way people look for normality. Okay. So this I'd say this is like Roughly normal and roughly the same spread. That's just what my emotional soul is telling me the boxes look like the same height and the lines are sort of in the middle. Basically. I want to see hard numbers. You can always create a table like this is a table of people ": [
            919.8, 
            944.7, 
            35
        ], 
        "null hypothesis So here that curve happens to be an F distribution in the particular of distribution euron is dictated by the two degree of Freedom parameters that you see right here. Okay, so I can actually draw a picture of what's going on if you want. Okay. Now we probably should go a couple F distribution. So you could see what they sort of look like. So their support ": [
            1956.2, 
            1978.7, 
            67
        ], 
        "numbers you can start to see you're studying whether you think the means are different, right? That's what this is all about. So if you look at these numbers or so, we'll have a feeling immediately. But the point of the Anova is to rigorously decide on what your soul is feeling there. I don't know. Do you think the designated hitters have higher batting average than the others? If ": [
            1028.2, 
            1052.9, 
            39
        ], 
        "of them and that histogram shouldn't look roughly normal in the same thing is true in all the groups. So that is a very strong Criterion all of your different populations setting have to be nice Ash. Just on Friday next door. We figured out how to fix that condition. It's rough. I'll tell you that it will Carson horrible transformation Taylor polynomials. Remember these 20 be cuz I cant ": [
            757.4, 
            786.6, 
            29
        ], 
        "on. Although I don't know if that affects anything about what's going on in this. We also don't know anything about the normality condition here because I haven't shown you a picture of the date on each of the five groups. So just tell me if you don't know something it turns out it doesn't really matter about the normality though. So in general we just need normality unless we ": [
            2935.3, 
            2956.0, 
            90
        ], 
        "one of these three bins. Now if the data are independent within it means you shouldn't be able to use one data point to infer the value of others. Within your same group and across means if I know this datapoint, it shouldn't help me make predictions about ones in the other bins. The easiest way to always get this is to fill up your study by choosing people randomly ": [
            708.2, 
            733.6, 
            27
        ], 
        "or equal to one cup of week Etc. So they have these different bins so you can see where you are. And then they measure people's exercise level per week. Now that could have been in lots of different things number of calories burned. So there's a continuous random variable. They used to Mets. Sorry America, you're not probably familiar with this stupid unit. But all the non American people ": [
            2541.1, 
            2567.5, 
            82
        ], 
        "people will hate today. Because I said here's the tool. You don't really get to understand why anything works. It's a super complicated tool with all these weird intricacies. And people get bothered why they when they don't know things are true. So if you're that kind of person 181b is waiting for you. Just have to get through 180 and 181 a first. Okay, they're quite doable and enjoyable. ": [
            2063.4, 
            2091.1, 
            71
        ], 
        "playing baseball. Where they've been grouped or put into populations by their position, so I will field infield designated hitter catcher. Okay. Now you have to be measuring something about them some quantitative idea. So here you can see that it's on base percentage, which is some sort of measure for batting average if you know that. It's probably needs her thing to think about she might wonder if the ": [
            944.7, 
            972.2, 
            36
        ], 
        "population are measuring here. It's women probably American women. Okay, and then Dino should be that all the new eyes are the same. If there's a lot of nuso buys you can write them all out and put equal sign between them or you can choose words. So it's totally fine to use words infected easiest to use words for the Anova null and alternative hypotheses. All the new eyes ": [
            2738.6, 
            2764.2, 
            85
        ], 
        "really know what you're doing, but it's very dangerous to put numbers in this column because our needs to know that it's a categorical variable a bidding way of dividing things up in our can get very confused if you put numbers in the year column. Okay, so then what are you do so then you run aov analysis of variance? And the first thing it wants is a It's ": [
            1724.0, 
            1751.4, 
            58
        ], 
        "right order. That's something a lot of people don't know. It's a group thing is the first spot in the F distribution in the errors II spot. What commands should we write in our we get an F stat of 4.2? don't you love the temptation of a It seems right in every way. Right? So people will select it before they read on. Okay are always resist lower Tails ": [
            2270.8, 
            2332.7, 
            75
        ], 
        "see the direction of Earth's magnetic field by looking and just measuring from some like true north or whatever angle measurements. So how's the direction of Earth's magnetic field changed? Overtime this is a nice way to check it because we have data that span for 400 years basically and you can see so these are three different measurements. They did from the 1669 eruption hardened lava. now let's try ": [
            1360.7, 
            1401.9, 
            51
        ], 
        "side if you don't want to. But I would recommend choosing the side if it's reasonably clear now. I'm I don't know. I mean with the pictures over here. I believed it more the numbers. I'm starting to get worried cuz numbers you look at them differently, right? Now you're not going to see normality in this list of numbers here, unfortunately. But what is cool about this list of ": [
            1003.0, 
            1028.2, 
            38
        ], 
        "so on for the others so, you know, it'd be super boring if they were all the same number. We wasted our life going back in time. And I would be super excited if one of them different from the rest because this would show a change in the magnetic field. In fact, lots of them could differ that would be even more exciting. It would show continual change. Not ": [
            1651.0, 
            1670.9, 
            55
        ], 
        "so this will run the Anova and it gives you back a very complicated object with tons of information in it. So then I just say give me the summary of this big complicated object. So this print out a table and this will be the Anova printout or read out and this is common to most pieces of software there actually is some uniformity in which people in the ": [
            1776.6, 
            1801.6, 
            60
        ], 
        "someone's going to see it. So I promise when you start doing stuff a lot. Randomness crazy random events just start happening. Because you have a large multiplier as you start to think something weird is going on when nothing inside weird is going on. So if you run lots of tests and we're going to start showing the significant difference. When really maybe nothing's going on that is a ": [
            585.4, 
            610.2, 
            22
        ], 
        "something with your soul. Hey average together the numbers in each column. And then ask yourself if you feel like they're different from each other. Does your soul have an answer? So we're going to run into Nova now and figure it out. And with the Anova is going to do is going to try to look beyond our data. Can you can do it? Like obviously the columns don't ": [
            1401.9, 
            1435.1, 
            52
        ], 
        "squared call him is telling you what MSG in Embassy are. So the way this is set up is one of the Rose will be about grouping ideas. So here's the year variables how you group things in case of this entire first row is group related things. That's why this number is MSG. You can see the MS from the column in the G comes from the year idea. ": [
            1824.9, 
            1848.9, 
            62
        ], 
        "standard errors the name for this occasion. This was Sigma / \u221a and where Sigma was being taken on by the role of standard deviation cuz you don't know if they So that was a nice thing we did there when we have two populations. We would do the same idea take the difference that's going on in your two samples and their averages subtract away what you think the ": [
            192.1, 
            216.0, 
            7
        ], 
        "terms of what what are the errors that are happening as you look at infielders. How did they vary? And this other one is looking at the variation in groups as they compared to an overall average. And then we have some ratio. So this is what the f stat let me show you how to use this in a nice example. This is one of my favorite ones that ": [
            1275.8, 
            1302.2, 
            48
        ], 
        "that are both trying to analyze variance. It's going on in queso on top MSG the stands for the mean squares across group. Now this is defined terms of a horrible formula. 1 / K - 1 * some summation I'm so what this summation is doing. Is taking XII bar? Okay, you can see what we're missing a bar here. There should be a bar over this xr80r know ": [
            1081.4, 
            1115.3, 
            41
        ], 
        "the 181b class. They just had their homework on a Nova and they had to write code in our that would generate such a dataset. So it's not obvious. Like how one would even have such it is that you have to like let the computer try to create such a thing. So it's so it shows that there's like not a perfect structure to all of our statistical testing ": [
            2458.8, 
            2478.4, 
            79
        ], 
        "the formula here. So on the left side and wants to know the quantitative variable and this Tilda you're going to tell it how to break up a partition the data into different groups. So use the year variable to split the angle data into different groups. And then everything comes from the volcano data set. So the thing I named this object the state of for him here. Okay, ": [
            1751.4, 
            1776.6, 
            59
        ], 
        "the most efficient way on the page. Now that you've already done something like this previously. Okay. Set up an average idea. It should be the average of the quantitative variable in the problem. So hear the quantitative variables The Meta this is metabolic equivalents tasks here. Now you need to tell me what the different groups are. So this is the different coffee categories that helps to tell what ": [
            2591.7, 
            2738.6, 
            84
        ], 
        "the possible values they can take on or on the horizontal axis so starts at deer and goes to Infinity. And the reason is the F stat is a ratio to mean squared ideas which always positive so that's why you can't get anything negative. And so I just chose to random Ones based on these different crazy degrees of freedom, and you can see in general. They sort of ": [
            1978.7, 
            2001.8, 
            68
        ], 
        "the salt and the chocolate chips are in perfect balance in order to use this. Okay, now some of these conditions are more and less important. Okay, the last one actually turns about to be really important. You've never seen anything like this before but no malady one is easier to a road. And in fact, we seen this in the past with the central limit theorem. I always say ": [
            814.5, 
            839.2, 
            31
        ], 
        "the spreads to be roughly equal physical homoscedasticity almost the same speed asked hicity for spread out in this. You're worried about the 25.5 right here. So if I were you I would write we need to spread to be roughly equal. I'm somewhat concerned about the 25.5. So there you told me told me what you need and you're not willing to even commit on this one is someone ": [
            2823.9, 
            2851.1, 
            87
        ], 
        "them whatever you want. Make sure they've Greek letters you can put the years of subscripts or you can just put numbers and then tell me what they relate to so if we were able to truly go back in time. And set up a device that would measure the direction of the magnetic field. We would get a value. In 1669 that value would be a new one. And ": [
            1627.9, 
            1651.0, 
            54
        ], 
        "thinking about averages and eventually we moved to this the t-statistic you saw me doing all the tats all the time. So this doesn't it takes your ex bar and it's standardized. Is it and moved it to an ice distribution the T distribution with n -1 degrees of freedom to subtract the meaning of what's going on in you / the spread out in this switch here. We use ": [
            168.9, 
            192.1, 
            6
        ], 
        "this residuals degree of freedom is always going to be in the total number of data points. You were studying. / - K the number of groups that you broke things into so here we get 9-3 or 6. Okay, so that's that column. Now. The last thing you're going to care about is the final thing. This is the P value. This helps you decide between the two hypotheses. ": [
            1901.6, 
            1928.0, 
            65
        ], 
        "this would be the most natural source of null and alternative hypotheses. So if you continue that you could imagine how one would set up a situation around more than two. You don't be really boring if they're all these populations and I go try to study what's going on and then all the averages come out to be the same. And sometimes they're not even different populations really anymore ": [
            92.8, 
            115.8, 
            3
        ], 
        "to manova or manova, depending on how do you pronounce add multiple analysis of variance in in man? Kova multiple analysis of covariance. So then you start classes, but the acronym Train goes for a long time. I'll tell you that. Okay. Now one thing you immediately asked the same. Why do I need your fancy new framework? Why don't we if we have lots of different populations. Just run ": [
            406.5, 
            437.0, 
            15
        ], 
        "to some value you not do your specifying so think about this constant and maybe you set up an alternative is it's not that value. And we realized that we could continue this idea into more than one population and we could do this as long as we started studying differences between two populations. Is it now there's two things we need to study. Maybe they're equal. Maybe they're not ": [
            66.5, 
            92.8, 
            2
        ], 
        "to study two of them. And you're going to study all possible combinations of two of them. Suddenly. We have K choose to which is about K squared. So now you're running about K Square different tests. How do you feel about running so many tests? Well, there's different emotional things to feel about this. The first is is it possible to run that many tests that sort of a ": [
            489.1, 
            519.6, 
            18
        ], 
        "tons of two sample T Test. Turn natural question, right? Why do I need to sort of compare everything at once with a single Hammer hitting all of the nails? Why can't I just look at them in pairs? Well, it's a nice idea except that you'll see if we have four different populations. You can already see that the number of comparisons is growing large. Okay, so here you ": [
            437.0, 
            467.0, 
            16
        ], 
        "very dangerous thing because you're going to start seeing some type 1 error showing up. Just a random chance. So yours the metaphor suppose we're trying to decide if a coin is fair and I have a big bunch of coins and they're all really fair. Okay, and I give everyone in the room of hair queen and I say here's how we're going to decide if these coins are ": [
            541.3, 
            562.9, 
            20
        ], 
        "way to correct for this at the end of the slide so we won't get too cuz bonferroni correction if you want to read about but anyhow, let's say you don't want to run a bunch of tests run one that does it all at once. This gets rid of What's called the multiple testing problem. Does it runs a single test? And it's check to see if all the ": [
            610.2, 
            632.4, 
            23
        ], 
        "week and a half to cover it and it takes about a month build up. So if you want to know why we don't really do this and Technical detail it's because it requires that much work in addition to two entire quarters before that of lead in material. So that's the amount of meat on the bone here if you like, okay, but we can at least get introduction ": [
            326.6, 
            351.5, 
            12
        ], 
        "what's going on. It was already a hot mess with one population with two we had to analyze differences and then suddenly the distribution you do everything on it's already getting complicated. So when it comes to three or more populations, we actually have to leave the T distribution behind and we're going to move to something called an F distribution. Named after sir, Ronald Fisher. That's where the F ": [
            240.3, 
            265.4, 
            9
        ], 
        "what? Now that's good. I love to ended on one that the majority gets wrong. Now you have very little. Hope of getting this right. I'll tell you that right now. Okay, and you probably song the explanation? It's possible to be a Nova to say something interesting is going on. But when you go try to find it using a bunch of pairwise tests. None of them can find ": [
            2373.1, 
            2424.6, 
            77
        ], 
        "when lava hardens because it's full of lots of magnetic things metal things. You can see the direction of Earth's magnetic field in the hardened lava because of the iron that appears in lava. Okay, so that's kind of cool. So what research is did is there were three different eruptions of Mount Etna in these years and you can go back in the fossil record record and you can ": [
            1335.1, 
            1360.7, 
            50
        ], 
        "why it's not there. That's weird Okay, so that's some number. And then it's impacting that from X bar, which is the overall average if you take all the players and don't think about them based on any group and just an average of all together. This is sort of what happens in baseball on average. Okay, so it's looking like what how did the outfielders differ from the overall ": [
            1115.3, 
            1141.8, 
            42
        ], 
        "women a bunch of questions. And then it is Tuesday to set and then people could just go study whatever they wanted. So this is study that came out about this to people who didn't drink different amounts of coffee each day. 10 to exercise different amounts It's what they do this in two cups of coffee and they made it a categorical variable it bend it. They less than ": [
            2515.1, 
            2541.1, 
            81
        ], 
        "world have decided to do despite the fact that we have different softwares. Okay. So when they have to be able to do is read this print out regardless of the software came from the F statistic, it's taking the ratio of MSG in mac, and it just happened to get the number 15. They said that's not hard because of the name is exactly above it. Now. The mean ": [
            1801.6, 
            1824.9, 
            61
        ], 
        "you compare them over to infielders or outfielders, you're like a .01 to compare them over to catchers. You like .02. Starting to look bigger. So that's what I was trying to tell us if these numbers really are different from each other assuming we made all these conditions. Now do you know but does this by calculating the F statistic? So this is a ratio of two weird Expressions ": [
            1052.9, 
            1081.4, 
            40
        ], 
        "you don't actually put it into our is a matrix. Like I gave him the previous thing. You put it into dataframe where all the angles are in one column and then the other column has the grouping variable. Will you showing me how does data split by year? That usually people will use some letter or something for the grouping variable that you can use numbers here. If you ": [
            1699.4, 
            1724.0, 
            57
        ]
    }, 
    "File Name": "Statistical_Methods___A00___Quarfoot__David_James___Spring_2019-lecture_22.flac", 
    "Full Transcript": "Morning, welcome back.  Hope you are emotionally recovering after the Game of Thrones finale.  I won't spoil it.  if you're still waiting to see  okay.  Today is a beautiful topic because it shows you the future of where you could go with all this.  So we're going to talk about something known as Anova analysis of variance.  And the right way to think about this straight off. Is it the generalization of things you've already seen before?  That's one thing you should always say like what are the limitations of my current tools and you'll see that all our testing around averages right now is limited to one or two Okay, so we've worked with  populations  for the rest of single one and we learned how to form hypotheses around these may be the average in your population is equal to some value you not do your specifying so think about this constant and maybe you set up an alternative is it's not that value.  And we realized that we could continue this idea into more than one population and we could do this as long as we started studying differences between two populations. Is it now there's two things we need to study. Maybe they're equal. Maybe they're not this would be the most natural source of null and alternative hypotheses. So if you continue that you could imagine how one would set up a situation around more than two. You don't be really boring if they're all these populations and I go try to study what's going on and then all the averages come out to be the same.  And sometimes they're not even different populations really anymore in relation to what you're trying to measure they're just a bunch of populations that happen to look identical across your quantitative variable that you're setting now things get strange here as you can see in the alternative.  If you're claiming a bunch of things are all identical the opposite of that is that there's at least one person or one idea 1 average whatever it is, you're studying it differs from the group.  This is a frustrating alternative because it doesn't tell you which one differs.  Okay, so as you moved to more than two populations, you can continue the infrastructure, but maybe the alternative is less satisfying than it used to be.  Okay, how do you test all these things? Well in one population?  We said okay, let's study axe bar. That seems like a good way of thinking about averages and eventually we moved to this the t-statistic you saw me doing all the tats all the time. So this doesn't it takes your ex bar and it's standardized. Is it and moved it to an ice distribution the T distribution with n -1 degrees of freedom to subtract the meaning of what's going on in you / the spread out in this switch here. We use standard errors the name for this occasion. This was Sigma / \u221a and where Sigma was being taken on by the role of standard deviation cuz you don't know if they  So that was a nice thing we did there when we have two populations. We would do the same idea take the difference that's going on in your two samples and their averages subtract away what you think the difference should be which is almost always zero for us because of what the no looks like the null says they are two things are the same and then divide how to spread out things were and here we were just on a more complicated distribution.  Now if your soul has good intuition here, it should tell you that we're about to make a big hot mess.  I mean look what's going on. It was already a hot mess with one population with two we had to analyze differences and then suddenly the distribution you do everything on it's already getting complicated.  So when it comes to three or more populations, we actually have to leave the T distribution behind and we're going to move to something called an F distribution.  Named after sir, Ronald Fisher. That's where the F comes from. One of the founders of Statistics did most of the important work in building the entire field between say 1920 and 1960.  Now that's a statistic is a complicated object that we won't fully study. But I'll tell you how to use this. Basically now, it's going to be the ratio of two other quantities MSG taste good, doesn't it?  And MSC whatever those are and you'll notice here that the particular F distribution you do the analysis on suddenly has two indexing parameters. Where's the T distribution? Just had a single one in case I have to tell me two different things to figure out the curve is df1 and df2.  If you take 181b which is what I teach right after this you learn what's going on Wheelie with a Nova and it takes about a week and a half to cover it and it takes about a month build up.  So if you want to know why we don't really do this and Technical detail it's because it requires that much work in addition to two entire quarters before that of lead in material. So that's the amount of meat on the bone here if you like, okay, but we can at least get introduction and get a good feel for what's going on. Okay. So what's up how you heard? All these things on the very left won't use the phrase one sample T Test? Okay, the one refers to the fact that you drew from one population. You were sampling from it. The teen tells you what distribution the analysis eventually happen. So, obviously this will be two sample T Test and over here people just use the phrase a Nova for an analysis of variation for variance.  Okay, as part of a huge framework where you study things by analyzing how they vary.  Aunt in the variation of an object that you come to truly understand it. So slick name now and has lots of extensions.  The first extension people usually learn this and Cova analysis of covariance.  Then it can go to manova or manova, depending on how do you pronounce add multiple analysis of variance in in man? Kova multiple analysis of covariance. So then you start classes, but the acronym  Train goes for a long time. I'll tell you that. Okay. Now one thing you immediately asked the same. Why do I need your fancy new framework? Why don't we if we have lots of different populations. Just run tons of two sample T Test.  Turn natural question, right? Why do I need to sort of compare everything at once with a single Hammer hitting all of the nails? Why can't I just look at them in pairs? Well, it's a nice idea except that you'll see if we have four different populations. You can already see that the number of comparisons is growing large.  Okay, so here you can count that. There are six possible tests and that comes from the number for choose two of the four ones choose to that you'd like to compare and the two symbol tells you how many ways you could choose those two out of the group of four?  Unfortunately The Chew symbol is nasty and get the big very fast. So if you have K different populations and you want to study two of them.  And you're going to study all possible combinations of two of them. Suddenly. We have K choose to which is about K squared.  So now you're running about K Square different tests.  How do you feel about running so many tests?  Well, there's different emotional things to feel about this. The first is is it possible to run that many tests that sort of a logistical question?  Now that your computer's you have no problem with telling the computer to do tons of work, right?  So maybe it's not so bad anymore. Do we have technology to help us? The other question is is it bad to run? A lot of tests? This is sort of a theoretical question and it turns out that if you do something a lot of times this is a very dangerous thing because you're going to start seeing some type 1 error showing up.  Just a random chance. So yours the metaphor suppose we're trying to decide if a coin is fair and I have a big bunch of coins and they're all really fair. Okay, and I give everyone in the room of hair queen and I say here's how we're going to decide if these coins are fair. You're going to flip them 10 times.  And then you going to tell me what happens know most of you will get 5 out of 10 heads. Nothing exciting happened, but there's one person in the room.  The lucky one that was just get 10 heads, right just like someone you know, it's like a one out of a hundred event or something, but we have a hundred people someone's going to see it. So I promise when you start doing stuff a lot.  Randomness crazy random events just start happening.  Because you have a large multiplier as you start to think something weird is going on when nothing inside weird is going on. So if you run lots of tests and we're going to start showing the significant difference.  When really maybe nothing's going on that is a way to correct for this at the end of the slide so we won't get too cuz bonferroni correction if you want to read about but anyhow, let's say you don't want to run a bunch of tests run one that does it all at once.  This gets rid of What's called the multiple testing problem. Does it runs a single test?  And it's check to see if all the means are equal. So this is known as an Omnibus test Omni for everything.  Now it's not the best kind of tests ever is everything equal in life.  Seems like a silly question, right? You almost know the answer is no immediately, but there are some cases where you want to ask such a thing. Unfortunately the drawback of running an Omnibus test as we said earlier is the alternative is to satisfying something differs.  What how many things differ I don't know but at least you have a starting point that you could then go off.  And maybe run some tests never going to use this infrastructure. There are a lot of assumptions. This is one of the frustrating things about it cuz he's all the conditions. First of all, you can't have garbage data.  So you need your data to be independent within and across groups. It was a tough raise the first time you see it within means if here's a picture of our data. So we have three different populations. One, two, three, and we're measuring something quantitative about him. I would come whatever that is. Okay doesn't really matter in this problem. Okay. So all of your data points, you can see you fall into one of these three bins. Now if the data are independent within it means you shouldn't be able to use one data point to infer the value of others.  Within your same group and across means if I know this datapoint, it shouldn't help me make predictions about ones in the other bins.  The easiest way to always get this is to fill up your study by choosing people randomly and to not choose to many. This is the randomization 10% condition. So the same thing is true here and that usually will give Independence both within all the different groups and across the different groups.  Okay next to the data in each group have to be nearly normal. So if you took these dots from say the first group those all represent numbers, you can make a histogram out of them and that histogram shouldn't look roughly normal in the same thing is true in all the groups. So that is a very strong Criterion all of your different populations setting have to be nice Ash.  Just on Friday next door. We figured out how to fix that condition. It's rough. I'll tell you that it will Carson horrible transformation Taylor polynomials. Remember these 20 be cuz I cant infrastructure in case it is a rough thing to lose and finally the spread in each group have to be roughly equal. So you look at how spread out the data are in group one. That's a number using the standard deviation use the variance. Whatever you want to me roughly the same ideas and group.  So this is like a very well controlled.  Structured cookie pay the flower and the salt and the chocolate chips are in perfect balance in order to use this.  Okay, now some of these conditions are more and less important.  Okay, the last one actually turns about to be really important. You've never seen anything like this before but no malady one is easier to a road. And in fact, we seen this in the past with the central limit theorem. I always say I need to do to be nearly normal.  Except it's okay. If they're a hot mess as long as there's enough of them. This isn't balanced, right they're always talking about to the same thing will turn out to be true here and Anova also the last one frustratingly. There's no nice way to fix if you don't have it except for Taylor polynomials and horrible transformations in another class. Okay, so then there's the set up there. So you checking the payments through the two conditions. We always do that. You can see the normality and the Equus prednis quite easily in one of the following two things. You can make me a box plot. They had side-by-side grouping.  Okay. Now in the box plot, the spread is just the height of this box. That's the IQR actually. So if all the IQR is look sort of similar now, that is an emotional judgment.  I need you to just make emotional judgment sometimes.  It's like I don't want to do that in a math class. Give me some rules. I can apply sorry.  Now normality if the central line is roughly in the middle, it means that there's a nice balance between the two halves. No doesn't mean it's normal. But that usually the way people look for normality. Okay. So this I'd say this is like  Roughly normal and roughly the same spread. That's just what my emotional soul is telling me the boxes look like the same height and the lines are sort of in the middle. Basically. I want to see hard numbers. You can always create a table like this is a table of people playing baseball.  Where they've been grouped or put into populations by their position, so I will field infield designated hitter catcher.  Okay. Now you have to be measuring something about them some quantitative idea. So here you can see that it's on base percentage, which is some sort of measure for batting average if you know that.  It's probably needs her thing to think about she might wonder if the different players when you group them based on where they play.  If that idea differs in terms of their batting average.  Okay. So here are the numbers that come out so sample standard deviation. Do these numbers look roughly the same?  something emotional in your soul  Okay, if you're worried, you can always tell me we are somewhat concerned about the equal spread condition. They don't have to choose a side if you don't want to.  But I would recommend choosing the side if it's reasonably clear now. I'm I don't know.  I mean with the pictures over here. I believed it more the numbers. I'm starting to get worried cuz numbers you look at them differently, right?  Now you're not going to see normality in this list of numbers here, unfortunately.  But what is cool about this list of numbers you can start to see you're studying whether you think the means are different, right? That's what this is all about. So if you look at these numbers or so, we'll have a feeling immediately.  But the point of the Anova is to rigorously decide on what your soul is feeling there. I don't know. Do you think the designated hitters have higher batting average than the others?  If you compare them over to infielders or outfielders, you're like a .01 to compare them over to catchers. You like .02.  Starting to look bigger. So that's what I was trying to tell us if these numbers really are different from each other assuming we made all these conditions.  Now do you know but does this by calculating the F statistic?  So this is a ratio of two weird Expressions that are both trying to analyze variance. It's going on in queso on top MSG the stands for the mean squares across group.  Now this is defined terms of a horrible formula.  1 / K - 1 * some summation  I'm so what this summation is doing.  Is taking XII bar?  Okay, you can see what we're missing a bar here.  There should be a bar over this xr80r know why it's not there. That's weird Okay, so that's some number.  And then it's impacting that from X bar, which is the overall average if you take all the players and don't think about them based on any group and just an average of all together. This is sort of what happens in baseball on average.  Okay, so it's looking like what how did the outfielders differ from the overall average and how did the infielders differ from the overall in a sort of accumulating?  This idea when you measure what's going on between the groups as they compare to the overall average.  Okay, and then discenza buys how many people were in a certain group? So how many infielders were in your study how many outfielders so it's a weeding factor in case I'm very exciting to get a huge difference between two ideas if there weren't many people that went into making the difference.  But if there's a huge difference in there's a million infielders that used to build that that's important. So this isn't a Sensei waited.  Weird idea. So there's a square that has to do with groups. And then in the end you take an average across all of those different ideas.  Okay, and you don't / k / K - 1 we seen the sort of like weirdness before.  That you don't really need to truly understand this at some point. You just have some emotional feeling that is trying to measure some idea that has to do with how groups differ.  From the overall average. Okay down here the embassy this is the mean squared error.  Or the sum of squares related to are terms that you've taken an average of okay. So this looks within each group and it says okay within each group. You'll see a lot of variability not all infielders. Look the same.  Okay, and this Essa by is the standard deviation going on or the spread out and it's within a given group. Okay, so our infielders really spread out or catchers really spread out.  And what you're going to do is sort of again wait, he's and then averaged together everything going on.  Now it's not obvious. We're doing any of these calculations is helpful. It's not obvious. Why taking their average gives you anything important at all.  Right now it's just sort of we're analyzing the variation that can happen.  And the variation here in MSC is all about variation sort of within a group in terms of what what are the errors that are happening as you look at infielders. How did they vary?  And this other one is looking at the variation in groups as they compared to an overall average.  And then we have some ratio.  So this is what the f stat let me show you how to use this in a nice example. This is one of my favorite ones that I've ever seen done with a Nova because it's a really cool result actually.  So if you know anything about volcanology have you taken of volcanoes course here?  Has anyone taken volcanoes?  Apparently it's like one of the most popular courses at UCSD.  I used to teach in the room right before the guy without volcanoes and was a crazy in there. He won't crazy for that class.  Anyhow, so when lava hardens because it's full of lots of magnetic things metal things. You can see the direction of Earth's magnetic field in the hardened lava because of the iron that appears in lava.  Okay, so that's kind of cool. So what research is did is there were three different eruptions of Mount Etna in these years and you can go back in the fossil record record and you can see the direction of Earth's magnetic field by looking and just measuring from some like true north or whatever angle measurements.  So  how's the direction of Earth's magnetic field changed?  Overtime this is a nice way to check it because we have data that span for 400 years basically and you can see so these are three different measurements. They did from the 1669 eruption hardened lava.  now  let's try something with your soul.  Hey average together the numbers in each column.  And then ask yourself if you feel like they're different from each other.  Does your soul have an answer?  So we're going to run into Nova now and figure it out. And with the Anova is going to do is going to try to look beyond our data.  Can you can do it? Like obviously the columns don't have exactly the same average, but the question is are they so far apart from each other that they must have come from populations that actually differ.  So let's eat first thing. Can you define me some variables and write me some hypotheses? Let's start building the infrastructure around this a minute here to do that.  Okay, three different populations.  1669 1780 1865 implies three different averages you can call them whatever you want. Make sure they've Greek letters you can put the years of subscripts or you can just put numbers and then tell me what they relate to so if we were able to truly go back in time.  And set up a device that would measure the direction of the magnetic field. We would get a value.  In 1669 that value would be a new one.  And so on for the others so, you know, it'd be super boring if they were all the same number. We wasted our life going back in time.  And I would be super excited if one of them different from the rest because this would show a change in the magnetic field.  In fact, lots of them could differ that would be even more exciting. It would show continual change. Not just a one-year change.  Okay now.  The calculations around an oval are horrible in complicated and they usually must be done with computer.  Guess I'm not going to ask you to like.  Find an F stat by hand or from the original data. This would be nightmarish. Okay, so let me show you what it looks like when we go put this into our especially if the structure correctly.  Now you don't actually put it into our is a matrix. Like I gave him the previous thing. You put it into dataframe where all the angles are in one column and then the other column has the grouping variable. Will you showing me how does data split by year?  That usually people will use some letter or something for the grouping variable that you can use numbers here. If you really know what you're doing, but it's very dangerous to put numbers in this column because our needs to know that it's a categorical variable a bidding way of dividing things up in our can get very confused if you put numbers in the year column.  Okay, so then what are you do so then you run aov analysis of variance?  And the first thing it wants is a  It's the formula here. So on the left side and wants to know the quantitative variable and this Tilda you're going to tell it how to break up a partition the data into different groups. So use the year variable to split the angle data into different groups.  And then everything comes from the volcano data set. So the thing I named this object the state of for him here.  Okay, so this will run the Anova and it gives you back a very complicated object with tons of information in it. So then I just say give me the summary of this big complicated object.  So this print out a table and this will be the Anova printout or read out and this is common to most pieces of software there actually is some uniformity in which people in the world have decided to do despite the fact that we have different softwares. Okay. So when they have to be able to do is read this print out regardless of the software came from the F statistic, it's taking the ratio of MSG in mac, and it just happened to get the number 15.  They said that's not hard because of the name is exactly above it. Now. The mean squared call him is telling you what MSG in Embassy are. So the way this is set up is one of the Rose will be about grouping ideas. So here's the year variables how you group things in case of this entire first row is group related things. That's why this number is MSG. You can see the MS from the column in the G comes from the year idea. The other row will be the error residual row.  Depending on what language they use here because of this Rose all about are ideas. So you can see Ms. Andy coming together 2.95 is the embassy. Okay, and you can even chat 45 / 3 is about 15.  Is it ain't doing any magic here if you understand what those calculations are now over here the degree of Freedom call him. It's helping keep track of what kind of curve are eventually going to be on.  Okay. Now the first one since this in the Years A grouping idea. This is known as DFG the degree of Freedom related to the group's. Okay. So it's the number of groups - 1 that's how you find this. So here we three different years. And so that's why they're getting to  Now this residuals degree of freedom is always going to be in the total number of data points. You were studying.  / - K the number of groups that you broke things into so here we get 9-3 or 6.  Okay, so that's that column. Now. The last thing you're going to care about is the final thing. This is the P value.  This helps you decide between the two hypotheses. You just wrote it and it's as it normally is if this is really small you move to the alternative we've seen incredibly weird. Aprecio is what this number the fact that is so small this is what it means.  Okay.  Now if you want to think about what a p-value always means it's always the area under some curve.  Based on your data or something more extreme.  assuming the null hypothesis  So here that curve happens to be an F distribution in the particular of distribution euron is dictated by the two degree of Freedom parameters that you see right here.  Okay, so I can actually draw a picture of what's going on if you want.  Okay. Now we probably should go a couple F distribution. So you could see what they sort of look like. So their support the possible values they can take on or on the horizontal axis so starts at deer and goes to Infinity.  And the reason is the F stat is a ratio to mean squared ideas which always positive so that's why you can't get anything negative. And so I just chose to random Ones based on these different crazy degrees of freedom, and you can see in general. They sort of go up have a hump and then come back down.  Is what these look like? So when you're calculating A P value for an anova all you do is draw the appropriate F distribution, and then you plop down your fstat or your value and then you just shaved to the right.  Now one thing is strange about these.  You're always going to shade to the right.  And there is no two-sided alternative. There is no symmetric shading both directions. There's none of that the hypothesis for an anova always look the same but no is all the means of the same. The alternative is something is different.  And when you do the shading you always popped on your fstab and shake it to the right. So the infrastructure is built around that happening.  How you feeling so far?  Some people will hate today.  Because I said here's the tool. You don't really get to understand why anything works. It's a super complicated tool with all these weird intricacies.  And people get bothered why they when they don't know things are true.  So if you're that kind of person 181b is waiting for you.  Just have to get through 180 and 181 a first. Okay, they're quite doable and enjoyable.  If you're not one of those people in your fine, like give me a tool I love using tools.  When is a great day for you and you enjoy using things like a black box? We don't really truly understand. What's going on underneath. Okay, how about we do some Socrative?  I wonder how you're going to do. Let's go see.  so if doing an F test that is an anova  how many groups?  Are the researchers dataset?  Hey, there we go.  So this just seeing if you have learned this expression for the degree of Freedom related to the grouping idea K - 1 so you can back derive the number of groups that way.  How many total data observations were in the data set?  20/20  Carrots are people breaking out the degree of Freedom related to the error formula and minus K and back driving 110 was from that.  collect salary info on a hundred 1012 200 people five different states  What curve is the p-value found on for this study?  His number should be subscripts here. But so proud of his so limited.  Can't really show it nicely.  Look at that 70% already. So here you're just using the you need to make sure you get the right order.  That's something a lot of people don't know. It's a group thing is the first spot in the F distribution in the errors II spot.  What commands should we write in our we get an F stat of 4.2?  don't you love the temptation of a  It seems right in every way. Right? So people will select it before they read on.  Okay are always resist lower Tails by default.  So you need to tell it to shade or the right side, which is width D is doing.  if a Nova suggest a move to the alternative  Then we will have identified the mean that is different than the rest.  So I said it a couple times already. The alternative is frustrating. It says something exciting is going on. That's all it says nothing more doesn't tell you what?  Now that's good. I love to ended on one that the majority gets wrong.  Now you have very little. Hope of getting this right. I'll tell you that right now.  Okay, and you probably song the explanation?  It's possible to be a Nova to say something interesting is going on. But when you go try to find it using a bunch of pairwise tests.  None of them can find it.  Okay. So here's this analogy corruption present in the government, but you can't exactly identify where it is.  Sometimes the totality of the experience flowing at you says excitement. If you can't pinpoint it for some reason maybe it's generated across the totality of lots of different comparisons are leading to that.  But when you dig deep it's not visible. So this is a very counterintuitive results.  In fact the 181b class. They just had their homework on a Nova and they had to write code in our that would generate such a dataset. So it's not obvious. Like how one would even have such it is that you have to like let the computer try to create such a thing. So it's so it shows that there's like not a perfect structure to all of our statistical testing here that we can figure out everything in life, basically.  Okay, should we try some more?  But try some more.  Try smoking problem. No, no still coffee problems. I like the coffee one more than the smoking one.  Okay. So here's an actual study. This is one of the freely accessible jumbo data sets on the internet that you can go get the women's health study.  Okay, so they asked women a bunch of questions.  And then it is Tuesday to set and then people could just go study whatever they wanted. So this is study that came out about this to people who didn't drink different amounts of coffee each day.  10 to exercise different amounts  It's what they do this in two cups of coffee and they made it a categorical variable it bend it. They less than or equal to one cup of week Etc. So they have these different bins so you can see where you are. And then they measure people's exercise level per week. Now that could have been in lots of different things number of calories burned. So there's a continuous random variable. They used to Mets. Sorry America, you're not probably familiar with this stupid unit.  But all the non American people are probably loving it Kristi noem eat. He's a little better 50739 women.  Okay, so here is sort of a summary table of what's going on. So we're going to run in an over here. It is to see if different amounts of coffee leads to differing amounts of exercise on average. So I need you to set up temporary amateur some hypotheses for me. Try to do this in the most efficient way on the page. Now that you've already done something like this previously.  Okay.  Set up an average idea. It should be the average of the quantitative variable in the problem. So hear the quantitative variables The Meta this is metabolic equivalents tasks here.  Now you need to tell me what the different groups are. So this is the different coffee categories that helps to tell what population are measuring here. It's women probably American women.  Okay, and then Dino should be that all the new eyes are the same. If there's a lot of nuso buys you can write them all out and put equal sign between them or you can choose words. So it's totally fine to use words infected easiest to use words for the Anova null and alternative hypotheses. All the new eyes are equal some at least one is different.  Okay, next thing let's talk about inference conditions. There were a bunch of these.  So if someone talked to me about any of these that you'd like to mention and whether you think it's met.  Help us who checks conditions anyway.  Let's be stopping garbage researchers. Never check anything.  We have three conditions.  Go ahead. Which one do you want?  Okay, Sweeney the spreads to be roughly equal physical homoscedasticity almost the same speed asked hicity for spread out in this.  You're worried about the 25.5 right here.  So if I were you I would write we need to spread to be roughly equal. I'm somewhat concerned about the 25.5.  So there you told me told me what you need and you're not willing to even commit on this one is someone else might be totally fine.  What are the other conditions?  Go ahead.  I'm sorry.  10 so 10% and randomization are trying to create the independence idea. How do you feel about  I agree there more than 50,000 women in the US drink coffee.  But we need more than 500,000 right when we need more than 50 thousand times 10. So I also think there's more than 500,000 Starbucks exists. What about the randomization?  How do you feel about that?  They are at why'd you say the randomly-selected? Where'd you get that?  Hoping and dreaming in your soul. So you have to go back.  I don't see anything about random. In fact, this was probably volunteer. I'm guessing.  Okay, we don't know about this condition because we don't know how the data were collected.  Volunteer bias is most likely what's going on. Although I don't know if that affects anything about what's going on in this. We also don't know anything about the normality condition here because I haven't shown you a picture of the date on each of the five groups.  So just tell me if you don't know something it turns out it doesn't really matter about the normality though.  So in general we just need normality unless we have large sample sizes and we have huge sample sizes. So these numbers 6017 thousand at cetera.  Now, how am I ever going to give you a problem like this on an exam?  It mostly involves computer stuff, right? We're here so you can give her problem like this on an exam. You can give the print out and say there's missing pieces.  Coffee stains are blurring things whatever. Okay and between this description of what's going on in this table, you should be able to fill in everything that's going on here.  So for example, you can get the DF column by just knowing how the study of structure there were five groups, right? So 5 - 1 is 4  There were 50739 people at the end. So if you subtract a from that you can get this value and you can always add Down The Columns to get the total idea.  Now this sum of squares called me even talked much about okay. It's really just an intermediary it to get to the main idea which divides the sum of squares column by the degree of Freedom column. So if I subtract I can get this number and then if I divide the sum of squares by the degrees of freedom, I'll get the mean square column, which is what we really care about. So we have to stop but I suggest you keep looking at these things and working on them.  Have a great day. I'll see you later. "
}