{
    "Blurbs": {
        "-1 coordinates and you can draw polynomial through them questions about that. What do you mean by reacting certainly the X be different and the Y coordinates have to be different. But I guess if you do get just a straight line then all of the coefficients will be zero up to that point. Okay, so these two representations have different benefits. They're easier to do certain things. So let's ": [
            1597.1,
            1652.5,
            41
        ],
        "A geometric series it behave differently depending on if its base is bigger than or equal to or less than 1. So if it's less than one then you get this whole some as a constant and you just get bigger than to the D. So that's good. If it's equal then each one of these is equal to one and you get You just get into the D X ": [
            302.7,
            329.2,
            6
        ],
        "And that takes big ol then again. So all in all it takes big oven login time to do this multiplication. Song now, let's see how it works. Because I swear doing this transformation back and forth really if you want to cuz I see that you're not think we need unlimited lines, but it wont. So really all we're doing is right and that's why I can't say that ": [
            1949.1,
            1990.0,
            49
        ],
        "Is that better? Yes. Wow. That'll wake you up, huh? Okay, so let's get started. Anybody have any questions before we begin? I posted the solutions for the homework or was that for so might be a good idea to take a look at those see how those proofs go. I also gave you some practice problems for the next Quiz. So I'll probably put those Solutions up tomorrow just ": [
            64.1,
            133.7,
            0
        ],
        "It's not going to be divided by 2. Actually, I should put it like this the size of x squared right because of a has let's say k elements been x squared is also have to have k-elements right because you're basically squaring all the things. So X. Let's just do like a little example. Let's say x is equal to 1 2 3. Then x squared has got to ": [
            2480.7,
            2511.4,
            61
        ],
        "Okay, then you set a x to be equal to this value for all x in x. That's the combined step. This is the whole thing. And the record the recurrence well now we know that x square the size of x squared is actually the size of x / 2. + so really you can think about the input as a size of an plus size of X and ": [
            2650.1,
            2677.9,
            66
        ],
        "a greater than 3 to the bugbee to the D. Until we get T of n is equal to Big O of n to the log base 3 of 5. Okay, and that's that's bigger of N2 the 1.43. So we have improved. And then like you said we can keep on going and keep on improving more and more the more you divide it up. There's one kind of ": [
            992.2,
            1024.7,
            24
        ],
        "a to the log base B of n? Why is that equal to n to the log base B of A. Does anybody know? TV program log as a fraction Okay. I guess so. You mean just using log rules? so one thing to notice is Right if you want I see what you're saying. So if you wanted to decompose. log base B Of end is actually equal to ": [
            446.1,
            521.5,
            10
        ],
        "again. But again now we're done. We've already proved it so we don't have to do this anymore. Okay, so Now we have our Master theorem and let's think about these these three different scenarios as different states. So we saw that we saw on what was that Friday the karatsuba multiplication and the grade school multiplication was bottom-heavy meaning that every level you go down you increase the number ": [
            573.0,
            607.6,
            12
        ],
        "and a o of alpha for all Alpha and x squared right where x squared is going to be the set of all x squared such that X is an X then you can combine them together, right? Because I need to know the x squared values in order to put them together to get the X values. Okay. So basically the runtime recurrence for this thing is going to ": [
            2403.8,
            2433.1,
            59
        ],
        "and buy in square. It's you're going to do at least and squared. Okay. So here's the here's the thing. The fast Fourier transform what it does is it takes coefficients to samples and back it's this transform where you can go back and forth and the power of it is because the reason that it's fat we call it fast is because it takes big O of n log ": [
            1854.0,
            1883.0,
            47
        ],
        "any Epsilon greater than zero you can design a multiplication algorithm that runs in Big O of n to the one point plus Epsilon time. Pretty cool, huh? I mean most of them are pretty impractical because of that linear term is a little bit too heavy. got any questions Well, we're thinking about the division thing as the algorithm we designed. So we're going to fix it. We're going ": [
            1306.3,
            1345.5,
            33
        ],
        "are grow a constant multiple from each other. Okay, then the steady-state one. We're going to see that one when we when we look at mergesort. Basically you're going to do the the same number of operations in each level. And so you're basically adding up all of those I'll log and amount of time because that's how many levels you have. And then you have the top-heavy one will ": [
            652.1,
            681.1,
            14
        ],
        "base, right? So you can you can use that and it gets a little complicated to get the exact run time because then you have to figure out okay. Am I getting enough Precision on these numbers and all that kind of stuff? I'm not going to go into that. Just trying to give you more of an overview because it's a beautiful divide and conquer algorithm also. Okay, so ": [
            1467.7,
            1491.6,
            37
        ],
        "be equal to what 149 right and they have the same number of elements. Is there any way that you can have? x squared have fewer elements than x Okay good. So if x is equal to -1 + 1 then x squared now is equal to just one. Thanks for this is the this is the key to get that recursion to go down. This is called a collapsing ": [
            2511.4,
            2557.9,
            62
        ],
        "be well. Let's base it on and and the size of x. Well, I do to recursive calls, right? T of n / 2 X-Plus Big O of n + x Be combined part, right? The non recursive part is just linear in in the number of coefficients plus the number of points you need to do. the problem here is that this thing here is not going to reduce. ": [
            2433.1,
            2480.7,
            60
        ],
        "coefficient space? also Big O of n o you have to do is add the the coefficients and then you get the Okay, how about multiplying two polynomials in coefficient space? Right kind of like the grade school method that we've seen is N squared. Okay now sample space how long does it take to evaluate in sample space? Well, if it's one of the samples and it takes constant ": [
            1698.4,
            1732.1,
            43
        ],
        "do is look at these as polynomials in themselves. Rehearse on them to solve them and then put them back together. Okay. Does anybody have a a way to do this to combine them? Okay. But then that first one would be a 3X to the 3 / 2. How about if you just X ax and then plug in x squared? Anybody have any questions about this? So that ": [
            2288.4,
            2367.6,
            57
        ],
        "equal to 2 N and this becomes Big O up and login. any questions Only live within a Pergola restaurant. Wouldn't there be some functions that and like what? not necessarily be able to represent everything that you only inflection points value of the function after exactly Circle at one point. There's only one point where it is possible to recycle there is no there's no way to know that. ": [
            2718.0,
            2791.1,
            68
        ],
        "gases? You can go from 4 down to three nine can go down to. 5 Wi-Fi seems like a good number. 5 is right in actually has to do with the amount of terms you have right? There are five terms. And the way that you do it, it's kind of complicated and the best way to describe it is using a bunch of Matrix algebra, and I'm not going ": [
            926.8,
            961.2,
            22
        ],
        "get all the roots of unity and what you want to do. If you want to compute the value of the polynomial at all of those roots of unity the way you do it split it up like this compute a e and Ayo for all Alpha and x squared and you know that this set has decreased by 2. So, you know that the recursion will keep on decreasing. ": [
            2625.9,
            2650.1,
            65
        ],
        "get exactly. But the next thing we're going to do. I'm going to give you a sort of a brief overview of the fast Fourier transform, which is also a multiplicative. Or multiplication algorithm. It's more focused on multiplying polynomials instead of integers. But if you think about what is a integer in base 10 or base 2, it's just a polynomial where the the variable is replaced by the ": [
            1429.4,
            1467.7,
            36
        ],
        "go. I'll give you guys like a few minutes to see if you can figure out what is this limit go to? You guys could probably guess right. But we saw when K was equal to 2 it was 1.58 right when K was equal to 3 it was 1.43. It keeps on kind of going down. Anybody have a guess? What's good? You could use l'hopital's Rule and get ": [
            1236.7,
            1278.5,
            31
        ],
        "in time. So if I want to multiply two polynomials and coefficients Pace, I can transform them into sample space multiply them and then transform them back. Answer this is kind of the idea. Let's call it a multiplication idea. SO2 coefficient polynomials Thank you transform. in two sample That takes big O of M login. Right, then you multiply. Which takes big O of n then you transform back? ": [
            1883.0,
            1943.5,
            48
        ],
        "just do it naively then you just get this you get this back and you're not really saving anyting but if you use the trick and there is a way to do it for nek to reduce K squared multiplications down to 2 K - 1 multiplications. There's always a way to do it. It just gets increasingly more complicated like I said before but it's still a constant time ": [
            1168.7,
            1197.1,
            29
        ],
        "lime or any other functions that only so every polynomial will have a value at every point on for every point along that Circle. so Are like that? Are imaginary x coordinate plane? So what kind of go into some complex analysis kind of stuff? Oh, well, I guess I sorta just did it backwards. I guess they just first-set x2b to end and then And then do it like ": [
            2792.9,
            2839.1,
            69
        ],
        "limit as K goes to Infinity of 2/2 K - 1 / 1 / K which is limit as K goes to Infinity of. 2 k / 2 K - 1 which is 1 okay, but this is a limit right? It doesn't mean that you can actually get down to one. It just means that you can get to as close to 1 as you want. So basically for ": [
            1278.5,
            1306.3,
            32
        ],
        "log base B event because that's how many times you're adding this up. And then if you have a is greater than b then you have a base that's bigger than one and so the a geometric series with a base bigger than one behaves like an exponential function. And it behaves exponential in terms of whatever the the last. indexes of the sun canceling you get this And I ": [
            329.2,
            364.0,
            7
        ],
        "log base a of n / log base a of B, right. Okay, so then this thing becomes. hey to the log base a of n / log base a of B that becomes into the log base a sorry to the 1 / log base a of B and that's equal to end to the log base B of A. Never thought those log rules would come in handy ": [
            521.5,
            573.0,
            11
        ],
        "master theorem. It should be Big O of n to the log base B of A. Now back to your comment. This be is actually really important to leave in there. Okay, so you got into the log base 2 of 4 which is bigger than squared. Not any Improvement to the grade school method but then if we change it to the karatsuba, then we get a is equal ": [
            733.8,
            761.5,
            17
        ],
        "multiplication is one-third the size of the original right and so you get this recurrence. It's still going to take linear time to put them together. So you have a is equal to 9 B is equal to 3 G is equal to one so you still have a bottom-heavy with a is greater than beat the D. And so we get T of n is equal to Big O ": [
            846.9,
            878.4,
            20
        ],
        "multiplying. integers can take n log in time because there is that Precision thing, but we're just going to kind of theoretically assume that we can have Unlimited Precision just to kind of get through the the the main point. Good point. Okay, so let's move on. So if I want to go from coefficient to samples I could I could just * this Matrix right? You guys can just ": [
            1990.0,
            2026.1,
            50
        ],
        "n to the D is the non recursive part. It's basically the part of like how to combine all your recursive calls to get to your final solution. So it's sort of the part. Hear these lines that's kind of like how long it takes to do that. So this is going to be Big O of n to the D, right. And then each one of these is going ": [
            202.0,
            228.5,
            3
        ],
        "need to know in order to draw a parabola? Three three points will describe a unique Parabola Four Points will describe a unique cubic 5.4 a cortech six points and so on. So basically if I give you What is it endpoint? Then you can tell me a unique and -1 degree polynomial. Okay, so instead of giving you the coefficients and -1 coefficients I can just give you And ": [
            1557.2,
            1597.1,
            40
        ],
        "not use the keiretsu by the kind of the naive way of using all four multiplications, can we plug in the values we get a is for B is 2 and D has one right in this recursion. And so you look at a is greater than b to the D. So you get the bottom heavy, which is what we thought. And so you just plug it into the ": [
            709.8,
            733.8,
            16
        ],
        "number of additions that you additional additions that you have to do. I think it grows exponentially. So it's it's kind of like this balance of Depending on how big your input is, is it really worth it to divide it up into more things because the constant time operation takes a long time. But that being said theoretically we are improving the asymptotic runtime. So let's let's push it ": [
            1097.8,
            1132.7,
            27
        ],
        "of n to the log base 3 of 9 which is what the golden Square That's just kind of what I have here. Okay, so that didn't help but there is a way just like in the karatsuba was able to decrease for multiplications down to three. You're you're able to decrease these 9 multiplications. Does anybody know how many you can decrease if I Would anybody have any gas ": [
            878.4,
            926.8,
            21
        ],
        "of operations you have to do and so most of the work is going to be done in the bottom level. And so that'll be bottom-heavy? Oh good point. So. Log base B of A N is Big O of log base. a of n for neamb bigger than one I think. Yeah, so usually we just say login and it sort of just means all logs. Because all logs ": [
            607.6,
            652.1,
            13
        ],
        "one edition, right? So we had to add like four more kind of addition steps, right? So I would say this is like around and then we had to add the two things together. So maybe it's like on the order of like maybe 10in or something 10 10 editions when you divide by Three then now it's on the order of like 80 editions. Okay, and so the the ": [
            1061.1,
            1097.8,
            26
        ],
        "other question. Applied or for Edinger Matrix of operation of linear and it gets covered up and evaluation. Yeah. Is M squared? Yeah. Yeah. I mean there's another there's other ways to do it, but you really you can't really get much better than M squared if you're just trying to do it directly from the samples. Right, but anytime you deal with a matrix. You have a square right ": [
            1800.0,
            1854.0,
            46
        ],
        "polynomial and also noticed that the the subscripts don't match the exponent anymore. Okay. So what I want you guys to do is to get into groups and figure out how would you if I knew these this information here. How would I use that to combine to get a of X ready go? and then Well, that's what you wanted to be at the end. So you split it ": [
            2157.6,
            2260.2,
            55
        ],
        "polynomial we all know what that is. It's basically this this is sort of the best way to describe it. Right. It's just a summation over powers of X. There's a polynomial in one variable and each power of X has a different coefficient. Okay. So another way to write it is like this. Now what information am I really dealing with is just the coefficients so you can just ": [
            1493.4,
            1525.3,
            38
        ],
        "see that to that just means that as the tree goes down. You're doing fewer and fewer calculations. So really the bulk of the calculations happens in the top level and that's why you have this is equal to just the the time it takes to come by. Just give you a sense of the three different cases. so let's let's up plug these things in so when we did ": [
            681.1,
            709.8,
            15
        ],
        "set. so Ex has one and every time x has one element or everytime you square X the number of elements decreases by a factor of 2. Okay. There's one great example of the set and it's the I think we already did it, right? Okay, what's the relationship between negative one and one and one is that these are the square roots of this do we keep on taking ": [
            2557.9,
            2594.8,
            63
        ],
        "skip over this just if you want this is basically just to kind of show you what the naive way is going to take Big O of N squared time. Okay, and back from samples to coefficients also big oven Square time. I just want to show you that the fast Fourier transform and it's worth it to do because if you try to try to do it and I ": [
            2026.1,
            2048.2,
            51
        ],
        "square roots over and over again? You get the you get I negative I want and -1 you go into the complex numbers. And really what we want to do is have xB. all of the roots of unity I all the complex numbers, but all the nth roots of 1. And that way the set will collapse down to one. Okay, so you get the coefficients and then you ": [
            2594.8,
            2625.9,
            64
        ],
        "step a to the k x basically where it all comes from then we some this so this is just rearranging and we sum it up and we get the total runtime from k equals 0 up to log B, then cuz that's how many levels there are. And so then you just look at the three different cases. It's all dependent on this value here because we saw that ": [
            269.9,
            301.3,
            5
        ],
        "take a look. So here are the three things that I would want to do with a with polynomials. Sometimes I would want to valuate a point. Okay? How long will it take to evaluate a point with the coefficient space? Oh, that's right. Basically you just plug it in and you have to add up a bunch of stuff. to go then how about to add two polynomials in ": [
            1652.5,
            1698.4,
            42
        ],
        "that the two polynomials are sampled at the same points, then you could just add those points together. Now, how about multiplying? Let's suppose that we had as many points as we wanted right then you can just multiply all those samples together and that would take in time also. Got any questions about that? Okay. So one of them is good at 1 and wanted to go to the ": [
            1766.5,
            1800.0,
            45
        ],
        "that. I guess I sort of confusing. Are there any other questions? Okay. Well, thanks for sticking with me through that. I knows sort of vague. I didn't go through all the details but I do think that it's an important divide and conquer algorithm that you should at least know kind of sort of what what's going on and will go into some more familiar stuff on Wednesday. ": [
            2839.1,
            2875.9,
            70
        ],
        "the combined step so if I can get those a of O and E, then this will just take a constant amount of time. Right? Just put them together on time for each x value. Okay, that's basically the algorithm basically the recursion there is one more cool trick that you might Overlook if if you're not careful. Okay, so Basically, here's the thing if I can compute a e ": [
            2367.6,
            2403.8,
            58
        ],
        "the runtime? Okay, we get close to that. Let's see. Let's see how that works. Let's think about splitting into 3 and see if we get any better. Okay, we split into thirds and now instead of multiplying binomials. We're multiplying trinomials, right? So how many multiplications are needed to multiply these two things? 9 right you do that and you get 9 9 multiplications means 9 recursive calls. Each ": [
            795.4,
            846.9,
            19
        ],
        "thing that we need to keep in mind is that this could bind step sure it's Big O of an which means it's a constant times and but that constant is going to get bigger and bigger and bigger the more you divide. So just to give you an idea. the width if you divide by to write. Dad, remember we had to add like a few subtractions and like ": [
            1027.4,
            1061.1,
            25
        ],
        "think about the polynomial as a vector or a in order to bowl or something like that that holds enough information to describe the entire polynomial. okay, so there's another representation that I like to talk about that I'll call the sample representation and it's basically basically works like this. How many points do you need to know in order to draw a line? To how many points do you ": [
            1525.3,
            1557.2,
            39
        ],
        "this this will have every time and you can plug it into the the master theorem and get this this is equal to What is it Big O of? + + x log And plus tax. And really what we want how many how many things do we want in that set? We really want to and element so we can multiply two things together. so set ask to be ": [
            2677.9,
            2718.0,
            67
        ],
        "time, but in general you kind of have to convert it into the coefficient space in order to evaluate something right? And so You can do it with matrices in basically Big O of N squared time. But the point here is that it's not efficient, right? It's not fast. Adding is fine. You just add all of the you add all of the sample points, right? This is assuming ": [
            1732.1,
            1766.5,
            44
        ],
        "to 3 now. It's still bottom-heavy. But now we get into the log base 2 of 3 and to the 1.58. We got a lot better now. can we do better than N2 the 1.58 or is this in the is this the best that you can do to multiply two integers? Can you just keep? 22in instead of splitting into two split into more. Yes, will that actually help ": [
            761.5,
            795.4,
            18
        ],
        "to as far as we can go. if we divided into case of problems, each of size n / k How many multiplications are there now? We have to multiply each one of them right so there should be K squared. Right cuz you have to do a k - 1 b k - 1 AK might it right you have to do all the possible combinations. So if you ": [
            1132.7,
            1168.7,
            28
        ],
        "to be Big O of an over 2 to the Dee Wright. And so on. So at every step you have to do that a x a squared x to the fourth times and saw a cube X and so on right and we talked about this already. I'm just going to skip over this part. Okay, so that means that at level K. You have to do. This combined ": [
            228.5,
            269.9,
            4
        ],
        "to fix K and then let and go off to infinity and see how that how that works. I mean of course and it's got to be bigger than K right for it to work. I mean you can maybe you could like make one up. It would make sense for me. all like a big old to depending on to did you play sports? Oh, yeah. Yeah, I guess ": [
            1345.5,
            1389.5,
            34
        ],
        "to go into it. But if you're interested we can kind of work it out. So There's a way to reduce from 9 multiplications down to just five now. The recursion becomes T of n is 5 * 2 of 8/3 foot bigger event. So by the master theorem now we have a is 5 b is 3 and D is one right and so he still get bottom-heavy with ": [
            961.2,
            992.2,
            23
        ],
        "to the log base B of n right and we have one over B to the log base B of n raised to the D. So as you can see do a little bit of rearranging you get B to the log base B of n that's just end to the D. So these things cancel. And all you're left with is a log base B event. So why is ": [
            410.7,
            446.1,
            9
        ],
        "told you guys that you would maybe do this as an exercise, but maybe let's do it now. Okay. Okay, so why is an to the D? A over B to the D log base B of NY is that equal to this thing? Okay, so let's do some rearranging. How does it go again Okay. So let's let's write it out. We have into the D. We have a ": [
            364.0,
            410.7,
            8
        ],
        "try them out first so that you can kind of see where you are before looking at the solutions. any questions All right. All right. Last time we kind of went fast through the master theorem. So I just wanted to kind of finish it up and then we'll look at some more examples. All right, so this is the master theorem. You guys can use this as much as ": [
            133.7,
            165.2,
            1
        ],
        "up into the even coefficients in the are coefficients, but those things are going to be these vectors, right? So they're going to describe polynomials in themselves. And the way that they described it is is like this. Right where the first coefficient is. The constant term II coefficient is going to be the linear term the third with the quadratic and so on because really what we want to ": [
            2260.2,
            2288.4,
            56
        ],
        "usually it's it's not as fast. Okay. So how we going to do this? We're going to split up the polynomial. Okay, we're given coefficients like this. and a set of points X and we want to compute Y is equal a of X for all x in x this is basically getting getting coordinates XY so how we going to split it up? So what we could do one ": [
            2048.2,
            2088.2,
            52
        ],
        "way to do it is to use the karatsuba multiplication and split it up into the left and right. We already know how fast it's going to be that's going to be one into the 1.58. And that's perfectly fine. But the fast Fourier transform what it does is it splits it up into evens and odds? Can some kind of like on unzips it? Okay, so You start off ": [
            2088.2,
            2121.3,
            53
        ],
        "with this guy here. I'm so ASAP. Rocky is going to be the even coordinate. So it's still this vector. Frank and that's going to be a polynomial. inex Right, so just notice here that we have a 2 * x a 4 * x squared and so on right and similarly the coefficients are going to give you a zero or eggs which are the odd coefficient of the ": [
            2121.3,
            2157.6,
            54
        ],
        "x a linear time operation. and so we get this as our run time and if you plug that into the master theorem you get a is equal to 2 K - 1B is equal to K & D is equal to one so you still get this bottom-heavy thing. If you plug it in. you got this run time so Does anybody know what this limit is? Let me ": [
            1197.1,
            1236.7,
            30
        ],
        "you could base it on I mean when we did the graph algorithms we based on two parameters the number of edges in the number of vertices. This I don't think so. This is more just like a theoretical bound because there is kind of an open question is is there algorithm that exist that can do multiplication in linear time. Nobody knows. We can get close but we can't ": [
            1389.5,
            1429.4,
            35
        ],
        "you want. without proof cuz we're going to prove it in class, but you can This one work. This is a great thing because most divide-and-conquer algorithm for going to have this recursion. So you can just plug in all the numbers and get get whatever you want out. So where does it come from? What kind of talked about this the other day, but basically this big O of ": [
            165.2,
            202.0,
            2
        ]
    },
    "File Name": "Design___Analysis_of_Algorithm___A00___Jones__Miles_E___Winter_2019-lecture_15.flac",
    "Full Transcript": "Is that better? Yes. Wow.  That'll wake you up, huh?  Okay, so let's get started. Anybody have any questions before we begin?  I posted the solutions for the homework or was that for so might be a good idea to take a look at those see how those proofs go. I also gave you some practice problems for the next Quiz. So I'll probably put those Solutions up tomorrow just try them out first so that you can kind of see where you are before looking at the solutions.  any questions  All right.  All right. Last time we kind of went fast through the master theorem. So I just wanted to kind of finish it up and then we'll look at some more examples. All right, so this is the master theorem. You guys can use this as much as you want.  without proof cuz we're going to prove it in class, but you can  This one work.  This is a great thing because most divide-and-conquer algorithm for going to have this recursion. So you can just plug in all the numbers and get get whatever you want out. So where does it come from? What kind of talked about this the other day, but basically this big O of n to the D is the non recursive part. It's basically the part of like how to combine all your recursive calls to get to your final solution. So it's sort of the part.  Hear these lines that's kind of like how long it takes to do that. So this is going to be Big O of n to the D, right.  And then each one of these is going to be Big O of an over 2 to the Dee Wright.  And so on.  So at every step you have to do that a x a squared x to the fourth times and saw a cube X and so on right and we talked about this already. I'm just going to skip over this part.  Okay, so that means that at level K.  You have to do.  This combined step a to the k x basically where it all comes from then we some this so this is just rearranging and we sum it up and we get the total runtime from k equals 0 up to log B, then cuz that's how many levels there are.  And so then you just look at the three different cases. It's all dependent on this value here because we saw that  A geometric series it behave differently depending on if its base is bigger than or equal to or less than 1.  So if it's less than one then you get this whole some as a constant and you just get bigger than to the D. So that's good.  If it's equal then each one of these is equal to one and you get  You just get into the D X log base B event because that's how many times you're adding this up.  And then if you have a is greater than b then you have a base that's bigger than one and so the a geometric series with a base bigger than one behaves like an exponential function.  And it behaves exponential in terms of whatever the the last.  indexes of the sun canceling you get this  And I told you guys that you would maybe do this as an exercise, but maybe let's do it now. Okay.  Okay, so why is an to the D?  A over B to the D log base B of NY is that equal to this thing?  Okay, so  let's do some rearranging.  How does it go again  Okay. So let's let's write it out. We have into the D. We have a to the log base B of n  right and we have one over B to the log base B of n raised to the D.  So as you can see do a little bit of rearranging you get B to the log base B of n that's just end to the D. So these things cancel.  And all you're left with is a log base B event. So why is a to the log base B of n?  Why is that equal to n to the log base B of A. Does anybody know?  TV program log as a fraction  Okay.  I guess so.  You mean just using log rules?  so one thing to notice is  Right if you want I see what you're saying. So if you wanted to decompose.  log base B  Of end is actually equal to log base a of n / log base a of B, right.  Okay, so then this thing becomes.  hey to the log base a of n / log base a of B that becomes into the log base a  sorry to the  1 / log base a of B and that's equal to end to the log base B of A.  Never thought those log rules would come in handy again. But again now we're done. We've already proved it so we don't have to do this anymore.  Okay, so  Now we have our Master theorem and let's think about these these three different scenarios as different states. So we saw that we saw on what was that Friday the karatsuba multiplication and the grade school multiplication was bottom-heavy meaning that every level you go down you increase the number of operations you have to do and so most of the work is going to be done in the bottom level.  And so that'll be bottom-heavy?  Oh good point. So.  Log base B of A N is Big O of log base.  a of n for neamb bigger than  one I think.  Yeah, so usually we just say login and it sort of just means all logs.  Because all logs are grow a constant multiple from each other.  Okay, then the steady-state one. We're going to see that one when we when we look at mergesort. Basically you're going to do the the same number of operations in each level. And so you're basically adding up all of those I'll log and amount of time because that's how many levels you have. And then you have the top-heavy one will see that to that just means that as the tree goes down. You're doing fewer and fewer calculations. So really the bulk of the calculations happens in the top level and that's why you have this is equal to just the the time it takes to come by.  Just give you a sense of the three different cases.  so  let's let's up plug these things in so when we did not use the keiretsu by the kind of the naive way of using all four multiplications, can we plug in the values we get a is for B is 2 and D has one right in this recursion. And so you look at a is greater than b to the D. So you get the bottom heavy, which is what we thought.  And so you just plug it into the master theorem. It should be Big O of n to the log base B of A.  Now back to your comment. This be is actually really important to leave in there.  Okay, so you got into the log base 2 of 4 which is bigger than squared.  Not any Improvement to the grade school method but then if we change it to the karatsuba, then we get a is equal to 3 now. It's still bottom-heavy. But now we get into the log base 2 of 3 and to the 1.58. We got a lot better now.  can we do better than N2 the 1.58 or is this in the  is this the best that you can do to multiply two integers?  Can you just keep?  22in instead of splitting into two split into more. Yes, will that actually help the runtime?  Okay, we get close to that. Let's see. Let's see how that works. Let's think about splitting into 3 and see if we get any better. Okay, we split into thirds and now instead of multiplying binomials. We're multiplying trinomials, right? So how many multiplications are needed to multiply these two things?  9 right  you do that and you get 9 9 multiplications means 9 recursive calls. Each multiplication is one-third the size of the original right and so you get this recurrence.  It's still going to take linear time to put them together.  So you have a is equal to 9 B is equal to 3 G is equal to one so you still have a bottom-heavy with a is greater than beat the D. And so we get T of n is equal to Big O of n to the log base 3 of 9 which is what  the golden Square  That's just kind of what I have here.  Okay, so that didn't help but there is a way just like in the karatsuba was able to decrease for multiplications down to three.  You're you're able to decrease these 9 multiplications. Does anybody know how many you can decrease if I  Would anybody have any gas gases?  You can go from 4 down to three nine can go down to.  5 Wi-Fi  seems like a good number.  5 is right in actually has to do with the amount of terms you have right? There are five terms.  And the way that you do it, it's kind of complicated and the best way to describe it is using a bunch of Matrix algebra, and I'm not going to go into it. But if you're interested we can kind of work it out. So  There's a way to reduce from 9 multiplications down to just five now. The recursion becomes T of n is 5 * 2 of 8/3 foot bigger event. So by the master theorem now we have a is 5 b is 3 and D is one right and so he still get bottom-heavy with a greater than 3 to the bugbee to the D.  Until we get T of n is equal to Big O of n to the log base 3 of 5.  Okay, and that's that's bigger of N2 the 1.43. So we have improved.  And then like you said we can keep on going and keep on improving more and more the more you divide it up. There's one kind of  thing that we need to keep in mind is that this could bind step sure it's Big O of an which means it's a constant times and but that constant is going to get bigger and bigger and bigger the more you divide. So just to give you an idea.  the width  if you divide by to write.  Dad, remember we had to add like a few subtractions and like one edition, right? So we had to add like four more kind of addition steps, right? So I would say this is like around  and then we had to add the two things together. So maybe it's like on the order of like maybe  10in or something 10 10 editions when you divide by  Three then now it's on the order of like 80 editions. Okay, and so the the number of additions that you additional additions that you have to do. I think it grows exponentially. So it's it's kind of like this balance of  Depending on how big your input is, is it really worth it to divide it up into more things because the constant time operation takes a long time.  But that being said theoretically we are improving the asymptotic runtime. So let's let's push it to as far as we can go.  if we divided into case of problems, each of size n / k  How many multiplications are there now?  We have to multiply each one of them right so there should be K squared.  Right cuz you have to do a k - 1 b k - 1 AK might it right you have to do all the possible combinations. So if you just do it naively then you just get this you get this back and you're not really saving anyting but if you use the trick and there is a way to do it for nek to reduce K squared multiplications down to 2 K - 1 multiplications. There's always a way to do it. It just gets increasingly more complicated like I said before but it's still a constant time x a linear time operation.  and so we get this as our  run time and if you plug that into the master theorem you get a is equal to 2 K - 1B is equal to K & D is equal to one so you still get this bottom-heavy thing.  If you plug it in.  you got  this run time  so  Does anybody know what this limit is? Let me go. I'll give you guys like a few minutes to see if you can figure out what is this limit go to?  You guys could probably guess right.  But we saw when K was equal to 2 it was 1.58 right when K was equal to 3 it was 1.43. It keeps on kind of going down.  Anybody have a guess?  What's good?  You could use l'hopital's Rule and get limit as K goes to Infinity of 2/2 K - 1 / 1 / K which is limit as K goes to Infinity of.  2 k / 2 K - 1 which is 1  okay, but this is a limit right? It doesn't mean that you can actually get down to one. It just means that you can get to as close to 1 as you want. So basically for any Epsilon greater than zero you can design a multiplication algorithm that runs in Big O of n to the one point plus Epsilon time.  Pretty cool, huh?  I mean most of them are pretty impractical because of that linear term is a little bit too heavy.  got any questions  Well, we're thinking about the division thing as the algorithm we designed. So we're going to fix it. We're going to fix K and then let and go off to infinity and see how that how that works.  I mean of course and it's got to be bigger than K right for it to work.  I mean you can maybe you could like make one up.  It would make sense for me.  all like a big old to depending on to  did you play sports?  Oh, yeah.  Yeah, I guess you could base it on I mean when we did the graph algorithms we based on two parameters the number of edges in the number of vertices.  This I don't think so. This is more just like a theoretical bound because there is kind of an open question is is there algorithm that exist that can do multiplication in linear time. Nobody knows.  We can get close but we can't get exactly.  But the next thing we're going to do.  I'm going to give you a sort of a brief overview of the fast Fourier transform, which is also a multiplicative.  Or multiplication algorithm. It's more focused on multiplying polynomials instead of integers. But if you think about what is a integer in base 10 or base 2, it's just a polynomial where the the variable is replaced by the base, right? So you can you can use that and it gets a little complicated to get the exact run time because then you have to figure out okay. Am I getting enough Precision on these numbers and all that kind of stuff? I'm not going to go into that. Just trying to give you more of an overview because it's a beautiful divide and conquer algorithm also.  Okay, so  polynomial we all know what that is. It's basically this this is sort of the best way to describe it. Right. It's just a summation over powers of X. There's a polynomial in one variable and each power of X has a different coefficient.  Okay.  So another way to write it is like this.  Now what information am I really dealing with is just the coefficients so you can just think about the polynomial as a vector or a in order to bowl or something like that that holds enough information to describe the entire polynomial.  okay, so there's another representation that I like to talk about that I'll call the sample representation and it's basically  basically works like this.  How many points do you need to know in order to draw a line?  To how many points do you need to know in order to draw a parabola?  Three three points will describe a unique Parabola Four Points will describe a unique cubic 5.4 a cortech six points and so on. So basically if I give you  What is it endpoint?  Then you can tell me a unique and -1 degree polynomial. Okay, so instead of giving you the coefficients and -1 coefficients I can just give you  And -1 coordinates and you can draw polynomial through them questions about that.  What do you mean by reacting certainly the X be different and the Y coordinates have to be different.  But I guess if you do get just a straight line then all of the coefficients will be zero up to that point.  Okay, so these two representations have different benefits. They're easier to do certain things. So let's take a look.  So here are the three things that I would want to do with a with polynomials. Sometimes I would want to valuate a point. Okay? How long will it take to evaluate a point with the coefficient space?  Oh, that's right. Basically you just plug it in and you have to add up a bunch of stuff.  to go then  how about to add two polynomials in coefficient space?  also Big O of n o you have to do is add the the coefficients and then you get the  Okay, how about multiplying two polynomials in coefficient space?  Right kind of like the grade school method that we've seen is N squared.  Okay now sample space how long does it take to evaluate in sample space? Well, if it's one of the samples and it takes constant time, but in general you kind of have to convert it into the coefficient space in order to evaluate something right? And so  You can do it with matrices in basically Big O of N squared time.  But the point here is that it's not efficient, right? It's not fast.  Adding is fine. You just add all of the you add all of the sample points, right? This is assuming that the two polynomials are sampled at the same points, then you could just add those points together.  Now, how about multiplying?  Let's suppose that we had as many points as we wanted right then you can just multiply all those samples together and that would take in time also.  Got any questions about that?  Okay. So one of them is good at 1 and wanted to go to the other question.  Applied or for Edinger Matrix of operation of linear and it gets covered up and evaluation.  Yeah.  Is M squared? Yeah. Yeah. I mean there's another there's other ways to do it, but you really  you can't really get much better than M squared if you're just trying to do it directly from the samples.  Right, but anytime you deal with a matrix. You have a square right and buy in square. It's you're going to do at least and squared.  Okay. So here's the here's the thing.  The fast Fourier transform what it does is it takes coefficients to samples and back it's this transform where you can go back and forth and the power of it is because the reason that it's fat we call it fast is because it takes big O of n log in time.  So if I want to multiply two polynomials and coefficients Pace, I can transform them into sample space multiply them and then transform them back.  Answer this is kind of the idea.  Let's call it a multiplication idea.  SO2 coefficient  polynomials  Thank you transform.  in two sample  That takes big O of M login.  Right, then you multiply.  Which takes big O of n then you transform back?  And that takes big ol then again. So all in all it takes big oven login time to do this multiplication.  Song now, let's see how it works.  Because I swear doing this transformation back and forth really if you want to cuz I see that you're not think we need unlimited lines, but it wont.  So really all we're doing is  right  and  that's why I can't say that multiplying.  integers can take n log in time because there is that Precision thing, but we're just going to kind of theoretically assume that we can have  Unlimited Precision just to kind of get through the the the main point.  Good point. Okay, so let's move on. So if I want to go from coefficient to samples I could I could just * this Matrix right? You guys can just skip over this just if you want this is basically just to kind of  show you what the naive way is going to take Big O of N squared time.  Okay, and back from samples to coefficients also big oven Square time. I just want to show you that the fast Fourier transform and it's worth it to do because if you try to try to do it and I usually it's it's not as fast.  Okay. So how we going to do this? We're going to split up the polynomial. Okay, we're given coefficients like this.  and a set of points X and we want to compute Y is equal a of X for all x in x this is basically getting  getting coordinates  XY  so how we going to split it up?  So what we could do one way to do it is to use the karatsuba multiplication and split it up into the left and right. We already know how fast it's going to be that's going to be one into the 1.58.  And that's perfectly fine.  But the fast Fourier transform what it does is it splits it up into evens and odds?  Can some kind of like on unzips it?  Okay, so  You start off with this guy here.  I'm so ASAP. Rocky is going to be the even coordinate. So it's still this vector.  Frank and that's going to be a polynomial.  inex  Right, so just notice here that we have a 2 * x a 4 * x squared and so on right and similarly the coefficients are going to give you a zero or eggs which are the odd coefficient of the polynomial and also noticed that the the subscripts don't match the exponent anymore.  Okay. So what I want you guys to do is to get into groups and figure out how would you if I knew these this information here. How would I use that to combine to get a of X ready go?  and then  Well, that's what you wanted to be at the end.  So you split it up into the even coefficients in the are coefficients, but those things are going to be these vectors, right? So they're going to describe polynomials in themselves. And the way that they described it is is like this.  Right where the first coefficient is. The constant term II coefficient is going to be the linear term the third with the quadratic and so on because really what we want to do is look at these as polynomials in themselves.  Rehearse on them to solve them and then put them back together.  Okay. Does anybody have a a way to do this to combine them?  Okay.  But then that first one would be a 3X to the 3 / 2.  How about if you just X ax and then plug in x squared?  Anybody have any questions about this?  So that the combined step so if I can get those a of O and E, then this will just take a constant amount of time. Right? Just put them together on time for each x value.  Okay, that's basically the algorithm basically the recursion there is one more cool trick that you might Overlook if if you're not careful.  Okay, so  Basically, here's the thing if I can compute a e and a o of alpha for all Alpha and x squared right where x squared is going to be the set of all x squared such that X is an X then you can combine them together, right? Because I need to know the x squared values in order to put them together to get the X values.  Okay. So basically the runtime recurrence for this thing is going to be well.  Let's base it on and and the size of x.  Well, I do to recursive calls, right?  T of n / 2  X-Plus Big O of n + x  Be combined part, right? The non recursive part is just linear in in the number of coefficients plus the number of points you need to do.  the problem here  is that  this thing here is not going to reduce. It's not going to be divided by 2. Actually, I should put it like this the size of x squared right because of a has let's say k elements been x squared is also have to have k-elements right because you're basically squaring all the things.  So X.  Let's just do like a little example. Let's say x is equal to 1 2 3.  Then x squared has got to be equal to what 149 right and they have the same number of elements.  Is there any way that you can have?  x squared have fewer elements than x  Okay good. So if x is equal to -1 + 1 then x squared now is equal to just one.  Thanks for this is the this is the key to get that recursion to go down.  This is called a collapsing set.  so  Ex has one and every time x has one element or everytime you square X the number of elements decreases by a factor of 2. Okay. There's one great example of the set and it's the  I think we already did it, right? Okay, what's the relationship between negative one and one and one is that these are the square roots of this do we keep on taking square roots over and over again? You get the you get I negative I want and -1 you go into the complex numbers. And really what we want to do is have xB.  all of the roots of unity  I all the complex numbers, but all the nth roots of 1.  And that way the set will collapse down to one.  Okay, so you get the coefficients and then you get all the roots of unity and what you want to do. If you want to compute the value of the polynomial at all of those roots of unity the way you do it split it up like this compute a e and Ayo for all Alpha and x squared and you know that this set has decreased by 2. So, you know that the recursion will keep on decreasing. Okay, then you set a x to be equal to this value for all x in x. That's the combined step. This is the whole thing.  And the record the recurrence well now we know that x square the size of x squared is actually the size of x / 2. + so really you can think about the input as a size of an plus size of X and this this will have every time and you can plug it into the  the master theorem and get this this is equal to  What is it Big O of?  + + x  log  And plus tax.  And really what we want how many how many things do we want in that set? We really want to and element so we can multiply two things together.  so set  ask to be equal to 2 N and this becomes Big O up and login.  any questions  Only live within a Pergola restaurant.  Wouldn't there be some functions that and like what?  not necessarily be able to represent everything that you only  inflection points  value of the function after exactly  Circle at one point. There's only one point where it is possible to recycle there is no there's no way to know that.  lime or any other functions that only  so every polynomial will have a value at every point on for every point along that Circle.  so  Are like that?  Are imaginary x coordinate plane?  So what kind of go into some complex analysis kind of stuff?  Oh, well, I guess I sorta just did it backwards. I guess they just first-set x2b to end and then  And then do it like that.  I guess I sort of confusing.  Are there any other questions?  Okay. Well, thanks for sticking with me through that. I knows sort of vague. I didn't go through all the details but I do think that it's an important divide and conquer algorithm that you should at least know kind of sort of what what's going on and will go into some more familiar stuff on Wednesday. "
}