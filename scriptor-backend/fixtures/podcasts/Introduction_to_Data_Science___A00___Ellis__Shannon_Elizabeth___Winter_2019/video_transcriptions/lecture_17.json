{
    "Blurbs": {
        "Alright, everybody. Let's quiet down. We'll get started as people filed in and get their seats. I already decided so. I miss you notes appear at these are just to make sure that everybody is on the same page and knows what's coming up as we come into the close of the quarter. So you have your forts assignment due this Friday. You do not have to do an analysis. ": [
            53.6, 
            83.6, 
            0
        ], 
        "And so now I'm going to try to put this all together and show that you can do multiple approaches in a single analysis and I will talk about what we can learn from pop music in the last 3 years. So we could use data and there are deer that I use for this that are not the perfect amount this because I did this on Sunday. And if ": [
            2726.6, 
            2745.5, 
            98
        ], 
        "Android and we see the different sentiments from the NRC lexicon here along the left the word that occur most frequently in the Android relative to the iPhone 10 to be negative have a sentiment of disgust sadness fear or anger relative to the more positive sentiments and a conclusion hair was that Trump Android account uses about 40 to 80% more words related to discuss sadness fear anger and ": [
            1570.9, 
            1595.6, 
            57
        ], 
        "Blue, you would expect more positive words to show up in the most negative words to show up in the league. So this is really just like do we see what we expect what I was more interested in seeing from this or what words are most important to the least response. And which ones are the most So if you look at the words for least you see things ": [
            3676.0, 
            3696.7, 
            134
        ], 
        "I be a few more seconds? 3 2 1 Okay, so half of the class said that a scatter plot plus two continuous variable and that is the correct answer. So it's got a pot is the plot where you are have to ask these and then their individual points. That show the relationship between two quantitative variables. So this requires you to know that quantitative and continuous variables are ": [
            234.5, 
            274.2, 
            7
        ], 
        "I got yourself and chat with your neighbor. Can I read a few more seconds? 3 2 1 okay. Okay. So the first three are the only three that are measures of central tendency that we talked about median mean and mode. These are measures of variance. So here we have to determine between these three which is the right one and what they should clue you into is the ": [
            551.0, 
            600.2, 
            15
        ], 
        "I were to be positive or negative so you wouldn't get anything about anger or fear or Joy. It would just that kind of like them into positive or negative. So they can you use affect the results you get dizzy all of the above. I said this is an example that we've looked at before. I realize that this may not be big enough for y'all to stay. Well, ": [
            1772.9, 
            1795.2, 
            64
        ], 
        "I'm going to analyze the text responses to this. So if you do a sentiment analysis on is it likely doesn't clean that much information for you. I asked you what do you like most and which do you like least? So this is really just a proof of do we see what we expect so broken down by the things you like least are in red and most in ": [
            3658.6, 
            3676.0, 
            133
        ], 
        "So I'm going to walk through each of these the answer is in fact, he all of the above. So I just to make sure it's clear what some limitations of sentiment analysis are. So back of the matter is that words in your data that may not be included in the Lexicon. You have a limited number of words and a word may be really important and convey strong ": [
            1712.3, 
            1731.4, 
            61
        ], 
        "So this black box with black line in the middle of your box, what is consistent so that you have about two hundred different words is a typical number of different words and we look at lexical density. How many words are there song It's So, how many are you? Relative to the number of words because he is about half of the song we saw this previously and Analysis ": [
            2979.7, 
            3001.1, 
            108
        ], 
        "So you have to propose how you would analyze the data. So if it is a question of inference, you would have to say that and explain what variables you would use what if you got some results of what you would expect to see what you would be looking for. So just walk through how you would do an analysis. It is not so that you do the analysis ": [
            83.6, 
            100.8, 
            1
        ], 
        "We're going to jump into a number of questions just like we did last lecture will do this again Thursday. So the first one is a different relaxation question. You are interpreting information from a scatter plot. How many of what type of variable have been plotted reminder? The frequency code is 80 Get rid of chat with each other. Take a long time figured out your self first. Can ": [
            183.2, 
            234.5, 
            6
        ], 
        "You could go through each and every one of them write down what you think the seams are you can use the texts themselves to figure out exactly what the themes are without having to painstakingly go through each one and categorize it on your own lot done this and the way they determine of JK Rowling with a likely author of The Cuckoo's calling and they looked at a ": [
            1160.1, 
            1177.7, 
            38
        ], 
        "a clear question the next one to chat with each other. What can you conclude from each ear regarding unique? All right, as you do that take a look at this quicker question and then conclude which of these is true. I'll give every few more seconds. 3 2 1 Okay, so the correct answer is be somebody who chose be explain to me or anybody who can figure it ": [
            3121.1, 
            3203.8, 
            114
        ], 
        "across all of them to these are all those words. They stop words that we talked about. So super, words will have zero TF IDF stand save her from the across all documents the words. I will have higher tf-idf you can see I'm wet fuck they came from and what the word was this now as you start seeing if you're familiar with Jane Austen, these will look like ": [
            2165.2, 
            2185.9, 
            79
        ], 
        "aim to be efficient and they don't as I mentioned with the car symbol, they don't have to accomplish a difficult task and they do not have to be self-contained. They can interact with other algorithms. Alright, okay. So before we get to say topic, I realize that I didn't actually go over there by where the end of actually there's a question. I just want to make sure that ": [
            887.3, 
            906.0, 
            25
        ], 
        "all to know exactly what you're looking at when you see this. So the size of the word is relative to its frequency. And the date is that the larger it is the more freaking it was in the input. And the same goes here so you can quickly see that in this word cloud V where data science and engineering shows up a lot in the day that used ": [
            2423.0, 
            2441.3, 
            88
        ], 
        "along here. The last one I want to take a look at this figure out what it is saying and then we will do a clicker question interpreting what's going on. So this is just a snapshot of an interactive visualization. And the question is to determine which of the following is true. What is it telling you? And what is what if we learn from it? Give it a ": [
            3949.7, 
            4023.7, 
            146
        ], 
        "and I'll review the example. We talked about earlier in Electra. This was in one of the very first lectures but to Define sentiment analysis. It is to programmatically infer emotional content of text. So take all of the words in your body of text and figure out what it conveys emotionally. Is it a happy text is it Angry is it sad? Is it frustrated and you can do ": [
            1263.8, 
            1287.6, 
            43
        ], 
        "and it looked at word by grandtheft word that In Paris right after each other how frequently do those words show up in the Cuckoo's calling and how much does that overlap with her other work with regards to common themes in the 19th century literature. They were trying to figure out what topics were common. They had to go and find co-occurring word the idea. There was that word ": [
            1199.3, 
            1221.8, 
            40
        ], 
        "and just walk through a text analysis or two. And the goal here is that you understand if you were given a bunch of texts on how you could analyze it. We're not going to talk about natural language processing which is an approach. It'll just be one step further than what we're going to discuss today and you should be able to discuss the limitations to analyzing data with ": [
            1078.0, 
            1098.0, 
            34
        ], 
        "and tell me what you see but this is telling us what you can conclude and if anything weird is going on. The child each other. What's the x-axis? What's the Y. What can you take away from it is anything weird? Are you can tell me one thing about this graph? What are we looking at? What do you conclude? Yeah, there's one outlier in a 2018 so you're ": [
            2837.4, 
            2898.9, 
            104
        ], 
        "and you're a year third assignment feedback or on China. Gradescope for the actual feedback for the assignment. I'm Cape Town out so start to fill those out. Those are really helpful for getting out how to adjust his course and where we can improve in the future and the exam 2 study guide if you have the slides are an awesome pipe posted on Triton head. Hey, what's up? ": [
            160.6, 
            183.2, 
            5
        ], 
        "are extremely common words such as the of two or words that have nothing to do with giving emotion or conveying emotion or sentiment. They just happened to be in text analysis. If your goal is to understand the emotional content of the text. These words can be removed as they are not helpful for analysis. So you take your text is it you break it down into Parts you ": [
            1417.6, 
            1438.4, 
            50
        ], 
        "are most important to that novel so would be a word like the that would happen and occur frequently across all of them. So what are we what where do you see the most every what where do you see the most in Emma? But not seen in all of the other. Other novel and the way you can measure this is what the term frequency inverse document frequency or ": [
            1941.4, 
            1959.9, 
            69
        ], 
        "back to a sentiment lexicon. So it doesn't like the con is a dataset where you have words or combination of words and then their corresponding sentence. How did that word generally make you make someone feel when it appears in text? This is an example of something that has the NRC sentiment lexicon so we can see is when you're looking at words. These would be the words in ": [
            1311.0, 
            1332.5, 
            45
        ], 
        "be helpful, but they can get very involved and be hard to figure out any information the larger they get that. I want you to take a look at this where we're looking at the six most most authentic ingredients for each region Cuisine the colors indicate the region and connections indicate relative, current and I want you all to chat and determine which of the following is true. Give ": [
            2582.0, 
            2666.0, 
            95
        ], 
        "came from which songs are what they matter with whether or not they color curtain stop having an understanding of the data can really help you in your interpretation. All right. So we've also been removing words from their context a lot to people and spent time recently trying to figure out how to put words back into their contact and how to visualize that and how the Allies it. ": [
            3771.6, 
            3794.8, 
            138
        ], 
        "can see, this is cfids. Any Einstein would be the word that has most uniquely Einstein. Okay, so far we talked about taking a bunch of text breaking it down into tokens comparing it back to a sentiment lexicon to get an idea of what emotions are conveyed in the text. We don't talk about tf-idf theme text possibly and determining how frequently word show up relative to an entire ": [
            2353.0, 
            2379.8, 
            85
        ], 
        "chapter and show the thickness of the interactions would be those that interact with most they also provide this walk through the chapter where it is a positive word. It's red. And if it's a negative, it's black as you hover over anyone part. It tells you what line of dialogue on Bernard. It happens at that point and you can follow it a long layer time. Set an example ": [
            3841.5, 
            3864.5, 
            141
        ], 
        "character. So this isn't all that surprising if you understand that we're looking at a novel and that word show up more frequently with regard to proper name for you. Don't just have to analyze a corpus of an author's books. You can look that Cross Classic physics text do we have here are a corpus of classic physics text and we break them down by who the text came ": [
            2207.6, 
            2231.9, 
            81
        ], 
        "characters in the books. So in the case of Jane Austen, it makes sense that character names which are important with this if novel and occur only in that novel not relative to all the other novels have a higher TF IDF. Taken plot what? This looks like across the novels and you can see that a lot of the words that are important to the novel's happened to the ": [
            2185.9, 
            2207.6, 
            80
        ], 
        "cloud and they did for work out as you would give the computer a bunch of text and it would determine which words occur most frequently and then size the word relative to its frequency. So you can quickly look and see cognitive learning language and interaction are the most common words people have very strong feelings about word cloud and whether or not there any good I wanted you ": [
            2401.5, 
            2423.0, 
            87
        ], 
        "data scientist stated in the question. So that is don't interact with their job is separate. That's not true because they interact with the algorithms that spot because their client to them and with the outer and determined to the clothing that they are picking from their selections on well that is a step. It happens. That is not the role of stylus stylus run the algorithm to determine who ": [
            786.7, 
            809.4, 
            22
        ], 
        "didn't talk about in this dataset was the fact that there is entirely different type of textual analysis you can do and has nothing to do with the sentiment attach two words, but has to do with what words are most important to a document relative to other documents. So this case for example, we are looking at Jane Austen novels and you want to say any novel what words ": [
            1921.2, 
            1941.4, 
            68
        ], 
        "different with a lot of these words being the most common and we've song count along the x axis. So! Up a lot and then we have other words as you think about pop music make sense. sister 2017 Let's take a look at 2018. So we still have lots of expletive but yeah has really shot up in 2018. So yeah, I was about half and now now we're ": [
            3041.9, 
            3069.0, 
            111
        ], 
        "distribution of word length. So others tend to have a similar vocabulary from one book to the next and similar style and they're looking to see if he's style in the Cuckoo's calling Baxter other works. They look at the distribution of word length and 100. Most common words in the text. They also look at the distribution of character for Graham. So number four letters that come up together ": [
            1177.7, 
            1199.3, 
            39
        ], 
        "do songs convey most frequently as I've been changed over time where the sentiments are broken down by number one song. I Won't words contribute to the sentiment of these number one song and I'm going to briefly mention what we would be talking about when were talking about 5 grams. So here take a look at this plot. We had to be sentiments from the NRC dataset along the ": [
            3280.1, 
            3300.4, 
            118
        ], 
        "emotion in your days that and if it's not present in the Lexicon, it won't be included in urinalysis. Are we not contacts in language matters, but it may be lost in sentiment analysis. What do you say something is not great and you break it down into words and suddenly great get the side is positive sentiment aside that I can you said something specifically was not great in ": [
            1731.4, 
            1750.6, 
            62
        ], 
        "everyone was clear on what this by Majid. Algorithms and when they can be dangerous to other individuals and that's when they are important and secret and can harm other individuals and in those cases. It's really important that we make sure that are algorithms should be better known as fat so fair meaning they like why is he is discriminatory outcomes. So if and this was what you all ": [
            906.0, 
            929.8, 
            26
        ], 
        "fact that there is an outlier value some values our way far apart from the rest of the values and in that case mean would be skewed. So you would use median when you have a bunch of outliers near Davis at nothing simple we talked about in class does an example of a question where you have to understand what central tendency is the difference between them and when ": [
            600.2, 
            616.8, 
            16
        ], 
        "few more seconds. I thought people get their final responses in closet in 3 2 1. Ticket to hear that we're looking at the breakdown of movies and over here are movies were 100% of the words are spoken by females over here by mail. We can see that it is largely male bias in Disney movies, which is what we're not sure if you hover over anyone movie, it ": [
            4023.7, 
            4054.7, 
            147
        ], 
        "five 9 somewhere by asked you on the mid course survey what you've enjoyed most about cause night so far and then what you have liked least so I did read through a question about cat. Sorry. Yeah, that's a great question. The question is if there's a few questions in there one is when you start to break it down into to work to buy Graham. Is there a ": [
            3605.2, 
            3636.3, 
            131
        ], 
        "for how rarely it is used. This will be bigger deer are the word is of the more unique. It is to your document which will increase when you multiply the numbers together leading to a bigger tf-idf forward that are important to a single document and not found in all the other documents. So there's a lot of us instead of just looking at the sentiment just looking at ": [
            2051.2, 
            2080.6, 
            74
        ], 
        "forever. If you do go above and beyond to do now that there's a chance for extra hard on the final project II exam is a week from today in the last 30 minutes of class will do a few questions without iclicker to get started to get warmed up to get some practice on what question on the exam maybe like I know they're going to question. Okay, answer ": [
            100.8, 
            121.4, 
            2
        ], 
        "from what it was Galileo Nikola Tesla Albert Einstein or Christiaan Huygens, and I'm going to have you all take a look at this child each other what you learn from this and we will chat about this on a clear cache on next month, but that's what are you looking at? And what are the conclusions is anything we are going on. No, it's so quiet today. Can lay ": [
            2231.9, 
            2259.5, 
            82
        ], 
        "generate different types of Pop based on what we looked at in class. So, is what you use for a categorical variable to generate a bar plot. Our next line given the weights of lots of different animals, you know that there are few animals whose weights are a lot bigger than most of the animals in a data set. How would you measure the central tendency for animal type? ": [
            528.7, 
            548.8, 
            14
        ], 
        "get and that there are different numbers of words for song. You don't really want this to be count. You would rather want this to be frequency. So when you look at the workout, it looks like there's a big drop-off in negative. I'm over the years and you see a decrease over time and positivity when you look at frequency, so you normalize if I how many words and ": [
            3322.4, 
            3341.9, 
            120
        ], 
        "going to briefly walkthrough. What TF IDF is throw down a term frequency. That is what you expected at Owl Creek only a word occurs in a document. But I mentioned that though it would happen all the time across all the documents and that's not really a word of Interest or try to figure out what word is most important to one document relative to all the documents in ": [
            1959.9, 
            1978.8, 
            70
        ], 
        "have 52 people saying a and 57 people things see so I didn't hear about to do chatting. So I wanted to try and convince your neighbor of the right answer and will revote on this one. So tell each other remote see if we can come to a consensus. I left your discussion make your vote. Give it a few more seconds 3 to 1. Okay, so we had ": [
            422.4, 
            509.4, 
            12
        ], 
        "have so much time on Sunday. So what I did was I took the two hundred top songs from Spotify and then I got the lyrics from this website genius.com the problem here. So I'm already finding that my data are limited. And if you're doing this correctly, you would want to make sure that you had all 200 songs. So some of the songs aren't in this database. So ": [
            2761.0, 
            2780.5, 
            100
        ], 
        "how many songs you have you can see that this decrease still stay there, but that decrease is now gone. So the important thing to keep in mind is when you're looking at total count overtime should you build me a total count or should you be looking at frequency and Ciara. I'm looking at frequency of positivity tend to be decreasing. It's kind of hard to interpret cuz there's ": [
            3341.9, 
            3361.5, 
            121
        ], 
        "how to focus on for a little bit longer y'all what are you going to Isengard is here? So people haven't started it and determine whether or not books can be summarized through their emotional trajectory. So they're going to look at sentiment analysis for going up there going to look at it all together over time and start to make some connections. So this is somebody who set out ": [
            3796.7, 
            3820.0, 
            139
        ], 
        "in fact efficient. We talk about this in should accomplish your goal in a few steps is possible. I would have candy Bowl complex or simple a simple one that would sort number still count as an algorithm and one that determines a complicated problem as to which clothing a client should wear on both Alvarez and they can be good albums by being simple or complex, but they should ": [
            867.7, 
            887.3, 
            24
        ], 
        "in the blue. The negative sentiments are here along the top and you can see words of sadness are things like badly crazy lost worst disaster Li bad killing unfair top, whereas the words that have to do with joy, like safe winning trupay in love tend to occur more frequently iPhone relative to the Android. So having said all of that having told you about what you do what ": [
            1620.1, 
            1644.8, 
            59
        ], 
        "in the next example of we're having knowledge about your data that allows you to interpret more Didn't hire the TF idea the more unique it is to the data set being displayed. We looked at number for diversity of words. What are the most common which ones are the most unique to each of the years? I'm so now we're going to start talking about sentiment. So what sentiment ": [
            3259.3, 
            3280.1, 
            117
        ], 
        "interact with algorithm. So we talked about the fact that a data scientist generate the platform that stylists use to figure out how to best do their job to figure out what style of people should use. So our lives are behind the experiments does a b testing used to determine the best system and The Stylist use that so this one's true everybody at Stitch fix is not a ": [
            765.1, 
            786.7, 
            21
        ], 
        "is used to being used on any information about code. It doesn't count. If you just give them the Code if they have no idea what it does just think carefully about what transparency is just giving the code may not be enough. You might even have some extra documentation to explain What it is you're doing with the algorithm when there is harm that could result from your algorithm ": [
            1013.8, 
            1034.9, 
            31
        ], 
        "it a few more seconds. thanks to 1 going to be getting there is pretty even clicking in between a through D. But then D took off. So if you look here it says parmesan cheese is authentic to Southern European Cuisine, but does not koelker mean there's not a link between other authentic ingredients. So parmesan cheese is emblematic of Southern European dishes, but there are no links to ": [
            2666.0, 
            2698.5, 
            96
        ], 
        "kind of complicated Boykin CRV words along the side here and what sentiment they convey so you can see here that in the anger category words, like smell or grab or crazy 10 to convey anger. Another limitation of using the sentiments is as I mentioned before that. Sometimes words are misclassified to for example discussed has the word boy and I think that is a Miss classification generally. So ": [
            3463.7, 
            3491.4, 
            126
        ], 
        "like discussion read exam confusing midterm dislike computer answers and examples word major life scientist Facebook easy apply example of where I know the date of pretty well because I teach the horse and because I read all of the responses so I know that people often said they don't like reading or reading quizzes and that they want more time to work on their assignments in discussion section. They ": [
            3696.7, 
            3726.6, 
            135
        ], 
        "lot of examples where you could train the data have a model that is accurate, but you actually over fit the data which is what this is called on. This is an example of me trying to predict your future success. I had high accuracy and the training hate it that but below 50% in the test that so this is possible. It just is not a very good model ": [
            656.6, 
            673.8, 
            18
        ], 
        "lots of colors and lots of bars. This is a gentle reminder that when you're looking at a trend of a number over time. It's often helpful to make these line plots. This is just reminding everybody about the data visualization stuff we talked about on here is I put most of the sentiments and gray and then just highlighted that Trend that we start to see the receipt decreasing ": [
            3361.5, 
            3381.6, 
            122
        ], 
        "make sense. You can see it at these words all kind of makes sense to be the most common in a bunch of tweets from Donald Trump on the campaign Trail. Listen to really talk anything about the sentiment of what's in the text and her question is whether or not tweets from the Android version. No Trump is using or more hyperbolic or angrier than the other tweets and ": [
            1482.0, 
            1503.9, 
            53
        ], 
        "material you'll have the opportunity to ask them before we start talking about Thursday's topic geospatial analysis. Today we're talking about text analysis. We're going to start by going through examples. Y'all have seen before but in more detail, so when was from the early way earlier on in the course and one was from a middle of the course lecture and go through a example. You haven't seen before ": [
            1057.8, 
            1078.0, 
            33
        ], 
        "most common and what words are most unique to each year. So this is where you would use TF IDF. Are we looking at here are the most frequently words are mostly going to use words and a top 200 songs. And because I put this on a slide I change some words to just say expletive so that I didn't show up so that Ibaka this would look very ": [
            3023.0, 
            3041.9, 
            110
        ], 
        "not frequently occur with dying cuz there is no line connecting the two when he starts looking that works. You can start to see and glean relationships between them but networks can get very complicated and I think you would have trouble determining exactly what's going on in this aside from the fact that categories different. You can determine that category 10 to Coker so fruits off and show up ": [
            2537.1, 
            2559.6, 
            93
        ], 
        "not generally bad for color blindness. And neither of these is true for deer. So the answer is c a r b happens to be true here because of the projector. Next on what type of plant would FNS count pot generate. This is an example of how we use the programming that we talked about in class. It'll be a few more seconds. 3 2 1 alright, so we ": [
            369.5, 
            422.4, 
            11
        ], 
        "number of words per song? a razor and if you think a hundred words of typical / song 300 500 some interpretations about 500 is the number of words that is typical about we do see that there's some distribution between very few and a lot more words and this song is new Patek, which I didn't know how to pronounce and had to listen to and didn't know what ": [
            2926.8, 
            2956.2, 
            106
        ], 
        "of how sometimes you want to keep word in the context of a matter over time or how they relate to one. Another is another example there been a lot of really great interactive visualization that look at text in their context. So this is an example that was from the pudding which is an awesome visual essay website and what they did was they took all of the letter ": [
            3864.5, 
            3887.8, 
            142
        ], 
        "of people said inconsistent color Toys R Us label? So if there is a cating is at UK is right in the first part and then that changes to China Inn II plot. What is a different color for either you hear China to make sure that people don't think that this red and this red represent the same information. So she is the correct choice. However, I can see ": [
            331.7, 
            348.7, 
            9
        ], 
        "of texts. What can you do with that to understand what's going in going on in the underlined text? So here to example questions of text analysis of people have done to answer interesting questions. So first one or two journalists set out to determine if JK Rowling wrote The Cuckoo's calling there was a rumor that JK Rowling was the person who wrote This Book despite the fact she ": [
            1114.7, 
            1137.1, 
            36
        ], 
        "of these ingredients. So what does he do the compounds differ across the lines connect between the flavor compounds and the ingredients so a single compound could be in lots of different ingredients and that's depicted by the lines relating the ingredients to the flavor, They used these data to then generate Vista Network. And when you're looking and networks, you need to visually look at a number of different ": [
            2486.4, 
            2512.3, 
            91
        ], 
        "okay give you guys another 30 second try and challenge each other about what you're interpreting from this figure. I'll use this question. I'll open it up as y'all chat about it to figure out which word is most uniquely Einstein. Give it a few more seconds. 3 2 1 okay, so by and large b-class said that relativity is the word that is most uniquely Einstein. So to come ": [
            2259.5, 
            2333.1, 
            83
        ], 
        "on with the sentiment in this analysis, which is what's plotted on the y-axis. if a child with each other how to get over a few more seconds 3 2 1 certainly would most people a majority of people get that be Mansfield Park which is this one is more negative toward the end of the novel as you can see the negative sentiment is down here and we see ": [
            1814.8, 
            1903.6, 
            66
        ], 
        "one individual you start to get a snapshot of what the person wrote to do a b this is a way to keep all the information and contacts and start to understand the trends if you were really looking at a histogram, so the number overtime but if you hover over at you keep all of the information in contact I didn't just do this for religion related issues. There ": [
            3907.5, 
            3926.1, 
            144
        ], 
        "other negative sentiments than the iPhone 8 phone account. analysis showing how you can take a bunch of free data and then assign some sentiment to the entire body of text by tokenizing it and then carrying out sentiment analysis it If you want to know what words exactly Drive each of these sentiments, you can see here again with Android being more frequently in the red and iPhone workers ": [
            1595.6, 
            1620.1, 
            58
        ], 
        "out why a is not the correct choice. Play okay. So where is overlap between the dataset? They just aren't the words that show up in the top most unique to that year. So it would be inaccurate to say that no words overlap across the years is data just like the fact that there are no words overlapping in this chart. So when we look here if these three ": [
            3203.8, 
            3240.7, 
            115
        ], 
        "people that are subjected to the algorithm. So it requires access to the album. This is where the class IV of the question came in last class is open to everybody. How can you make money and my argument was if you are able to harm somebody that it is more important that people are the people you are using album on that information rather than you make money that ": [
            971.2, 
            992.2, 
            29
        ], 
        "positivity. Now if we wanted to really determine if this were true we would have to be more thorough and are now since we want to look back in history. See if that trend has been down word for a while if it continues on past this I really do it in a more rigorous way than what I've done here. I'm bored being is that across most of the ": [
            3381.6, 
            3397.8, 
            123
        ], 
        "pretty close. So yeah, it is gaining in popularity has changed a little bit and it's 2019. Yes, and yeah takes over. So yeah shows up a whole lot in the lyrics from 2019. But I forgot that what words are unique to each year. I'm about to get a sense of what shows are most commonly in the songs and is an example of where you could make a ": [
            3069.0, 
            3096.4, 
            112
        ], 
        "probably talking about this one right here. Okay, so we have one song in 2019 that is far away for the number of words in the song relative to the rest of the night. Any idea what that would be a popular song right now with lots of words. Hey, buddy. What else do we learn from this? Any general Trends does it change over time? What what's a typical ": [
            2898.9, 
            2926.8, 
            105
        ], 
        "process of breaking it down into tokens is tokenization and you use this for now, you take the entire bunch of texts known as a corpus split it up into tokens and he's choking Tempe words as I showed you in the last example worth of single word. I could be by Grand words next one next to each other. So this will have to work until 4. So people ": [
            1375.4, 
            1394.5, 
            48
        ], 
        "real real world examples for these capture the same thing largely people said that these examples where applicable to their major and to their life a lot of people like the examples about scientist and Facebook example that we talked about the ethics lecture. So because I know the day that I know what to interpret from this unlike and the lyrics where I didn't really know exactly what lyrics ": [
            3747.8, 
            3771.6, 
            137
        ], 
        "remove the soft words, and then you can do your sentiment analysis. The example I used her leader in the course and then we're going to revisit here was a question of whether the angrier and more hyperbolic tweets come from Trump himself rather than his staff and we know that Trump uses an Android phone. So they looked at all the trees coming from Trump's official account. And this ": [
            1438.4, 
            1460.1, 
            51
        ], 
        "right about last week if you want to terminating or whether or not somebody is likely to go back to jail and then imprisoning them based on this that is likely be found that has been shown that this is based on historical data to a wind-up discriminating against individual. So therefore it is not fair so you should always think about who does Alberta fail fail for how do ": [
            929.8, 
            949.7, 
            27
        ], 
        "said for Stitch fix you people are going to be harmed by the clothing decisions that are being made their so companies like that. It's fine either Our Lives aren't out there and that they are making money off of those algorithms. Welcome to being transparent being open about how why and how and why particular decisions were made. And this has to do with the fact that it's who ": [
            992.2, 
            1013.8, 
            30
        ], 
        "see ingredients in two different recipes here on the left. So you have shrimp scampi and tomato boil and then season muscles at the bottom. So the only ingredients that overlaps between these two is garlic but this is like a scientific papers there has to be more to it than just looking what ingredients over like overlap and they really are interested in what flavor compounds come from each ": [
            2464.0, 
            2486.4, 
            90
        ], 
        "set of documents. We will find a word that are most important to that text but while filtering like words like the and the ones that are not unique to that text to talk about before getting into an example of something you haven't seen are two ways to visualize textural day that we haven't talked about yet. So you'll have probably seen something like this. This is a word ": [
            2379.8, 
            2401.5, 
            86
        ], 
        "some movement but not a time. So we have 53% getting the right answer. So as an account that is a what you would use to generate a bar plot for a categorical variable for histogram. You would use SNS. Just what if you're struggling with this I would say go back to the workbook or to the coding is demonstrating class to make sure you're familiar with how we ": [
            509.4, 
            528.7, 
            13
        ], 
        "that I showed a while ago looking at the uniqueness of songs and how much they compress tons of change here, maybe to decreasing a tiny bit. Three start now we have a toll number words on the change of time does uniqueness change over time. That means looks pretty stable design diversity change or the density that looks pretty stable. Now. We want to look at what words are ": [
            3001.1, 
            3023.0, 
            109
        ], 
        "that are not used very much in a collection of Corpus of documents. Again, we're trying to find a word that are important and common but not too, So what were their unique to one novel and not being in the rest is what TF IDF is set out to measure. CC the same break down here where each panel is a different novel. And right now we're just plotting ": [
            2104.2, 
            2124.6, 
            76
        ], 
        "that co-worker are topics and then they looked at those topics where they occur and look at what other words occur around on some of the topics they found that was common in the 19th century literature was the co-occurring word female fashion and then the word they found that came along with a male fashion wear gown silk dress lace and ribbons and they did this for hundreds of ": [
            1221.8, 
            1240.3, 
            41
        ], 
        "that was and it's a watch and now I know okay take it doesn't change a time from your ear is always between 31 and 33% So the words the average words unique number of words per song doesn't change a ton from year to year. When we look at next school diversity. We can see that we're looking at Fox spots here. So we have x-axis is time. Median. ": [
            2956.2, 
            2979.7, 
            107
        ], 
        "the Lexicon and these will be the sentiments attached to those words. So you can see that an individual words such as abandonment might have more than one sentiment attached to it. So each word can have more than one sentiment and there are lots and lots of words in this data set, but you can see that there are some 27,000 words and they're a lot more than 27,000 ": [
            1332.5, 
            1353.8, 
            46
        ], 
        "the approaches that I'm talking about today in this so far in this course was really focus on data that are either continuously of a bunch of numbers. You look to see if those numbers differ between groups or he looked at categories C do the how does that differ on between one category the next I'm So today, we're really talking about what if you just have a bunch ": [
            1098.0, 
            1114.7, 
            35
        ], 
        "the novel. So we have to use tf-idf to separate out which ones of these are unique to the novel and show up a lot, but that don't show up in the other books. Southern Mansion you can measure tfidf this way and those words that show up frequently. So have a higher TF their term frequency they show up a lot but they are show up all the time ": [
            2144.6, 
            2165.2, 
            78
        ], 
        "the number of word frequency. So we'll look at the frequency distribution. So there are lots of words that don't show up frequently and then there are some that show up much more frequently with in a novel. So this long tail would be all those very frequent words, but then those and the A's and the Ants would be out here along with the words that are unique to ": [
            2124.6, 
            2144.6, 
            77
        ], 
        "the other ingredients in that part of the network. So this demonstrates that it's not always the easiest to clean information from a network, but it's hard to demonstrate occurrences between words in a dataset in a simpler way. Okay, so we talked about sentiment analysis. We talked about tf-idf to look at term frequency relative to the rest of the document. We've also looked at work loud and networks. ": [
            2698.5, 
            2726.6, 
            97
        ], 
        "the process is. You take your body of text you break it down into tokens. And then you assign sentiment to it. I want you all to think about what limit limitation of sentiment analysis. You're free to chat with each other. Give it a few more seconds. 3 2 1 Okay, so we have more than half of the class the majority saying it's e all of the above. ": [
            1644.8, 
            1712.3, 
            60
        ], 
        "the progression through the novel on the x-axis. I realized that this was small and that may have contributed to people not being sure and it's an example we looked at before when we were talking about the types of data through talking about Text data and that you can use a corpus of text a bunch of novels from a single author and look at sentiment overtime. But we ": [
            1903.6, 
            1921.2, 
            67
        ], 
        "the sentiment across the novel's week and then say what words really captured the differences between the novel. Forgot the question. What are the most commonly used words in Jane Austen novels relative to each novel. So we're going to use this tf-idf approach to find the important words for the content of each document by decreasing to wait for commonly used words an increase in the weight for words ": [
            2080.6, 
            2104.2, 
            75
        ], 
        "their clients are going to be that happens before the client gets to The Stylist. So that is an algorithm is generated by somebody else at the company Sony is the correct answer here. At last one I think maybe his own life is so complete the sentence good algorithms are. I got a job with your neighbor. Very few more seconds. 3 2 1 Okay, so good algorithms are ": [
            809.4, 
            867.7, 
            23
        ], 
        "then you can read and see if someone else you can help you figure out what the these say. I'll read them quickly for everybody. And this one is Sense and Sensibility. This one is Pride and Prejudice. This one's Mansfield Park Emma Abby and then persuasion and here we have an example of what they're so we've looked at this before and I want you to interpret what's going ": [
            1795.2, 
            1814.8, 
            65
        ], 
        "there may be missing for my days had two were really looking at between 127 and 152 songs from each day. That there's funny cheer and this is going to make up a dataset songs and I get all of the lyrics for each of the songs in a data set from 2017 and 2018 and 2019. I didn't go to ask lots of questions. So we're going to go ": [
            2780.5, 
            2800.8, 
            101
        ], 
        "there on the left side. And knowing Trump's lexicon. This likely makes sense that these words would be something that he would type and that these would likely come from staff or there's a lot more hashtags that they're using. But that doesn't tell the sentiments. We have the most frequent words. We know which ones occur more frequently on an Android relative to an iPhone, but now we need ": [
            1524.1, 
            1547.0, 
            55
        ], 
        "these are things that there are biases within the sentiments that you are looking at. So that is the first one. This is a more recent song. 1 seconds So get how uncool. I am. I have never heard that song before. I do know who Ariana Grande is any kind of sense in this song and when you look into it and I done had to like look up ": [
            3491.4, 
            3537.4, 
            127
        ], 
        "thing. The first is the size of the circle. So the size of the circle shows how prevalent the ingredient is, so you can quickly see garlic shows up a lot in in cooking and then you want to see what the lines represent so why is he represent Biko occurrence of the ingredients for garlic and scallions 10 to show up in the same recipe whereas soy sauce does ": [
            2512.3, 
            2537.1, 
            92
        ], 
        "think there's more uniqueness in on the pop charts now than there was in 2017 is there left this is that diversity or the density? We have more words for song do we have more unique words for song see the types of questions you can start to ask when you have a bunch of song lyric data. I'm going to give you all a second to look at this ": [
            2818.1, 
            2837.4, 
            103
        ], 
        "this is the correct answer. I'm going to go through why so this is an example of question. That's likely a little harder than something. I would include on an exam but there would be one or two questions may be about this level of difficulty to the reason it's over asking about how become a stylist at Stitch fix which we talked about throughout a long example in class ": [
            746.3, 
            765.1, 
            20
        ], 
        "this number for words that are frequent and it's going to make this number bigger for words that are unique to a single document in your purpose. So bigger number for words that are less frequent. This is what we just talked about for this is idea to calculate TF IDF. You multiply the term frequency. So how frequent the word is in your document and then adjusted by that ": [
            2004.8, 
            2030.7, 
            72
        ], 
        "this programmatically meaning you put into a computer and you get out your results based on the text that you're analyzing so very simply to simplest form. You have your text a disease of the input you break these down into individual words or combination of words and we'll talk about what that process is. And then you take those individual words or combinations of words and you compare them ": [
            1287.6, 
            1311.0, 
            44
        ], 
        "this was right after the exam in the mid-term so exam in midterm are talking about the same thing here and people mentioned a few things that they disliked or what that were confusing. So this all made sense to me but it quickly gives us an idea of what the class on mass like the least about the course. Or hear a lot of people say they like the ": [
            3726.6, 
            3747.8, 
            136
        ], 
        "through all this so I won't read them all. Now the first three that were going to focus on his do the total number of words change so you should start to think about this. Do you think that knowing anything about pop music which I don't what you learn from. This is do the phone number where chance have they decreased increased over the past three years. Do you ": [
            2800.8, 
            2818.1, 
            102
        ], 
        "to Dear Abby. If you never heard of your average, she has been an advice columnist for 30 years at the people waiting to Dear Abby and then she publishes their question and then her answer and we're looking here is at time and how many were religion related issues over time so you can see it's generally there's been a decrease from the mid-80s and you hover over any ": [
            3887.8, 
            3907.5, 
            143
        ], 
        "to assign sentiment to the words. Do that again, which organized and we get our individual words from all the tweets and then we look to see how emotion in the androids occurs relative to the iphone, please and when you see here when you're looking at the percent increase and Android relative to iPhone, so the further you are to the right the more often, it occurs in an ": [
            1547.0, 
            1570.9, 
            56
        ], 
        "to generate the word cloud. Okay, outside of word clouds networks can also be helpful and showing you how words relate to each other. So this starts to get out the context issue that we were talking about before so instead of breaking things down into their individual contacts, maybe want to ask how the words relate to one another. So this has to do with recipes so you can ": [
            2441.3, 
            2464.0, 
            89
        ], 
        "to know the song once I was into it. I just didn't know beforehand. So this is from Ed Sheeran and you could be at the pretty upbeat song with the words in his song were classified as positive. But there are words that contribute to fear disgust sadness and anger. She might want to know what words those are. I need to do that. So it's hard to get ": [
            3443.4, 
            3463.7, 
            125
        ], 
        "to that conclusion, they looked and they said okay. This one's Einstein and then which one has the largest tf-idf so this would be the word that shows up a lot and Einstein's writings which make sense. He came up with the theory of relativity and it doesn't show up in the other physicists are scientists writing. And so the word with the top hot is TF IDF, as you ": [
            2333.1, 
            2353.0, 
            84
        ], 
        "to visualize The Hobbit and what you're looking at are the relationships between characters in The Hobbit where the lines demonstrate in a given cat in a given chapter who they interact with and this is an interactive graphic if you go to this URL and if you hover over one of the characters, it will show up in Black and it'll show up who it interacts with in the ": [
            3820.0, 
            3841.5, 
            140
        ], 
        "topics to really get a full picture of what teams were most common in nineteenth-century literature example of the types of questions, you can ask with text analysis and it involves not just doing exploratory analysis or inference involves really getting on understanding the words that are in the texts themselves. What are the ways in which you can do? This is sentiment analysis. We talked about this briefly before ": [
            1240.3, 
            1263.8, 
            42
        ], 
        "trends most the sentiments the trend is pretty flat from year to year but positivity seems to be going down across the data that we have. Okay. So this was the number one date of that number one song from 2 years in 2017. So very briefly cuz I so listen to just a part of the song Maybe. best place to find a lover Okay, so I would like ": [
            3397.8, 
            3443.4, 
            124
        ], 
        "turn in your final project, which is a compilation of all of your assignments at online instructional video details for this in the assignment. I don't know conclusion and explain who did Wyatt in the project and that will be your final project. So it should be mostly done despite definition of some edits and a bit more information. Answer every knows you're good reading grades are out on China ": [
            139.5, 
            160.6, 
            4
        ], 
        "urinalysis. So by removing the contacts from the language, you don't always get the sentiment assigned correctly. And that is the same explanation for why C is a limitation. The last one I wanted to point out is that I showed you the results from one sentiment lexicon, which is NRC. There's more than one the results you get are sensitive to the Lexicon you use some leprechauns only if ": [
            1750.6, 
            1772.9, 
            63
        ], 
        "use different ways of breaking down the text so we could be single words more than one word multiple words and then compare that back to the sentiment. How do you break down into its component? You would likely want to remove words that are not helpful for analysis of words that don't convey any sentiment. These are going to stop words and there were hundreds of them, but they ": [
            1394.5, 
            1417.6, 
            49
        ], 
        "used the pen name Robert Robert Galbraith, and so they did a text analysis. And found out to determine that most likely JK Rowling was Robert Galbraith and she later confirmed that was true. And then what teams are coming in the 19th century literature. What is of what was written and his most famous from the 19th century and you want to know what feelings come out in them. ": [
            1137.1, 
            1160.1, 
            37
        ], 
        "was when he was on the campaign Trail and we look here first. It's just at what words are most common so you can see the words here along the y-axis and how frequently they occur in all the tweets that they analyzed so you can say that some words as you would expect like crooked Hillary Clinton witch Trump said Lots on the campaign Trail and hashtag Trump 2016 ": [
            1460.1, 
            1482.0, 
            52
        ], 
        "way to classify which ones are most important answer is yes, but we're not going to talk about it a time or you can take them all at random. And is there a lexicon that you could compare them to an address? That is also yet. Yeah. Okay to talk about the music example, and now your question sorry that I may have missed. Questions that I asked and then ": [
            3636.3, 
            3658.6, 
            132
        ], 
        "we make sure that doesn't happen and you can do that by verifying sure that being sure that you're dating you have is correct and not biased. What this means is remove some individual from the date of that if your predictions change wildly that means that it is likely unfair and that is not a steep stable algorithm. By the way, you wanted me to be accountable to the ": [
            949.7, 
            971.2, 
            28
        ], 
        "weight. We just talked about so if it is a comment word that is unique to the document you're looking at this number will be bigger. You'll then multiply it by your big turn privacy. So if it appears a lot and is unique you will have a larger tf-idf. So take your Turnpike is 8 m per second in frequency or tf-idf measures the frequency of a term adjusted ": [
            2030.7, 
            2051.2, 
            73
        ], 
        "were lots of issues that they included in this essay. Oh, I thought we talked about this previously was looking at artist uniqueness so you can plant how unique and individual artists words are and then you can put it relative to other famous works like Moby-Dick or Shakespeare and you can see that some artists are have much more much larger vocabulary relative to most individuals would fall somewhere ": [
            3926.1, 
            3949.7, 
            145
        ], 
        "what are plotted on a scatter plot and you identify what how many different variables are on that type of what? Our next question is what is wrong with the colors in His image base when we talked about in class, so we through there and try to get out yourself and chat with your neighbor. Going to be a few more seconds. 3 2 1 What is the 70% ": [
            274.2, 
            331.7, 
            8
        ], 
        "what the song is about and it's about her and her friends getting Rings after her engagement broke off. So I learned a lot about pop culture that you can see that there are positive and negative words really contributing to the sentiment within this song. I just talked about the talk to her she's had but how should come out on top so you can see that sentiment distribution ": [
            3537.4, 
            3557.7, 
            128
        ], 
        "what they noticed was that there was a difference between the words used by those treats that came from an iPhone with they presume to be his staff and the Android which we know Trump uses we're looking at here is a ratio between the words used on the Android which when there are more frequently on the Android they're over here and when they're more freaking on the iPhone ": [
            1503.9, 
            1524.1, 
            54
        ], 
        "when it happened. I hope this one is about the algorithms lecture. So I'll read the question that you read the answers at Stitch fix algorithms are the heart of the company. How do stylus 2 aren't data scientist interact with l? It looks like they've most responsive. I'll give you a few more seconds. 3 2 1 So we have almost half the 43% of the class saying and ": [
            673.8, 
            746.3, 
            19
        ], 
        "when you look at the sentiments from the words in the song. I got to break it down by word and look at what words are contributing to each of the sentiment. I want to mention just very briefly is we've only been looking at single words, but you can look at what by grants so when you have two words, you can't open eyes it to look at two ": [
            3557.7, 
            3578.4, 
            129
        ], 
        "why people would say be here. So this gray is too late to project so that would be a fine answer given what you see here, but in general for data visualizations great as a really helpful color to distract you from the stuff, you don't want them to focus on and really wanna know what you do want people to see in your visualization. So red and blue are ": [
            348.7, 
            369.5, 
            10
        ], 
        "will show up down here where the Wild Bunch has 100% male dialogue and 0% female making the correct response here. Is it example of how you can keep text in contact and visualize it to convey a point without on Thursday. For the last time I got cut off but it's just if they're like that. ": [
            4054.7, 
            4112.2, 
            148
        ], 
        "with one another and vegetables tend to show up and have related compounds. One another so this just demonstrates that there's as big Network things are connected have similar compounds that they share between within a category and you can start to look at the colors and see that fruits are further away from vegetable. So they tend not to have a ton of overlapping compound. So that works can ": [
            2559.6, 
            2582.0, 
            94
        ], 
        "word cloud. Are they going to eat that yes shows up a lot and it doesn't change a time from year to year. Okay, so that's an example of using figure what words occur most often which ones and then visually looking at it in a word cloud. I want you to take a look at this figure out what's going on in this plot, and then we will have ": [
            3096.4, 
            3121.1, 
            113
        ], 
        "words are the most unique words to the 2019 data and is an example here where I looked at this and I realized that I had no idea which songs were really contributing this or what words went with which song does an example of where if I had more knowledge about the data itself. I would better be able to interpret my results which will come back to that ": [
            3240.7, 
            3259.3, 
            116
        ], 
        "words at a time and see which of those that most people initiated that the 80s and sentiment analysis can all show up all be done using 5 grams or three words or forwards and so forth. Okay, that was my music example where I learned Lots about pop culture. So you are helping me learn more about that here. I want to just summarize what y'all thoughts are about ": [
            3578.4, 
            3605.2, 
            130
        ], 
        "words in the English language. I don't see how this text you break it down into pieces and then you want to assign some sentiment to the text. I send you break it down into words or pairs of words. This is called breaking it down into Tokens. The Tokens are going to be what you analyze when doing sentiment analysis to go to text breaking down into tokens. The ": [
            1353.8, 
            1375.4, 
            47
        ], 
        "x-axis and then we have the word count here and we broken it down by year 2017 2018-2019. And when you see counts be compared to cost time. The first thing you should think of is are the same number of words in each of these data said and if you remember back to the beginning, we had a different number of songs on Whose lyrics we were able to ": [
            3300.4, 
            3322.4, 
            119
        ], 
        "you have to act minimum make sure that it is fair accountable and transparent. I'm in that has a bare minimum and I could still be biased even if it is fair Cowboy transparent. So any questions about the exam about the first questions, we had there or generally about algorithms. Will start lecture on Thursday with some more of these and if you have any questions then about the ": [
            1034.9, 
            1057.8, 
            32
        ], 
        "you were to do this correctly would take a little bit longer. My question is how to change the last three or so. I just took the lyrics to the most popular songs from the like end of February for the last three years. If you really want to do this you would maybe summarize across the entire year or you would take it from each week, but I only ": [
            2745.5, 
            2761.0, 
            99
        ], 
        "you would use them so he's put a lot of Concepts together. All rights to this one true false a model can have high accuracy in a training dataset and low accuracy in a test that Add people are clicking and quickly go over a few more seconds 3 to 1. Overwhelmingly people here said this is in fact true, which is the correct answer. So we talked about a ": [
            616.8, 
            656.6, 
            17
        ], 
        "your Corpus. So in order to adjust and account for how frequently and how important the word is in a document you have to adjust by inverse intended to measure how important a word is to a document. So we talked about IDF. So just measuring the IDF portion, you take the natural log of the frequency within a document relative to how many documents contained that term. So finish ": [
            1978.8, 
            2004.8, 
            71
        ], 
        "your final project is due on the day of your final exam, but it's not due until 11:59 p.m. I just want to make sure that everybody is clear that that is a Thursday since most of the deadlines have been Fridays in the class of your final project is due today of the final exam and you do not have to show up anywhere. But you do have to ": [
            121.4, 
            139.5, 
            3
        ]
    }, 
    "File Name": "Introduction_to_Data_Science___A00___Ellis__Shannon_Elizabeth___Winter_2019-lecture_17.flac", 
    "Full Transcript": "Alright, everybody. Let's quiet down. We'll get started as people filed in and get their seats.  I already decided so.  I miss you notes appear at these are just to make sure that everybody is on the same page and knows what's coming up as we come into the close of the quarter. So you have your forts assignment due this Friday. You do not have to do an analysis. So you have to propose how you would analyze the data. So if it is a question of inference, you would have to say that and explain what variables you would use what if you got some results of what you would expect to see what you would be looking for. So just walk through how you would do an analysis. It is not so that you do the analysis forever. If you do go above and beyond to do now that there's a chance for extra hard on the final project II exam is a week from today in the last 30 minutes of class will do a few questions without iclicker to get started to get warmed up to get some practice on what question on the exam maybe like  I know they're going to question. Okay, answer your final project is due on the day of your final exam, but it's not due until 11:59 p.m. I just want to make sure that everybody is clear that that is a Thursday since most of the deadlines have been Fridays in the class of your final project is due today of the final exam and you do not have to show up anywhere. But you do have to turn in your final project, which is a compilation of all of your assignments at online instructional video details for this in the assignment. I don't know conclusion and explain who did Wyatt in the project and that will be your final project. So it should be mostly done despite definition of some edits and a bit more information.  Answer every knows you're good reading grades are out on China and you're a year third assignment feedback or on China. Gradescope for the actual feedback for the assignment.  I'm Cape Town out so start to fill those out. Those are really helpful for getting out how to adjust his course and where we can improve in the future and the exam 2 study guide if you have the slides are an awesome pipe posted on Triton head.  Hey, what's up? We're going to jump into a number of questions just like we did last lecture will do this again Thursday. So the first one is a different relaxation question. You are interpreting information from a scatter plot. How many of what type of variable have been plotted reminder? The frequency code is 80  Get rid of chat with each other. Take a long time figured out your self first.  Can I be a few more seconds?  3 2 1  Okay, so half of the class said that a scatter plot plus two continuous variable and that is the correct answer. So it's got a pot is the plot where you are have to ask these and then their individual points.  That show the relationship between two quantitative variables.  So this requires you to know that quantitative and continuous variables are what are plotted on a scatter plot and you identify what how many different variables are on that type of what?  Our next question is what is wrong with the colors in His image base when we talked about in class, so we through there and try to get out yourself and chat with your neighbor.  Going to be a few more seconds.  3 2 1  What is the 70% of people said inconsistent color Toys R Us label? So if there is a cating is at UK is right in the first part and then that changes to China Inn II plot. What is a different color for either you hear China to make sure that people don't think that this red and this red represent the same information. So she is the correct choice. However, I can see why people would say be here. So this gray is too late to project so that would be a fine answer given what you see here, but in general for data visualizations great as a really helpful color to distract you from the stuff, you don't want them to focus on and really wanna know what you do want people to see in your visualization. So red and blue are not generally bad for color blindness. And neither of these is true for deer. So the answer is c a r b happens to be true here because of the projector.  Next on what type of plant would FNS count pot generate. This is an example of how we use the programming that we talked about in class.  It'll be a few more seconds.  3 2 1  alright, so we have 52 people saying a and 57 people things see so I didn't hear about to do chatting. So I wanted to try and convince your neighbor of the right answer and will revote on this one.  So tell each other remote see if we can come to a consensus.  I left your discussion make your vote.  Give it a few more seconds 3 to 1.  Okay, so we had some movement but not a time. So we have 53% getting the right answer. So as an account that is a what you would use to generate a bar plot for a categorical variable for histogram. You would use SNS. Just what if you're struggling with this I would say go back to the workbook or to the coding is demonstrating class to make sure you're familiar with how we generate different types of Pop based on what we looked at in class. So, is what you use for a categorical variable to generate a bar plot.  Our next line given the weights of lots of different animals, you know that there are few animals whose weights are a lot bigger than most of the animals in a data set. How would you measure the central tendency for animal type?  I got yourself and chat with your neighbor.  Can I read a few more seconds?  3 2  1  okay. Okay. So the first three are the only three that are measures of central tendency that we talked about median mean and mode. These are measures of variance. So here we have to determine between these three which is the right one and what they should clue you into is the fact that there is an outlier value some values our way far apart from the rest of the values and in that case mean would be skewed. So you would use median when you have a bunch of outliers near Davis at nothing simple we talked about in class does an example of a question where you have to understand what central tendency is the difference between them and when you would use them so he's put a lot of Concepts together.  All rights to this one true false a model can have high accuracy in a training dataset and low accuracy in a test that  Add people are clicking and quickly go over a few more seconds 3 to 1.  Overwhelmingly people here said this is in fact true, which is the correct answer. So we talked about a lot of examples where you could train the data have a model that is accurate, but you actually over fit the data which is what this is called on. This is an example of me trying to predict your future success. I had high accuracy and the training hate it that but below 50% in the test that so this is possible. It just is not a very good model when it happened.  I hope this one is about the algorithms lecture. So I'll read the question that you read the answers at Stitch fix algorithms are the heart of the company. How do stylus 2 aren't data scientist interact with l?  It looks like they've most responsive. I'll give you a few more seconds.  3 2 1  So we have almost half the 43% of the class saying and this is the correct answer. I'm going to go through why so this is an example of question. That's likely a little harder than something. I would include on an exam but there would be one or two questions may be about this level of difficulty to the reason it's over asking about how become a stylist at Stitch fix which we talked about throughout a long example in class interact with algorithm.  So we talked about the fact that a data scientist generate the platform that stylists use to figure out how to best do their job to figure out what style of people should use. So our lives are behind the experiments does a b testing used to determine the best system and The Stylist use that so this one's true everybody at Stitch fix is not a data scientist stated in the question. So that is don't interact with their job is separate. That's not true because they interact with the algorithms that spot because their client to them and with the outer and determined to the clothing that they are picking from their selections on well that is a step. It happens. That is not the role of stylus stylus run the algorithm to determine who their clients are going to be that happens before the client gets to The Stylist. So that is an algorithm is generated by somebody else at the company Sony is the correct answer here.  At last one I think maybe his own life is so complete the sentence good algorithms are.  I got a job with your neighbor.  Very few more seconds.  3 2 1  Okay, so good algorithms are in fact efficient. We talk about this in should accomplish your goal in a few steps is possible. I would have candy Bowl complex or simple a simple one that would sort number still count as an algorithm and one that determines a complicated problem as to which clothing a client should wear on both Alvarez and they can be good albums by being simple or complex, but they should aim to be efficient and they don't as I mentioned with the car symbol, they don't have to accomplish a difficult task and they do not have to be self-contained. They can interact with other algorithms.  Alright, okay. So before we get to say topic, I realize that I didn't actually go over there by where the end of actually there's a question. I just want to make sure that everyone was clear on what this by Majid. Algorithms and when they can be dangerous to other individuals and that's when they are important and secret and can harm other individuals and in those cases. It's really important that we make sure that are algorithms should be better known as fat so fair meaning they like why is he is discriminatory outcomes. So if and this was what you all right about last week if you want to terminating or whether or not somebody is likely to go back to jail and then imprisoning them based on this that is likely be found that has been shown that this is based on historical data to a wind-up discriminating against individual. So therefore it is not fair so you should always think about who does Alberta fail fail for how do we make sure that doesn't happen and you can do that by verifying sure that being sure that you're dating you have is correct and not biased.  What this means is remove some individual from the date of that if your predictions change wildly that means that it is likely unfair and that is not a steep stable algorithm.  By the way, you wanted me to be accountable to the people that are subjected to the algorithm. So it requires access to the album. This is where the class IV of the question came in last class is open to everybody. How can you make money and my argument was if you are able to harm somebody that it is more important that people are the people you are using album on that information rather than you make money that said for Stitch fix you people are going to be harmed by the clothing decisions that are being made their so companies like that. It's fine either Our Lives aren't out there and that they are making money off of those algorithms. Welcome to being transparent being open about how why and how and why particular decisions were made. And this has to do with the fact that it's who is used to being used on any information about code. It doesn't count. If you just give them the Code if they have no idea what it does just think carefully about what transparency is just giving the code may not be enough. You might even have some extra documentation to explain  What it is you're doing with the algorithm when there is harm that could result from your algorithm you have to act minimum make sure that it is fair accountable and transparent. I'm in that has a bare minimum and I could still be biased even if it is fair Cowboy transparent. So any questions about the exam about the first questions, we had there or generally about algorithms.  Will start lecture on Thursday with some more of these and if you have any questions then about the material you'll have the opportunity to ask them before we start talking about Thursday's topic geospatial analysis.  Today we're talking about text analysis. We're going to start by going through examples. Y'all have seen before but in more detail, so when was from the early way earlier on in the course and one was from a middle of the course lecture and go through a example. You haven't seen before and just walk through a text analysis or two.  And the goal here is that you understand if you were given a bunch of texts on how you could analyze it. We're not going to talk about natural language processing which is an approach. It'll just be one step further than what we're going to discuss today and you should be able to discuss the limitations to analyzing data with the approaches that I'm talking about today in this so far in this course was really focus on data that are either continuously of a bunch of numbers. You look to see if those numbers differ between groups or he looked at categories C do the how does that differ on between one category the next I'm So today, we're really talking about what if you just have a bunch of texts. What can you do with that to understand what's going in going on in the underlined text? So here to example questions of text analysis of people have done to answer interesting questions. So first one or two journalists set out to determine if JK Rowling wrote The Cuckoo's calling there was a rumor that JK Rowling was the person who wrote This Book despite the fact she used the pen name Robert Robert Galbraith, and so they did a text analysis.  And found out to determine that most likely JK Rowling was Robert Galbraith and she later confirmed that was true. And then what teams are coming in the 19th century literature. What is of what was written and his most famous from the 19th century and you want to know what feelings come out in them. You could go through each and every one of them write down what you think the seams are you can use the texts themselves to figure out exactly what the themes are without having to painstakingly go through each one and categorize it on your own lot done this and the way they determine of JK Rowling with a likely author of The Cuckoo's calling and they looked at a distribution of word length. So others tend to have a similar vocabulary from one book to the next and similar style and they're looking to see if he's style in the Cuckoo's calling Baxter other works. They look at the distribution of word length and 100. Most common words in the text. They also look at the distribution of character for Graham. So number four letters that come up together and it looked at word by grandtheft word that  In Paris right after each other how frequently do those words show up in the Cuckoo's calling and how much does that overlap with her other work with regards to common themes in the 19th century literature. They were trying to figure out what topics were common. They had to go and find co-occurring word the idea. There was that word that co-worker are topics and then they looked at those topics where they occur and look at what other words occur around on some of the topics they found that was common in the 19th century literature was the co-occurring word female fashion and then the word they found that came along with a male fashion wear gown silk dress lace and ribbons and they did this for hundreds of topics to really get a full picture of what teams were most common in nineteenth-century literature example of the types of questions, you can ask with text analysis and it involves not just doing exploratory analysis or inference involves really getting on understanding the words that are in the texts themselves.  What are the ways in which you can do? This is sentiment analysis. We talked about this briefly before and I'll review the example. We talked about earlier in Electra. This was in one of the very first lectures but to Define sentiment analysis. It is to programmatically infer emotional content of text. So take all of the words in your body of text and figure out what it conveys emotionally. Is it a happy text is it Angry is it sad? Is it frustrated and you can do this programmatically meaning you put into a computer and you get out your results based on the text that you're analyzing so very simply to simplest form. You have your text a disease of the input you break these down into individual words or combination of words and we'll talk about what that process is. And then you take those individual words or combinations of words and you compare them back to a sentiment lexicon. So it doesn't like the con is a dataset where you have words or combination of words and then their corresponding sentence.  How did that word generally make you make someone feel when it appears in text?  This is an example of something that has the NRC sentiment lexicon so we can see is when you're looking at words. These would be the words in the Lexicon and these will be the sentiments attached to those words. So you can see that an individual words such as abandonment might have more than one sentiment attached to it. So each word can have more than one sentiment and there are lots and lots of words in this data set, but you can see that there are some 27,000 words and they're a lot more than 27,000 words in the English language.  I don't see how this text you break it down into pieces and then you want to assign some sentiment to the text. I send you break it down into words or pairs of words. This is called breaking it down into Tokens. The Tokens are going to be what you analyze when doing sentiment analysis to go to text breaking down into tokens. The process of breaking it down into tokens is tokenization and you use this for now, you take the entire bunch of texts known as a corpus split it up into tokens and he's choking Tempe words as I showed you in the last example worth of single word. I could be by Grand words next one next to each other. So this will have to work until 4. So people use different ways of breaking down the text so we could be single words more than one word multiple words and then compare that back to the sentiment.  How do you break down into its component? You would likely want to remove words that are not helpful for analysis of words that don't convey any sentiment. These are going to stop words and there were hundreds of them, but they are extremely common words such as the of two or words that have nothing to do with giving emotion or conveying emotion or sentiment. They just happened to be in text analysis. If your goal is to understand the emotional content of the text. These words can be removed as they are not helpful for analysis. So you take your text is it you break it down into Parts you remove the soft words, and then you can do your sentiment analysis.  The example I used her leader in the course and then we're going to revisit here was a question of whether the angrier and more hyperbolic tweets come from Trump himself rather than his staff and we know that Trump uses an Android phone. So they looked at all the trees coming from Trump's official account. And this was when he was on the campaign Trail and we look here first. It's just at what words are most common so you can see the words here along the y-axis and how frequently they occur in all the tweets that they analyzed so you can say that some words as you would expect like crooked Hillary Clinton witch Trump said Lots on the campaign Trail and hashtag Trump 2016 make sense. You can see it at these words all kind of makes sense to be the most common in a bunch of tweets from Donald Trump on the campaign Trail.  Listen to really talk anything about the sentiment of what's in the text and her question is whether or not tweets from the Android version. No Trump is using or more hyperbolic or angrier than the other tweets and what they noticed was that there was a difference between the words used by those treats that came from an iPhone with they presume to be his staff and the Android which we know Trump uses we're looking at here is a ratio between the words used on the Android which when there are more frequently on the Android they're over here and when they're more freaking on the iPhone there on the left side.  And knowing Trump's lexicon. This likely makes sense that these words would be something that he would type and that these would likely come from staff or there's a lot more hashtags that they're using.  But that doesn't tell the sentiments. We have the most frequent words. We know which ones occur more frequently on an Android relative to an iPhone, but now we need to assign sentiment to the words.  Do that again, which organized and we get our individual words from all the tweets and then we look to see how emotion in the androids occurs relative to the iphone, please and when you see here when you're looking at the percent increase and Android relative to iPhone, so the further you are to the right the more often, it occurs in an Android and we see the different sentiments from the NRC lexicon here along the left the word that occur most frequently in the Android relative to the iPhone 10 to be negative have a sentiment of disgust sadness fear or anger relative to the more positive sentiments and a conclusion hair was that Trump Android account uses about 40 to 80% more words related to discuss sadness fear anger and other negative sentiments than the iPhone 8 phone account.  analysis showing how you can take a bunch of free data and then assign some sentiment to the entire body of text by tokenizing it and then carrying out sentiment analysis it  If you want to know what words exactly Drive each of these sentiments, you can see here again with Android being more frequently in the red and iPhone workers in the blue. The negative sentiments are here along the top and you can see words of sadness are things like badly crazy lost worst disaster Li bad killing unfair top, whereas the words that have to do with joy, like safe winning trupay in love tend to occur more frequently iPhone relative to the Android.  So having said all of that having told you about what you do what the process is. You take your body of text you break it down into tokens. And then you assign sentiment to it. I want you all to think about what limit limitation of sentiment analysis.  You're free to chat with each other.  Give it a few more seconds.  3 2 1  Okay, so we have more than half of the class the majority saying it's e all of the above. So I'm going to walk through each of these the answer is in fact, he all of the above. So I just to make sure it's clear what some limitations of sentiment analysis are.  So back of the matter is that words in your data that may not be included in the Lexicon. You have a limited number of words and a word may be really important and convey strong emotion in your days that and if it's not present in the Lexicon, it won't be included in urinalysis.  Are we not contacts in language matters, but it may be lost in sentiment analysis. What do you say something is not great and you break it down into words and suddenly great get the side is positive sentiment aside that I can you said something specifically was not great in urinalysis. So by removing the contacts from the language, you don't always get the sentiment assigned correctly. And that is the same explanation for why C is a limitation. The last one I wanted to point out is that I showed you the results from one sentiment lexicon, which is NRC. There's more than one the results you get are sensitive to the Lexicon you use some leprechauns only if I were to be positive or negative so you wouldn't get anything about anger or fear or Joy. It would just that kind of like them into positive or negative. So they can you use affect the results you get dizzy all of the above.  I said this is an example that we've looked at before. I realize that this may not be big enough for y'all to stay.  Well, then you can read and see if someone else you can help you figure out what the these say. I'll read them quickly for everybody. And this one is Sense and Sensibility. This one is Pride and Prejudice. This one's Mansfield Park Emma Abby and then persuasion and here we have an example of what they're so we've looked at this before and I want you to interpret what's going on with the sentiment in this analysis, which is what's plotted on the y-axis.  if a child with each other  how to get over a few more seconds  3 2 1  certainly would most people a majority of people get that be Mansfield Park which is this one is more negative toward the end of the novel as you can see the negative sentiment is down here and we see the progression through the novel on the x-axis. I realized that this was small and that may have contributed to people not being sure and it's an example we looked at before when we were talking about the types of data through talking about Text data and that you can use a corpus of text a bunch of novels from a single author and look at sentiment overtime. But we didn't talk about in this dataset was the fact that there is entirely different type of textual analysis you can do and has nothing to do with the sentiment attach two words, but has to do with what words are most important to a document relative to other documents. So this case for example, we are looking at Jane Austen novels and you want to say any novel what words are most important to that novel so would be a word like the that would happen and occur frequently across all of them. So what are we what where do you see the most every what where do you see the most in Emma?  But not seen in all of the other. Other novel and the way you can measure this is what the term frequency inverse document frequency or going to briefly walkthrough. What TF IDF is  throw down a term frequency. That is what you expected at Owl Creek only a word occurs in a document. But I mentioned that though it would happen all the time across all the documents and that's not really a word of Interest or try to figure out what word is most important to one document relative to all the documents in your Corpus. So in order to adjust and account for how frequently and how important the word is in a document you have to adjust by inverse intended to measure how important a word is to a document.  So we talked about IDF. So just measuring the IDF portion, you take the natural log of the frequency within a document relative to how many documents contained that term. So finish this number for words that are frequent and it's going to make this number bigger for words that are unique to a single document in your purpose. So bigger number for words that are less frequent.  This is what we just talked about for this is idea to calculate TF IDF. You multiply the term frequency. So how frequent the word is in your document and then adjusted by that weight. We just talked about so if it is a comment word that is unique to the document you're looking at this number will be bigger. You'll then multiply it by your big turn privacy. So if it appears a lot and is unique you will have a larger tf-idf.  So take your Turnpike is 8 m per second in frequency or tf-idf measures the frequency of a term adjusted for how rarely it is used. This will be bigger deer are the word is of the more unique. It is to your document which will increase when you multiply the numbers together leading to a bigger tf-idf forward that are important to a single document and not found in all the other documents.  So there's a lot of us instead of just looking at the sentiment just looking at the sentiment across the novel's week and then say what words really captured the differences between the novel.  Forgot the question. What are the most commonly used words in Jane Austen novels relative to each novel. So we're going to use this tf-idf approach to find the important words for the content of each document by decreasing to wait for commonly used words an increase in the weight for words that are not used very much in a collection of Corpus of documents. Again, we're trying to find a word that are important and common but not too, So what were their unique to one novel and not being in the rest is what TF IDF is set out to measure.  CC the same break down here where each panel is a different novel. And right now we're just plotting the number of word frequency. So we'll look at the frequency distribution. So there are lots of words that don't show up frequently and then there are some that show up much more frequently with in a novel.  So this long tail would be all those very frequent words, but then those and the A's and the Ants would be out here along with the words that are unique to the novel. So we have to use tf-idf to separate out which ones of these are unique to the novel and show up a lot, but that don't show up in the other books.  Southern Mansion you can measure tfidf this way and those words that show up frequently. So have a higher TF their term frequency they show up a lot but they are show up all the time across all of them to these are all those words. They stop words that we talked about. So super, words will have zero TF IDF stand save her from the across all documents the words. I will have higher tf-idf you can see I'm wet fuck they came from and what the word was this now as you start seeing if you're familiar with Jane Austen, these will look like characters in the books. So in the case of Jane Austen, it makes sense that character names which are important with this if novel and occur only in that novel not relative to all the other novels have a higher TF IDF.  Taken plot what? This looks like across the novels and you can see that a lot of the words that are important to the novel's happened to the character. So this isn't all that surprising if you understand that we're looking at a novel and that word show up more frequently with regard to proper name for you. Don't just have to analyze a corpus of an author's books.  You can look that Cross Classic physics text do we have here are a corpus of classic physics text and we break them down by who the text came from what it was Galileo Nikola Tesla Albert Einstein or Christiaan Huygens, and I'm going to have you all take a look at this child each other what you learn from this and we will chat about this on a clear cache on next month, but that's what are you looking at? And what are the conclusions is anything we are going on.  No, it's so quiet today.  Can lay okay give you guys another 30 second try and challenge each other about what you're interpreting from this figure.  I'll use this question. I'll open it up as y'all chat about it to figure out which word is most uniquely Einstein.  Give it a few more seconds.  3 2 1  okay, so by and large b-class said that relativity is the word that is most uniquely Einstein. So to come to that conclusion, they looked and they said okay. This one's Einstein and then which one has the largest tf-idf so this would be the word that shows up a lot and Einstein's writings which make sense. He came up with the theory of relativity and it doesn't show up in the other physicists are scientists writing. And so the word with the top hot is TF IDF, as you can see, this is cfids. Any Einstein would be the word that has most uniquely Einstein.  Okay, so far we talked about taking a bunch of text breaking it down into tokens comparing it back to a sentiment lexicon to get an idea of what emotions are conveyed in the text. We don't talk about tf-idf theme text possibly and determining how frequently word show up relative to an entire set of documents. We will find a word that are most important to that text but while filtering like words like the and the ones that are not unique to that text to talk about before getting into an example of something you haven't seen are two ways to visualize textural day that we haven't talked about yet. So you'll have probably seen something like this. This is a word cloud and they did for work out as you would give the computer a bunch of text and it would determine which words occur most frequently and then size the word relative to its frequency. So you can quickly look and see cognitive learning language and interaction are the most common words people have very strong feelings about word cloud and whether or not there any good  I wanted you all to know exactly what you're looking at when you see this. So the size of the word is relative to its frequency. And the date is that the larger it is the more freaking it was in the input.  And the same goes here so you can quickly see that in this word cloud V where data science and engineering shows up a lot in the day that used to generate the word cloud.  Okay, outside of word clouds networks can also be helpful and showing you how words relate to each other. So this starts to get out the context issue that we were talking about before so instead of breaking things down into their individual contacts, maybe want to ask how the words relate to one another. So this has to do with recipes so you can see ingredients in two different recipes here on the left. So you have shrimp scampi and tomato boil and then season muscles at the bottom. So the only ingredients that overlaps between these two is garlic but this is like a scientific papers there has to be more to it than just looking what ingredients over like overlap and they really are interested in what flavor compounds come from each of these ingredients. So what does he do the compounds differ across the lines connect between the flavor compounds and the ingredients so a single compound could be in lots of different ingredients and that's depicted by the lines relating the ingredients to the flavor,  They used these data to then generate Vista Network. And when you're looking and networks, you need to visually look at a number of different thing. The first is the size of the circle. So the size of the circle shows how prevalent the ingredient is, so you can quickly see garlic shows up a lot in in cooking and then you want to see what the lines represent so why is he represent Biko occurrence of the ingredients for garlic and scallions 10 to show up in the same recipe whereas soy sauce does not frequently occur with dying cuz there is no line connecting the two when he starts looking that works. You can start to see and glean relationships between them but networks can get very complicated and I think you would have trouble determining exactly what's going on in this aside from the fact that categories different. You can determine that category 10 to Coker so fruits off and show up with one another and vegetables tend to show up and have related compounds.  One another so this just demonstrates that there's as big Network things are connected have similar compounds that they share between within a category and you can start to look at the colors and see that fruits are further away from vegetable. So they tend not to have a ton of overlapping compound. So that works can be helpful, but they can get very involved and be hard to figure out any information the larger they get that. I want you to take a look at this where we're looking at the six most most authentic ingredients for each region Cuisine the colors indicate the region and connections indicate relative, current and I want you all to chat and determine which of the following is true.  Give it a few more seconds.  thanks to  1  going to be getting there is pretty even clicking in between a through D. But then D took off. So if you look here it says parmesan cheese is authentic to Southern European Cuisine, but does not koelker mean there's not a link between other authentic ingredients. So parmesan cheese is emblematic of Southern European dishes, but there are no links to the other ingredients in that part of the network. So this demonstrates that it's not always the easiest to clean information from a network, but it's hard to demonstrate occurrences between words in a dataset in a simpler way.  Okay, so we talked about sentiment analysis. We talked about tf-idf to look at term frequency relative to the rest of the document. We've also looked at work loud and networks. And so now I'm going to try to put this all together and show that you can do multiple approaches in a single analysis and I will talk about what we can learn from pop music in the last 3 years.  So we could use data and there are deer that I use for this that are not the perfect amount this because I did this on Sunday. And if you were to do this correctly would take a little bit longer. My question is how to change the last three or so. I just took the lyrics to the most popular songs from the like end of February for the last three years. If you really want to do this you would maybe summarize across the entire year or you would take it from each week, but I only have so much time on Sunday. So what I did was I took the two hundred top songs from Spotify and then I got the lyrics from this website genius.com the problem here. So I'm already finding that my data are limited. And if you're doing this correctly, you would want to make sure that you had all 200 songs. So some of the songs aren't in this database. So there may be missing for my days had two were really looking at between 127 and 152 songs from each day. That there's funny cheer and this is going to make up a dataset songs and I get all of the lyrics for each of the songs in a data set from 2017 and 2018 and 2019.  I didn't go to ask lots of questions. So we're going to go through all this so I won't read them all. Now the first three that were going to focus on his do the total number of words change so you should start to think about this. Do you think that knowing anything about pop music which I don't what you learn from. This is do the phone number where chance have they decreased increased over the past three years. Do you think there's more uniqueness in on the pop charts now than there was in 2017 is there left this is that diversity or the density? We have more words for song do we have more unique words for song see the types of questions you can start to ask when you have a bunch of song lyric data.  I'm going to give you all a second to look at this and tell me what you see but this is telling us what you can conclude and if anything weird is going on.  The child each other. What's the x-axis? What's the Y. What can you take away from it is anything weird?  Are you can tell me one thing about this graph?  What are we looking at? What do you conclude?  Yeah, there's one outlier in a 2018 so you're probably talking about this one right here. Okay, so we have one song in 2019 that is far away for the number of words in the song relative to the rest of the night. Any idea what that would be a popular song right now with lots of words.  Hey, buddy.  What else do we learn from this?  Any general Trends does it change over time? What what's a typical number of words per song?  a razor and if you think a hundred words of typical / song  300 500 some interpretations about 500 is the number of words that is typical about we do see that there's some distribution between very few and a lot more words and this song is new Patek, which I didn't know how to pronounce and had to listen to and didn't know what that was and it's a watch and now I know okay take it doesn't change a time from your ear is always between 31 and 33% So the words the average words unique number of words per song doesn't change a ton from year to year. When we look at next school diversity. We can see that we're looking at Fox spots here. So we have x-axis is time. Median. So this black box with black line in the middle of your box, what is consistent so that you have about two hundred different words is a typical number of different words and we look at lexical density. How many words are there song It's So, how many are you?  Relative to the number of words because he is about half of the song we saw this previously and Analysis that I showed a while ago looking at the uniqueness of songs and how much they compress tons of change here, maybe to decreasing a tiny bit.  Three start now we have a toll number words on the change of time does uniqueness change over time. That means looks pretty stable design diversity change or the density that looks pretty stable. Now. We want to look at what words are most common and what words are most unique to each year. So this is where you would use TF IDF.  Are we looking at here are the most frequently words are mostly going to use words and a top 200 songs. And because I put this on a slide I change some words to just say expletive so that I didn't show up so that Ibaka this would look very different with a lot of these words being the most common and we've song count along the x axis. So! Up a lot and then we have other words as you think about pop music make sense.  sister 2017  Let's take a look at 2018. So we still have lots of expletive but yeah has really shot up in 2018. So yeah, I was about half and now now we're pretty close. So yeah, it is gaining in popularity has changed a little bit and it's 2019. Yes, and yeah takes over.  So yeah shows up a whole lot in the lyrics from 2019.  But I forgot that what words are unique to each year.  I'm about to get a sense of what shows are most commonly in the songs and is an example of where you could make a word cloud.  Are they going to eat that yes shows up a lot and it doesn't change a time from year to year.  Okay, so that's an example of using figure what words occur most often which ones and then visually looking at it in a word cloud.  I want you to take a look at this figure out what's going on in this plot, and then we will have a clear question the next one to chat with each other. What can you conclude from each ear regarding unique?  All right, as you do that take a look at this quicker question and then conclude which of these is true.  I'll give every few more seconds.  3 2 1  Okay, so the correct answer is be somebody who chose be explain to me or anybody who can figure it out why a is not the correct choice.  Play okay. So where is overlap between the dataset? They just aren't the words that show up in the top most unique to that year. So it would be inaccurate to say that no words overlap across the years is data just like the fact that there are no words overlapping in this chart. So when we look here if these three words are the most unique words to the 2019 data and is an example here where I looked at this and I realized that I had no idea which songs were really contributing this or what words went with which song does an example of where if I had more knowledge about the data itself. I would better be able to interpret my results which will come back to that in the next example of we're having knowledge about your data that allows you to interpret more  Didn't hire the TF idea the more unique it is to the data set being displayed.  We looked at number for diversity of words. What are the most common which ones are the most unique to each of the years? I'm so now we're going to start talking about sentiment. So what sentiment do songs convey most frequently as I've been changed over time where the sentiments are broken down by number one song. I Won't words contribute to the sentiment of these number one song and I'm going to briefly mention what we would be talking about when were talking about 5 grams.  So here take a look at this plot. We had to be sentiments from the NRC dataset along the x-axis and then we have the word count here and we broken it down by year 2017 2018-2019. And when you see counts be compared to cost time. The first thing you should think of is are the same number of words in each of these data said and if you remember back to the beginning, we had a different number of songs on Whose lyrics we were able to get and that there are different numbers of words for song. You don't really want this to be count. You would rather want this to be frequency. So when you look at the workout, it looks like there's a big drop-off in negative. I'm over the years and you see a decrease over time and positivity when you look at frequency, so you normalize if I how many words and how many songs you have you can see that this decrease still stay there, but that decrease is now gone. So the important thing to keep in mind is when you're looking at total count overtime should you build me a total count or should you be looking at frequency and Ciara.  I'm looking at frequency of positivity tend to be decreasing. It's kind of hard to interpret cuz there's lots of colors and lots of bars. This is a gentle reminder that when you're looking at a trend of a number over time. It's often helpful to make these line plots. This is just reminding everybody about the data visualization stuff we talked about on here is I put most of the sentiments and gray and then just highlighted that Trend that we start to see the receipt decreasing positivity. Now if we wanted to really determine if this were true we would have to be more thorough and are now since we want to look back in history. See if that trend has been down word for a while if it continues on past this I really do it in a more rigorous way than what I've done here. I'm bored being is that across most of the trends most the sentiments the trend is pretty flat from year to year but positivity seems to be going down across the data that we have.  Okay. So this was the number one date of that number one song from 2 years in 2017. So very briefly cuz I so  listen to just a part of the song Maybe.  best place to find a lover  Okay, so I would like to know the song once I was into it. I just didn't know beforehand. So this is from Ed Sheeran and you could be at the pretty upbeat song with the words in his song were classified as positive. But there are words that contribute to fear disgust sadness and anger. She might want to know what words those are.  I need to do that. So it's hard to get kind of complicated Boykin CRV words along the side here and what sentiment they convey so you can see here that in the anger category words, like smell or grab or crazy 10 to convey anger. Another limitation of using the sentiments is as I mentioned before that. Sometimes words are misclassified to for example discussed has the word boy and I think that is a Miss classification generally. So these are things that there are biases within the sentiments that you are looking at.  So that is the first one. This is a more recent song.  1 seconds  So get how uncool. I am. I have never heard that song before. I do know who Ariana Grande is any kind of sense in this song and when you look into it and I done had to like look up what the song is about and it's about her and her friends getting Rings after her engagement broke off. So I learned a lot about pop culture that you can see that there are positive and negative words really contributing to the sentiment within this song. I just talked about the talk to her she's had but how should come out on top so you can see that sentiment distribution when you look at the sentiments from the words in the song.  I got to break it down by word and look at what words are contributing to each of the sentiment.  I want to mention just very briefly is we've only been looking at single words, but you can look at what by grants so when you have two words, you can't open eyes it to look at two words at a time and see which of those that most people initiated that the 80s and sentiment analysis can all show up all be done using 5 grams or three words or forwards and so forth.  Okay, that was my music example where I learned Lots about pop culture. So you are helping me learn more about that here. I want to just summarize what y'all thoughts are about five 9 somewhere by asked you on the mid course survey what you've enjoyed most about cause night so far and then what you have liked least so I did read through a question about cat. Sorry.  Yeah, that's a great question. The question is if there's a few questions in there one is when you start to break it down into to work to buy Graham. Is there a way to classify which ones are most important answer is yes, but we're not going to talk about it a time or you can take them all at random. And is there a lexicon that you could compare them to an address? That is also yet. Yeah.  Okay to talk about the music example, and now your question sorry that I may have missed.  Questions that I asked and then I'm going to analyze the text responses to this. So if you do a sentiment analysis on is it likely doesn't clean that much information for you. I asked you what do you like most and which do you like least? So this is really just a proof of do we see what we expect so broken down by the things you like least are in red and most in Blue, you would expect more positive words to show up in the most negative words to show up in the league. So this is really just like do we see what we expect what I was more interested in seeing from this or what words are most important to the least response. And which ones are the most  So if you look at the words for least you see things like discussion read exam confusing midterm dislike computer answers and examples word major life scientist Facebook easy apply example of where I know the date of pretty well because I teach the horse and because I read all of the responses so I know that people often said they don't like reading or reading quizzes and that they want more time to work on their assignments in discussion section. They this was right after the exam in the mid-term so exam in midterm are talking about the same thing here and people mentioned a few things that they disliked or what that were confusing. So this all made sense to me but it quickly gives us an idea of what the class on mass like the least about the course.  Or hear a lot of people say they like the real real world examples for these capture the same thing largely people said that these examples where applicable to their major and to their life a lot of people like the examples about scientist and Facebook example that we talked about the ethics lecture. So because I know the day that I know what to interpret from this unlike and the lyrics where I didn't really know exactly what lyrics came from which songs are what they matter with whether or not they color curtain stop having an understanding of the data can really help you in your interpretation.  All right. So we've also been removing words from their context a lot to people and spent time recently trying to figure out how to put words back into their contact and how to visualize that and how the Allies it.  how to focus on for a little bit longer y'all  what are you going to Isengard is here?  So people haven't started it and determine whether or not books can be summarized through their emotional trajectory. So they're going to look at sentiment analysis for going up there going to look at it all together over time and start to make some connections. So this is somebody who set out to visualize The Hobbit and what you're looking at are the relationships between characters in The Hobbit where the lines demonstrate in a given cat in a given chapter who they interact with and this is an interactive graphic if you go to this URL and if you hover over one of the characters, it will show up in Black and it'll show up who it interacts with in the chapter and show the thickness of the interactions would be those that interact with most they also provide this walk through the chapter where it is a positive word. It's red. And if it's a negative, it's black as you hover over anyone part. It tells you what line of dialogue on Bernard. It happens at that point and you can follow it a long layer time.  Set an example of how sometimes you want to keep word in the context of a matter over time or how they relate to one. Another is another example there been a lot of really great interactive visualization that look at text in their context. So this is an example that was from the pudding which is an awesome visual essay website and what they did was they took all of the letter to Dear Abby. If you never heard of your average, she has been an advice columnist for 30 years at the people waiting to Dear Abby and then she publishes their question and then her answer and we're looking here is at time and how many were religion related issues over time so you can see it's generally there's been a decrease from the mid-80s and you hover over any one individual you start to get a snapshot of what the person wrote to do a b this is a way to keep all the information and contacts and start to understand the trends if you were really looking at a histogram, so the number overtime but if you hover over at you keep all of the information in contact  I didn't just do this for religion related issues. There were lots of issues that they included in this essay.  Oh, I thought we talked about this previously was looking at artist uniqueness so you can plant how unique and individual artists words are and then you can put it relative to other famous works like Moby-Dick or Shakespeare and you can see that some artists are have much more much larger vocabulary relative to most individuals would fall somewhere along here.  The last one I want to take a look at this figure out what it is saying and then we will do a clicker question interpreting what's going on. So this is just a snapshot of an interactive visualization.  And the question is to determine which of the following is true. What is it telling you? And what is what if we learn from it?  Give it a few more seconds.  I thought people get their final responses in closet in 3 2 1.  Ticket to hear that we're looking at the breakdown of movies and over here are movies were 100% of the words are spoken by females over here by mail. We can see that it is largely male bias in Disney movies, which is what we're not sure if you hover over anyone movie, it will show up down here where the Wild Bunch has 100% male dialogue and 0% female making the correct response here. Is it example of how you can keep text in contact and visualize it to convey a point without on Thursday.  For the last time I got cut off but it's just if they're like that. "
}