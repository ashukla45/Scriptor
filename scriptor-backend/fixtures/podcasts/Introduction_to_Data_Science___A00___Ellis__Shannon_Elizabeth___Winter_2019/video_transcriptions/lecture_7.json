{
    "Blurbs": {
        "8Ball it is a lot of fun, but it's usually a lot of money to go. However through the school would you get passes for 90% off? Ye so exciting right? That's the energy in this room guys. Anyway, it's usually $50 a person do the school to $10 a ticket in each tickets to people in so it's only $5 a person this includes everything you need to play ": [
            0.0, 
            23.9, 
            0
        ], 
        "All right, so that's a Geordie of you said available data. Second most popular with a TI and then some people said Manuel stration are who wants to go with what explain why they pick the option that they did. Okay, so funny, but you didn't hear the argument was for using the API because if you're making commits to get Hub you would be able to pull that information ": [
            1808.7, 
            1843.8, 
            77
        ], 
        "And HTML is characterized by these tags that open with these carrots and an end with a carrots with a Flash and you use these to specify what information to pull from a web page. That's like the highest level of web scraping. There are python packages to help you do this. There are packages help you do this and you can use these to go and get the data ": [
            1588.8, 
            1611.2, 
            70
        ], 
        "As long as we tell them the right thing. The manual curation is not going to be the idea for your creation of any dataset, but will sometimes be necessary. I'm so it's not the answer to any of these just because those aren't any of our three 1st Avenues. Any questions so far on anything I talked about this far and I put that in the beginning. I already ": [
            2086.1, 
            2108.4, 
            85
        ], 
        "Collins second operations in Row. The Third is that there should be one table for each type of data. These are the same two types that I showed before and I forwarded that if you have more than one type of data or more than one table, you need a column in each with the same column label that allow them to be joining merged these might seem obvious looking ": [
            2514.7, 
            2533.5, 
            101
        ], 
        "HTTP method we're going to use and then we specify what we want to get using a specific URL that we determine from the documentation from the provider. We're going to get that data back in Json format. So before we can get that data back, you need to authorize yourself. So you need to some way to say that you are who you say you are. I did do ": [
            1077.1, 
            1099.3, 
            46
        ], 
        "Okay, we're not going to do that one just to summarize taking data that are a mess and getting them into that format is called Data wrangling. We'll talk about the Technics techniques used to do that next lecture, but I'm going to send the last few minutes talking about the first exam. Call by 30 multiple choice questions last 30 minutes of class a week from today. I wanted ": [
            3001.7, 
            3024.5, 
            119
        ], 
        "So in your team name, I learned a new nickname for UCSD. I got to solve a riddle and lots of you I had data science in your team name. I appreciated it among other creative things. Degrading is happening this week. Just so you all are aware. The way grading is happening. Is that the same TA or I will grade your project throughout the quarter. This is the ": [
            228.2, 
            250.0, 
            8
        ], 
        "The question is this guy? What color is the sky? And you say blue? How to say the first line of defense for your setting should be the lecture material you can be your notes it can be you re listening to the podcast it can be reviewing clicker questions vast majority of the questions on the exam will come from what was discussed in lecture. The readings are fair ": [
            3104.6, 
            3126.6, 
            124
        ], 
        "a few clicker questions and have discussions about this. Montana that opens up Christmas tree one second not working yet. You guys can't cooking right kind of Ken? Set a reminder frequency code is AD that's on the board. A person even super busy on get held and you want to write a blog post summarizing what you've been up to. Get ready a few more seconds. 3 2 1 ": [
            1715.9, 
            1805.4, 
            76
        ], 
        "a obvious examples, but this happens all the time in the real world. So it's really important to understand these ahead of time to know what to look for. So consistency is the first rule of incredibly important make sure that your variable names are consistent from one table to the next but also within a variable or variables should be coded the same way. So here if your you ": [
            2601.0, 
            2623.3, 
            105
        ], 
        "about health information. So if you want to search through there for any project you never have to do a data. It's a good place to start. What do you think data from 5:38 in the course? They are referring to be unfamiliar with 538 a new source, they cover politics and they covers and they cover science and health and they released a detective High some of their article ": [
            650.2, 
            671.5, 
            27
        ], 
        "about this a bit last lecture and this is from last lecture we talked about that phones lies example where they had the day and then the quote and then the medical explanation as to why it was an untruth with a URL in it. Where you at scrape is and this is just the basics of it behind everything. You see on the internet. There is HTML behind it. ": [
            1567.4, 
            1588.8, 
            69
        ], 
        "add them to breathe part of me thinks that you just did this. So I had to put dookie on a slide and you should make sure next time that your group members have been added. I added them this time. I will not do that going forward you make sure your group members are added on Crisco. Most of you this does not apply to if you did not ": [
            342.0, 
            363.3, 
            13
        ], 
        "all do this everyday you sit down at your computer you type in the URL and then magically request goes to a website and then the website is displayed. So if you can get websites using a swear to call can we get other data or information you would want from the internet? And obviously this is in a lecture you can and the way you do. This is using ": [
            850.6, 
            871.3, 
            36
        ], 
        "an exam study guide. It's not comprehensive. It don't have everything on there. That's a good place to check your understanding. It's broken down by lecture has questions from lecture from the readings from python. If you understand all of those and have a good understanding all the questions and Concepts on Barrett, you should be okay. What start with questions on Thursday that you have that any material? So ": [
            3167.6, 
            3188.5, 
            127
        ], 
        "an interactive Graphics though on this website. You can hover over any point and see the conversation that happened so very quickly can understand all of the conversation that happened and if I were to summarize this briefly hear there was in the beginning some wording clarification, but some people will remember the question was do you want somebody else to send you a Google invite for coffee or lunch ": [
            1311.8, 
            1332.7, 
            57
        ], 
        "and those can be accessed online. I mentioned will work with some of this this if you got to fly to earlier than an hour ago will not be in the size you downloaded but it is up there now. I'm at with a AR Library liaison this morning and I learned about a new resource and the library has at UCSD a lot of data sets by Topic in ": [
            671.5, 
            692.0, 
            28
        ], 
        "any questions why something that is a number like cylinders would be categorical and not quantitative. You need one of each in your project in the dataset. This is up here for your information. Just so y'all can plan ahead. This is what you'll be doing in section each week. We're currently in week 4, there was a Monday section that already met they were we behind so they did ": [
            513.1, 
            538.3, 
            21
        ], 
        "application programming interface. API. API can do a whole lot and we're only going to focus on a small capability of API today in the context of getting data. If you guys are ways for software and applications to talk to one another so their rules for interaction. For purposes the way to access an API is to First choose a method. We're watching that's been build a URL and ": [
            871.3, 
            899.5, 
            37
        ], 
        "apply on the website for the API on to what you want access to tell in this case get home who you are and retrieve authorization so that they know I can control who's getting what from their from their database. You then create an Olaf application from this you will get with none of the client ID and a client secret either would have provided by GitHub or Google ": [
            1116.9, 
            1138.6, 
            48
        ], 
        "ask an interesting question. You can likely get data that are of interest to you. Okay, so I feel like bananas Corso sound like a broken record on this the common theme of just couldn't because you can get the data doesn't mean you should get it or use it on their terms and agreement that we use for all of the apps that we use in all of the ": [
            1460.6, 
            1485.3, 
            64
        ], 
        "assignment so far get ready to your second assignment any questions about anything I said so far. What are we talking about getting data? We're going to be very generally discussing with an API it is and how it works. And then we're going to start talking about where you can look for data and get interesting data set. To review where unit definition that data are anything that can ": [
            560.2, 
            588.0, 
            23
        ], 
        "at the reading and the programme practice. Everybody else will just get programming practice infection this week you always as we've been doing discuss the readings the week after you submit the reading quiz, and you'll get time to work on your projects as time permits in section. This is just for y'all to plan. Whole lot of housekeeping staff today whole lot of reminders good work on the first ": [
            538.3, 
            560.2, 
            22
        ], 
        "at the spreadsheets were showing right here, but I have seen countless examples where this would be Capital ID in one and then over here this week, maybe remember underscore ID and then it's up to me to figure out already the same for the same people across the date of that says it's a different number that they've used. So it's really for they use consistent labeling and that ": [
            2533.5, 
            2550.9, 
            102
        ], 
        "at your questions not you can find or use a different way to set you find something that will work better you find another it is that they will complement your first day that you're welcome to add that in on your second assignment. You must include feedback and programming is required on this assignment and 1/3 assignment, not the 4th assignment. Okay On Assignment, it says that your day ": [
            420.4, 
            443.9, 
            17
        ], 
        "be easier than b or c because the data exist. Skip one. I trust one. I just had a super interesting article on 5:38 and I want to explore the data myself is the best place to start. Are closing in a few seconds? 3 2 1 All right. We had some disagreement here who wants to defend their position here. What were your thoughts on choosing the choice you ": [
            1952.7, 
            2006.4, 
            81
        ], 
        "be stored in a computer. I've been using this term tabular data and spreadsheets and I've been using them interchangeably the purpose of this course. Those are the same. We're really going to discuss what tabular data are today and I've mentioned that these are the types of data that day of scientist work with most often. These are data that are of what you would store in Microsoft Excel ": [
            588.0, 
            607.4, 
            24
        ], 
        "because let's say you were working with data about cars and you had some information about the number of cylinders for a vehicle. That number is limited even though the value it's really a categorical data location to take this example cuz I don't know that much about cars, but you can have 4 or 6 cylinders are probably another value but there are only a few options. Are there ": [
            492.4, 
            513.1, 
            20
        ], 
        "call on date? if I show hands anybody for be very face of anybody for bo4 anyone 4D I will sleep in class. That's fine. Panther hear Envy you would want to as we said put the unit up in the column header. So I wanted to have some discussions. So the best option would BP you just let the values for your specify the unit in the column header. ": [
            2976.0, 
            3001.7, 
            118
        ], 
        "can be used to answer an interesting question. And that is the definition of data wrangling getting it so you can answer the question you want. Give me a few more seconds. 3 2 okay. So here are the results a lot of you said we're somewhere in the 50 to 75 range. But we have a spread across all of these. I would gather that if you didn't know ": [
            2206.1, 
            2260.7, 
            88
        ], 
        "can get it right from the beginning and you are the data scientist can spend less time working on this part over right now and for the past 10 years, this is what it's been. So here's that other caveat to this is that we spend eighty 90% of our time trying to get the data in the form we want. And we don't get to do the more interesting ": [
            2281.3, 
            2300.1, 
            90
        ], 
        "can get rid of it. And these are the methods that you would use on a left to make these requests. They were just going to focus on getting information from an API. So the only method were to be talking about today is get So when you're sitting at your computer and decide that there's not a decent out there for you, but there is information out there that ": [
            940.0, 
            961.4, 
            40
        ], 
        "can use available methods to get that information for yourself. So I'm going to very high-level talk about HTTP and apis today and I'm going to compare this to my undergraduate biology and I learned a lot of things and then I got to grad school and learned that almost everything was taught with kind of true but there wasn't there a lot under the hood that I wasn't taught ": [
            805.8, 
            829.0, 
            34
        ], 
        "cell. Don't use phone color and highlighting is data hear. The person is trying to save this value is way different than all these other valleys. It might be a mistake rather than using font color. It's best to have another variable that says hey this one might be an outlier. Is included for you to look through I won't go through all of this, but it's a summary of ": [
            2774.4, 
            2795.6, 
            113
        ], 
        "change your date at that. If you look at one of these sources and say hey, they might have something really interesting for my project most of your familiar with cargo. There's a very similar platform called date of that world data is plural is a big ol Google Sheets filled with more than 600 datasets across a ton of different topics. They have policing information. They have politics information ": [
            629.8, 
            650.2, 
            26
        ], 
        "collect from each in a safe person. So variables are stored in column. And it's observations are stored in rows. In this case. These observations are people that they don't have to be an everyday to set this year. We can see that we have for observation. So seven variable seven columns for observations for Rose. And I'm going to talk about types of data here. For example, you might ": [
            2419.5, 
            2445.6, 
            97
        ], 
        "crazy way in which you can enter date or time should be used under the dates and times are difficult to work with is because time zones exist. So 2 p.m. Here is not 2 p.m. Worldwide dates are hard because people have chosen lots of different crazy ways to unspecified 8 spermatically. We'll talk about the details later. So a boy that they sell in spreadsheets and your glucose ": [
            2688.7, 
            2714.6, 
            109
        ], 
        "date? Hear these two boxes are people saying why wouldn't that be the first point of contact but if we have discussed it and then they sent me at to verify. That's great. So there's some clarification happened here. There was this tweet about people having very strong feelings about online calendar etiquette, which is something that lots of people apparently have very strong feelings for down here is some ": [
            1332.7, 
            1353.2, 
            58
        ], 
        "did everybody like us 32nd minute break talk to each other and then I'm gonna have everybody Focus back in cuz I know this is a lot of lecture so you guys have a minute to talk to each other again, and I'll have you focus II. All right. For the second half we're going to do the first star. That was very East Coast 2nd half second half we're ": [
            2108.4, 
            2183.4, 
            86
        ], 
        "discussion about the fact that having kids makes your mind. Goofy. There is over here at the point that I have anything invite you're not going to have confusion about time or place. There's something special about who uses to do managers hear people talking about paper journals. And then there's a soft-shoe talking about Microsoft Office product and wedding tangent about somebody play all of this. You can quickly ": [
            1353.2, 
            1374.8, 
            59
        ], 
        "down here. This is something that would not be very easy to work with. Once you read it into python the process of taking it from this untidy format to a tidy data format, which we're going to Define is through the process of data wrangling. So I talked about in the programming lecture what the differences are between a data scientist in a computer scientist when it comes to ": [
            2373.6, 
            2394.6, 
            95
        ], 
        "everything. We talked about better approaches to bad naming. I have it in here to summarize your points that are made in the Tidy data paper from Hadley Wickham This is just so your notes are complete. I won't walk through all of these now. I'm going to have you all do one of the two here. So the first one take a look carefully at these spreadsheets and it's ": [
            2795.6, 
            2817.9, 
            114
        ], 
        "feel free to bring them to get any questions about the exam. Those are great for Thursday to call Thursday. ": [
            3188.5, 
            3193.3, 
            128
        ], 
        "first assignment you will get feedback by this weekend and you will have your grades for the first assignment by this weekend for second assignment just to make sure we're on the same page when you open the second assignment. It might look slightly different than before because there will be some cells that are collapsed. You can click on these and see all the content these are class just ": [
            381.7, 
            403.1, 
            15
        ], 
        "first exam is a week from today. It is multiple choice. It will happen the last 30 minutes of class. And your second assignment is due next Friday. Just that is so you all can plan ahead. There are programming covered in three lectures. That is this Thursday is next Thursday's and Thursdays after lecture. So my goal at the end of that is you can all read a dataset ": [
            164.4, 
            186.4, 
            5
        ], 
        "for choosing D because you if you hear you clarify that your unit is talented maybe pounce universally would not be the way in which you would enter weight because most countries would use a kilogram. Okay, so that's one for anybody have a thought against d Yes. Okay. So the argument here is 4B that you just want the value in here and you can specify up here in ": [
            2904.0, 
            2942.2, 
            116
        ], 
        "for the entire day. The only additional cost is that you have to purchase paint which is very inexpensive. His tickets are also valid for 2 years. I know everybody in here is very busy with school and work. They must be part of this school to purchase. These tickets are Weber anybody is more than welcome to go. So you can bring family and friends you can even give ": [
            23.9, 
            40.4, 
            1
        ], 
        "from the API and then be able to analyze it there. So all the information about everything you did not get help would be accessible through their API. So on this one I'm going to argue with that is the correct answer. The reason that available data are there is the days that you wanted probably doesn't exist as a dataset yet. There is no easy spreadsheet to get you ": [
            1843.8, 
            1863.1, 
            78
        ], 
        "game. Only the first two readings will have questions specifically from them, but I don't want you to memorize dates or names or specifics. I really want to make sure did you get the big Concepts from the readings note that while tidy data and data wrangling at the topic or covered? I won't be asking anything specific from that reading because you haven't discussed it in section yet. You ": [
            3126.6, 
            3148.6, 
            125
        ], 
        "getting data from Twitter all the time and using this answer questions that are much more in-depth than the simple example. I explained here They're also research project that goes through GitHub and take people say online jupyter notebook and analyze how people are using them and what's included in each of them. So getting a sense of how people are analyzing data left an example of how you would ": [
            1417.8, 
            1439.0, 
            62
        ], 
        "getting that type of information and the fact that you can use a provider's API to get data that you wouldn't have had. Otherwise we talked about the basic methods for getting that information. So briefly, what would you use this for the return back to that example I started with this is a simple example where you have a very specific question. What is the larger context around this ": [
            1245.3, 
            1267.6, 
            54
        ], 
        "go check it afterwards. So I'm not just texted testing 1 topic. I am not out to get anybody when I write advance, but I'm also not out to ask questions like is the sky blue and have one option bb19 be blue when I have to be 17 and then something else that doesn't make sense. So I need to test that were actually learning something and that everything's ": [
            3042.8, 
            3061.1, 
            121
        ], 
        "going to be talking about tidy data and data wrangling the idea of how to make spreadsheet data at work for you rather than against you and how to get data that aren't good into a good format and we're going to clarify what all of that means today and on Thursday. Thermostat says the question is what percentage of a data scientist time is spent preparing data that I ": [
            2183.4, 
            2206.1, 
            87
        ], 
        "had NV data lecture. The categories are quantitative and qualitative quantitative our numbers. So I called you before continuing. I was trying to avoid using quantitative and qualitative cuz they are such similar words and they don't actually conjure up exactly what they are for everybody. But we'll use these quantitative think quantities of some sort of number qualitative think some sort of Category 3 how to make that is ": [
            465.0, 
            492.4, 
            19
        ], 
        "have the options of female and male they should be Mount you shouldn't write lowercase T mail for one uppercase email for another and an F for another individual. It's much easier to summarize information if everything is typed consistently Bus you to choose good names for things there rules about avoiding Extra Spaces. So programmatically mail without a face is different than male with a space underneath of it ": [
            2623.3, 
            2648.6, 
            106
        ], 
        "have to go retrieve it from the API. All right. next one You want to analyze where geographically calls to police have occurred in Albuquerque, New Mexico same options? Closet in a few seconds 3 2 1 All right, so overwhelmingly people said available data, but BNC were possible options here who wants to explain their thought process here. I'll tell you that the majority got it right on this ": [
            1863.1, 
            1921.2, 
            79
        ], 
        "have to know what information you want and then you have to use the documentation to figure out how to get it. And there are different ways. I'm at each of the API. So here we have Twitter and then Google's API and a GitHub API and we should know is that these versions change often? So you may have this really great script that gets all the information you ": [
            1006.2, 
            1026.4, 
            43
        ], 
        "how to do it. So that's why I'm sending a number of a bunch of time talking about how to take data that looks something like this which we're going to all know with Messy in a second and then why and get it into a usable form at this is a spreadsheet that came from the Australian government and its is a blog post from how many miles in ": [
            2337.7, 
            2356.2, 
            93
        ], 
        "in and you can Wrangle the date of that and make some basic visualisations. That's the goal of what programming I want you to be able to do. I also have contact information. I'm in the second bullet point hear from the person who made announcements at beginning of class last lecture. All right. So as everybody is settling I want to recap the first assignment y'all did really well ": [
            186.4, 
            208.1, 
            6
        ], 
        "in there a lot exceptions all the rules. I told Todd was taught the same to be true about this high-level be an API beer to go courses later on and learn a lot more details me like she didn't really tell me the whole story and that's true. Okay, that's good morning transfer protocol. This is a way that allow messages to be sent on the internet. So you ": [
            829.0, 
            850.6, 
            35
        ], 
        "information after I authorize our self and then it's up to you to practice their spot. The first part of this lecture was about getting data from a source or you just get your tabular data right away. This one takes a lot more steps and of a lot more work, but you can get data that you wouldn't have had otherwise, so just to recap the same process that ": [
            1201.8, 
            1223.2, 
            52
        ], 
        "information and their way through that we won't talk about in detail. There are lots of popular platforms that will cut you off if you take too much information from you than at any one point in time so know that that is likely to happen if you don't look into whether or not it's okay to scrape information from a website first. All right, See you in data science ": [
            1675.0, 
            1696.3, 
            74
        ], 
        "information you can get back for free, but if you want a lot of information or to make you pay which makes sense, so if you post these after getting them on the Internet, somebody else can take them and then charge your credit card for all the data that they just got for free and you just paid for so we talked about getting a previous should never be ": [
            1160.6, 
            1178.3, 
            50
        ], 
        "is don't reinvent the wheel if the information is already out there don't work harder than you need to to get it. So always start by looking for available dataset, if those don't exist, your next step would be going to an API and then only go to web scraping if it's allowed and if you can't get it from the initial resources. All right. So we're going to do ": [
            1696.3, 
            1715.9, 
            75
        ], 
        "is it has to have variables that are numerical and categorical. I just wanted to clarify this a little bit. The reason I want both is because when you make visualizations, they differ based on the type of variable you have I want you to experience with both so you need some sort of number and you need some sort of categorical value. I'm going to clarify a slide I ": [
            443.9, 
            465.0, 
            18
        ], 
        "is over here on the left. We have one value of a person's weight in pounds on in a Cell over here while this looks like one piece of information really up to you have the value and you have the unit. It's much better to put the unit up at the top. Everybody's clear. This includes weight in pounds rather than having two pieces of information in a single ": [
            2755.8, 
            2774.4, 
            112
        ], 
        "it as a gift is $50 value right on the ticket. Once again, we get a limited number of these passes in the beginning of the semester had them out first come first serve until we sell out. It's for giant paintball is a lot of fun $10 for one ticket gets to people in and you have two years to bring anybody you like on any day of the ": [
            40.4, 
            57.0, 
            2
        ], 
        "made? Okay. So the argument here is for a available data and the statement made was at 5:38 post all of their data online. I'm going to put a sleigh carry-on on that. They don't make all of their data available that they make a lot of their data available. So I agree that a is possible here. I'm anybody would explain why they chose be. Okay, so nervous. I ": [
            2006.4, 
            2044.2, 
            82
        ], 
        "mention that it's available, but I can take through which Avenue and that was somewhat intentional so that I could ask this question later on and we could have this discussion so you can directly download their data sets from data. Fivethirtyeight.com. But they also host them on GitHub. So you could use an API to access those. Toby is also correct. I'm going to disagree with web scraping because ": [
            2044.2, 
            2065.7, 
            83
        ], 
        "not obvious. So I get the balance between asking questions that make you recall information and questions that make you apply Concepts. We asked without everything being obvious so that everybody gets it 100 cuz then I have no idea whether or not he's actually learnt or learned anything and if I taught anything BB recall questions where you just have to remember stuff that have been presented to you ": [
            3061.1, 
            3082.3, 
            122
        ], 
        "of anything that you're not supposed to have or not supposed to be using. Okay. Lots of spreadsheet talk we discussed basics of apis and HTTP methods we talked about getting information from an API. We walked through a quick example the last know what if there's no way you can't get that do you want from it? This is where the case of web scraping comes in. We talked ": [
            1545.4, 
            1567.4, 
            68
        ], 
        "of death and get them in this format. Any questions about the general principles of Tidy data and what it means to take a messy dataset into a tidy format. Okay. Their number rules for good spreadsheets. This is also part of your reading for this week. We're going to walk through the examples of the seven of these now and again keep these in mind again. I'll show you ": [
            2574.3, 
            2601.0, 
            104
        ], 
        "one and even defend their thought process there. The data should be available should not Diamond. It's all going to disappear little bit. But agree on the second part of what you said. So I left out a key piece of information here. Is that Albuquerque New Mexico has and open data portal the data are available through their portal and that's what you would ask this and that would ": [
            1921.2, 
            1952.7, 
            80
        ], 
        "or Twitter or whatever you're trying to get the data from. After you have a client ID and secret fuel generator token and this access token is what ultimately will provide you access to the API to get the data that you want and verify who you are who you say you are. Alright morning. These should never be shared or posted publicly with apis. You have some amount of ": [
            1138.6, 
            1160.6, 
            49
        ], 
        "or what you would store in Google Sheets their information entered in rows and columns and they're often stored in spreadsheet. So I'm saying tabular data or spreadsheet data. Those are the same thing. Beat type of data can be found in lots and lots of different places on your first assignment you were directed at a few places. I'm going to mention a few more today. You're welcome to ": [
            607.4, 
            629.8, 
            25
        ], 
        "pain and he showed the fact that you have information kind of all over the place. So you have his header appear that doesn't really include data that you'd want to analyze and then you have what you actually want in your columns over here and I assure you of groups that you would actually want that to be a column itself there lots of missing spots on his junk ": [
            2356.2, 
            2373.6, 
            94
        ], 
        "part at the end that said Cynthia wrangling is a big part of the job. You have to be good at it. You have to not hate it cuz you'll spend a lot of time doing it and you have to be good at it over time. So this is a quote from DJ patil. He was a chief scientist. I'm under the Obama Administration and the first Chief data ": [
            2300.1, 
            2316.9, 
            91
        ], 
        "question? So if you want to understand all of his receipts read tweets and responses. You could use for the API to explore this so you can ask Twitter for all of the responses to the street all of the Retreats and all of the conversations that happened as a result of those retweets and you can visualize them by Lucy Big Island Casino who is a form of tasty ": [
            1267.6, 
            1289.5, 
            55
        ], 
        "save this information was information collected from somebody who entered a doctor's office and they had to fill out a survey and give this information to the doctor. This may have been measurements taken at your visit that was taken by the nurse or by The Physician and these are two different types of data because this came from a survey and this came from the physician during your meeting ": [
            2445.6, 
            2464.7, 
            98
        ], 
        "scientist, and he said that good data scientist understand any deep way that the heavy lifting a cleanup and preparation isn't something that gets in the way of solving the problem. It is the problem. This means that we should all be really good at it. And I think right now since they to science isn't evolving feel that we don't always watch it. But we expect everybody to know ": [
            2316.9, 
            2337.7, 
            92
        ], 
        "sephiroth in the 20 retweet and then conversations that happened off of the retweets and it's not really easy. Just looking on Twitter to get a sense of what everybody said. They don't have the data to answer that question. If you really want to understand the full scope of the question. You have the initial vote for that sit at a glance. The way you can do is you ": [
            785.4, 
            805.8, 
            33
        ], 
        "should understand the code for that in a class. The best way to do it is go back to the notebooks run the code changed a little bit anticipate what the output should be after you change it be if that happens that will really ensure that you understand it. I'm actually it's up on Triton head but it's also click link clickable on this link from the slides. There's ": [
            3148.6, 
            3167.6, 
            126
        ], 
        "so you can see where you should focus your attention the sections that are not collapse are those we have to add new information where you have to have to read your data set in we have to Wrangle the data set on the 2nd of submission. You can change edit update your question you should do so if you get feedback telling you from your t a r i ": [
            403.1, 
            420.4, 
            16
        ], 
        "software that we use and we just kind of like go through and click and say I will follow those when you're getting data from somebody else that you can use in either research or your own for your own purposes those agreements matter a lot more and it's up to you to make sure you understand what you're agreeing to whenever you're accessing an API house rules on how ": [
            1485.3, 
            1506.8, 
            65
        ], 
        "stored. I'll get how people write crawlers that go through and look for these types of information and will then use them to their advantage and to your detriment So that covers the getting authorization part. So now we know that we're going to use me get methods to get information. We're going to build our URL that's going to return us some Jason information and we can get that ": [
            1178.3, 
            1201.8, 
            51
        ], 
        "submit it as a PDF if you print it out something and and it using an app or you took pictures of your computer screen. Make sure you refer back to the slides at the end of the third lecture to see how to submit it as a PDF or stop by during office hours to make sure you know how to do that going forward. That's it for the ": [
            363.3, 
            381.7, 
            14
        ], 
        "submitting. I think only three people didn't submit a project or an assignment the first assignment which is good. And those three people I'm not sure are in a class. So that's great. If y'all did really well with what I've seen so far. I've gone through look at your question in just looked at your sources for data says I'm I also appreciate a lot of your team name. ": [
            208.1, 
            228.2, 
            7
        ], 
        "swapping information about individuals in the United States that are collected from the dentist. But yet and it's not just Federal day that you're limited to cities more and more are rolling out open City-Data portals. And here's a list of a few where you can go in and get information about a city in particular. That summer is here. If you want to look at cities across and see ": [
            716.4, 
            738.1, 
            30
        ], 
        "system so that you are getting feedback consistently from somebody who already has seen your question before I can help guide you throughout it. And that's why we're going with infection. You should know your point of contact. I will monitor for grading fairness across graders and see what that should anything to rise dough using a rubric to grade but they're providing you feedback specific to your projects that ": [
            250.0, 
            269.5, 
            9
        ], 
        "that you'll see that this is not necessarily the case and we'll come back to that and examples about a second last name first name one piece of information stored in each column for each variable. The second is that every observation for each variable should be stored in a different row. So this row includes information for a single observation and that's consistent across the row. Purple vegetables and ": [
            2487.8, 
            2514.7, 
            100
        ], 
        "that. The most common way is using oauth or open authorization. Resume compromise so that you're familiar with the terms when you go to a tempest in the future. You've at least heard them but know that they will be steps along the way they will have to figure out when you go to do this. So when you are they should the first thing I have to do is ": [
            1099.3, 
            1116.9, 
            47
        ], 
        "the actual position and data scientist have to be really really good and quick at working with these tabular data. Imma, spend a little bit of time talking about the terms that were going to use. So the first here's a snapshot of a spreadsheet. These are rose. These are columns. The information that are stored in columns are going to be variables. In this case. This is information. You ": [
            2394.6, 
            2419.5, 
            96
        ], 
        "the column maybe that this is town or somewhere else great. Anybody else have a thought as to why either a or D is not the right answer. Yes. Okay, so be this is easier to calculate on because it's just the values and then D the option here. It's hard because it has the units. So how do we decide between B&D? So who wants to make the final ": [
            2942.2, 
            2976.0, 
            117
        ], 
        "the helpful variable name that would save you a lot of time. Gates are hard to work with her romantically. We'll talk about this at some point. But there is a standard and is the standard you should try to use the iso 8601 is the way to approach it where you have four values for year to four months and two four digit in that order. So no other ": [
            2667.2, 
            2688.7, 
            108
        ], 
        "the link is down here. So you can search for data from now Austin or linking two other places, but there are some proprietary data set that you would only have access to with your UCSD login information. So this is another great resource for Topix. After that is now up and running you can get information from here. Now that the government is open and the US Census provide ": [
            692.0, 
            716.4, 
            29
        ], 
        "the number of projects along the y-axis go out of you are asking questions about for the most popular which are the NBA and the Olympics was the second one with the most uncommon topic being gun control or gun violence. A lot of people are asking questions about the tech industry and employment in the tech industry phone number project about economics education entertainment and then a number of ": [
            292.8, 
            318.0, 
            11
        ], 
        "the other to exist and they are easier and then manual curation as this is the last one is always a possibility by manual Fusion. I mean, you can go look for the data and then look at the article and then type in the information yourself or copy and paste but that is prone to making errors cuz we're human and the computers are better at making fewer hours. ": [
            2065.7, 
            2086.1, 
            84
        ], 
        "the presumably this person's glucose for this ID value was taken on some days are like that shouldn't be missing a dose of the Assumption we have to make is that this date still down here and this date still down here and so forth. The Raven make those assumptions which could be incorrect. What is the ordering changed of this? It's important to explicitly State what information is in ": [
            2714.6, 
            2735.8, 
            110
        ], 
        "their similarly. This is example of what if I were to go round the outside. This was have like camel hump or something on it. This is an example and in the morning you'll have something that's untidy but that could be entered in a tidy four methods easier to work with. Schedule, this is Imagine for this is one of the title principles just one thing in a cell ": [
            2735.8, 
            2755.8, 
            111
        ], 
        "then get authorization. We have to determine how you're going to get the data. Then you have to say what day do you exactly want and then you have to promise them that you are who you say you are and then demonstrate that you are who you say you are and that's the third step. 7th and H E to pay because these are the types of methods that ": [
            899.5, 
            917.0, 
            38
        ], 
        "this state of scientist at Stitch fix name Hilary Parker and she posted this pool at the D like getting Google Calendar invites from your friends for lunch is coffee Etc. And then the other two options were absolutely not a mess. So there's some disagreement among the respondents and there were twenty-five hundred votes because there's also a lot of discussion that happened so there were forty or fight ": [
            764.5, 
            785.4, 
            32
        ], 
        "though the application where I've caught something and you have to apply the concept. Again, my goal is not to trick you or to deceive you in any of these. So I want to understand a topic. These are the topics covered we're through seven. So we'll finish 8 on Thursday. I do want you to think during the exam right? Like I don't want it to be so obvious. ": [
            3082.3, 
            3104.6, 
            123
        ], 
        "to all of the websites and clicking on them individually, and I'm figuring out and copying the numbers down manually and entering them in a spreadsheet and then looking all them. This is a case where you would use web scraping to do that systematically for you. Same disclaimer western saddle is okay. It is up to you to determine whether or not a website allows you to scrape their ": [
            1652.4, 
            1675.0, 
            73
        ], 
        "to give you all I thought after the process in the way that I developed exam the Bible and development exam to take everything. We've learned and take a few questions from each. And so I wanted to be balanced across the material that we talked about so will be a similar number of questions from each topic we talked about like I said that out the first and I'll ": [
            3024.5, 
            3042.8, 
            120
        ], 
        "to handle it. If you go and take a ton of data to answer your super interesting question. And then what is half of those users that you've taken that data has been deleted all those tweets. Is it okay for you who know I have these data to continue to analyze them despite the fact that somebody else deleted those so they have rules about getting rid of deleted ": [
            1506.8, 
            1524.9, 
            66
        ], 
        "to talk about getting data. Then we'll be talking a little bit about wrangling date of going to start on that topic will finish it on Thursday and then I'll cover a little bit about the exam to help you out at all figure out what you should study what you should be focusing on. So Court reminders as per usual assignment due this Friday is a reading assignment. The ": [
            141.1, 
            164.4, 
            4
        ], 
        "today herbal store in Collins operations store in Rose in different types of data. Generally about this this week for your reading whoever hasn't done that so far. You'll get more details on this but there are four roles. The first real tidy data is at each variable should be stored in a single column. This may seem obvious looking at this dataset when you look at other people stated ": [
            2464.7, 
            2487.8, 
            99
        ], 
        "topics on being covered by fewer group. So if you want to see somebody else is working on a similar question or similar data be happy to put you in contact with another group. Okay, this is going to be out there really quickly here. These are the names of teens who did not put their team members on their commission their names were on the document but they didn't ": [
            318.0, 
            342.0, 
            12
        ], 
        "tuna at Vanderbilt health department and we have here is a visualization of all of the treats that happened in response to this initial sweet to each circle is a treat and right ear is the initial sweets. You can see their number conversations that were spawned off of that first and then there are some long branches were there was a lot of conversation that happened and this is ": [
            1289.5, 
            1311.8, 
            56
        ], 
        "tweets from your analysis before using them for research the distinct. All of this is covered in the developer agreement for Twitter and it changes. So when you're using these types of data for project is up to you to make sure you're in agreement with their Twitter policy or whatever ATI are using their policy. You're following the rules and you're going back to the data and getting rid ": [
            1524.9, 
            1545.4, 
            67
        ], 
        "underscores should be used instead of spaces and meaningful names of this comes back to our variable naming. So this is a better column header than F1 which doesn't give any information about what stored in it. You will see this all the time and date in the real world. And then it's your job to go figure out what information is stored in there or if you just had ": [
            2648.6, 
            2667.2, 
            107
        ], 
        "understand what happened as a result of this printer pole. That's not just the quick 51% so you can understand it much more deeply when you do something like this. This is a sentiment analysis which dispatchers what we talked about previously for sentiment analysis. You can take all the words and all of the tweets related to this. You can be overwhelming with the words had a positive sentiment ": [
            1374.8, 
            1396.3, 
            60
        ], 
        "up to you to determine which of these spreadsheets is there fast. Feel free to talk to each other and discuss why. I'll give you a few more seconds. 3 2 1 alright, so we have lots of people think D and fewer people saying D and decides you can see it. He wants to explain a thinking and choosing their off their answer. Okay. So the argument here was ": [
            2817.9, 
            2904.0, 
            115
        ], 
        "use an API to do the science of data science that we talked about and very simple you can also use these to organize your Google Drive account. So you can interact with it without clicking and dragging all the stuff you have to do on Google Drive. I just wanted a few populatie. I fear this is by no means exhaustive. There are lots of places where you can ": [
            1439.0, 
            1460.6, 
            63
        ], 
        "use consistent number when you're storing data. What type of tiny data and spreadsheets your data or rectangular what I mean? Is that all of the data fit into a rectangle and I don't have to loop around to avoid cells were stuff missing so I could data set is a rectangular data set and these are the easiest to work with programmatically the goal of the Pecos Messi date ": [
            2550.9, 
            2574.3, 
            103
        ], 
        "want from a source from a provider and then they change your API and you have to redo everything so know that the documentation are invaluable and a source of frustration when you're working with API. So so far we talked about the fact that you were going to use get to retrieve the information. That could be the method. We use we're going to send Bill to said if ": [
            1026.4, 
            1050.6, 
            44
        ], 
        "we use for getting a website. Is it same idea we use for making a request to an API. You determine your method you then interpret the mass of a p i interpret the method and gets the information you request and then it's up to you to process what you get from the a p i n Okay, so far we talked about tabular data loss of sources for ": [
            1223.2, 
            1245.3, 
            53
        ], 
        "week like what shows has she would like to go? Okay guys. If you like a ticket, please raise your hands and I will come to you. Thank you. Thank you. Art as we get settled there are a number of things were going to come cover today. So we're to talk about some housekeeping stuff and a brief it about the first and second assignment where they ain't going ": [
            57.0, 
            141.1, 
            3
        ], 
        "what types of day do they have that is available for the United States. And globally there's an open Beta index. There's no shortage of data that are found in tabular format in spreadsheet that can be retrieved from the internet. But what if the data aren't in a spreadsheet ready and waiting for you? example block information on Twitter people write lots of things people make polls. So there's ": [
            738.1, 
            764.5, 
            31
        ], 
        "what's going on with the houses being listed in your neighborhood, you could use web scraping to get a bunch of information quickly from all the different sources rather than relying on anyone individually. Or you can use it for your background research. If you decide you want to start a car dealership, you would want to get some information about what other people are charging and rather than going ": [
            1630.4, 
            1652.4, 
            72
        ], 
        "which makes the most people felt positively about this but there was a faction that had some negative feelings about this. Okay. So first one is a simple example here of how you would use for this API to get a very limited set of Jada but social scientists are using Twitter to act very interesting questions about how we work with each other and a social network. They are ": [
            1396.3, 
            1417.8, 
            61
        ], 
        "you are all that specifies the data we're going to retrieve and when those data are returned to us. They are most often in Json format and we talked about this last lecture. So Jason santor JavaScript object notation and just as a reminder, Jason is returned in key value Paris between races with colon in between I told you we're going to get stuff. So that's the HD Phim ": [
            1050.6, 
            1077.1, 
            45
        ], 
        "you can incorporate that in your next assignment in your assignment this week. Should be incorporated in your submission next week. Very briefly. This is a summary of the topics. Y'all have chosen for your projects. So I just went through and took all of the project questions. You are a thing and gave it a general topic so you can see all the different sites along the x-axis and ": [
            269.5, 
            292.8, 
            10
        ], 
        "you can use to access information from an API. These are some of the most popular methods so if you want to get information which accomplishes reading information from a source, this is a method we're going to be focusing on today, but I know that there are other ways in which you can interact with an API, you can create one create a new resource. You can update you ": [
            917.0, 
            940.0, 
            39
        ], 
        "you want from a website. So, where would you go ahead and use at what if you were a real estate agent and you're new to a neighborhood and you want to figure out what's going on in the neighborhood there are lots of websites where you go for information like Zillow or Redfin and there are lots of others and if you wanted to get a sense as to ": [
            1611.2, 
            1630.4, 
            71
        ], 
        "you want to get you would use these HTTP Methods at your computer to bend connect to and retrieve the data that are stored somewhere else. So nothing ever going to be using is get that's going to retrieve it. Now there's a bill instead of URL that are going to return the data. So you had to tell the the place where the data are stored. What day do ": [
            961.4, 
            984.7, 
            41
        ], 
        "you want? You can just ask for everything and get all of the information in all of Twitter's I do it. So what you have to do for this, did you have to go to their API documentation and there are Developer documentation for lots of different popular website and you have to go through all of this and figure out how to specifically specify the information he wants you ": [
            984.7, 
            1006.2, 
            42
        ], 
        "you would probably get see cuz it's in the middle of the range in that the state's guess just talked about it all the time. And that is at 80 to 90% of their time is working on getting the data from formatted into the form. They can actually use it in and it said hopefully improve over time. So if more people understand how data should look then they ": [
            2260.7, 
            2281.3, 
            89
        ]
    }, 
    "File Name": "Introduction_to_Data_Science___A00___Ellis__Shannon_Elizabeth___Winter_2019-lecture_7.flac", 
    "Full Transcript": "8Ball it is a lot of fun, but it's usually a lot of money to go. However through the school would you get passes for 90% off? Ye so exciting right? That's the energy in this room guys. Anyway, it's usually $50 a person do the school to $10 a ticket in each tickets to people in so it's only $5 a person this includes everything you need to play for the entire day. The only additional cost is that you have to purchase paint which is very inexpensive. His tickets are also valid for 2 years. I know everybody in here is very busy with school and work. They must be part of this school to purchase. These tickets are Weber anybody is more than welcome to go. So you can bring family and friends you can even give it as a gift is $50 value right on the ticket. Once again, we get a limited number of these passes in the beginning of the semester had them out first come first serve until we sell out. It's for giant paintball is a lot of fun $10 for one ticket gets to people in and you have two years to bring anybody you like on any day of the week like what shows has she would like to go? Okay guys. If you like a ticket, please raise your hands and I will come to you. Thank you.  Thank you.  Art as we get settled there are a number of things were going to come cover today.  So we're to talk about some housekeeping stuff and a brief it about the first and second assignment where they ain't going to talk about getting data. Then we'll be talking a little bit about wrangling date of going to start on that topic will finish it on Thursday and then I'll cover a little bit about the exam to help you out at all figure out what you should study what you should be focusing on.  So Court reminders as per usual assignment due this Friday is a reading assignment. The first exam is a week from today. It is multiple choice. It will happen the last 30 minutes of class. And your second assignment is due next Friday. Just that is so you all can plan ahead. There are programming covered in three lectures. That is this Thursday is next Thursday's and Thursdays after lecture. So my goal at the end of that is you can all read a dataset in and you can Wrangle the date of that and make some basic visualisations. That's the goal of what programming I want you to be able to do. I also have contact information. I'm in the second bullet point hear from the person who made announcements at beginning of class last lecture.  All right. So as everybody is settling I want to recap the first assignment y'all did really well submitting. I think only three people didn't submit a project or an assignment the first assignment which is good. And those three people I'm not sure are in a class. So that's great. If y'all did really well with what I've seen so far. I've gone through look at your question in just looked at your sources for data says I'm I also appreciate a lot of your team name. So in your team name, I learned a new nickname for UCSD. I got to solve a riddle and lots of you I had data science in your team name. I appreciated it among other creative things.  Degrading is happening this week. Just so you all are aware. The way grading is happening. Is that the same TA or I will grade your project throughout the quarter. This is the system so that you are getting feedback consistently from somebody who already has seen your question before I can help guide you throughout it. And that's why we're going with infection. You should know your point of contact. I will monitor for grading fairness across graders and see what that should anything to rise dough using a rubric to grade but they're providing you feedback specific to your projects that you can incorporate that in your next assignment in your assignment this week. Should be incorporated in your submission next week.  Very briefly. This is a summary of the topics. Y'all have chosen for your projects. So I just went through and took all of the project questions. You are a thing and gave it a general topic so you can see all the different sites along the x-axis and the number of projects along the y-axis go out of you are asking questions about for the most popular which are the NBA and the Olympics was the second one with the most uncommon topic being gun control or gun violence. A lot of people are asking questions about the tech industry and employment in the tech industry phone number project about economics education entertainment and then a number of topics on being covered by fewer group.  So if you want to see somebody else is working on a similar question or similar data be happy to put you in contact with another group.  Okay, this is going to be out there really quickly here. These are the names of teens who did not put their team members on their commission their names were on the document but they didn't add them to breathe part of me thinks that you just did this. So I had to put dookie on a slide and you should make sure next time that your group members have been added. I added them this time. I will not do that going forward you make sure your group members are added on Crisco. Most of you this does not apply to if you did not submit it as a PDF if you print it out something and and it using an app or you took pictures of your computer screen. Make sure you refer back to the slides at the end of the third lecture to see how to submit it as a PDF or stop by during office hours to make sure you know how to do that going forward.  That's it for the first assignment you will get feedback by this weekend and you will have your grades for the first assignment by this weekend for second assignment just to make sure we're on the same page when you open the second assignment. It might look slightly different than before because there will be some cells that are collapsed. You can click on these and see all the content these are class just so you can see where you should focus your attention the sections that are not collapse are those we have to add new information where you have to have to read your data set in we have to Wrangle the data set on the 2nd of submission. You can change edit update your question you should do so if you get feedback telling you from your t a r i at your questions not you can find or use a different way to set you find something that will work better you find another it is that they will complement your first day that you're welcome to add that in on your second assignment. You must include feedback and programming is required on this assignment and 1/3 assignment, not the 4th assignment.  Okay On Assignment, it says that your day is it has to have variables that are numerical and categorical. I just wanted to clarify this a little bit. The reason I want both is because when you make visualizations, they differ based on the type of variable you have I want you to experience with both so you need some sort of number and you need some sort of categorical value.  I'm going to clarify a slide I had NV data lecture.  The categories are quantitative and qualitative quantitative our numbers. So I called you before continuing. I was trying to avoid using quantitative and qualitative cuz they are such similar words and they don't actually conjure up exactly what they are for everybody. But we'll use these quantitative think quantities of some sort of number qualitative think some sort of Category 3 how to make that is because let's say you were working with data about cars and you had some information about the number of cylinders for a vehicle. That number is limited even though the value it's really a categorical data location to take this example cuz I don't know that much about cars, but you can have 4 or 6 cylinders are probably another value but there are only a few options. Are there any questions why something that is a number like cylinders would be categorical and not quantitative.  You need one of each in your project in the dataset.  This is up here for your information. Just so y'all can plan ahead. This is what you'll be doing in section each week. We're currently in week 4, there was a Monday section that already met they were we behind so they did at the reading and the programme practice. Everybody else will just get programming practice infection this week you always as we've been doing discuss the readings the week after you submit the reading quiz, and you'll get time to work on your projects as time permits in section. This is just for y'all to plan.  Whole lot of housekeeping staff today whole lot of reminders good work on the first assignment so far get ready to your second assignment any questions about anything I said so far.  What are we talking about getting data? We're going to be very generally discussing with an API it is and how it works. And then we're going to start talking about where you can look for data and get interesting data set.  To review where unit definition that data are anything that can be stored in a computer.  I've been using this term tabular data and spreadsheets and I've been using them interchangeably the purpose of this course. Those are the same. We're really going to discuss what tabular data are today and I've mentioned that these are the types of data that day of scientist work with most often. These are data that are of what you would store in Microsoft Excel or what you would store in Google Sheets their information entered in rows and columns and they're often stored in spreadsheet. So I'm saying tabular data or spreadsheet data. Those are the same thing.  Beat type of data can be found in lots and lots of different places on your first assignment you were directed at a few places. I'm going to mention a few more today. You're welcome to change your date at that. If you look at one of these sources and say hey, they might have something really interesting for my project most of your familiar with cargo. There's a very similar platform called date of that world data is plural is a big ol Google Sheets filled with more than 600 datasets across a ton of different topics. They have policing information. They have politics information about health information. So if you want to search through there for any project you never have to do a data. It's a good place to start.  What do you think data from 5:38 in the course? They are referring to be unfamiliar with 538 a new source, they cover politics and they covers and they cover science and health and they released a detective High some of their article and those can be accessed online. I mentioned will work with some of this this if you got to fly to earlier than an hour ago will not be in the size you downloaded but it is up there now. I'm at with a AR Library liaison this morning and I learned about a new resource and the library has at UCSD a lot of data sets by Topic in the link is down here. So you can search for data from now Austin or linking two other places, but there are some proprietary data set that you would only have access to with your UCSD login information. So this is another great resource for Topix.  After that is now up and running you can get information from here. Now that the government is open and the US Census provide swapping information about individuals in the United States that are collected from the dentist. But yet and it's not just Federal day that you're limited to cities more and more are rolling out open City-Data portals. And here's a list of a few where you can go in and get information about a city in particular.  That summer is here. If you want to look at cities across and see what types of day do they have that is available for the United States. And globally there's an open Beta index. There's no shortage of data that are found in tabular format in spreadsheet that can be retrieved from the internet.  But what if the data aren't in a spreadsheet ready and waiting for you?  example  block information on Twitter people write lots of things people make polls. So there's this state of scientist at Stitch fix name Hilary Parker and she posted this pool at the D like getting Google Calendar invites from your friends for lunch is coffee Etc. And then the other two options were absolutely not a mess. So there's some disagreement among the respondents and there were twenty-five hundred votes because there's also a lot of discussion that happened so there were forty or fight sephiroth in the 20 retweet and then conversations that happened off of the retweets and it's not really easy. Just looking on Twitter to get a sense of what everybody said.  They don't have the data to answer that question. If you really want to understand the full scope of the question. You have the initial vote for that sit at a glance.  The way you can do is you can use available methods to get that information for yourself.  So I'm going to very high-level talk about HTTP and apis today and I'm going to compare this to my undergraduate biology and I learned a lot of things and then I got to grad school and learned that almost everything was taught with kind of true but there wasn't there a lot under the hood that I wasn't taught in there a lot exceptions all the rules. I told Todd was taught the same to be true about this high-level be an API beer to go courses later on and learn a lot more details me like she didn't really tell me the whole story and that's true. Okay, that's good morning transfer protocol. This is a way that allow messages to be sent on the internet. So you all do this everyday you sit down at your computer you type in the URL and then magically request goes to a website and then the website is displayed.  So if you can get websites using a swear to call can we get other data or information you would want from the internet? And obviously this is in a lecture you can and the way you do. This is using application programming interface. API. API can do a whole lot and we're only going to focus on a small capability of API today in the context of getting data.  If you guys are ways for software and applications to talk to one another so their rules for interaction.  For purposes the way to access an API is to First choose a method.  We're watching that's been build a URL and then get authorization. We have to determine how you're going to get the data. Then you have to say what day do you exactly want and then you have to promise them that you are who you say you are and then demonstrate that you are who you say you are and that's the third step.  7th and H E to pay because these are the types of methods that you can use to access information from an API.  These are some of the most popular methods so if you want to get information which accomplishes reading information from a source, this is a method we're going to be focusing on today, but I know that there are other ways in which you can interact with an API, you can create one create a new resource. You can update you can get rid of it. And these are the methods that you would use on a left to make these requests. They were just going to focus on getting information from an API. So the only method were to be talking about today is get  So when you're sitting at your computer and decide that there's not a decent out there for you, but there is information out there that you want to get you would use these HTTP Methods at your computer to bend connect to and retrieve the data that are stored somewhere else.  So nothing ever going to be using is get that's going to retrieve it.  Now there's a bill instead of URL that are going to return the data. So you had to tell the the place where the data are stored. What day do you want? You can just ask for everything and get all of the information in all of Twitter's I do it.  So what you have to do for this, did you have to go to their API documentation and there are Developer documentation for lots of different popular website and you have to go through all of this and figure out how to specifically specify the information he wants you have to know what information you want and then you have to use the documentation to figure out how to get it.  And there are different ways. I'm at each of the API. So here we have Twitter and then Google's API and a GitHub API and we should know is that these versions change often? So you may have this really great script that gets all the information you want from a source from a provider and then they change your API and you have to redo everything so know that the documentation are invaluable and a source of frustration when you're working with API.  So so far we talked about the fact that you were going to use get to retrieve the information. That could be the method. We use we're going to send Bill to said if you are all that specifies the data we're going to retrieve and when those data are returned to us. They are most often in Json format and we talked about this last lecture. So Jason santor JavaScript object notation and just as a reminder, Jason is returned in key value Paris between races with colon in between  I told you we're going to get stuff. So that's the HD Phim HTTP method we're going to use and then we specify what we want to get using a specific URL that we determine from the documentation from the provider. We're going to get that data back in Json format. So before we can get that data back, you need to authorize yourself. So you need to some way to say that you are who you say you are.  I did do that. The most common way is using oauth or open authorization.  Resume compromise so that you're familiar with the terms when you go to a tempest in the future. You've at least heard them but know that they will be steps along the way they will have to figure out when you go to do this. So when you are they should the first thing I have to do is apply on the website for the API on to what you want access to tell in this case get home who you are and retrieve authorization so that they know I can control who's getting what from their from their database.  You then create an Olaf application from this you will get with none of the client ID and a client secret either would have provided by GitHub or Google or Twitter or whatever you're trying to get the data from.  After you have a client ID and secret fuel generator token and this access token is what ultimately will provide you access to the API to get the data that you want and verify who you are who you say you are. Alright morning. These should never be shared or posted publicly with apis. You have some amount of information you can get back for free, but if you want a lot of information or to make you pay which makes sense, so if you post these after getting them on the Internet, somebody else can take them and then charge your credit card for all the data that they just got for free and you just paid for so we talked about getting a previous should never be stored. I'll get how people write crawlers that go through and look for these types of information and will then use them to their advantage and to your detriment  So that covers the getting authorization part. So now we know that we're going to use me get methods to get information. We're going to build our URL that's going to return us some Jason information and we can get that information after I authorize our self and then it's up to you to practice their spot. The first part of this lecture was about getting data from a source or you just get your tabular data right away. This one takes a lot more steps and of a lot more work, but you can get data that you wouldn't have had otherwise, so just to recap the same process that we use for getting a website. Is it same idea we use for making a request to an API. You determine your method you then interpret the mass of a p i interpret the method and gets the information you request and then it's up to you to process what you get from the a p i n  Okay, so far we talked about tabular data loss of sources for getting that type of information and the fact that you can use a provider's API to get data that you wouldn't have had. Otherwise we talked about the basic methods for getting that information. So briefly, what would you use this for the return back to that example I started with this is a simple example where you have a very specific question. What is the larger context around this question? So if you want to understand all of his receipts read tweets and responses.  You could use for the API to explore this so you can ask Twitter for all of the responses to the street all of the Retreats and all of the conversations that happened as a result of those retweets and you can visualize them by Lucy Big Island Casino who is a form of tasty tuna at Vanderbilt health department and we have here is a visualization of all of the treats that happened in response to this initial sweet to each circle is a treat and right ear is the initial sweets. You can see their number conversations that were spawned off of that first and then there are some long branches were there was a lot of conversation that happened and this is an interactive Graphics though on this website. You can hover over any point and see the conversation that happened so very quickly can understand all of the conversation that happened and if I were to summarize this briefly hear there was in the beginning some wording clarification, but some people will remember the question was do you want somebody else to send you a Google invite for coffee or lunch date?  Hear these two boxes are people saying why wouldn't that be the first point of contact but if we have discussed it and then they sent me at to verify. That's great. So there's some clarification happened here. There was this tweet about people having very strong feelings about online calendar etiquette, which is something that lots of people apparently have very strong feelings for down here is some discussion about the fact that having kids makes your mind. Goofy. There is over here at the point that I have anything invite you're not going to have confusion about time or place. There's something special about who uses to do managers hear people talking about paper journals. And then there's a soft-shoe talking about Microsoft Office product and wedding tangent about somebody play all of this. You can quickly understand what happened as a result of this printer pole. That's not just the quick 51% so you can understand it much more deeply when you do something like this.  This is a sentiment analysis which dispatchers what we talked about previously for sentiment analysis. You can take all the words and all of the tweets related to this. You can be overwhelming with the words had a positive sentiment which makes the most people felt positively about this but there was a faction that had some negative feelings about this. Okay. So first one is a simple example here of how you would use for this API to get a very limited set of Jada but social scientists are using Twitter to act very interesting questions about how we work with each other and a social network. They are getting data from Twitter all the time and using this answer questions that are much more in-depth than the simple example. I explained here  They're also research project that goes through GitHub and take people say online jupyter notebook and analyze how people are using them and what's included in each of them. So getting a sense of how people are analyzing data left an example of how you would use an API to do the science of data science that we talked about and very simple you can also use these to organize your Google Drive account. So you can interact with it without clicking and dragging all the stuff you have to do on Google Drive.  I just wanted a few populatie. I fear this is by no means exhaustive. There are lots of places where you can ask an interesting question. You can likely get data that are of interest to you.  Okay, so I feel like bananas Corso sound like a broken record on this the common theme of just couldn't because you can get the data doesn't mean you should get it or use it on their terms and agreement that we use for all of the apps that we use in all of the software that we use and we just kind of like go through and click and say I will follow those when you're getting data from somebody else that you can use in either research or your own for your own purposes those agreements matter a lot more and it's up to you to make sure you understand what you're agreeing to whenever you're accessing an API house rules on how to handle it. If you go and take a ton of data to answer your super interesting question. And then what is half of those users that you've taken that data has been deleted all those tweets. Is it okay for you who know I have these data to continue to analyze them despite the fact that somebody else deleted those so they have rules about getting rid of deleted tweets from your analysis before using them for research the distinct.  All of this is covered in the developer agreement for Twitter and it changes. So when you're using these types of data for project is up to you to make sure you're in agreement with their Twitter policy or whatever ATI are using their policy. You're following the rules and you're going back to the data and getting rid of anything that you're not supposed to have or not supposed to be using.  Okay.  Lots of spreadsheet talk we discussed basics of apis and HTTP methods we talked about getting information from an API. We walked through a quick example the last know what if there's no way you can't get that do you want from it? This is where the case of web scraping comes in. We talked about this a bit last lecture and this is from last lecture we talked about that phones lies example where they had the day and then the quote and then the medical explanation as to why it was an untruth with a URL in it.  Where you at scrape is and this is just the basics of it behind everything. You see on the internet. There is HTML behind it. And HTML is  characterized by these tags that open with these carrots and an end with a carrots with a Flash and you use these to specify what information to pull from a web page.  That's like the highest level of web scraping. There are python packages to help you do this. There are packages help you do this and you can use these to go and get the data you want from a website.  So, where would you go ahead and use at what if you were a real estate agent and you're new to a neighborhood and you want to figure out what's going on in the neighborhood there are lots of websites where you go for information like Zillow or Redfin and there are lots of others and if you wanted to get a sense as to what's going on with the houses being listed in your neighborhood, you could use web scraping to get a bunch of information quickly from all the different sources rather than relying on anyone individually.  Or you can use it for your background research. If you decide you want to start a car dealership, you would want to get some information about what other people are charging and rather than going to all of the websites and clicking on them individually, and I'm figuring out and copying the numbers down manually and entering them in a spreadsheet and then looking all them. This is a case where you would use web scraping to do that systematically for you.  Same disclaimer western saddle is okay. It is up to you to determine whether or not a website allows you to scrape their information and their way through that we won't talk about in detail. There are lots of popular platforms that will cut you off if you take too much information from you than at any one point in time so know that that is likely to happen if you don't look into whether or not it's okay to scrape information from a website first.  All right, See you in data science is don't reinvent the wheel if the information is already out there don't work harder than you need to to get it. So always start by looking for available dataset, if those don't exist, your next step would be going to an API and then only go to web scraping if it's allowed and if you can't get it from the initial resources.  All right. So we're going to do a few clicker questions and have discussions about this.  Montana that opens up  Christmas tree  one second not working yet.  You guys can't cooking right kind of Ken?  Set a reminder frequency code is AD that's on the board.  A person even super busy on get held and you want to write a blog post summarizing what you've been up to.  Get ready a few more seconds.  3 2 1  All right, so that's a Geordie of you said available data.  Second most popular with a TI and then some people said Manuel stration are who wants to go with what explain why they pick the option that they did.  Okay, so funny, but you didn't hear the argument was for using the API because if you're making commits to get Hub you would be able to pull that information from the API and then be able to analyze it there. So all the information about everything you did not get help would be accessible through their API. So on this one I'm going to argue with that is the correct answer. The reason that available data are there is the days that you wanted probably doesn't exist as a dataset yet. There is no easy spreadsheet to get you have to go retrieve it from the API.  All right.  next one  You want to analyze where geographically calls to police have occurred in Albuquerque, New Mexico same options?  Closet in a few seconds  3 2 1  All right, so overwhelmingly people said available data, but BNC were possible options here who wants to explain their thought process here.  I'll tell you that the majority got it right on this one and even defend their thought process there.  The data should be available should not Diamond. It's all going to disappear little bit. But agree on the second part of what you said. So I left out a key piece of information here. Is that Albuquerque New Mexico has and open data portal the data are available through their portal and that's what you would ask this and that would be easier than b or c because the data exist.  Skip one. I trust one.  I just had a super interesting article on 5:38 and I want to explore the data myself is the best place to start.  Are closing in a few seconds?  3 2 1  All right. We had some disagreement here who wants to defend their position here. What were your thoughts on choosing the choice you made?  Okay. So the argument here is for a available data and the statement made was at 5:38 post all of their data online. I'm going to put a sleigh carry-on on that. They don't make all of their data available that they make a lot of their data available. So I agree that a is possible here. I'm anybody would explain why they chose be.  Okay, so nervous. I mention that it's available, but I can take through which Avenue and that was somewhat intentional so that I could ask this question later on and we could have this discussion so you can directly download their data sets from data. Fivethirtyeight.com. But they also host them on GitHub. So you could use an API to access those. Toby is also correct. I'm going to disagree with web scraping because the other to exist and they are easier and then manual curation as this is the last one is always a possibility by manual Fusion. I mean, you can go look for the data and then look at the article and then type in the information yourself or copy and paste but that is prone to making errors cuz we're human and the computers are better at making fewer hours. As long as we tell them the right thing. The manual curation is not going to be the idea for your creation of any dataset, but will sometimes be necessary. I'm so it's not the answer to any of these just because those aren't any of our three 1st Avenues.  Any questions so far on anything I talked about this far and I put that in the beginning.  I already did everybody like us 32nd minute break talk to each other and then I'm gonna have everybody Focus back in cuz I know this is a lot of lecture so you guys have a minute to talk to each other again, and I'll have you focus II.  All right.  For the second half we're going to do the first star. That was very East Coast 2nd half second half we're going to be talking about tidy data and data wrangling the idea of how to make spreadsheet data at work for you rather than against you and how to get data that aren't good into a good format and we're going to clarify what all of that means today and on Thursday.  Thermostat says the question is what percentage of a data scientist time is spent preparing data that I can be used to answer an interesting question. And that is the definition of data wrangling getting it so you can answer the question you want.  Give me a few more seconds.  3 2  okay. So here are the results a lot of you said we're somewhere in the 50 to 75 range. But we have a spread across all of these. I would gather that if you didn't know you would probably get see cuz it's in the middle of the range in that the state's guess just talked about it all the time. And that is at 80 to 90% of their time is working on getting the data from formatted into the form. They can actually use it in and it said hopefully improve over time. So if more people understand how data should look then they can get it right from the beginning and you are the data scientist can spend less time working on this part over right now and for the past 10 years, this is what it's been. So here's that other caveat to this is that we spend eighty 90% of our time trying to get the data in the form we want.  And we don't get to do the more interesting part at the end that said Cynthia wrangling is a big part of the job. You have to be good at it. You have to not hate it cuz you'll spend a lot of time doing it and you have to be good at it over time. So this is a quote from DJ patil. He was a chief scientist. I'm under the Obama Administration and the first Chief data scientist, and he said that good data scientist understand any deep way that the heavy lifting a cleanup and preparation isn't something that gets in the way of solving the problem. It is the problem.  This means that we should all be really good at it. And I think right now since they to science isn't evolving feel that we don't always watch it. But we expect everybody to know how to do it. So that's why I'm sending a number of a bunch of time talking about how to take data that looks something like this which we're going to all know with Messy in a second and then why and get it into a usable form at this is a spreadsheet that came from the Australian government and its is a blog post from how many miles in pain and he showed the fact that you have information kind of all over the place. So you have his header appear that doesn't really include data that you'd want to analyze and then you have what you actually want in your columns over here and I assure you of groups that you would actually want that to be a column itself there lots of missing spots on his junk down here. This is something that would not be very easy to work with. Once you read it into python the process of taking it from this untidy format to a tidy data format, which we're going to Define is through the process of data wrangling.  So I talked about in the programming lecture what the differences are between a data scientist in a computer scientist when it comes to the actual position and data scientist have to be really really good and quick at working with these tabular data.  Imma, spend a little bit of time talking about the terms that were going to use. So the first here's a snapshot of a spreadsheet. These are rose. These are columns.  The information that are stored in columns are going to be variables. In this case. This is information. You collect from each in a safe person. So variables are stored in column.  And it's observations are stored in rows. In this case. These observations are people that they don't have to be an everyday to set this year. We can see that we have for observation. So seven variable seven columns for observations for Rose.  And I'm going to talk about types of data here. For example, you might save this information was information collected from somebody who entered a doctor's office and they had to fill out a survey and give this information to the doctor. This may have been measurements taken at your visit that was taken by the nurse or by The Physician and these are two different types of data because this came from a survey and this came from the physician during your meeting today herbal store in Collins operations store in Rose in different types of data.  Generally about this this week for your reading whoever hasn't done that so far. You'll get more details on this but there are four roles. The first real tidy data is at each variable should be stored in a single column. This may seem obvious looking at this dataset when you look at other people stated that you'll see that this is not necessarily the case and we'll come back to that and examples about a second last name first name one piece of information stored in each column for each variable.  The second is that every observation for each variable should be stored in a different row. So this row includes information for a single observation and that's consistent across the row.  Purple vegetables and Collins second operations in Row. The Third is that there should be one table for each type of data. These are the same two types that I showed before and I forwarded that if you have more than one type of data or more than one table, you need a column in each with the same column label that allow them to be joining merged these might seem obvious looking at the spreadsheets were showing right here, but I have seen countless examples where this would be Capital ID in one and then over here this week, maybe remember underscore ID and then it's up to me to figure out already the same for the same people across the date of that says it's a different number that they've used. So it's really for they use consistent labeling and that use consistent number when you're storing data.  What type of tiny data and spreadsheets your data or rectangular what I mean? Is that all of the data fit into a rectangle and I don't have to loop around to avoid cells were stuff missing so I could data set is a rectangular data set and these are the easiest to work with programmatically the goal of the Pecos Messi date of death and get them in this format.  Any questions about the general principles of Tidy data and what it means to take a messy dataset into a tidy format.  Okay.  Their number rules for good spreadsheets. This is also part of your reading for this week. We're going to walk through the examples of the seven of these now and again keep these in mind again. I'll show you a obvious examples, but this happens all the time in the real world. So it's really important to understand these ahead of time to know what to look for. So consistency is the first rule of incredibly important make sure that your variable names are consistent from one table to the next but also within a variable or variables should be coded the same way. So here if your you have the options of female and male they should be Mount you shouldn't write lowercase T mail for one uppercase email for another and an F for another individual. It's much easier to summarize information if everything is typed consistently  Bus you to choose good names for things there rules about avoiding Extra Spaces. So programmatically mail without a face is different than male with a space underneath of it underscores should be used instead of spaces and meaningful names of this comes back to our variable naming. So this is a better column header than F1 which doesn't give any information about what stored in it. You will see this all the time and date in the real world. And then it's your job to go figure out what information is stored in there or if you just had the helpful variable name that would save you a lot of time.  Gates are hard to work with her romantically. We'll talk about this at some point. But there is a standard and is the standard you should try to use the iso 8601 is the way to approach it where you have four values for year to four months and two four digit in that order. So no other crazy way in which you can enter date or time should be used under the dates and times are difficult to work with is because time zones exist. So 2 p.m. Here is not 2 p.m. Worldwide dates are hard because people have chosen lots of different crazy ways to unspecified 8 spermatically. We'll talk about the details later.  So a boy that they sell in spreadsheets and your glucose the presumably this person's glucose for this ID value was taken on some days are like that shouldn't be missing a dose of the Assumption we have to make is that this date still down here and this date still down here and so forth.  The Raven make those assumptions which could be incorrect. What is the ordering changed of this? It's important to explicitly State what information is in their similarly. This is example of what if I were to go round the outside. This was have like camel hump or something on it. This is an example and in the morning you'll have something that's untidy but that could be entered in a tidy four methods easier to work with.  Schedule, this is Imagine for this is one of the title principles just one thing in a cell is over here on the left. We have one value of a person's weight in pounds on in a Cell over here while this looks like one piece of information really up to you have the value and you have the unit. It's much better to put the unit up at the top. Everybody's clear. This includes weight in pounds rather than having two pieces of information in a single cell.  Don't use phone color and highlighting is data hear. The person is trying to save this value is way different than all these other valleys. It might be a mistake rather than using font color. It's best to have another variable that says hey this one might be an outlier.  Is included for you to look through I won't go through all of this, but it's a summary of everything. We talked about better approaches to bad naming.  I have it in here to summarize your points that are made in the Tidy data paper from Hadley Wickham This is just so your notes are complete. I won't walk through all of these now.  I'm going to have you all do one of the two here. So the first one take a look carefully at these spreadsheets and it's up to you to determine which of these spreadsheets is there fast. Feel free to talk to each other and discuss why.  I'll give you a few more seconds.  3 2 1  alright, so we have lots of people think D and fewer people saying D and decides you can see it. He wants to explain a thinking and choosing their off their answer.  Okay. So the argument here was for choosing D because you if you hear you clarify that your unit is talented maybe pounce universally would not be the way in which you would enter weight because most countries would use a kilogram. Okay, so that's one for anybody have a thought against d  Yes.  Okay. So the argument here is 4B that you just want the value in here and you can specify up here in the column maybe that this is town or somewhere else great. Anybody else have a thought as to why either a or D is not the right answer. Yes.  Okay, so be this is easier to calculate on because it's just the values and then D the option here. It's hard because it has the units. So how do we decide between B&D? So who wants to make the final call on date?  if I show hands anybody for be  very face of anybody for bo4 anyone 4D  I will sleep in class. That's fine. Panther hear Envy you would want to as we said put the unit up in the column header. So I wanted to have some discussions. So the best option would BP you just let the values for your specify the unit in the column header. Okay, we're not going to do that one just to summarize taking data that are a mess and getting them into that format is called Data wrangling. We'll talk about the Technics techniques used to do that next lecture, but I'm going to send the last few minutes talking about the first exam.  Call by 30 multiple choice questions last 30 minutes of class a week from today. I wanted to give you all I thought after the process in the way that I developed exam the Bible and development exam to take everything. We've learned and take a few questions from each. And so I wanted to be balanced across the material that we talked about so will be a similar number of questions from each topic we talked about like I said that out the first and I'll go check it afterwards. So I'm not just texted testing 1 topic. I am not out to get anybody when I write advance, but I'm also not out to ask questions like is the sky blue and have one option bb19 be blue when I have to be 17 and then something else that doesn't make sense. So I need to test that were actually learning something and that everything's not obvious. So I get the balance between asking questions that make you recall information and questions that make you apply Concepts. We asked without everything being obvious so that everybody gets it 100 cuz then I have no idea whether or not he's actually learnt or learned anything and if I taught anything  BB recall questions where you just have to remember stuff that have been presented to you though the application where I've caught something and you have to apply the concept.  Again, my goal is not to trick you or to deceive you in any of these. So I want to understand a topic. These are the topics covered we're through seven. So we'll finish 8 on Thursday.  I do want you to think during the exam right? Like I don't want it to be so obvious. The question is this guy? What color is the sky? And you say blue?  How to say the first line of defense for your setting should be the lecture material you can be your notes it can be you re listening to the podcast it can be reviewing clicker questions vast majority of the questions on the exam will come from what was discussed in lecture.  The readings are fair game. Only the first two readings will have questions specifically from them, but I don't want you to memorize dates or names or specifics. I really want to make sure did you get the big Concepts from the readings note that while tidy data and data wrangling at the topic or covered? I won't be asking anything specific from that reading because you haven't discussed it in section yet.  You should understand the code for that in a class. The best way to do it is go back to the notebooks run the code changed a little bit anticipate what the output should be after you change it be if that happens that will really ensure that you understand it.  I'm actually it's up on Triton head but it's also click link clickable on this link from the slides. There's an exam study guide. It's not comprehensive. It don't have everything on there. That's a good place to check your understanding. It's broken down by lecture has questions from lecture from the readings from python. If you understand all of those and have a good understanding all the questions and Concepts on Barrett, you should be okay.  What start with questions on Thursday that you have that any material? So feel free to bring them to get any questions about the exam. Those are great for Thursday to call Thursday. "
}