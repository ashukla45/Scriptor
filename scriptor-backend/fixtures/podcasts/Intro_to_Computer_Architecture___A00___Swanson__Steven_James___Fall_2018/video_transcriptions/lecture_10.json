{
  "Blurbs": {
    "2.242 and I have three decimal places to convince you that it's not the same I should have chosen larger number so that they would be like everything is not the right answer and I have this speed up so I can apply the L1 optimization that means this value changes because I no longer accounts for 3% Right. It's actually a little bit larger fraction because if I go ": [
      1214.6,
      1257.7,
      34
    ],
    "500 instructions beef and then they're looking for this instruction level parallelism across all those instructions trying to hide the latency is long latency memory operations and so forth. They told me was that the decode process which is right up here. This is about a 5% area power and performance overhead for Intel processors. So that's the cost of x86. It's pretty small nowadays. I noticed for their presentation ": [
      358.0,
      391.3,
      5
    ],
    "All right, let's get it started. So. As you all know the mid-term is in just a hundred and 68th hours. right Is it 168 hours? No, yes. 168 hours is 7 * 24. The seven days but it's not $160. Does anyone know why? Because you get an extra hour for daylight savings time. What's wrong with daylight quiet folks? We are actually having class. Thank you. 159 hours ": [
      173.8,
      224.4,
      0
    ],
    "All right. so we have all right. So now we're going to have to shift gears a little bit. so the processor that we have been talking about in this class are there called Von Neumann processors is because they were the first one was invented by a guy named John Von Neumann who is kind of a jerk, but he did kind of invent the digital computer so will ": [
      2144.9,
      2178.0,
      61
    ],
    "Fire and then eventually the store when we actually get the value of being here at the bottom and they are secured Wireless according to this data photographed it explicitly and codes all of them. So when we saw the first as a little similar to order superscalar does the same thing right to remember we said we should we have this instruction window and you look for the instructions ": [
      2645.7,
      2680.9,
      79
    ],
    "One of the earlier videos. I saw on the internet with someone cooking a process of cooking an egg on a processor 2004. This is like the peak of Madness as far as clock rates. We got to about three point six gigahertz 31 stages deep deep pipeline three-wide out of order 103 watts, and then we came back down 2006 and been impounded this spot ever since your processor ": [
      540.4,
      570.6,
      11
    ],
    "Pipeline. Since 04. So this is the i-46. This was released in 1989. So I heard that I had five state is it was actually pretty similar to the five-stage pipeline. It was one wide it was not out of order had one corner at 5 watts of power. So you could have your finger on the 46. No problem. It wouldn't even have been particularly warm computers didn't used ": [
      453.6,
      481.9,
      8
    ],
    "Right? So the obvious thing to do is we have SL2 that's the speed if we get for the L2 cache XL 2.3. That's the number of are the amount of time we spend in memory operations. This is the the thing this is the fraction of things that the Ellen does not help we divided by 2 because that's the the fraction of the remaining instructions that the L2 ": [
      1138.3,
      1164.2,
      32
    ],
    "So far we go through and do the math. Let's look at this in pictures. So this is drawn to scale. So this whole thing is the total execution time. execution time of the program 70% of 70% write 30% of our instructions are actually on time. So I said just a minute ago. It was the instruction counts not actually the execution time and that's going to be important ": [
      971.4,
      1009.5,
      27
    ],
    "Susan who won the the Nobel Prize of architecture. He now spends all of his time on his boat. He's very tan looks very Haggard. Anyway, so what we have we have 18 minutes left. Any questions about the midterm we can start the review early. Yes. Yes. Because I'm with the Greensheet is the front page of your textbook. I'll probably just be reproduced as part of the test. ": [
      3720.7,
      3759.8,
      117
    ],
    "The the processor was re Computing all of these dependents edges so that I could do the calculation about who is ready to fire. Can I bring a lot of power? I'm fetching things over and over again. That seems like kind of a drag. I'm doing a registry name and it's also Burns Power. And so if I represent my program, that's why I'm actually saving the processor or ": [
      2705.7,
      2725.5,
      81
    ],
    "There's a strict limits have to keep our way down there any ice an hour like 130 Watt, that's a big server processor 1 gigahertz vs. 1.6 gigahertz 1/4 vs 4 Cor No floating point in our make or send a little bit older arm cores now have floating point in then they have their they're out of order. Both of them. The Intel processor can do a couple more ": [
      660.7,
      687.1,
      16
    ],
    "They got an answer. That is not right. The answer this is a point one was. Hey, so this is 1 / 0.9, which is like 1.1 or something. So this is not quite the right answer. so what went wrong does anyone have a gas not the person that I talked to in my office about this who knows the answer? Animals law is a law. like a physical ": [
      1863.7,
      1902.3,
      55
    ],
    "Yes. Oh my gosh. You should definitely not memorize the names of control lines. That's like super useless. Yes. calculator How many of you actually own a stand-alone calculator? Yeah. All right. We will allow stand-alone calculators. No phones, no laptops, obviously. No, Siri, yes. I honestly do not know. Yes. Say that again. I would say I will accept a fraction of like one number over another number. That's ": [
      3759.8,
      3833.2,
      118
    ],
    "a couple months in my processor will just get 8% faster just through technology scaling now 8% is about two years worth of speed of so, they have this kind of cool idea where they could get about two years worth of performance boost by doing some clever stuff and how the processor manages memory. Staying in relevance for talking about in here clock Rafe and some other things on ": [
      421.2,
      453.6,
      7
    ],
    "a new micro of a new Intel processor about 30. He said people are in charge of doing the high level design and there's about 200 people that are in charge of actually implemented really building a circuits and testing and all that remains in Costa Rica number of the number we can bury the number of Pease and he's the man we can bury the number of instructions for ": [
      3170.1,
      3197.4,
      99
    ],
    "a trade-off among multiple different constraints. So here we are performance. Turn off the print off some of those lines are in this graph are they are the upper left boundary of the cloud of points. So we're print off a little sign what it says if there is no other design that is better by both of my metrics. So in this case, you know, this one is not ": [
      3579.4,
      3606.1,
      112
    ],
    "a whole lot of work. And maybe that'll make it more energy efficient and will also see their likes they going to be able to avoid fetching instructions all together or at least almost all the time. I'm going to skip ahead a little bit here. So we the one that we set about building a processor to execute these kinds of programs and the idea was to be abstract ": [
      2725.5,
      2749.4,
      82
    ],
    "actually seems pretty old. Now. This is an example processor from Intel Core i7 came out in 2009 or something like that. But we are watching the Intel processors is called to reorder buffer 128 entries in 2009. And now this is up to 500 entries in the latest processor. So the current Intel processors are there means they have to fetch and decode and Branch predict and other stuff ": [
      329.8,
      358.0,
      4
    ],
    "allocate so that way you would bring in programs write the code for a particular thread and you can put you can run one big thread by bloating in 30. Mm instructions from one thread or you can load them smaller threads and all sorts of different sizes. So you actually bring in the program and the program live in the processor with the machine. The program does not live ": [
      3085.8,
      3106.7,
      96
    ],
    "and it has some instructions that it's an operator's like Adam subtract and multiply and divide and they have some input these represent. The true dependents has right. So we talked about true to Kevin says in the instructions said so these are the true dependent there's if there's no subtract up here and maybe I multiply up here if there's a narrow here that means there is a true ": [
      2428.4,
      2449.2,
      71
    ],
    "animals law. This is sort of like pretty review for the midterm going to couple of you coming to talk to me about different questions about animals on the performance equation. Is there going to be a little bit of that then I am going to talk to you about a timer. Maybe I'll talk to you about a completely different way of building processors that will give you sort ": [
      836.8,
      857.7,
      22
    ],
    "are the whole ship is going to have lots of little chips instead of all my chips. Constraints constraints. Yes power would be a great constraints. Read it can't be too hot. I could also use power is Alter Ego energy. per instruction Maybe Cuz that's the kind of determines how efficiently I can execute what else any others. performance on crisis maybe I don't know how to run Crysis ": [
      3312.7,
      3352.2,
      104
    ],
    "as much math is willing to do while grading. Otherwise, you should just do that do the calculations. Yes. fair game You mean for me to add to the midterm Oh, yes, most definitely. I might have said to my Tas. Who are in charge of creating homeworks generate me six questions each based on the homework. So you've built that are similar but not identical. I might have said ": [
      3833.2,
      3868.2,
      119
    ],
    "at 1 right that's just the definition of speed up. So I can I do. I can rewrite us to a wine about it again by the definition of speed up to BT over. I just did this problem on paper to make sure I wasn't screwed up in class. 15 l Alright, so Tina just substitutes in. / a stoat one these cancel this comes up here and I ": [
      1465.9,
      1509.2,
      42
    ],
    "awful graphic design looks like from around 2006. And also we're going to see a different way of executing programs are also going to see how some of the stuff we've been talking about about performance modeling and trade-offs goes into designing kind of a new processor from scratch Princeton research. This is the kind of stuff that you might get to do in research. All of this particular product ": [
      2267.8,
      2293.7,
      65
    ],
    "back to this picture play go up here. This piece has the same but the overall size of this the toilets the kitchen timer shrunk, so it should actually be a little bit bigger. Right, and so that's why I got the wrong answer. So there are two ways to solve this will go through both of them. So the first one is to use a sort of generalized form ": [
      1257.7,
      1285.2,
      35
    ],
    "basic algorithm for how we executed Epps program instruction for memory. We run it through a Pipeline and construction of the next instruction and everything about pipe lining and out-of-order execution and everything else is just a way of making up process more efficient. The fundamental algorithm is still the same. Your program's execution is a very long list of individual instructions that appear to execute sequentially, right? So that's ": [
      2205.5,
      2233.6,
      63
    ],
    "but with that and no one else understands, maybe but like it sounds like an interesting idea. To that from you, so I don't know. Maybe you should go and think about it support. So will specific with architecture questions be on the midterm like x86 or like weight scale are that type of thing? There won't be I mean you need to be able to read mips assembly for ": [
      4266.9,
      4302.5,
      129
    ],
    "came out of his 14 stages for white out of order. It had two cars on a diet is often more coarse and I die now, we'll talk about that some more later and the power consumption actually came down to something a little bit more reasonable. This is more or less where we are today. These other machines. These are we'll talk about this more later. You need to ": [
      570.6,
      588.5,
      12
    ],
    "can't help and we get that XL 2 equals 0 3 we can plug that into 1.015. Right. Now if we're going to combine these two should multiply them together and so we will get 1.237. Yes. Sorry. We need what are you? There is no arithmetic are on the slide. There is a conceptual are on this side. So this is not right. So this is not equal to ": [
      1164.2,
      1214.6,
      33
    ],
    "case if I'm trying to do some trying to build a thing in the world, there are constraints I in summer. Yes. Cost I'm going to call that area because cost is basically proportional to actually it's more than proportional area because the bigger something gets the harder it is to manufacture a perfectly right. I'm so if I have a big big chip and I have like one little ": [
      3285.0,
      3312.7,
      103
    ],
    "clone Quantum bits. So that's like a that's one of the fundamental things. We actually had to move this like physical entity around. You know, I couldn't you couldn't like transmitted somewhere you had to move it so I can actually two atoms and they do Quantum dots and they do so But just what I feel like the colonists they won in 06. doctor we just saw this earlier ": [
      4189.8,
      4232.5,
      127
    ],
    "couple different directions. And then as I would say you just tile the Earth with clusters right until you can build something with thousands and thousands of processing elements that can hold thousands and thousands of instructions. And then we danced around the entire thing with L2 cache. So I have lots of memory around the outside. I'm so this thing was going to be 400 square millimeters. It was ": [
      3015.8,
      3036.8,
      93
    ],
    "data phone machines. There are a bunch of reasons that people can actually build these but it requires knowing all bit more about how memory works to see how that plays out. So they will come back to this later on in the quarter and if we go through here we can trace all of the dependence is the true dependence is between all of these instructions and they're actually ": [
      2536.5,
      2568.5,
      75
    ],
    "dependents between this multiply and subtract and this Edition operation know what's going to happen. Is that when we compute a value If I computed value it will get sent to this instruction. And it's instruction instead of to the basic algorithm fresh candidate program is not that I think it's the East instructions sits. By itself independently of all the other instructions and it waits for all of its ": [
      2449.2,
      2483.4,
      72
    ],
    "divided sections is 2 and 34 and then what is the expected speed up? So the first thing that we need to figure out in this problem is what is the CPI of the other instructions? So we can do that by we have the CPI for the whole program. We know that you compute the CPI for if we don't want to the formula for computing the average CPI ": [
      1648.2,
      1679.6,
      48
    ],
    "ever in this there's no false dependents is here and we could draw a graph. So there's a graph that graph represents that complication on the left and then a dataflow instruction sets. It's not some funny thing. We're like, there are some arrows in here that you could draw on these arrows kind of emerged as a program executes. This is actually the way that you represent the program ": [
      2568.5,
      2591.8,
      76
    ],
    "facility because we'll be designing Anthem in a processor during the midterm. So I think they have them at the bookstore. They cost three billion dollars each, but you can sell them back to use for just a billion dollars so you can recoup most of it later and yourselves. All right. That is all schools out early. Religion of Nod to relative quantum computer. So I think some of ": [
      4045.0,
      4099.8,
      124
    ],
    "find that the speed up is 1.25. All right. About the spinner. And remember this problem was way back. Yes. Sure. So here than what I need to figure out so it's hard to hear what they're what's going to think about. This question is that I don't know everything that I usually know. I don't know what the CPI is it all the other instructions, which is usually how ": [
      1739.1,
      1779.1,
      51
    ],
    "for defining best. So what would be one metric we could use to find the best? Yeah. performance On benchmarks performance build the fastest processor possible for waves Kayla. That means I'll just build the biggest processor possible. I mean certainly would have more space to swap things in and out very much. So performance can't be the only choice. The only thing I'm worried about this is usually the ": [
      3243.9,
      3285.0,
      102
    ],
    "get that X new equals x 2 x Esther 1. All right, so that's the new. coverage for X2 balkanization to and then I can just apply Imo's login with the new value of X2. directions and then if we go through and do it Hello. Thank you. We get so this is the math we had in the previous slide to get 1.295. Then we calculate the new x ": [
      1509.2,
      1544.6,
      43
    ],
    "going to be Whole 30 mm instructions and it could run about a gigahertz. So by today's standards that would probably be running at this is 90 nanometer an hour at 14-nanometer. So if we built this again this might run out of 7 times faster, so maybe this would be 7 gigahertz. Except it would melt. That's not good. That was a processor. So our claim with this was ": [
      3036.8,
      3065.5,
      94
    ],
    "good or was bad and so once we did first weekend of search by hand, we should have made some gases that seems reasonable or we could try to do a complete and systematic search. And try to find the best processor. Now what? How should we measure this? We're trying to find the best way of scalar processor when we talked earlier about some metrics that we could use ": [
      3219.5,
      3243.9,
      101
    ],
    "happen to waves together there a couple of fall on papers and then all the students graduated went on to bigger and better things. One of the students is like VP of Technology mongodb guys know about mongodb. Yeah, so he's in charge of that another one of those students is a professor at Columbia University in New York. Another one teaches the University of Toronto. My other Weiser Not ": [
      3695.0,
      3720.7,
      116
    ],
    "has already been done so hands-off. Play Skillet has finished. All right between 2003 and 2006. This is right way and conventional Prosser designs were hitting the Powerball and everything was kind of falling apart. So that's the kind of thing. We were concerned about these chip multiprocessors or you build lots of processors on a single die were very popular ever is very excited about them. And what are ": [
      2293.7,
      2329.0,
      66
    ],
    "here we had pretty quick communication so we can pass stuff ran inside of domain. This is a big wide interconnect that we have there. And then we just read about this is for making the memory go faster. This is a thing called a story about 4, then we'll talk about later. That's also for memory, but you can talk out of these out of the cluster in a ": [
      2988.6,
      3015.8,
      92
    ],
    "ideas super time. I may be on the scale of like a strip down even version of the NIT score of the first five stage meth pipe light and that we'd studied by the standards of 2003 and 2006 are they is super tiny so we'll give each of each instruction. The program will have its own ale you write will have if there's a billing instructions in front of ": [
      2749.4,
      2773.7,
      83
    ],
    "in a datafile inspection set how many people here use Pearl nunu's Pearl a couple of you. It was awesome and had a theme song because it was so slow. It was the Chariots of Fire theme song If you know what that is, but one of my mates actually when you ran it would play Chariots of Fire through your computer because it took so long to run the ": [
      2591.8,
      2620.3,
      77
    ],
    "in the processor lives in the memory and weight scaler. It actually lives in the processor itself. And then we would do if there's there's always more than you can fit so we would have this way of swapping them in and out. What is the thing called cashing which will see certain example of a memory as well? It took a long time there like five of us working ": [
      3106.7,
      3134.3,
      97
    ],
    "in this example the next one. So this is where here is the fraction of the program that the L1 speeds up. So this is 80% This is beside it is 80% this is 80% of 30% which comes out to 24% of the total execution time. This is the L2 time the fraction of time that LDL to can speed up and this is a fraction of time but ": [
      1009.5,
      1046.8,
      28
    ],
    "independently of all the other instructions in the program. And so there's no centralization essential serialized bottleneck. We can have massive parallelism, right? We can execute thousands of instructions at once maybe. And they're like an apparently so then the question you could ask yourself is like where are the dataflow machines? Right if it's so awesome what happened to them? I'm not going to talk about why there no ": [
      2509.7,
      2536.5,
      74
    ],
    "inputs to be available right here waiting doing nothing and it seems one value come in and tell me what that is not enough for me. I need to values and when the second value appears, then it can fire. We called this instruction firing and then executes and it sends its result on waiting about this is that it's totally decentralized write each and each instruction is making decisions ": [
      2483.4,
      2509.7,
      73
    ],
    "instruction sit over here. The data sets over here and one of value comes in if we get a value in this slot or they have the destruction here we go to Value here in about you here in about you hear through the ale you and I will pop out the bottom it'll flow on all tell the other instructions. So I'm here with leave, but at least Gaylor ": [
      2862.4,
      2890.3,
      87
    ],
    "instructions are memory operation writing laws about speed up speed up is about latency. And so Anthony to be working in seconds Can erase all this? Let's try again. so we need to know is how much time we are going to spend doing divide instructions? So the way we can do that. Is we can apply the performance equation just to The Divide instructions? And we know that so ": [
      1949.5,
      2006.9,
      57
    ],
    "instructions per cycle. Pipeline depth is about the same. This is more aggressive out-of-order execution was speculation just called Static in order. That means that. Basically, there's some they don't do fill out of order and they just started manage some houses that show up there goes to level. This one is a lot bigger though, and then we'll talk about the memory systems later on, but you can see ": [
      687.1,
      716.6,
      17
    ],
    "is a separate outputs Jaden Smith. 64 instructions can sit there at one time. We took two of these and we stuck together. My advisor was extremely happy with this pun advisor like that. We are two peas in a pod size are her name is Susan Eggers. one of my two advisors She just one like the Nobel Prize in our computer architecture a couple weeks ago. So she's ": [
      2914.1,
      2942.5,
      89
    ],
    "is homework one for my think homework problem 1 4 homework 3, so we have a process or a program that runs for 30 billion instructions that takes 20 seconds on a 3 gigahertz machine on the program consists of 20% divided instructions. And as a CPI, and then the CPI of a divided section is for so the first question which is not going to go through any Taylor ": [
      1596.5,
      1623.5,
      46
    ],
    "is kind of a same feeling as It's fitting in certain that things are not as they are. I'll come by in the same time. How often should I say? So here's I mean, I don't know but here's the here's the thing. The way that like great research projects are born is that people have these like brain waves in their head that are like this is like this ": [
      4232.5,
      4266.9,
      128
    ],
    "is the processor design that we came up with. So this is a big city of these things called processing element. That's what we called them. And they are a little piece of Hardware that connects several instructions, which one of the instructions to sort of independent of all the other instructions are so you can think of this as a big sea of instructions just waiting for their inputs ": [
      2801.0,
      2821.8,
      85
    ],
    "it go about calculating things. So when I get over here what I need if I want to ride down the performance equation for one of these I need this term separated out which means I need the CPI of the other instructions and the situation where I know that answer is back here in the original version of the problem. So I start off for the CPI is equal ": [
      1779.1,
      1802.3,
      52
    ],
    "it stays there. It said that it waits for more inputs to come right? It could execute again. I can be in a loop and we'll execute it a second time and some more values will show up and we'll execute again. It's a 5-stage pipeline. There's no decode stage. So there's no decode. There's no fetch right because he was already there in like three or four stages here ": [
      2890.3,
      2914.1,
      88
    ],
    "just seems like a little bit of magic. So I'm going to write down first a text to equals x 2. We all on board with that part. That was easy part 1 Then I'm going to rewrite 1 as X2 is T / t. Alright, so this all this is sort of tautological E True when so this is the total execution time Big T is the total execution ": [
      1390.2,
      1419.7,
      40
    ],
    "just to figure out the average CPI of the program. And so you can just plug it use the performance equation to plug in those numbers and you get the average CPI is to That's previous routes. You need that in a moment. And then we have a new breakthrough technology that reduces the CPI divide instructions by a factor of 2. So the new the new CPI is 4 ": [
      1623.5,
      1648.2,
      47
    ],
    "kind of things you get to do in graduate school computer sees the program an execution goes like this. So the inputs come in from somewhere up above. Not shown and they flow through here. So the first Ranch all of the first of the first step all the multipliers have all their input. So they fire then all the ads fire next up the load in the puss in ": [
      2620.3,
      2645.7,
      78
    ],
    "know, one of these is going to be best by that measure and that might give us another way to evaluate them. All right. That might be it. That's all there is about wife's killer any questions. Who would buy a wave scanner processor? Oh. My poor thesis no one wants to buy my thesis. Okay, I got me a job. Alright, so what happened to waste go nothing really ": [
      3650.1,
      3695.0,
      115
    ],
    "law So what what does who remembers the definition of speed up? That is right. So there the definition of speed up. old over new and these are latent he's So these are seconds. not instructions So this is why back on the previous example, I went back and I talked about how it's the memory operations take 30% of the program's execution time. Not that 30% of the program's ": [
      1902.3,
      1949.5,
      56
    ],
    "life after is the current list prediction for each processor? ": [
      4333.3,
      4337.6,
      131
    ],
    "low-complexity because it's built out of some really simple building blocks. Right? All you need to design really is this thing and then you just stamp out a whole bunch of them is much much easier to Big going to be complicated out of work or For the way this would work just a little bit of a lots of them flexible parallels and flex my location that you could ": [
      3065.5,
      3085.8,
      95
    ],
    "many millions of dollars and they will have hundreds of fpgas in them and they will come with the software that lets you take something like an Intel, you know, the latest Intel which has I don't know how many billions of transistors and will map it into a big box of fpgas and you can run it but that's how much fpj Hardware it takes to do something equivalent ": [
      3966.0,
      3985.5,
      122
    ],
    "memory operations by a factor of 4? So some fraction of our instructions are memory operations and some fraction of those memory operations. It speeds up by 80% I actually should say it of the remaining. Rightful is remaining 20% speed up half of what's left over so half of the memory operations, but the First Cash didn't help this one helps and its speed those up by a factor ": [
      908.4,
      944.4,
      25
    ],
    "messy x86 instructions and they dynamically convert them. Hello. Dynamically convert them into things that look a lot like mix instructions that we've seen. All of course and they take a throw them into the pipeline metal happens often to decode stage and then the process of more or less behaves like the process that we've seen so far there a law that be pipelines or white sugar out of ": [
      257.5,
      292.2,
      2
    ],
    "millimeters. This is really big for a processor not inconceivably big but it is pretty big and then we came up with a total of 201 reasonable designs. And then we went to evaluate them. So here are all the designs. And I plot of them so this is area. So we're optimizing for area versus something called aipc. So this is actually this is very relevant. We've seen this ": [
      3376.5,
      3405.1,
      106
    ],
    "nothing can speed up. It's just slow. So now all we do is we speed up this by a factor of 4. And then we speed up this. Five factor of 2 and then we will get there and execution time is 08505. And if we do the speed if it comes out to 1.242. What's the actual speed up? That we get by a flying instructor positions at the ": [
      1046.8,
      1082.4,
      29
    ],
    "of 2. price of a level 1 Cache bl1 cast speed up some of the instructions so the memory operations 80% of them and they all two speeds up 10% of the remaining teeth in as soon as I buy factor to so what is the total speed of right now? We have two different optimizations ever going to perform at the same time to different parts of the program. ": [
      944.4,
      970.4,
      26
    ],
    "of Campbell's law. And so I'll just write it out here. So it will be easier to see but if I have two optimizations I can write us toad equals one over. X1 / X1 + X2 / S2 Plus One see now. I ran out of space 1 - x 1 - 2 All right, so just expanded out the thing on the bottom and now I am going ": [
      1285.2,
      1315.6,
      36
    ],
    "of an idea of what you know, what the alternatives are to the living building today and also show you how some of the metrics and measurement stuff. We've looked at plays out in the in the real world. So here is there some slides I cut for my original lecture on animals law will go through them now because they find out some useful stuff. So this is example, ": [
      857.7,
      882.6,
      23
    ],
    "on Wave scalar. The truth is wife's killer in a very small number of carefully tuned workload. So we were definitely going to run Crysis. So the constraints we used. We fix the cycle time. Just kind of we only had one design. I'm so redoing the cycle time would have really complicated. We fix an area budget. So we fix the maximum size of the chip would 400 square ": [
      3352.2,
      3376.5,
      105
    ],
    "on it took I don't know three or four years. so one of the things that we did Is we had this design? Sorry I added at least like the earlier and then my computer crashed so All right. So we had we had to figure out what the right design parameters were. So if I'm designing a new processor people 30 people are in charge of the design of ": [
      3134.3,
      3170.1,
      98
    ],
    "one is best for Splash to? Yes. oops This one up here. The very biggest processor with the very biggest. Well, you said the biggest one that is not actually the biggest one. the biggest one Is here that's the biggest one. Is that the best one? Now that is definitely not the best one. I'm the best one of the fastest one is up here. What if I didn't ": [
      3505.1,
      3541.4,
      110
    ],
    "one program counter over and over again and it was the one place that I have to do all this complication Branch prediction and know the outcomes of my branches and so forth affair with an X values going to be it's also fun. You can imagine that if I wanted executed lots of things at once that's not a great place to start from right so I might want ": [
      2379.1,
      2401.9,
      69
    ],
    "order that floating Point Judith and Becky and everything else. We had some visitors here yesterday from Intel talking about some of their new processors some interesting facts the latest processor from Intel. Remember we talked about me see if I can find these Where is this Thursday prepper all shy the next? slide so I skip a little bit. So this is the this is the latest this is ": [
      292.2,
      329.8,
      3
    ],
    "p and there are a couple of things we can do as well but there were thousands and thousands of different proteins potential different designs that we could build and we wanted to find the best ones. We've been building a lot of intuition about what a good processor would look like because we've been building them for 30 years. We have no intuition. We didn't really know what was ": [
      3197.4,
      3219.5,
      100
    ],
    "part of the sequential execution is preserved. So that's not the only way to build a processor. So I'm going to tell you about another way to go to processor and there's nothing processors built like this for a long time. I have some vested interest in building processors in this way. This is what my PhD dissertation was about. What is going to show you both? What's a really ": [
      2233.6,
      2267.8,
      64
    ],
    "pay attention right now. They take some other interesting things. What is the start of where we are 15 between 15 and 20 stages find the lower end of that for pipeline jobs? And that's kind of our building today is that the number of cores is increasing we'll talk about that and also things like the number of instructions in the instruction when they're also increasing. So I was ": [
      588.5,
      617.4,
      13
    ],
    "physical constraints. I guess it is sort of you would take these ions that held cubits and you would sort of moving around and so you would ship them from one place to another so that since it was quite similar like a Quantum processing pipeline. That the reason that doesn't work is because you think about a pipeline They're copying data a lot in the pipeline, but you can't ": [
      4160.3,
      4189.8,
      126
    ],
    "ready to fire Billy and transistors or something. So this is a processing elements and weight scaler. It's really simple takes about half-a-million transistors to build. This is simpler than what you're going to build in 141. I'll probably use here and these are aware of the values flow in there's an actually there's one sort of slice across. This is a single instruction. And basically this is the the ": [
      2821.8,
      2862.4,
      86
    ],
    "right because the coverage here is changed because I have accelerated I've ever do stay over on execution time of the program and X2. It turns out the new x 2 equals x 2 times the speed up from flying the first appointment first optimization. How do I get there? So I'm going to do the kind of algebra trick that I used to hate in math class because it ": [
      1363.7,
      1390.2,
      39
    ],
    "right now there's one of two dominant families of our processors. There is arm, which is on your phone's remember I said, this is everything smaller than your laptop is running arm. Everything bigger than your laptop is running in quite a big difference right there too hot. So if it's your phone your phone actually has a very strict power budget because if you take the number of Watts ": [
      617.4,
      639.4,
      14
    ],
    "same time as all about applying differin optimization optimization support the program's what's total hours to come up with this answer right? We shouldn't have to go and draw these pictures. So let's do this the first way be kind of obvious way. So we have two optimizations we have is one a next one. That's one is the speed up for the L1 cache xs-1 is a speaker for ": [
      1082.4,
      1110.1,
      30
    ],
    "service bar Jackie? Fpga is are very inefficient in terms of energy and performance for implementing CPUs. They're really good at implementing particular circuits, which is what they're mostly used for. So we built we prototype prototype to take a lot of fpga. So like I don't know what the divisor is where you can buy these big boxes are like the size of a refrigerator cost. You know, how ": [
      3921.3,
      3966.0,
      121
    ],
    "share a secret with you because I have not written the midterm yet. So I can't say for sure that that will be the case, but it's certainly something that can happen. Alright sure thing. I was I want to go over with one of the homework questions in a couple of people came and asked me about minutes highlights are useful. Kind of subtlety of Amber's law to this ": [
      1574.5,
      1596.5,
      45
    ],
    "so a i p c stood for Alpha. IPC Who are members of the coolest processor ever built was? It was the alpha. So when's killoran the kind of Leverage an existing instructions that works with the alpha instructions said I didn't explain why but we had to add some extra instruction therapure overhead instructions. They do things like Implement branches branches are really complicated to get right and so ": [
      3405.1,
      3441.8,
      107
    ],
    "so these are all the long latency operations like floating points and Vector operations, and then throw the first stage of the pipeline. It's all the simple stuff and they have a few functionally as it can get memory operations. There's again there's a memory stuff in here that we're going to talk about next quarter. And this is I think you know, this is 15 stages. I believe is ": [
      777.8,
      802.0,
      20
    ],
    "so we can break it out by parts. We have .2 is divided and point H is the rest of it. And then if we solve for the other stuff, we will get to the other stuff. The average CPI is 1.5. Tell the next part of the question. Is the expected speed at this so this is the exam. This is the answer from the key performance equation. And ": [
      1679.6,
      1710.5,
      49
    ],
    "so we have a situation where we have 30% of the instructions in our program are memory operation. So they Lowe's in stores. We have a new widget call the cash to talk a lot about in the second half of the class, but they were just going to call the cash, you know exactly what it does. But what it does effective. Is it at speeds up 80% of ": [
      882.6,
      908.4,
      24
    ],
    "started getting a lot faster 1997 200 megahertz 10 stages 3 wind out of order 30 Watts. 2001 terrain of 2 gigahertz 20 suitcase 22 stages d-33 wide out of order still one course 75 Watts. So somewhere in here things became too hot for you to touch a process that was running it would burn your finger. You could actually like cook an egg on a prostitute Harrison videos. ": [
      508.2,
      540.4,
      10
    ],
    "super famous and very nice his his only adviser is Susan tiger. So she also invited us into you just don't hyper-threading. So we could take Four Paws. We can buy them in something called The Domain here. So we so within us within a single PE. What we try to do is we try to put instructions that were you know, if I had an instruction here. I need ": [
      2942.5,
      2969.9,
      90
    ],
    "sure. Maybe write some simple nips assembly. kind of pseudo nips assembly I don't know what I could possibly ask about where she's going. I don't know. There's no benefit if if we do anything about x86 will give you enough information to know that the instructions for us to do a problem. So I put pressure on my phone number six. So are we going to rain the penalty ": [
      4302.5,
      4333.3,
      130
    ],
    "that have all of their their dependence is satisfied and you execute those instructions. So that is a date of execution model. Remember we had to do to get there every time we wanted executed instruction. We had to fetch it out of memory. We had to decode it. We had to rename the registers and then we had a drop of the instruction q and every cycle every cycle. ": [
      2680.9,
      2705.7,
      80
    ],
    "that there's a lot more memory on the chip for the Intel processors for the arm processor. We can look at a couple of these pipelines little bit more detail. So here we have another thing we talked about the front and the back end. So is about three stages of fat should an arm processor that is is a bunch of decode that takes about three cycles. And then ": [
      716.6,
      738.6,
      18
    ],
    "that they were telling us about this new effort and they were very excited because they got an 8% speed up used to be that 8% was not really enough to get too excited about if you were computer architecture research her because back when performance was increasing at 60% a year 8% is like a month of speed up. Brian sounds like I don't care I'll wait, you know ": [
      391.3,
      421.2,
      6
    ],
    "that x equals. 8 / 20 which equals point for that we need for MBA. Is 1 / 0.4 / 2 cuz up to speed up plus point 6 which is 1 - 24 and that answer comes out to be 1.25. Thank goodness. Although if we had found a counterexample to andaz, Lobby would be famous. roll guitar picture on the cover of Byte magazine or something. Very exciting. ": [
      2096.7,
      2138.4,
      60
    ],
    "that your phone dissipates that means the processor in the memory in the radio in the screen and everything if that gets above some number, I don't know exactly what it is. You can take the number of Watson / the surface area of your phone and if that gets hot enough that number gets high enough watts per square centimeter will become too hot for you to touch right. ": [
      639.4,
      660.7,
      15
    ],
    "that. I may or may not use them. But I might have said that also all of the reading quiz questions are fair game as well. As is all the content that you've read in all of my lecture notes, so, you know. other question Yes. Oh, I think in the slides. Any questions about computer architecture? Maybe yes. Oh, yeah, there's someone. Do you think that the most modern ": [
      3868.2,
      3921.3,
      120
    ],
    "the alarm cash X1 is the amount of the program that affects so this is point to form and that is 4X here is the equation for angles law and we will get out to feed speed up for the L1 cache. I think if we go back here, I was like, this is 1.219 V. X I'm so that works out fine. And then we can apply the L2. ": [
      1110.1,
      1138.3,
      31
    ],
    "the problems was? It look like we were going to build more and more of these chip multi-process really like Hunters of course and a chip and nobody knew how to program them because there were just too many threads. And so we had this idea that we were going to adopt a new kind of execution model. For how programs were going to execute the next instruction. I'm going ": [
      2329.0,
      2357.9,
      67
    ],
    "them a billion ale use the Hub direct communication between. Some Communication channel so that when they produce one of them produces a value it can send it to its friends that need it and then everything will be great week sometime. Practically we can't build billions of values. We can we maybe build thousands of Ale use a few of them and will reuse them somehow. That's so here ": [
      2773.7,
      2801.0,
      84
    ],
    "them are sad thing is kind of love feels like Wheeler Dealer seems similar to Quantum computational I don't know how the current quantum computers work. I know there has been my one of my advisers actually has been a lot of architecture work on computers. Like how do you move give it around like operate on them and so forth and it's totally different because I'm very very different ": [
      4099.8,
      4160.3,
      125
    ],
    "there's several pipelines kind of parallel pipelines for doing the various arithmetic and logical operations casino. This is the longer multiplies and so forth. I might take three or four Cycles. Here's the Intel processor look for the silver already, but they have this big reorder buffer a bunch of functional units and you can see so hundred and twenty eight bits hundred 2828 bit. SSL leaders are labeled. I'm ": [
      738.6,
      777.8,
      19
    ],
    "this one is pretty optimal because there is nothing that is both smaller and faster, right? We're sort of at the upper boundary this one the biggest one is not afraid of optimal because there are a whole bunch of designs that are up and to the left from that one. So you can pop them all out. They go from some very small ones some very big one and ": [
      3606.1,
      3629.5,
      113
    ],
    "time if I want to go to calculate the new x22 this thing is going to stay the same because that's the amount of time. Covered by X2 so I can write x 2 x Big T. But this tea is going to be smaller because I have some speed out that I got from applying the first optimization. So teeny, This is over tinu tinu. equals T / best ": [
      1419.7,
      1465.9,
      41
    ],
    "to 4 I use that to break out the CPI of everything else and then I know that that's going to stay the same and then I apply the optimization to the just to divide instructions. All right in the questions on this part. All right. So now the question is right. So at least one person who can you talk to me once through and try to do this ": [
      1802.3,
      1825.2,
      53
    ],
    "to a modern processor. It's a lot. All right, as I said my algorithm for a few sessions is it when people stop asking questions I leave. So we are going to see each other question. Yes. Hellion You don't have the answers. interesting point I will. what to do about that I will provide some answers for the reading quiz. Yes. You need to bring a 13 mm fabrication ": [
      3985.5,
      4045.0,
      123
    ],
    "to break out each of the pieces for the individual optimizations and computer speed up. I subtract all of them from the whole this is still the leftover piece and then I take the inverse and if we go through and do all that math, we will show that we get 2.1.2 for 2. I don't get the right answer cuz you're applying both of them simultaneously. That's one way ": [
      1315.6,
      1338.8,
      37
    ],
    "to cycle time is 1 / 3 billion. Find Catherine do this math. I have it handily recorded here that this. and that equal to It alright to the CPI for the execution time for divide instructions is 8 Seconds. Now I can go through and I can then calculate the the coverage for the instructions, so that is just ax equals 8 * 20 Carol I can't even read ": [
      2047.7,
      2096.7,
      59
    ],
    "to do it. The other way to do it is we can actually apply them. Seriously. We just need to be a little bit more clever about it. So here is the kind of iterative method. So I have to use speedups to coverages X1 and X2 into speedups S1 and S2. I can apply us one. That's just normal and Ohs law. I need to calculate the new X2 ": [
      1338.8,
      1363.7,
      38
    ],
    "to execute and I can come at this one at a time and where that program counter goes. That's the sequence of instructions in my program is going to execute someone problem with this is that the program counter is fundamentally centralized. There is one program counter that lives in one place on the chip. This is why Branch prediction is important because I'm updating this one. I'm updating this ": [
      2357.9,
      2379.1,
      68
    ],
    "to have fans. I definitely didn't used to have big heatsinks. The Pentium came out in 1993 has an important transition you can see that until I got in the marketing bandwagon because Pentium sounds much cooler than 486. So that the Pentium about twice as fast as the same pipeline. There was too wide not out of order in about 10 wants the Pentium Pro. This is when machine ": [
      481.9,
      508.2,
      9
    ],
    "to start with a model that has a little bit more inherent parallelism in it. Show me alternate with something called data flow. We didn't invent data for it was actually better than 1975. I got injected us. Batavia is that we're going to store program not as a sequence of instructions and will walk through with the first encounter. We are going to store it is just a graph ": [
      2401.9,
      2428.4,
      70
    ],
    "to talk to her instruction. I would try to put both of these into the same PE and then they could communicate with each other. This is a forwarding Network. Right? We have the same forwarding idea just like we did at MEPS to affording Loop their the peas good for and stuff to each other so we could get some kind of parallelism in short communication there and then ": [
      2969.9,
      2988.6,
      91
    ],
    "to the next to equal 0.036 V. And then we apply us to just using handles law we get this and then we take the product of this. Not and we get 1.2 for 2. Alright, so there are two methods. I would not be surprised if there was a problem on the midterm about calculating the effective to optimization simultaneously that is something we could totally happen. I will ": [
      1544.6,
      1574.5,
      44
    ],
    "want to spend 400 mm if I wanted a smaller processor? What if I wanted a 200 square millimeter processor better phone is best. That's the one right here. So in general the the good designs. Play around this boundary. so at least have a very particular name, these are called the parade o optimal designs Sofrito optimality is a very useful concept when you're trying to make up for ": [
      3541.4,
      3579.4,
      111
    ],
    "we can see t or c e t i c e x c e p i x c t. All right. So that is the performance equation the instruction count. What is the instruction count of divide instructions? It is what point two times. 30 billion that's in section count. What is the CPI for divide instructions? It is for that's in the problem. And what is the cycle time ": [
      2006.9,
      2047.7,
      58
    ],
    "we wanted to make a compare performance using IPC which is instructions per cycle. It's one of her CPI towel Architects off and measure performance. What if we just compared instructions per cycle on Wave scalar to instructions per cycle on a conventional processor Advantage because even though you're useless, right? So Alpha IBC was the number of equivalent Alpha instructions such the start of a measure of how much ": [
      3441.8,
      3472.2,
      108
    ],
    "what p.m. The book talks about. All right, so that brings us to the end of processor design. Are you all ready to go and build your own processor? I hope so because some of your in 141 L. How is 141 L going? Going great. going poorly So we're going to do for the rest of today. Cuz I'm going to go through a couple of more problems with ": [
      802.0,
      836.8,
      21
    ],
    "what you see is that we have the two sisters the instruction count at the cycle time are the clock rate. And then what we do is we take the for the old we have the old CPI for the Divide instructions divide CPI. We have the everything else and then for the new CPI for the same thing for everything else. I need to go to the math. You'll ": [
      1710.5,
      1739.1,
      50
    ],
    "who doesn't like daylight savings time. people on the ballot Interesting. All right. So I'll last time we were talking about how people make x 36 go fast. But quiet please one conversation. I can hear lots of Europe Saint Severin else can hear you two. So how you make x96 go fast and the kind of amazing trick that they do is that they take all of these Very ": [
      224.4,
      257.5,
      1
    ],
    "with animals law which totally seems like it should work and in fact, it should work because by law. And we never break handles law. We always heat and has lost 20% of the instructions are divided instructions. So they said that I stole it was going to equal 1/2 / 2 cuz that's a speed up + 1 - 0.2. And they do this when I didn't calculate this. ": [
      1825.2,
      1861.8,
      54
    ],
    "work and how much useful work. The processor was doing a IPC. That's the last time I say IBC we can get this is the area that we are spending. And the green dots are all the designs. Which one is best? Yes. Good question or good point this is on so this is on a benchmark Suites called Splash to to the parallel benchmarks late. So let's say which ": [
      3472.2,
      3505.1,
      109
    ],
    "you know give him a little bit of a pass and the key feature of a Von Neumann processor or if I know Computer is that you store the the instructions and memory and then you fetch them into a processor and you execute the instructions one at a time and you just keep doing this until the program stops. And that's how the processor run. So this is the ": [
      2178.0,
      2205.5,
      62
    ],
    "you know, the question of whether whether one of those is better than another really depends on what you want. Like, what is your threshold in terms of performance or what? How much are you willing to spend or maybe how much power like we get out another access here? We can measure all these by power efficiency or maybe even better would be energy. So energy per instruction, you ": [
      3629.5,
      3650.1,
      114
    ]
  },
  "File Name": "Intro_to_Computer_Architecture___A00___Swanson__Steven_James___Fall_2018-lecture_10.flac",
  "Full Transcript": "All right, let's get it started. So. As you all know the mid-term is in just a hundred and 68th hours.  right  Is it 168 hours?  No, yes.  168 hours is 7 * 24.  The seven days but it's not $160. Does anyone know why?  Because you get an extra hour for daylight savings time.  What's wrong with daylight quiet folks? We are actually having class. Thank you.  159 hours who doesn't like daylight savings time.  people on the ballot  Interesting. All right. So I'll last time we were talking about how people make x 36 go fast. But quiet please one conversation. I can hear lots of Europe Saint Severin else can hear you two. So how you make x96 go fast and the kind of amazing trick that they do is that they take all of these  Very messy x86 instructions and they dynamically convert them.  Hello.  Dynamically convert them into things that look a lot like mix instructions that we've seen. All of course and they take a throw them into the pipeline metal happens often to decode stage and then the process of more or less behaves like the process that we've seen so far there a law that be pipelines or white sugar out of order that floating Point Judith and Becky and everything else. We had some visitors here yesterday from Intel talking about some of their new processors some interesting facts the latest processor from Intel. Remember we talked about me see if I can find these  Where is this Thursday prepper all shy the next?  slide  so I skip a little bit. So this is the  this is the latest this is actually seems pretty old. Now. This is an example processor from Intel Core i7 came out in 2009 or something like that. But we are watching the Intel processors is called to reorder buffer 128 entries in 2009. And now this is up to 500 entries in the latest processor. So the current Intel processors are there means they have to fetch and decode and Branch predict and other stuff 500 instructions beef and then they're looking for this instruction level parallelism across all those instructions trying to hide the latency is long latency memory operations and so forth. They told me was that the decode process which is right up here.  This is about a 5% area power and performance overhead for Intel processors. So that's the cost of x86. It's pretty small nowadays. I noticed for their presentation that they were telling us about this new effort and they were very excited because they got an 8% speed up used to be that 8% was not really enough to get too excited about if you were computer architecture research her because back when performance was increasing at 60% a year 8% is like a month of speed up.  Brian sounds like I don't care I'll wait, you know a couple months in my processor will just get 8% faster just through technology scaling now 8% is about two years worth of speed of so, they have this kind of cool idea where they could get about two years worth of performance boost by doing some clever stuff and how the processor manages memory.  Staying in relevance for talking about in here clock Rafe and some other things on Pipeline. Since 04. So this is the i-46. This was released in 1989. So I heard that I had five state is it was actually pretty similar to the five-stage pipeline. It was one wide it was not out of order had one corner at 5 watts of power. So you could have your finger on the 46. No problem. It wouldn't even have been particularly warm computers didn't used to have fans. I definitely didn't used to have big heatsinks. The Pentium came out in 1993 has an important transition you can see that until I got in the marketing bandwagon because Pentium sounds much cooler than 486.  So that the Pentium about twice as fast as the same pipeline. There was too wide not out of order in about 10 wants the Pentium Pro. This is when machine started getting a lot faster 1997 200 megahertz 10 stages 3 wind out of order 30 Watts.  2001 terrain of 2 gigahertz 20 suitcase 22 stages d-33 wide out of order still one course 75 Watts. So somewhere in here things became too hot for you to touch a process that was running it would burn your finger. You could actually like cook an egg on a prostitute Harrison videos. One of the earlier videos. I saw on the internet with someone cooking a process of cooking an egg on a processor 2004. This is like the peak of Madness as far as clock rates. We got to about three point six gigahertz 31 stages deep deep pipeline three-wide out of order 103 watts, and then we came back down 2006 and been impounded this spot ever since your processor came out of his 14 stages for white out of order. It had two cars on a diet is often more coarse and I die now, we'll talk about that some more later and the power consumption actually came down to something a little bit more reasonable.  This is more or less where we are today. These other machines. These are we'll talk about this more later. You need to pay attention right now. They take some other interesting things.  What is the start of where we are 15 between 15 and 20 stages find the lower end of that for pipeline jobs?  And that's kind of our building today is that the number of cores is increasing we'll talk about that and also things like the number of instructions in the instruction when they're also increasing.  So I was right now there's one of two dominant families of our processors. There is arm, which is on your phone's remember I said, this is everything smaller than your laptop is running arm. Everything bigger than your laptop is running in quite a big difference right there too hot. So if it's your phone your phone actually has a very strict power budget because if you take the number of Watts that your phone dissipates that means the processor in the memory in the radio in the screen and everything if that gets above some number, I don't know exactly what it is. You can take the number of Watson / the surface area of your phone and if that gets hot enough that number gets high enough watts per square centimeter will become too hot for you to touch right. There's a strict limits have to keep our way down there any ice an hour like 130 Watt, that's a big server processor 1 gigahertz vs. 1.6 gigahertz 1/4 vs 4 Cor  No floating point in our make or send a little bit older arm cores now have floating point in then they have their they're out of order. Both of them. The Intel processor can do a couple more instructions per cycle. Pipeline depth is about the same.  This is more aggressive out-of-order execution was speculation just called Static in order. That means that.  Basically, there's some they don't do fill out of order and they just started manage some houses that show up there goes to level. This one is a lot bigger though, and then we'll talk about the memory systems later on, but you can see that there's a lot more memory on the chip for the Intel processors for the arm processor.  We can look at a couple of these pipelines little bit more detail.  So here we have another thing we talked about the front and the back end. So is about three stages of fat should an arm processor that is is a bunch of decode that takes about three cycles. And then there's several pipelines kind of parallel pipelines for doing the various arithmetic and logical operations casino. This is the longer multiplies and so forth. I might take three or four Cycles.  Here's the Intel processor look for the silver already, but they have this big reorder buffer a bunch of functional units and you can see so hundred and twenty eight bits hundred 2828 bit. SSL leaders are labeled. I'm so these are all the long latency operations like floating points and Vector operations, and then throw the first stage of the pipeline. It's all the simple stuff and they have a few functionally as it can get memory operations.  There's again there's a memory stuff in here that we're going to talk about next quarter.  And this is I think you know, this is 15 stages.  I believe is what p.m. The book talks about.  All right, so that brings us to the end of processor design. Are you all ready to go and build your own processor?  I hope so because some of your in 141 L. How is 141 L going?  Going great.  going poorly  So we're going to do for the rest of today.  Cuz I'm going to go through a couple of more problems with animals law. This is sort of like pretty review for the midterm going to couple of you coming to talk to me about different questions about animals on the performance equation. Is there going to be a little bit of that then I am going to talk to you about a timer. Maybe I'll talk to you about a completely different way of building processors that will give you sort of an idea of what you know, what the alternatives are to the living building today and also show you how some of the metrics and measurement stuff. We've looked at plays out in the in the real world.  So here is there some slides I cut for my original lecture on animals law will go through them now because they find out some useful stuff. So this is example, so we have a situation where we have 30% of the  instructions in our program are memory operation. So they Lowe's in stores.  We have a new widget call the cash to talk a lot about in the second half of the class, but they were just going to call the cash, you know exactly what it does. But what it does effective. Is it at speeds up 80% of memory operations by a factor of 4?  So some fraction of our instructions are memory operations and some fraction of those memory operations. It speeds up by 80%  I actually should say it of the remaining.  Rightful is remaining 20% speed up half of what's left over so half of the memory operations, but the First Cash didn't help this one helps and its speed those up by a factor of 2.  price of a level 1 Cache  bl1 cast speed up some of the instructions so the memory operations 80% of them and they all two speeds up 10% of the remaining teeth in as soon as I buy factor to so what is the total speed of right now? We have two different optimizations ever going to perform at the same time to different parts of the program.  So far we go through and do the math. Let's look at this in pictures. So this is drawn to scale.  So this whole thing is the total execution time.  execution time of the program 70% of 70%  write 30% of our instructions are actually on time. So I said just a minute ago. It was the instruction counts not actually the execution time and that's going to be important in this example the next one. So this is where here is the fraction of the program that the L1 speeds up. So this is 80% This is beside it is 80%  this is 80% of  30% which comes out to 24% of the total execution time. This is the L2 time the fraction of time that LDL to can speed up and this is a fraction of time but nothing can speed up. It's just slow.  So now all we do is we speed up this by a factor of 4.  And then we speed up this.  Five factor of 2 and then we will get there and execution time is 08505. And if we do the speed if it comes out to 1.242.  What's the actual speed up?  That we get by a flying instructor positions at the same time as all about applying differin optimization optimization support the program's what's total hours to come up with this answer right? We shouldn't have to go and draw these pictures.  So let's do this the first way be kind of obvious way. So we have two optimizations we have is one a next one.  That's one is the speed up for the L1 cache xs-1 is a speaker for the alarm cash X1 is the amount of the program that affects so this is point to form and that is 4X here is the equation for angles law and we will get out to feed speed up for the L1 cache. I think if we go back here, I was like, this is 1.219 V.  X  I'm so that works out fine. And then we can apply the L2. Right? So the obvious thing to do is we have SL2 that's the speed if we get for the L2 cache XL 2.3. That's the number of are the amount of time we spend in memory operations. This is the the thing this is the fraction of things that the Ellen does not help we divided by 2 because that's the the fraction of the remaining instructions that the L2 can't help and we get that XL 2 equals 0 3 we can plug that into 1.015.  Right. Now if we're going to combine these two should multiply them together and so we will get 1.237.  Yes.  Sorry.  We need what are you?  There is no arithmetic are on the slide.  There is a conceptual are on this side. So this is not right.  So this is not equal to 2.242 and I have three decimal places to convince you that it's not the same I should have chosen larger number so that they would be like everything is not the right answer and I have this speed up so I can apply the L1 optimization that means this value changes because I no longer accounts for 3%  Right. It's actually a little bit larger fraction because if I go back to this picture  play go up here. This piece has the same but the overall size of this the toilets the kitchen timer shrunk, so it should actually be a little bit bigger.  Right, and so that's why I got the wrong answer.  So there are two ways to solve this will go through both of them.  So the first one is to use a sort of generalized form of Campbell's law.  And so I'll just write it out here. So it will be easier to see but if I have two optimizations I can write us toad equals one over.  X1 / X1 + X2 / S2 Plus One see now. I ran out of space 1 - x 1 - 2  All right, so just expanded out the thing on the bottom and now I am going to break out each of the pieces for the individual optimizations and computer speed up. I subtract all of them from the whole this is still the leftover piece and then I take the inverse and if we go through and do all that math, we will show that we get 2.1.2 for 2.  I don't get the right answer cuz you're applying both of them simultaneously.  That's one way to do it. The other way to do it is we can actually apply them. Seriously. We just need to be a little bit more clever about it. So here is the kind of iterative method. So I have to use speedups to coverages X1 and X2 into speedups S1 and S2. I can apply us one. That's just normal and Ohs law.  I need to calculate the new X2 right because the coverage here is changed because I have accelerated I've ever do stay over on execution time of the program and X2. It turns out the new x 2 equals x 2 times the speed up from flying the first appointment first optimization. How do I get there? So I'm going to do the kind of algebra trick that I used to hate in math class because it just seems like a little bit of magic. So I'm going to write down first a text to equals x 2.  We all on board with that part. That was easy part 1  Then I'm going to rewrite 1 as X2 is T / t.  Alright, so this all this is sort of tautological E True when so this is the total execution time Big T is the total execution time if I want to go to calculate the new x22 this thing is going to stay the same because that's the amount of time. Covered by X2 so I can write x 2 x Big T. But this tea is going to be smaller because I have some speed out that I got from applying the first optimization. So teeny,  This is over tinu tinu.  equals T /  best at 1 right that's just the definition of speed up.  So I can I do.  I can rewrite us to a wine about it again by the definition of speed up to BT over.  I just did this problem on paper to make sure I wasn't screwed up in class.  15 l  Alright, so Tina just substitutes in.  / a stoat  one these cancel this comes up here and I get that X new equals x 2 x Esther 1.  All right, so that's the new.  coverage for X2  balkanization to and then I can just apply Imo's login with the new value of X2.  directions  and then if we go through and do it  Hello. Thank you.  We get so this is the math we had in the previous slide to get 1.295. Then we calculate the new x to the next to equal 0.036 V. And then we apply us to just using handles law we get this and then we take the product of this.  Not and we get 1.2 for 2.  Alright, so there are two methods.  I would not be surprised if there was a problem on the midterm about calculating the effective to optimization simultaneously that is something we could totally happen. I will share a secret with you because I have not written the midterm yet.  So I can't say for sure that that will be the case, but it's certainly something that can happen.  Alright sure thing. I was I want to go over with one of the homework questions in a couple of people came and asked me about minutes highlights are useful.  Kind of subtlety of Amber's law to this is homework one for my think homework problem 1 4 homework 3, so we have a process or a program that runs for 30 billion instructions that takes 20 seconds on a 3 gigahertz machine on the program consists of 20% divided instructions. And as a CPI, and then the CPI of a divided section is for  so the first question which is not going to go through any Taylor just to figure out the average CPI of the program. And so you can just plug it use the performance equation to plug in those numbers and you get the average CPI is to  That's previous routes. You need that in a moment. And then we have a new breakthrough technology that reduces the CPI divide instructions by a factor of 2. So the new the new CPI is 4 divided sections is 2 and 34 and then what is the expected speed up?  So the first thing that we need to figure out in this problem is  what is the CPI of the other instructions?  So we can do that by we have the CPI for the whole program. We know that you compute the CPI for if we don't want to the formula for computing the average CPI so we can break it out by parts. We have .2 is divided and point H is the rest of it. And then if we solve for the other stuff, we will get to the other stuff. The average CPI is 1.5.  Tell the next part of the question.  Is the expected speed at this so this is the exam. This is the answer from the key performance equation. And what you see is that we have the two sisters the instruction count at the cycle time are the clock rate. And then what we do is we take the for the old we have the old CPI for the Divide instructions divide CPI. We have the everything else and then for the new CPI for the same thing for everything else. I need to go to the math. You'll find that the speed up is 1.25. All right.  About the spinner. And remember this problem was way back. Yes.  Sure.  So here than what I need to figure out so it's hard to hear what they're what's going to think about. This question is that I don't know everything that I usually know. I don't know what the CPI is it all the other instructions, which is usually how it go about calculating things. So when I get over here what I need if I want to ride down the performance equation for one of these I need this term separated out which means I need the CPI of the other instructions and the situation where I know that answer is back here in the original version of the problem. So I start off for the CPI is equal to 4 I use that to break out the CPI of everything else and then I know that that's going to stay the same and then I apply the optimization to the just to divide instructions. All right in the questions on this part.  All right. So now the question is right. So at least one person who can you talk to me once through and try to do this with animals law which totally seems like it should work and in fact, it should work because by law.  And we never break handles law. We always heat and has lost 20% of the instructions are divided instructions. So they said that I stole it was going to equal 1/2 / 2 cuz that's a speed up + 1 - 0.2.  And they do this when I didn't calculate this.  They got an answer. That is not right.  The answer this is a point one was.  Hey, so this is 1 / 0.9, which is like 1.1 or something.  So this is not quite the right answer.  so  what went wrong does anyone have a gas not the person that I talked to in my office about this who knows the answer?  Animals law is a law.  like a physical law  So what what does who remembers the definition of speed up?  That is right. So there the definition of speed up.  old over new and these are latent he's  So these are seconds.  not instructions  So this is why back on the previous example, I went back and I talked about how it's the memory operations take 30% of the program's execution time. Not that 30% of the program's instructions are memory operation writing laws about speed up speed up is about latency. And so  Anthony to be working in seconds  Can erase all this?  Let's try again.  so  we need to know is  how much time we are going to spend doing divide instructions?  So the way we can do that.  Is we can apply the performance equation just to The Divide instructions?  And we know that so we can see t or c e t i c e x c e p i x c t. All right. So that is the performance equation the instruction count. What is the instruction count of divide instructions?  It is what point two times.  30 billion  that's in section count. What is the CPI for divide instructions?  It is for that's in the problem. And what is the cycle time to cycle time is 1 / 3 billion.  Find Catherine do this math. I have it handily recorded here that this.  and that  equal to  It alright to the CPI for the execution time for divide instructions is 8 Seconds.  Now I can go through and I can then calculate the  the coverage for the instructions, so that is just ax equals 8 * 20  Carol I can't even read that x equals.  8 / 20 which equals point for that we need for MBA.  Is 1 / 0.4 / 2 cuz up to speed up plus point 6 which is 1 - 24 and that answer comes out to be 1.25.  Thank goodness. Although if we had found a counterexample to andaz, Lobby would be famous.  roll guitar picture on the cover of  Byte magazine or something. Very exciting.  All right.  so we have  all right. So now we're going to have to shift gears a little bit.  so the processor that we have been talking about in this class are there called Von Neumann processors is because they were the first one was invented by a guy named John Von Neumann who is kind of a jerk, but he did kind of invent the digital computer so will you know give him a little bit of a pass and the key feature of a Von Neumann processor or if I know  Computer is that you store the the instructions and memory and then you fetch them into a processor and you execute the instructions one at a time and you just keep doing this until the program stops. And that's how the processor run. So this is the basic algorithm for how we executed Epps program instruction for memory. We run it through a Pipeline and construction of the next instruction and everything about pipe lining and out-of-order execution and everything else is just a way of making up process more efficient. The fundamental algorithm is still the same. Your program's execution is a very long list of individual instructions that appear to execute sequentially, right? So that's part of the sequential execution is preserved.  So that's not the only way to build a processor. So I'm going to tell you about another way to go to processor and  there's nothing processors built like this for a long time.  I have some vested interest in building processors in this way. This is what my PhD dissertation was about. What is going to show you both? What's a really awful graphic design looks like from around 2006. And also we're going to see a different way of executing programs are also going to see how some of the stuff we've been talking about about performance modeling and trade-offs goes into designing kind of a new processor from scratch Princeton research. This is the kind of stuff that you might get to do in research. All of this particular product has already been done so hands-off.  Play Skillet has finished. All right between 2003 and 2006. This is right way and conventional Prosser designs were hitting the Powerball and everything was kind of falling apart. So that's the kind of thing. We were concerned about these chip multiprocessors or you build lots of processors on a single die were very popular ever is very excited about them.  And what are the problems was? It look like we were going to build more and more of these chip multi-process really like Hunters of course and a chip and nobody knew how to program them because there were just too many threads. And so we had this idea that we were going to adopt a new kind of execution model.  For how programs were going to execute the next instruction. I'm going to execute and I can come at this one at a time and where that program counter goes. That's the sequence of instructions in my program is going to execute someone problem with this is that the program counter is fundamentally centralized. There is one program counter that lives in one place on the chip. This is why Branch prediction is important because I'm updating this one. I'm updating this one program counter over and over again and it was the one place that I have to do all this complication Branch prediction and know the outcomes of my branches and so forth affair with an X values going to be it's also fun. You can imagine that if I wanted executed lots of things at once that's not a great place to start from right so I might want to start with a model that has a little bit more inherent parallelism in it.  Show me alternate with something called data flow. We didn't invent data for it was actually better than 1975.  I got injected us.  Batavia is that we're going to store program not as a sequence of instructions and will walk through with the first encounter. We are going to store it is just a graph and it has some instructions that it's an operator's like Adam subtract and multiply and divide and they have some input these represent. The true dependents has right. So we talked about true to Kevin says in the instructions said so these are the true dependent there's if there's no subtract up here and maybe I multiply up here if there's a narrow here that means there is a true dependents between this multiply and subtract and this Edition operation know what's going to happen. Is that when we compute a value  If I computed value it will get sent to this instruction.  And it's instruction instead of to the basic algorithm fresh candidate program is not that I think it's the East instructions sits.  By itself independently of all the other instructions and it waits for all of its inputs to be available right here waiting doing nothing and it seems one value come in and tell me what that is not enough for me. I need to values and when the second value appears, then it can fire. We called this instruction firing and then executes and it sends its result on waiting about this is that it's totally decentralized write each and each instruction is making decisions independently of all the other instructions in the program. And so there's no centralization essential serialized bottleneck.  We can have massive parallelism, right? We can execute thousands of instructions at once maybe.  And they're like an apparently so then the question you could ask yourself is like where are the dataflow machines? Right if it's so awesome what happened to them?  I'm not going to talk about why there no data phone machines. There are a bunch of reasons that people can actually build these but it requires knowing all bit more about how memory works to see how that plays out. So they will come back to this later on in the quarter and if we go through here we can trace all of the dependence is the true dependence is between all of these instructions and they're actually ever in this there's no false dependents is here and we could draw a graph. So there's a graph that graph represents that complication on the left and then a dataflow instruction sets.  It's not some funny thing. We're like, there are some arrows in here that you could draw on these arrows kind of emerged as a program executes. This is actually the way that you represent the program in a datafile inspection set how many people here use Pearl nunu's Pearl a couple of you. It was awesome and had a theme song because it was so slow. It was the Chariots of Fire theme song If you know what that is, but one of my mates actually when you ran it would play Chariots of Fire through your computer because it took so long to run the kind of things you get to do in graduate school computer sees the program an execution goes like this. So the inputs come in from somewhere up above.  Not shown and they flow through here. So the first Ranch all of the first of the first step all the multipliers have all their input. So they fire then all the ads fire next up the load in the puss in Fire and then eventually the store when we actually get the value of being here at the bottom and they are secured Wireless according to this data photographed it explicitly and codes all of them.  So when we saw the first as a little similar to order superscalar does the same thing right to remember we said we should we have this instruction window and you look for the instructions that have all of their their dependence is satisfied and you execute those instructions. So that is a date of execution model. Remember we had to do to get there every time we wanted executed instruction. We had to fetch it out of memory. We had to decode it. We had to rename the registers and then we had a drop of the instruction q and every cycle every cycle. The the processor was re Computing all of these dependents edges so that I could do the calculation about who is ready to fire.  Can I bring a lot of power? I'm fetching things over and over again. That seems like kind of a drag. I'm doing a registry name and it's also Burns Power. And so if I represent my program, that's why I'm actually saving the processor or a whole lot of work.  And maybe that'll make it more energy efficient and will also see their likes they going to be able to avoid fetching instructions all together or at least almost all the time.  I'm going to skip ahead a little bit here.  So we the one that we set about building a processor to execute these kinds of programs and the idea was to be abstract ideas super time. I may be on the scale of like a strip down even version of the NIT score of the first five stage meth pipe light and that we'd studied by the standards of 2003 and 2006 are they is super tiny so we'll give each of each instruction. The program will have its own ale you write will have if there's a billing instructions in front of them a billion ale use the Hub direct communication between. Some Communication channel so that when they produce one of them produces a value it can send it to its friends that need it and then everything will be great week sometime.  Practically we can't build billions of values. We can we maybe build thousands of Ale use a few of them and will reuse them somehow.  That's so here is the processor design that we came up with.  So this is a big city of these things called processing element. That's what we called them. And they are a little piece of Hardware that connects several instructions, which one of the instructions to sort of independent of all the other instructions are so you can think of this as a big sea of instructions just waiting for their inputs ready to fire Billy and transistors or something.  So this is a processing elements and weight scaler. It's really simple takes about half-a-million transistors to build. This is simpler than what you're going to build in 141. I'll probably use here and these are aware of the values flow in there's an actually there's one sort of slice across. This is a single instruction. And basically this is the the instruction sit over here.  The data sets over here and one of value comes in if we get a value in this slot or they have the destruction here we go to Value here in about you here in about you hear through the ale you and I will pop out the bottom it'll flow on all tell the other instructions. So I'm here with leave, but at least Gaylor it stays there.  It said that it waits for more inputs to come right? It could execute again. I can be in a loop and we'll execute it a second time and some more values will show up and we'll execute again. It's a 5-stage pipeline. There's no decode stage. So there's no decode. There's no fetch right because he was already there in like three or four stages here is a separate outputs Jaden Smith. 64 instructions can sit there at one time. We took two of these and we stuck together. My advisor was extremely happy with this pun advisor like that. We are two peas in a pod size are her name is Susan Eggers.  one of my two advisors  She just one like the Nobel Prize in our computer architecture a couple weeks ago. So she's super famous and very nice his his only adviser is Susan tiger. So she also invited us into you just don't hyper-threading.  So we could take Four Paws. We can buy them in something called The Domain here. So we so within us within a single PE. What we try to do is we try to put instructions that were you know, if I had an instruction here. I need to talk to her instruction. I would try to put both of these into the same PE and then they could communicate with each other. This is a forwarding Network. Right? We have the same forwarding idea just like we did at MEPS to affording Loop their the peas good for and stuff to each other so we could get some kind of parallelism in short communication there and then here we had pretty quick communication so we can pass stuff ran inside of domain.  This is a big wide interconnect that we have there. And then we just read about this is for making the memory go faster. This is a thing called a story about 4, then we'll talk about later. That's also for memory, but you can talk out of these out of the cluster in a couple different directions. And then as I would say you just tile the Earth with clusters right until you can build something with thousands and thousands of processing elements that can hold thousands and thousands of instructions. And then we danced around the entire thing with L2 cache. So I have lots of memory around the outside.  I'm so this thing was going to be 400 square millimeters. It was going to be Whole 30 mm instructions and it could run about a gigahertz. So by today's standards that would probably be running at this is 90 nanometer an hour at 14-nanometer. So if we built this again this might run out of 7 times faster, so maybe this would be 7 gigahertz.  Except it would melt. That's not good. That was a processor.  So our claim with this was low-complexity because it's built out of some really simple building blocks. Right? All you need to design really is this thing and then you just stamp out a whole bunch of them is much much easier to Big going to be complicated out of work or  For the way this would work just a little bit of a lots of them flexible parallels and flex my location that you could allocate so that way you would bring in programs write the code for a particular thread and you can put you can run one big thread by bloating in 30. Mm instructions from one thread or you can load them smaller threads and all sorts of different sizes. So you actually bring in the program and the program live in the processor with the machine. The program does not live in the processor lives in the memory and weight scaler. It actually lives in the processor itself. And then we would do if there's there's always more than you can fit so we would have this way of swapping them in and out.  What is the thing called cashing which will see certain example of a memory as well?  It took a long time there like five of us working on it took I don't know three or four years.  so one of the things that we did  Is we had this design? Sorry I added at least like the earlier and then my computer crashed so  All right.  So we had we had to figure out what the right design parameters were. So if I'm designing a new processor people 30 people are in charge of the design of a new micro of a new Intel processor about 30. He said people are in charge of doing the high level design and there's about 200 people that are in charge of actually implemented really building a circuits and testing and all that remains in Costa Rica number of the number we can bury the number of Pease and he's the man we can bury the number of instructions for p and there are a couple of things we can do as well but there were thousands and thousands of different proteins potential different designs that we could build and we wanted to find the best ones. We've been building a lot of intuition about what a good processor would look like because we've been building them for 30 years.  We have no intuition. We didn't really know what was good or was bad and so once we did first weekend of search by hand, we should have made some gases that seems reasonable or we could try to do a complete and systematic search.  And try to find the best processor.  Now what?  How should we measure this? We're trying to find the best way of scalar processor when we talked earlier about some metrics that we could use for defining best. So what would be one metric we could use to find the best?  Yeah.  performance  On benchmarks performance build the fastest processor possible for waves Kayla. That means I'll just build the biggest processor possible. I mean certainly would have more space to swap things in and out very much. So performance can't be the only choice. The only thing I'm worried about this is usually the case if I'm trying to do some trying to build a thing in the world, there are constraints I in summer.  Yes.  Cost I'm going to call that area because cost is basically proportional to actually it's more than proportional area because the bigger something gets the harder it is to manufacture a perfectly right. I'm so if I have a big big chip and I have like one little are the whole ship is going to have lots of little chips instead of all my chips. Constraints constraints. Yes power would be a great constraints.  Read it can't be too hot. I could also use power is Alter Ego energy.  per instruction Maybe  Cuz that's the kind of determines how efficiently I can execute what else any others.  performance on crisis  maybe I don't know how to run Crysis on Wave scalar. The truth is wife's killer in a very small number of carefully tuned workload. So we were definitely going to run Crysis.  So the constraints we used.  We fix the cycle time. Just kind of we only had one design. I'm so redoing the cycle time would have really complicated. We fix an area budget. So we fix the maximum size of the chip would 400 square millimeters. This is really big for a processor not inconceivably big but it is pretty big and then  we came up with a total of 201 reasonable designs.  And then we went to evaluate them.  So here are all the designs.  And I plot of them so this is area. So we're optimizing for area versus something called aipc. So this is actually this is very relevant. We've seen this so a i p c stood for Alpha.  IPC  Who are members of the coolest processor ever built was?  It was the alpha.  So when's killoran the kind of Leverage an existing instructions that works with the alpha instructions said I didn't explain why but we had to add some extra instruction therapure overhead instructions. They do things like Implement branches branches are really complicated to get right and so we wanted to make a compare performance using IPC which is instructions per cycle. It's one of her CPI towel Architects off and measure performance.  What if we just compared instructions per cycle on Wave scalar to instructions per cycle on a conventional processor Advantage because even though you're useless, right? So Alpha IBC was the number of equivalent Alpha instructions such the start of a measure of how much work and how much useful work. The processor was doing a IPC.  That's the last time I say IBC we can get this is the area that we are spending.  And the green dots are all the designs.  Which one is best?  Yes.  Good question or good point this is on so this is on a benchmark Suites called Splash to to the parallel benchmarks late. So let's say which one is best for Splash to?  Yes.  oops  This one up here.  The very biggest processor with the very biggest. Well, you said the biggest one that is not actually the biggest one.  the biggest one  Is here that's the biggest one. Is that the best one?  Now that is definitely not the best one.  I'm the best one of the fastest one is up here.  What if I didn't want to spend 400 mm if I wanted a smaller processor? What if I wanted a 200 square millimeter processor better phone is best.  That's the one right here.  So in general the the good designs.  Play around this boundary.  so at least have a very particular name, these are called the parade o  optimal  designs  Sofrito optimality is a very useful concept when you're trying to make up for a trade-off among multiple different constraints. So here we are performance.  Turn off the print off some of those lines are in this graph are they are the upper left boundary of the cloud of points. So we're print off a little sign what it says if there is no other design that is better by both of my metrics. So in this case, you know, this one is not this one is pretty optimal because there is nothing that is both smaller and faster, right? We're sort of at the upper boundary this one the biggest one is not afraid of optimal because there are a whole bunch of designs that are up and to the left from that one.  So you can pop them all out. They go from some very small ones some very big one and you know, the question of whether whether one of those is better than another really depends on what you want. Like, what is your threshold in terms of performance or what? How much are you willing to spend or maybe how much power like we get out another access here? We can measure all these by power efficiency or maybe even better would be energy. So energy per instruction, you know, one of these is going to be best by that measure and that might give us another way to evaluate them.  All right.  That might be it.  That's all there is about wife's killer any questions.  Who would buy a wave scanner processor?  Oh.  My poor thesis no one wants to buy my thesis.  Okay, I got me a job.  Alright, so  what happened to waste go nothing really happen to waves together there a couple of fall on papers and then all the students graduated went on to bigger and better things. One of the students is like VP of Technology mongodb guys know about mongodb. Yeah, so he's in charge of that another one of those students is a professor at Columbia University in New York.  Another one teaches the University of Toronto.  My other Weiser Not Susan who won the the Nobel Prize of architecture. He now spends all of his time on his boat.  He's very tan looks very Haggard.  Anyway, so what we have we have 18 minutes left.  Any questions about the midterm we can start the review early. Yes.  Yes.  Because I'm with the Greensheet is the front page of your textbook.  I'll probably just be reproduced as part of the test. Yes.  Oh my gosh.  You should definitely not memorize the names of control lines.  That's like super useless. Yes.  calculator  How many of you actually own a stand-alone calculator?  Yeah.  All right. We will allow stand-alone calculators.  No phones, no laptops, obviously.  No, Siri, yes.  I honestly do not know.  Yes.  Say that again.  I would say I will accept a fraction of like one number over another number. That's as much math is willing to do while grading.  Otherwise, you should just do that do the calculations.  Yes.  fair game  You mean for me to add to the midterm Oh, yes, most definitely.  I might have said to my Tas.  Who are in charge of creating homeworks generate me six questions each based on the homework. So you've built that are similar but not identical. I might have said that.  I may or may not use them.  But I might have said that also all of the reading quiz questions are fair game as well.  As is all the content that you've read in all of my lecture notes, so, you know.  other question  Yes.  Oh, I think in the slides.  Any questions about computer architecture?  Maybe yes.  Oh, yeah, there's someone.  Do you think that the most modern service bar Jackie?  Fpga is are very inefficient in terms of energy and performance for implementing CPUs. They're really good at implementing particular circuits, which is what they're mostly used for. So we built we prototype prototype to take a lot of fpga. So like I don't know what the divisor is where you can buy these big boxes are like the size of a refrigerator cost. You know, how many millions of dollars and they will have hundreds of fpgas in them and they will come with the software that lets you take something like an Intel, you know, the latest Intel which has  I don't know how many billions of transistors and will map it into a big box of fpgas and you can run it but that's how much fpj Hardware it takes to do something equivalent to a modern processor. It's a lot.  All right, as I said my algorithm for a few sessions is it when people stop asking questions I leave.  So we are going to see each other question.  Yes.  Hellion  You don't have the answers.  interesting point  I will.  what to do about that  I will provide some answers for the reading quiz.  Yes.  You need to bring a 13 mm fabrication facility because we'll be designing Anthem in a processor during the midterm.  So I think they have them at the bookstore. They cost three billion dollars each, but you can sell them back to use for just a billion dollars so you can recoup most of it later and yourselves.  All right.  That is all schools out early.  Religion of Nod to relative quantum computer. So I think some of them are sad thing is kind of love feels like  Wheeler Dealer seems similar to  Quantum computational  I don't know how the current quantum computers work. I know there has been my one of my advisers actually has been a lot of architecture work on computers. Like how do you move give it around like operate on them and so forth and it's totally different because I'm very very different physical constraints. I guess it is sort of you would take these ions that held cubits and you would sort of moving around and so you would ship them from one place to another so that since it was quite similar like a Quantum processing pipeline.  That the reason that doesn't work is because you think about a pipeline They're copying data a lot in the pipeline, but you can't clone Quantum bits.  So that's like a that's one of the fundamental things. We actually had to move this like physical entity around. You know, I couldn't you couldn't like transmitted somewhere you had to move it so I can actually two atoms and they do Quantum dots and they do so  But just what I feel like the colonists they won in 06.  doctor we just saw this earlier is kind of a same feeling as  It's fitting in certain that things are not as they are.  I'll come by in the same time.  How often should I say?  So here's I mean, I don't know but here's the here's the thing.  The way that like great research projects are born is that people have these like brain waves in their head that are like this is like this but with that and no one else understands, maybe but like it sounds like an interesting idea.  To that from you, so I don't know. Maybe you should go and think about it support.  So will specific with architecture questions be on the midterm like x86 or like weight scale are that type of thing? There won't be I mean  you need to be able to read mips assembly for sure. Maybe write some simple nips assembly.  kind of pseudo nips assembly  I don't know what I could possibly ask about where she's going.  I don't know. There's no benefit if if we do anything about x86 will give you enough information to know that the instructions for us to do a problem.  So I put pressure on my phone number six.  So are we going to rain the penalty life after is the current list prediction for each processor? "
}