{
  "Blurbs": {
    "10 seconds 3 2 1 10 seconds 4 3 2 1 4 3 2 1 10 seconds 5 4 3 2 1 Oh, sorry. It sounds like I'm for sale. 5 4 3 2 1 that's out of the quiz. The salad from the last time I asked this. 5 4 3 2 1 All right. So happily. It appears that very few people are cheating on the clicker quizzes. ": [
      154.4,
      460.0,
      0
    ],
    "Alright gpus are good. Bigger vectors must be better. So gpus take vectors support to the extreme. So gpus are these coprocessors back when I was in college, they were just for graphics and they were what you got if you wanted to have video games look good. I did I went to the University of Iowa for the summer and it's in research and they had an early. GPU ": [
      3310.0,
      3346.0,
      87
    ],
    "Cuz if I change one thing right, so if I were to say just right and nocuous Lee plus plus right there I can probably would probably get confused and stop being able to vectorize the code. All right. So, how do I add this to the pipeline? Right actually pretty easy. Usually we just put it in here. Reflective actors in at the Flying Point Unit. So number one ": [
      2785.7,
      2811.4,
      70
    ],
    "Cycles. That's true. It would increase CPI which is cycles per instruction, but cycle time remember functional limitation so it would not end. It definitely won't miss prediction won't increase cycle time. It won't increase increase cycle time. If there was a specially the front in the front end for the front end dispatch and the instruction memory on decode and the critical path of the processor is usually up ": [
      970.3,
      1000.7,
      17
    ],
    "I mean if you wanted to go to a bigger machine near the rear end all your software, which was a real bummer. So this is a big indention very important and initially they use in Focus was on usability by humans because there weren't good compilers. And so you would actually write a lot of assembly and so there were lots of user-friendly instructions things that could encode pretty ": [
      3566.2,
      3597.1,
      96
    ],
    "I only like 5 instructions? And he said yeah, but they're all moved which is about right. So move if I move icons on the media in to register, that's like try to create a constant smooth is so this move is a load because I'm moving from a register or a memory kind of operon to register. I can move I think I can move from memory location to ": [
      4557.3,
      4581.6,
      131
    ],
    "I see all the operations that are encoded in my Vector instructions and I execute them individually, then I have to fetch and decode and read the register file for each of those instructions to schedule an order processor. So this is energy breakdown for I didn't order mips processor 7 stages is a processor be used for a research prototype machine that we built. This is just the pure ": [
      3074.6,
      3110.0,
      79
    ],
    "I use it, right? Cuz I'm about to override it with this result is called destructive right at the register file the Monstrous register file. This is the original x86 register file. There are 8 registers right here. Emeril have special uses sir register a is called the accumulator register bee is called the base. There's a can of this day to register the stack pointer. This is the frame ": [
      4368.9,
      4404.6,
      124
    ],
    "I want to take the sum. I want to add up all those results are three in this case would be the the The sum I would put our three over here as well. And I just run this write down my vector and I will get the dot product for free and I've saved having to execute the extra extra add instructions for slots. It has three inputs and ": [
      2085.6,
      2118.9,
      45
    ],
    "It's called compiler. Explorer Dot-com on some guy's website, but it's really cool and it will show you the output of the assembly for the compiler, but you can choose between like 40 different compilers. So in a bunch of different instruction sets, you can do nips and x86 and Allen 32 and a bunch of other stuff and you can also pass it all kinds of command line options ": [
      2181.7,
      2216.5,
      48
    ],
    "Looking Graphics. Like they've gotten smaller and smaller and smaller eventually got programmable. And then after they got programmable if we can do real and then they got done on purpose. So now they're actually called gpgp use for general-purpose graphic processing units, which is a little bit of an oxymoron because of the general purpose, why are the graphics are called and they are basically giant Vector processors and ": [
      3373.2,
      3402.0,
      89
    ],
    "Loop iteration is actually independent and then we'll do a pretty good job of finding that parallelism. And so this is member I said last time the reason we have smt is because in a lot of for a lot of programs, we can't find you know, the five instructions are in a big out of order superscalar can execute a recycle but one place where you can often find ": [
      1929.2,
      1950.4,
      39
    ],
    "Not the number of slots in the vector since integers are 4X long. This gives me 1/4 month doctor. So this is the little array when you can actually index it so I could say a zero and I would get a value but then I can declare some values of that type and I can have them together. And now this is actually equivalent to that Loop. I thought ": [
      2665.9,
      2696.8,
      65
    ],
    "Point operations will take variable numbers of cycles. And if you look at you can get tables of instructional agencies for processors and they'll be kind of all over the place 12416 sine and cosine or like 50 or 60 Cycles or something like that a square root is pretty complicated as well. And so some of these instruction sit there for a really long time. We talked about all ": [
      1791.3,
      1817.0,
      34
    ],
    "Rex and started the Rex prefixes for take you through and figure out why you should perish you might need some extra operations for the displacement of the base scaling factor and also their stuff with the total mess. So, how do we do that? Can I get through this in 2 minutes? Maybe they take the x86 code and they converted in the decode unit in to order essentially ": [
      4707.3,
      4742.2,
      137
    ],
    "So I appreciate that. a little bit of confusion on some of them Savannah's law does it limit the speed of speed limit station can cause No, 100% of you got that. Correct. Very good. I should just say that everything is the most important thing in the class and everything will be on the final and then people remember remember all of it. Yes. It does. Is this the ": [
      460.0,
      515.5,
      1
    ],
    "So do you schedule to do some extra work after schedule the instructions? so the way that back it's implemented is that you add more pipeline stages to do that extra work? I'm so I guess you know if if I were one of you and I was arguing like I did with my instructors and I was in college, I would say yes, dr. Swanson, but you could just ": [
      657.3,
      682.9,
      6
    ],
    "Yes, it is. This Ables law know this is some terrible version of Handel's law. And so the answer is BNC. Very good. Sorry to definitely be that is definitely see. Hello. All right the performance equation. Is this the performance equation? No, because this should be cycle time not megahertz. This could be one over megahertz. And that would be a good answer. I guess technically it should be ": [
      517.8,
      552.3,
      2
    ],
    "a 32-bit swine. There is a 32in sure and free Vector register file. So now I have 32 vectors and they have names like v-0 and V1 and so far. It's like we had for the floating point in the end of the registry in a bunch of different way so you can have to 64-bit values or 4:30 to bedias rate 16 desires or Sixteen Acres values. So you ": [
      2493.3,
      2524.2,
      59
    ],
    "a floating points program. That number might be 50% might be 60% if it's really floating Point intensive and now I'm going to software it's just going to be a complete disaster in a slow down 60% of my program by 52 x right that's really really bad performance is going to be something like 30 times worse. If I do it in Hardware, then I'll you know, every time ": [
      1714.2,
      1737.1,
      31
    ],
    "a look at the answer? 37 seconds or so 5 5 4 3 2 1 descent. All right, so discuss amongst yourselves Give me another 15 seconds or so. 5 4 3 2 1 this is exciting. little more company All right, let's go through and do the math. It's a bit tricky. Alright, so so we know the things we know we know the speed up versus. The speed ": [
      1336.0,
      1573.1,
      27
    ],
    "a lot of that last performance as a big win. So how do we integrate it challenges is floating Point operations. Take a while. They can take a variable number of Cycles the dead guy found for I think it's until the Halen was 92203 cycles for a floating-point divide the reason to take so long as they do. As you know from elementary school long division, you know, it ": [
      1737.1,
      1762.6,
      32
    ],
    "a on a given processor and I'll have a different version free CPU and uses magic piece of code, and it goes really fast. Today's a very common as lost as if they're worth while speeding outfits also using Graphics, they can use RGB values. So RGB for color are usually start Hazard GPA is a transparency value for slot Vector usually of 4 integers on so they can use ": [
      2408.0,
      2435.1,
      56
    ],
    "a register which is the name of a register have an immediate value. I could have a label if I want to have a branch have a displacement. This is very similar to what nips has. So this is a register Value Plus Value Plus some offset. So we've seen that already. This is based off set this I take two registers and I add them together. Right and I ": [
      4482.0,
      4505.6,
      128
    ],
    "a separate floating-point register file so that I actually sort of being a separates at least was the separate set of architectural registers or may not be a separate set of physical registers. And remember you register renaming and also sometimes have separate renaming and scheduling logic. So in some processor is it actually could have two pipelines one for doing floating-point one for doing integer? Yes. How does graphics ": [
      1846.4,
      1871.3,
      36
    ],
    "a vector operation as a sequence of operations on safe for a doctor's I could maybe make them a lot lot faster. So here is what support for this looks like in the in the instruction set. So nips nips 32 too. We've been looking at doesn't actually support vectors 64 does this is a 64-bit version of nips. Basically the same except the registers are 64-bit slide and set ": [
      2459.0,
      2493.3,
      58
    ],
    "a whole Vector so I can do that paralyzed and the additions and I get lots of value out. That's the you know, the the dot product of those about the portion of the of the of the big array. What they do, they do a clever trick and this happens actually a lot is they actually overlap part of the floating points the poor part of the vector register ": [
      2549.8,
      2571.4,
      61
    ],
    "and they were going to have like an 8-bit off code or something and he was raining all the instructions that they were going to need and he wrote them down and the head of implementation team got till like instruction. 240mm is like we're stopping Refuse to go on right? This is too much work. And so there were some upcodes left unused at the end of the x86 ": [
      4280.3,
      4306.6,
      121
    ],
    "answer was because we only had five bits to name the registers one way around that is to have a completely separate set of floating-point separate set of registers and basically have an implicit in the instructions are executing which of the two big set. The registers are so sexy cute. So floating Point operations, operating floating-point registers, and therefore we get to effectively doubled the number of registers in ": [
      2009.2,
      2032.9,
      42
    ],
    "arguments to functions, but from the architectures perspective all of the register the same I can add two things for going to have any to register together and put the result in any register not the case in x86 bunch of complicated instructions stream copy. I-76 includes instructions for something called binary coded decimal were instead of using two's complement arithmetic use each for bits to represent the digits 0 ": [
      4071.3,
      4097.0,
      115
    ],
    "back when this was on Vinted and we can get low CP I should be possible but only for simple instruction. So these old processors they were actually built from a bunch of discrete chips on a circuit board. So it's not all integrated on a single chip and very long And things are going to be slow but now we can integrate things tightly enough. This is actually what ": [
      3747.5,
      3771.1,
      103
    ],
    "be even higher so probably more than half at least 4 or 8 or 16 or even 32 operations if I'm using narrow integer types, and so I save all of that fetch energy and so that reduces the total. Reduces that's over had by twin 75 or 90% or maybe even more than that for small data sizes with pictures of lots of lots and lots of big win ": [
      3131.9,
      3167.9,
      81
    ],
    "beautiful. It's all because of these ideas with So pom pom pom. I wish I could draw Darth Vader's head. I'll have to get a Darth Vader Darth Vader example of Sisk there many others long ago 36th Ave survived because then tell us the best CP manufacturer in the world and so they can have him get us at the rules. There are many many instruction formats. There are ": [
      4015.1,
      4050.8,
      113
    ],
    "big so you can get you know, something like 14 x with a moderately smart compiler and some Cindy acceleration instructions. That's way more than you know, you're going to get from basically anything else we've seen. Pipe lining or I guess if you turned off your branch for more than that, but this is a big big win. Four vectors is Energy Efficiency or if I execute instructions that ": [
      3035.8,
      3074.6,
      78
    ],
    "build a worst pipeline that have the same number of stages right? I could just cram schedule into decoder something and the length of a pipeline would stay the same. So maybe I'll add. to maintain cycle time and I would definitely be the case because there's fundamentally more work we have to do for instruction. Right usually when we do from the pipeline when I go to Safeway you ": [
      682.9,
      711.4,
      7
    ],
    "build it until clones as well. But giving this ride will take you know, it's a huge effort all of these Corner cases that you need to cover because somewhere there's some piece of code that actually going to rely on binary coded decimal and it's not going to work on your processor. so x86 is a poorly designed is a by modern standards and breaks every rule. There's nothing ": [
      4126.0,
      4151.7,
      117
    ],
    "build something else. So whoever that is responsible for all that has come after. All right. That is the sad story. All right, who is one destination in one source, and all of the operations are destructive. So destination equals destination opsource. This is what's called an accumulator. Does an accumulator architecture and so this means that if I wanted to keep going to have to copy it somewhere before ": [
      4329.1,
      4368.9,
      123
    ],
    "built into it until sort of hard codes and Max were they started I think part of this is an arm came a lot later the game. So they say in the beginning until start out with him is the same as like a 64 right now. Police are everywhere and they can use a lot so cold you ride may not actually do this. But you know, if you're ": [
      2949.1,
      2981.9,
      75
    ],
    "bunch of powers and multiply time to write the things got really out of hand. Turn the modern era compilers are really good. So we don't need humans humans are right Assembly Language anymore. So you should chew your is a to be good for the compiler rather than good for the human memory is cheap. Soco density is not so important, right? We have terabytes of d'ram now megabytes ": [
      3723.4,
      3747.5,
      102
    ],
    "but the compiler I couldn't actually house using compiler Explorer my new favorite tool trying to get it to admit vectorize code for Mid 64 by giving it looks like this. It looks like this up here. I couldn't actually get it to produce the doctor outfit I wanted so. This is one of these places where you know. If I want to use this if you either after I ": [
      2744.5,
      2770.1,
      68
    ],
    "can slice it up in a bunch of different ways Sandy. There's a bunch of instructions. There's Vector loads in stores as vectors add. There's some other operations to kind of moved me out with some track elements from doctors and shuffle them around and so forth. Turn up 64-bit an x86. I believe there's no actually. Product instructions. So again, this is like the X operation but it's for ": [
      2524.2,
      2549.8,
      60
    ],
    "can still go through and find those instructions and the order would go something like that, but I could execute them in and it will see as instruction latency to get really long as I got it with memory. So let's say I have this have a few instructions like this and this one is a load up here. Right. I have a dependency there and this is going to ": [
      833.7,
      858.5,
      12
    ],
    "come with questions right last time. Alright, so Ariana Grande this problem we have is we have this case where we spend 1% of our time doing some integer addition inside some Keely for my program. So it wasn't a lot so just 1% I didn't do a lot of it and I decided that I don't know maybe do improve the accuracy of something I decided album. I see ": [
      1187.8,
      1228.4,
      23
    ],
    "complicated operation. So, you know accessing the element from an array, you know, something like this a guy plus plus it's like a single instruction for that right? Because that's something you might want to do. There were instructions that roughly correspond to loops and so forth good thing we wanted was a memory was really expensive stove code. NC was very important, right if you could get by with ": [
      3597.1,
      3621.2,
      97
    ],
    "could go to another meeting during our class time next Thursday, and I was like, I can't miss class and I was thinking in terms of you. I wouldn't be great. I just put up a video of Leo Daily Reporter doing one of his mid-term review. I will answer questions and tell them no more questions and then we will get out of class early. So make sure you ": [
      1157.9,
      1187.8,
      22
    ],
    "could like this or you have to go in if he's a library with the devious thing. If you want do it yourself you after I could like this which is a little bit cumbersome or you have to go in and look at the assembly that the compiler is generating for your program to make sure it's doing what you wanted to do. I was in very scalable, right? ": [
      2770.1,
      2785.7,
      69
    ],
    "couple of instructions for moving things between the integer register file and the flame point register file so you can Shuffle step back and forth. That's how you put the volume. Here's a quick example. I found this amazing website. I wasn't going I was thinking I meant to tell you about if I don't think we're going to actually do a lot of translating code to Assembly a homework. ": [
      2154.3,
      2181.7,
      47
    ],
    "depends on what the last comparison executed is. So now I need to instructions to execute a branch and this thing has put the destination of a condition code. Best way to go so I got them right into it. And now if I wanted to go out of order something I have to virtualize and rename the condition code because I kind of multiple branches going on at the ": [
      4632.5,
      4651.9,
      134
    ],
    "destination in Beck's cuz you're going to copy from one place to another I need to keep track of both of them. There's the instruction pointer with the points to the current construction said These are different names depending on how you like to use them. So if I say l then I get eight bits if I say acts like it's 16 bits. If I say E X I ": [
      4432.3,
      4452.7,
      126
    ],
    "do x86 is mostly King. So your laptop's everything that runs in the cloud almost all of it is x86 but your tablets and your phones are running arm and your Apple watch stuff like that. So basically anything smaller than a laptop is running or anything bigger than a laptop is running x86. Has managed to do in Helena Mt. Have managed to do at a considerable cost is ": [
      4178.0,
      4202.8,
      119
    ],
    "doing watching movies or doing graphics and stuff this stuff is getting used all over the place. The performance gains are pretty big. This is just a graph from your textbook. So this is for Matrix X. This is an optimized code. So this is without the any of the optimization to be talking about this is what they've EXs is the latest version of Intel's instruction set extensions in ": [
      2981.9,
      3009.7,
      76
    ],
    "don't realize I'm joking and then I feel bad cuz it like it was on the final. It's not going to be on the final. not on the final hybrids, very very complicated. So the decoded stage has to deal with this. Right? Right, and it depends like this is for Vector instructions. I think or maybe it's the only here's the vector instruction prefix. I don't know what the ": [
      4679.2,
      4707.3,
      136
    ],
    "early 2000, but a lot of it is because they start off with this fundamental energy and disadvantage because I have to deal with x 36 in the scalp. ": [
      4792.3,
      4799.6,
      140
    ],
    "file overlaps with a floating-point register file. So this is the boiling point register file right here with 64-bit values. The vector register file is the same except that adds another $64 on to the end. This means it's super easy to move stuff between the flooding in the back to register file because it's already there right if I want to move out of a floating-point value. I can ": [
      2571.4,
      2592.8,
      62
    ],
    "from Energy Efficiency perspective. Yes. This is my operation. So should I should be cooler right? I mean that's what it seems like having your computer is because the places were Vector operations get used most are in these tight highly optimize Loops where you're doing something that took a lot of takes a lot of computation. And so that usually means that the code is very highly optimized. It ": [
      3167.9,
      3204.3,
      82
    ],
    "from one place to another and it encodes a little Loop and it'll run along and do a bunch of operations. So what that means it's sleepy eyes are super super high instruction count is low because I could write down one instruction to do string copy. And so what all this led to was a whole bunch of different directions with lots of bells and whistles variable and variable ": [
      3644.8,
      3666.7,
      99
    ],
    "get a 32-bit then if I say RX I will get 64-bit to that register want to work on and then this carries over into the oh actually were going to step outside. These are what the arguments can be. There's a bunch of different argument in this can go to any instruction more or less. Well, of course not that would imply some sort of regularity. I can have ": [
      4452.7,
      4482.0,
      127
    ],
    "going to multiply R1 and R2 together and then we are going to add the results are three. So this instruction is purpose-built for doing. Products and convolutions, if you know anything about digital signal processing so you can imagine if I'm doing it. Products. Product I take to raise two vectors and I want to compute the product of each of the paralyzed entries in the array and then ": [
      2061.1,
      2085.6,
      44
    ],
    "good for computational intensive code is vectors. So vectors are a little arrays of numbers. So, you know usually for for going to talk about the usually between 4 and 8 inches long what we can do with them as we can specify operations on them. So this could be a vector a Henrico dr. B And we could say C equals a plus b and this will do the ": [
      2324.2,
      2354.9,
      53
    ],
    "have 8 64-bit floats the gold standard for a high-precision say scientific calculations so I can work on 8 slot directors of those big data types. How much is what's in your phone also has these they're called scalable Vector in extensions or SV PS. Ve but they support a variable length of vector up at 2 and 2048-bit. I'm so the is a actually has a lot of scalability ": [
      2917.8,
      2949.1,
      74
    ],
    "have a structural makes my logic simpler decoding a super easy simple. We can do this. This is the algorithm for executing any risk Construction in nips instruction. Write the only kind of weird wrinkle. Is it some of them don't do memory operations, but that's a small price to pay for the beautiful Pipeline and we were able to implement compiling is easy all the instructions look the same ": [
      3948.0,
      3978.5,
      111
    ],
    "have learned a lot about how to design is and so we can start over Davis MEPS I miss is an infant or something called risk, which is reduced instructions that Computing so these previous things. We're called Sisk. So this is Sisk for complex instructions that can do this is reduced instructions to Computing. And the goal was fast clock slow CPI in a simple is a dove easy ": [
      3794.1,
      3836.2,
      105
    ],
    "house. We saw the floating Point register file overlaps with the back to register file so we can just watch everything in their Vector operations are complicated the floating Point operations are complicated. And also we do a lot of floating Point operations on vectors. And so let it kind of happens is that the scalar floating Point operations just become special cases of the vector floating Point operations, and ": [
      2811.4,
      2834.1,
      71
    ],
    "if I wanted to know where the inputs and the hardware is simple. So when I used to treat 141l still in 10 weeks and you can compiler for it if 33 instructions that we use and you can Target is GCC and you could run programs, right? So I took longer than 10 weeks. We have better tools now, but this is not a hard thing to implement right ": [
      3978.5,
      4015.1,
      112
    ],
    "if I were most mere mortals don't usually write code like this, but we usually happen is that you will use some library right that has a matrix multiply function or. Product function and inside they will do this and they will try to hide it from you. If you're using say something like python, you know, this thing will get Wrapped Up Inside by which is high-speed numeric applicator ": [
      2696.8,
      2721.9,
      66
    ],
    "if he doesn't have floating-point support. And so then what is a speed up and we saw from some data, I grabbed off the web that a software floating-point add cost 52 cycles and some processor. I found some Intel processor Hardware in the Yeah in the Elliott turned out to be a .66% speed up, which is a 30. 3% slowed down and this is because Amanda's law says ": [
      1228.4,
      1269.9,
      24
    ],
    "in a machine from a company called silicon Graphics was used to build the coolest Graphics workstations in the world and the computer that had the GPU in it was a size of a refrigerator. I guess bag and it's sad over there and you're like sign up for time to use it then had one floating-point Pipeline and it could do like what looks like now pretty pretty crazy. ": [
      3346.0,
      3373.2,
      88
    ],
    "in this format arithmetic goes up here and then we need us for some weird stuff like jumps and so forth. Beautiful girl for a bite song everything is a line. I always know where my next instruction is going to be. This is like a paragon of Elegance, right? So here's a list of all the things that we can say about this there is always one or if ": [
      3908.7,
      3929.3,
      109
    ],
    "instruction fetch overhead. So I have to read Safari I cash and I have to fetch and decoded that accounts for 42% of the energy that I spend for instruction. So if I go and do a vector actually more than this because this is an inner core. So if I was doing a modern processor out after registering a minion out of order scheduling on top of that would ": [
      3110.0,
      3131.9,
      80
    ],
    "instruction set. And the way that they have extended x86 is you take some of those unused opcode then use it to say if it's a stop code then go get another bite and I will tell you more of the opcode if that implementation team had just finished their work. There would have been no room for the extension and they would have had to throw away x86 and ": [
      4306.6,
      4329.1,
      122
    ],
    "is definitely not this one and not that one but it's definitely not one. I don't want her execution. Do we still use out-of-order execution? Heck yeah, I'm podcasting so I have to say hack. hacks we use the music ride out of out of order sticks qution. All right. I'm not the case. Does it require a longer pipeline? I don't want no execution requires scheduling scheduling is fundamentally ": [
      588.8,
      626.1,
      4
    ],
    "is making Augusta might be educated guess but it's just a guess. It doesn't affect cycle timer. that is My life and that's where it doesn't affect cycle time or pipeline. What did I say? The answer? That one was I'm not even sure I wrote this quiz like 5 minutes ago. Yes. So that was sent to your question. So he has Winterfell friction cause increase the number of ": [
      934.8,
      970.3,
      16
    ],
    "it. I answer the question because I know a lot of please don't you put that away faster faster and so the computers actually doing more work per unit time. Which means the energy of the power dissipation goes up witches Watts for time and that means that it gets hotter because that's what happens when the power goes out more quickly Paradox or the final maybe for extra credit. ": [
      3276.7,
      3310.0,
      86
    ],
    "just one over hurts, right because it's all the decimal point there for a CPU or is mathematical model for CB performance. This is and has lost so it doesn't look to limit the impact of optimizations. Most architectural optimizations do not affect all three terms equally. And it can actually account for the effects of memory operations. It shows up in. CPI as we will see shortly. So it ": [
      552.3,
      588.8,
      3
    ],
    "just use slot zero of the vector and it's already there conveniently. I don't have to move it. Here is basically the code for US Written haven't seen and then here is so this is kind of interesting. So one question right beside you specify this in your programming. You need some like pretty serious black magic for the compiler. At least if you want to guarantee that he gets ": [
      2592.8,
      2629.6,
      63
    ],
    "know. Maybe we should name the Swansons Paradox. I like that Swanson paradox. I'm so often times when you add things to your computer that make it go faster or you answering that you use less energy the computer gets hotter. So it's happened with solid state drives as well. So solid state drives replacement for hard drives these vastly less power than hard drives a hard drive a few ": [
      3227.2,
      3255.2,
      84
    ],
    "length instructions to save space. So the common instructions like add would be one bite The Uncommon instructions will be longer because of who says short instruction should be for common case are the common case should be start instructions. Wow, that's an excellent point. It is kind of like Huffman encoding but we don't cover instructions that right. If we have lots of them. They should take up a ": [
      3666.7,
      3697.5,
      100
    ],
    "library for Python. And this is what happens down in the bottom. Someone writes this code to make sure that things get back to Rice. I was also do this on its own but it's a little bit, you know, it relies on the loops being very well-behaved for the compiler can tell what's going on. But nothing is going to go wrong if it tries to do things actors, ": [
      2721.9,
      2744.5,
      67
    ],
    "little space as possible. And so their success had some downside the essays of all kind of organically. They got Messier and Messier and more complex. The Pinnacle of complexity was in an essay called backs. And there was actually the instruction that would evaluate a polynomial so you can give it the coefficients on a polynomial and it would take an invite and it would raise it to a ": [
      3697.5,
      3723.4,
      101
    ],
    "means it's doing good things in the memory system and it will probably has been scheduled aggressively maybe by the compiler by hand since I'm getting lots of instructions to the pipeline. I'm doing lots of work. And so the more work I do the more energy I burning of the more power dissipated then my processor gets hotter. Who started this is actually a little I guess. I don't ": [
      3204.3,
      3227.2,
      83
    ],
    "more than one coprocessor, but that's of these guys are doing one of the book you're looking pretty much the same if we had suffered a floating-point. This would become a function call. The compiler would actually insert a function call for you and it would take a long time to execute like Fifty Cycles. All right. So that's fine point the other thing that modern processors have because it's ": [
      2299.3,
      2324.2,
      52
    ],
    "more work in the pipeline. So I will say yes, it does require a longer pipeline. So what was this summer like to volunteer? What was the confusion or would like to disagree with my my answer on that one? Yes, I like to stand up in front you ascribe. Why is that not correct? So if we look inside the five stages pipeline right acute memory and right back. ": [
      626.1,
      657.3,
      5
    ],
    "need to read? How many arithmetic operations do I need to perform ride? If you can't Implement all of this in a nice pipeline equipment that you need way more harder than it actually is another interesting piece. We have to move instruction. This is like I was suggesting you one of my colleagues that we should teach 141 and x86. He was skeptical as you might imagine. Why can ": [
      4535.6,
      4557.3,
      130
    ],
    "night and we're going to multiply 1 * a * B and put the result in to see The Interlopers happens. This is actually basically a bunch of. Products. You can see it's using our multiply accumulate instruction. So it's using that m a. S. So it's multiply a positive single Precision floating point. There can be 32-bit floating-point and also a 64-bit floating Point values if you want more ": [
      2237.2,
      2275.5,
      50
    ],
    "nips operations. They're called microwatts. They're not actually there's some internal thing that is not documented outside of Intel but it takes them and it converts them and some of them that converts into multiple microwaves and this is what actually gets executed. So an x86 processor is a big complicated thing that understands x86 instructions and translated it into an internal risk. I say and then the rest of ": [
      4742.2,
      4768.0,
      138
    ],
    "of like so current processors grew more or less directly out of MEPS and some other similar see if he was in the early eighties that is a pretty straight path of evolution between them and where we are today and we've covered most of its see how that plays played out the graphics processing world started out as this fixed function thing and slowly became more general-purpose inside. You ": [
      3425.2,
      3449.0,
      91
    ],
    "of you. Right? So you have to keep this thing spinning they make noise cuz you're spending all the time that burns a lot of power ssds nothing spend as much much less power when they first came out and people start putting them into a laptop service like my laptop got hotter Sophie going Cora and you Google for why do ssds bring my laptop harder? You will find ": [
      3255.2,
      3276.7,
      85
    ],
    "on suction and it works on multiple. Data, this is something called this from something called Flynn's. taxonomy But you can go read about if you're curious. What are some other alternatives to send me there's Mindy and thisbe and Mincey and some other stuff tension do this is Advanced Battery instructions 512 seven this latest version of Intel's they have left a 512-bit Becker's so that means you can ": [
      2881.4,
      2917.8,
      73
    ],
    "one bite instructions, that would be it would have been much better cuz it cost less space to a hold them. And then there's also this implementation technique that people used to use called micro coating reconstruction actually triggered the execution of a sequence of instructions kind of internally instructions to be really complicated, right they could do a bunch of stuff is actually instruction x86 for copying a string ": [
      3621.2,
      3644.8,
      98
    ],
    "one. either of those All right, last one floating Point operations over and got this pretty much with their most important scientific codes and software. They can totally be pipeline. This is nonsense. So if you're ever curious and you want to burn 5 minutes go look up. Turbo in cab you later. That's a good way to waste 5 minutes on the internet as if we needed more of ": [
      1065.6,
      1101.4,
      20
    ],
    "our processor, so it's kind of We get some new instruction formats. I'll get to explaining what one of them is down here in a moment actors add and subtract and multiply and so forth one very useful operation for him when it's called a multiply accumulate and hear what we do is this looks very different from previous instructions because it has three inputs and went outside. So we're ": [
      2032.9,
      2061.1,
      43
    ],
    "output. I'm not sure if anyone else uses this besides multiply accumulate what their price several flavors a little complicated logic of how you do math on floating Point numbers is very complicated. There's a huge fat called IEEE 1334 I believe that handles things like underflow and infinity and dividing by zero North other stuff. You need to make things, right? All right, so and then there's also a ": [
      2118.9,
      2154.3,
      46
    ],
    "paralyzed edition of all of those entries, right? Are everywhere Inns in scientific Elder also everywhere in graphics Graphics fundamentally. Almost all of Graphics is about 4 by 4 matrix multiplication. That's it. So they represent points in three dimensional space of the forest slot nectar for a very clever reason called homogeneous coordinates that makes everything work amazingly well and then the transformation to like rotate something in 3 ": [
      2354.9,
      2386.8,
      54
    ],
    "pointer. There are some general purpose registers. Oh, this is actually store in here. Originally. This isn't this is he's already x86 64 which is a 64-bit extension. They had no they added some where they could this is the end X Predator which is used for the string operations. So this actually increments this is where it keeps track of the loop and Excellence copying string. This is the ": [
      4404.6,
      4432.3,
      125
    ],
    "precision, and then these instructions right here. Load Word xb1x. Do you want a little bit see one in Star Wars who won these our operations to move things between the integer register clown the flame point register. I was actually stands for his co-processor one because of you. They used to model the floating point you to the start of a separate coprocessor when you can maybe maybe I'm ": [
      2275.5,
      2299.3,
      51
    ],
    "registering evening or they might be true and false data and or false the penalties if we haven't but the algorithm is that we look for instructions that don't have any incoming edges. So they don't have any man satisfied appendices and we can do that just fine. So there's a single instruction and I can assure you that instruction if I could only execute one instruction for cycle. I ": [
      814.1,
      833.7,
      11
    ],
    "regular predictor of syntax. We don't know if we're not going to teach us to teach you guys to read x86 assembly. We don't do that. I can do that in this class. It's the most widely used as in your phone is probably running on more processors the next 36 is now but a lot of them are pretty deeply embedded but as far as like been complications you ": [
      4151.7,
      4178.0,
      118
    ],
    "ride at 1% The Slowdown is 1/4. Plus .99 that's the one minus X for the for the rest of the program and this thing equals. as it turns out .97 so that means that the speed up of Hardware vs. Original is .97. What means lose about 3% of performance by going to floating Point instead of integers. That's pretty that's not bad and then to find out the ": [
      1622.7,
      1656.0,
      29
    ],
    "right now so far been talking about MEPS and he said, you know, it's a spare that has 3 instruction form beautiful simple instruction sets get more complicated as that happens. Whenever you start having floating points country floating-point register file. That's a separate set of values. Now last time I said, we were talking about registering Amy and I said what why can't we add more registers in the ": [
      1982.0,
      2009.2,
      41
    ],
    "same piece of code branches. Are we really broken up? So there's a compare and discuss the comparison to record the outcome in the special thing called the condition code register and then I do a branch. I do a conditional jump depending on the contents of the condition. So this is like a jump of greater than jump of greater than equal to jump with less than and that ": [
      4610.0,
      4632.5,
      133
    ],
    "same time right now. They're big Matt's here is a little flowchart that I found. This is the kind of syntax structure for an x86 instruction. They can be between 15 and 1B long is the shortest ones are one by the longest one you can have is 15x long and there's a bunch of stuff in here. Y'all three producers on the final say, okay. Don't worry. Some people ": [
      4651.9,
      4679.2,
      135
    ],
    "saw how this makes everything easy to decode write G-Code is almost free to take the oxycodone. They control you just pull out these register values. The register values are always in the same place. So we just slice them off of the instruction where that we fetch her memory and everything is perfect. And then, you know, we can encode branches and loads and stores and a meat operations ": [
      3888.2,
      3908.7,
      108
    ],
    "single issue out of order processor. That's totally possible. Ways to show the clock cycle time may or may not sell something answer. So go back and I'll give us so I'll give you guys credit for B or C cuz it's a little bit unclear. But usually if you want to increase the pipeline. 4 out of order Alright Branch prediction. It is just a guess, right the processor ": [
      901.8,
      934.8,
      15
    ],
    "so you can connect to look and see what you know vectorization looks like or what comments of expression Play She Looks like looks like what the different level optimizations do when they process the code. I highly recommend it. I mean You know, it's not something I would do about something. I might do on a Saturday night if that's something they recommend for you guys on a Saturday ": [
      2216.5,
      2237.2,
      49
    ],
    "somehow make all of this. Okay, and I have fixed all these problems. Where are we TimeWise? 10 minutes All right, we're going to go through and talk a little bit about x86. Just to show you how terrible it is. We're going to use this world of pain 6 processor whose name. I cannot remember and the lead architect came up with the Was writing down all the instructions ": [
      4202.8,
      4280.3,
      120
    ],
    "space or move the camera in 3 space is you can specify that is a four-by-four vector and you just multiply points to do the Leah transformation to move stuff around that's really like all it is and because of that you can go to Intel's website and you can download a library that among other things has the fastest possible way to multiply to four by four vectors on ": [
      2386.8,
      2408.0,
      55
    ],
    "speed up of software support versus harder than for flame point. We just take the ratio of those two and that is a .974 / 0.66. and this is Hardware vs software and that equals 1.46. So that is that one and that one? Renaissance the questions about that All right, so that's why we might want to add floating-point support to our processor to let's say heavy heavy duty ": [
      1656.0,
      1714.2,
      30
    ],
    "still couldn't work on it and it's evolved a completely different kind of passed until the terminology and the architect of the terminology is totally different the Ferrari models. Totally different architectures are beginning to look more similar in some ways, but they really pretty different be so we will talk about that. Hopefully, you're the end of the quarter only have two more time. All right, but that is ": [
      3449.0,
      3469.5,
      92
    ],
    "take a I don't know a hundred Cycles. So I can execute the load o actually looks make the air to here instead. He'll be more interesting. Seven already here and here it doesn't have any dependents is so I'm going to execute the load. The load is going to take a hundred Cycles text a kid. So this dependence is going to remain there for a hundred Cycles. In ": [
      858.5,
      881.5,
      13
    ],
    "takes a variable number of steps depending on what the numbers are like so very long pipeline. This is a rough sketch of Intel Core i7 pipeline. I think the one your textbook and then you sort of stage of a bunch of different values. So you'll maybe have a couple that can do memory you'll have some for energy and you'll have some for floating point and the floating ": [
      1762.6,
      1791.3,
      33
    ],
    "tend to fetch fewer instructions for thread and that means it was less critical that our Branch predictor be good to require the process. the CPUs require speculation Yes, right. So we're guessing that the CPU is going to do something and then we speculated that's cracked. Then we proceed accordingly. So I said that A and D are correct. File give you a n d or B on that ": [
      1034.4,
      1065.6,
      19
    ],
    "that even if you have a very small amount of your program and you slow it down by a whole lot. This is actually a 52 x slow down. That you can actually have a big effect on your overall overall program performance. So what's up girl that question this one is for you. So I'll have the same setup. But now the question is what is the speed up ": [
      1269.9,
      1295.5,
      25
    ],
    "that on out of our superscalar the out-of-order super kills a pretty good at finding that parallelism so you can imagine if I have a loop it goes from 1 to 10 and it fetches going to take the garbage. There's two values for memory will the Flies them together and then writes them back to another spot. Another array has a lot of parallelism because each one of those ": [
      1909.5,
      1929.2,
      38
    ],
    "the beginning of 141 the IFA was invented in 1964 King and Computing trajectory tables and so forth in 1940-1964 system 360 which has remember and available slot helped invent decided decided they were going to have a family and machines from wimpy Little Machines to big big machines 20 machine was like the size of this lectern baby and the big machine would have made me feel this room. ": [
      3520.9,
      3566.2,
      95
    ],
    "the meantime. I connect to get this instruction Navy some other ones on down the line. So I still won't be executing more than one instruction for cycle would rather than being stuck waiting for a hundred Cycles before I can execute this instruction. I can go and execute some other stuff. And so I will effectively hide the latency for that long load operation. So you can have a ": [
      881.5,
      901.8,
      14
    ],
    "the memory location or maybe not you know, because it's not very regular and all these are destructive. So this one So here's a store. I'm going to move this label into this under the stack pointer what I can do with one move operation. Arithmetic, it's the same thing. I can add and subtract among all of these things. So I can do memory operations and arithmetic in the ": [
      4581.6,
      4610.0,
      132
    ],
    "the pipeline looks basically like for the rest of the quarter That's the trick is a big giant mess. It's a huge power sink right energy sink. I have to do all this work just pure overhead. This is a big reason why arm is running on your phone instead of x86 is a bunch of business reasons as well and some poor strategic Decisions by Intel's back in the ": [
      4768.0,
      4792.3,
      139
    ],
    "the question is how do we Implement? X86 because we started with Nets and mint is this simple elegant looking inspection set. So I don't want to do for the rest of today is to explain to you how we got to where we are at Exit 86. Why does a sad sad story and how we deal with it in modern processors? so in the beginning back far before ": [
      3495.2,
      3520.9,
      94
    ],
    "the same operations to work on those. They are everywhere and modern applications we're going to do is we're going to take this this Vector here. So the slice of the array we're going to go right there and then we're going to go and then we're going to go to play at times this phone and this one and this one will fill it in over there. This is ": [
      2435.1,
      2459.0,
      57
    ],
    "them is in big floating point codes for there's a lots and lots of instructions. I got xq2000 instructions to do some complicated Matrix calculation. The branches are really predictable and everything and so we can get pretty close to Peak. Instructions per cycle or the lowest possible CP I ended up working really well for floating point and Graphics is off-putting point. All right instructions had somehow and so ": [
      1950.4,
      1982.0,
      40
    ],
    "there somewhere and it's because of this are critical for the critical little bit too critical Loop, including the next program counter chairs in there so that I could increase of cycle time because it could add some more crushed up here in the front end. So I will say that one is correct. No, and this is because of that 70 we fetch instructions for more thread. So we ": [
      1000.7,
      1034.4,
      18
    ],
    "they work on very very large vectors have Peugeot models that are all built around very very large vectors. And so there's no use for graphics. They're really good at graphic. This is what is in your graphics card still obviously. They look very different from the processors that we talked about so far and we will go through and look at how they work. It's really interesting. It's kind ": [
      3402.0,
      3425.2,
      90
    ],
    "this complicated. Scheduling logic handles all of this pretty much for free, right? Because remember we had these instructions waiting and they were just waiting for their inputs to arrive and if the inputs take an extra 10 Cycles to arrive, that's fine the instructional just sit there for an extra ten Cycles waiting for if they didn't show up this all works together pretty nicely. Alright some architectures have ": [
      1817.0,
      1846.4,
      35
    ],
    "this is a blue body and instead of Stamp Out multiple copies of it. So I get more static instructions, but then I'm allowed more flexibility and how that those instructions get scheduled and it also makes a lot easier to explore these Vector instructions because I can see you're bigger group of a bigger group of instructions to optimize across and so the end up performance gains are pretty ": [
      3009.7,
      3035.8,
      77
    ],
    "this much. It was actually quite a bit of confusion. I have no idea how I did that. All right. Oh even worse. I don't know how I'm going to get it down. Cuz I can't get my amount of my pen over there. Interesting actually, okay. Oh, okay. That's a start. Okay. All right, there's some confusion about B. I just talked about be so the question is see ": [
      737.1,
      785.5,
      9
    ],
    "those. All right, and they're not just as fast as an integer operations. All right, so I reminder a midterm. in 7 days in a world where your midterm is in 7 Days much like this one true not true mid-term review. Superman games I wish I planned that. Hello world. I'm like next Thursday. Please come with. Questions. Yeah, okay come with questions. Someone asked me today if I ": [
      1101.4,
      1157.9,
      21
    ],
    "through 9 and you do math like that the disaster still there. They combine memory and arithmetic operations are special purpose registers and there are many many many many instructions. Implementing x36 correctly is almost intractable. This is actually a big strategic Advantage for Intel. It's very difficult for someone else to go along and build an x86 processor AMD is. There was another company called spirits that used to ": [
      4097.0,
      4126.0,
      116
    ],
    "to compile form. So there were two one of them is called risk nipples milk at Stanford. They were designed by Go to the authors of your textbook there were two competing project Hennessy and Patterson recent winners of the Turing award mostly for this work and they were very similar, right? So the same kinds of ideas. There a compiler friendly not user-friendly simple regular is a everything is ": [
      3836.2,
      3862.9,
      106
    ],
    "to take them to a 10 stage pipeline the fundamental like I'm out of work that we do for instruction doesn't change because we're still just fetching decoding executing doing the memory operation running back, but here actually doing something new. How about requires multiple issue? But actually I go ahead I think quite a people quite a few people said this. How is it I have trouble telling you ": [
      711.4,
      737.1,
      8
    ],
    "triggers a lot of this was a good finally fit an entire process or under a single piece of silicon. The first one to do this commercial with the until 40 the Intel 4040, which is followed by the Intel 8080 huge step forward with that man. But if you want to have low CPR instructions, and we also by the time you were like 30 years and now we ": [
      3771.1,
      3794.1,
      104
    ],
    "up soccer playing playing versus the original program, which you already know. So this is versus both software versus original equals .66 that's because we took 1% of our program and we slowed it down by a factor of 52.66 need to know is the the speed up of Hardware vs original. Hey that we can use animals law. And we will get one over. .01. So that's the X ": [
      1573.1,
      1622.7,
      28
    ],
    "use that as the address I'm going to access. This is a scaled offset. So this is a base address plus a second read a bass register base. I just registered + x to the end so I can jump in big slices. I see these faces are just like like I ate a bug Add a sound of the end. So this is like how many registers do I ": [
      4505.6,
      4535.6,
      129
    ],
    "used. So this is a type of sea called V4 s i This stands for four slots. find Integer, that's just the name. I chose or this example shows. I pulled this off the web. And then this is like a little bit of compiler black magic about this this declaration. I just made and it's going to be a vector of size 16. This is the number of bytes. ": [
      2629.6,
      2664.6,
      64
    ],
    "variable length will see some examples of that in a minute there a bunch of complex rules about rich registers can be used and when in mips all the instructions are the same there is a kind of a on as far as the instructions go there is a protocol which you probably look good for something similar forearm or we pass arguments and so forth and certain instructions passing ": [
      4050.8,
      4071.3,
      114
    ],
    "very simple and straightforward a few simple flexible flask operations of the compiler can come by easily on a nail stopping to Steph Curry data access from data manipulation. That's why we have ADD instructions and load instructions and the arithmetic operations never touch memory and vice-versa that makes everything simpler. We've seen the three beautiful instructions to the instructions for Mass. Right how elegant right? Just three things. We ": [
      3862.9,
      3888.2,
      107
    ],
    "we can just do them over here. So we don't need a big other special piece of hardware. how to do it All right. So all different processors have different sorts of extensions for this Intel has their own as a whole sequence of them. I forget if mxr SSC came first, I think maybe SSE came first. assistance for single Construction multiple data 951 instruction say Vector ad sing ": [
      2834.1,
      2881.4,
      72
    ],
    "where you do. That's where you do really big big floating Point math. If you want to do big big floating-point math and Vector math. All right. So we have talked about having commitment. I have shown you how to go from the IRS area to a single cycle processor and then all the way through a deeply pipelined out of order vectorized superscalar processor. So one set of open ": [
      3469.5,
      3495.2,
      93
    ],
    "why the summer confusion about teeth at whether or not it requires multiple issue. Border doesn't require multiple issue. So the reason is if we go back and remember what we were the algorithm right was we have this. I'm we build this instruction window right even 3 to these are instructions and we have some dependents has between them. And they can be true to penances if we've done ": [
      785.5,
      814.1,
      10
    ],
    "with floating Point versus the original program? With the should say Hardware floating points if we have hardware and the hardware than what is the speed up of Hardware flying Point versus the original programs. And what is the speed up of Hardware floating Point vs. Software playing point on that program? All right. So where can I fill the bed and then you guys were kind of groups in ": [
      1295.5,
      1336.0,
      26
    ],
    "with gpus with Graphics? So we're going to get there. Hopefully eventually we are going to get a step closer to Graphics in the next set of slides and we'll see this if you slide but when I do something like a vector operation or so. Products, I'm doing a little tight Loop and I'm going in multiple multiplying a bunch of numbers together in an array. If I run ": [
      1871.3,
      1909.5,
      37
    ],
    "your arithmetic operations solo they do an ad to compute the offset, right? They take the address. They had the immediate and that's the address that we used. There's only one arithmetic operation one memory access to register reads one register ride in one branch never more than that. And that means that I I don't have any if I if I provide her to do that, I will never ": [
      3929.3,
      3948.0,
      110
    ]
  },
  "File Name": "Intro_to_Computer_Architecture___A00___Swanson__Steven_James___Fall_2018-lecture_9.flac",
  "Full Transcript": "10 seconds  3 2 1  10 seconds  4 3 2 1  4 3 2 1  10 seconds  5 4 3 2 1  Oh, sorry.  It sounds like I'm for sale.  5 4 3 2 1  that's out of the quiz.  The salad from the last time I asked this.  5 4 3 2 1  All right.  So happily.  It appears that very few people are cheating on the clicker quizzes. So I appreciate that.  a little bit of confusion on some of them  Savannah's law  does it limit the speed of speed limit station can cause  No, 100% of you got that. Correct. Very good.  I should just say that everything is the most important thing in the class and everything will be on the final and then people remember remember all of it. Yes. It does. Is this the  Yes, it is. This Ables law know this is some terrible version of Handel's law.  And so the answer is BNC.  Very good. Sorry to definitely be that is definitely see.  Hello. All right the performance equation. Is this the performance equation?  No, because this should be cycle time not megahertz. This could be one over megahertz.  And that would be a good answer. I guess technically it should be just one over hurts, right because it's all the decimal point there for a CPU or is mathematical model for CB performance. This is and has lost so it doesn't look to limit the impact of optimizations.  Most architectural optimizations do not affect all three terms equally.  And it can actually account for the effects of memory operations. It shows up in.  CPI as we will see shortly. So it is definitely not this one and not that one but it's definitely not one.  I don't want her execution. Do we still use out-of-order execution? Heck yeah, I'm podcasting so I have to say hack.  hacks  we use the music ride out of out of order sticks qution.  All right. I'm not the case. Does it require a longer pipeline?  I don't want no execution requires scheduling scheduling is fundamentally more work in the pipeline. So I will say yes, it does require a longer pipeline.  So what was this summer like to volunteer? What was the confusion or would like to disagree with my my answer on that one?  Yes, I like to stand up in front you ascribe.  Why is that not correct? So if we look inside the five stages pipeline right acute memory and right back. So do you schedule to do some extra work after schedule the instructions?  so  the way that back it's implemented is that you add more pipeline stages to do that extra work?  I'm so I guess you know if if I were one of you and I was arguing like I did with my instructors and I was in college, I would say yes, dr. Swanson, but you could just build a worst pipeline that have the same number of stages right? I could just cram schedule into decoder something and the length of a pipeline would stay the same. So maybe I'll add.  to maintain  cycle time  and I would definitely be the case because there's fundamentally more work we have to do for instruction. Right usually when we do from the pipeline when I go to Safeway you to take them to a 10 stage pipeline the fundamental like I'm out of work that we do for instruction doesn't change because we're still just fetching decoding executing doing the memory operation running back, but here actually doing something new.  How about requires multiple issue? But actually I go ahead I think quite a people quite a few people said this.  How is it I have trouble telling you this much. It was actually quite a bit of confusion.  I have no idea how I did that. All right. Oh even worse. I don't know how I'm going to get it down.  Cuz I can't get my amount of my pen over there.  Interesting actually, okay.  Oh, okay. That's a start.  Okay.  All right, there's some confusion about B. I just talked about be so the question is see why the summer confusion about teeth at whether or not it requires multiple issue.  Border doesn't require multiple issue. So the reason is if we go back and remember what we were the algorithm right was we have this.  I'm we build this instruction window right even 3 to these are instructions and we have some dependents has between them.  And they can be true to penances if we've done registering evening or they might be true and false data and or false the penalties if we haven't but the algorithm is that we look for instructions that don't have any incoming edges. So they don't have any man satisfied appendices and we can do that just fine. So there's a single instruction and I can assure you that instruction if I could only execute one instruction for cycle. I can still go through and find those instructions and the order would go something like that, but I could execute them in and it will see as instruction latency to get really long as I got it with memory. So let's say I have this have a few instructions like this and this one is a load up here.  Right. I have a dependency there and this is going to take a I don't know a hundred Cycles.  So I can execute the load o actually looks make the air to here instead. He'll be more interesting.  Seven already here and here it doesn't have any dependents is so I'm going to execute the load. The load is going to take a hundred Cycles text a kid. So this dependence is going to remain there for a hundred Cycles. In the meantime. I connect to get this instruction Navy some other ones on down the line. So I still won't be executing more than one instruction for cycle would rather than being stuck waiting for a hundred Cycles before I can execute this instruction. I can go and execute some other stuff. And so I will effectively hide the latency for that long load operation. So you can have a single issue out of order processor. That's totally possible.  Ways to show the clock cycle time may or may not sell something answer.  So go back and I'll give us so I'll give you guys credit for B or C cuz it's a little bit unclear. But usually if you want to increase the pipeline.  4 out of order  Alright Branch prediction. It is just a guess, right the processor is making Augusta might be educated guess but it's just a guess. It doesn't affect cycle timer.  that is  My life and that's where it doesn't affect cycle time or pipeline.  What did I say? The answer? That one was I'm not even sure I wrote this quiz like 5 minutes ago. Yes.  So that was sent to your question. So he has Winterfell friction cause increase the number of Cycles. That's true. It would increase CPI which is cycles per instruction, but cycle time remember functional limitation so it would not end. It definitely won't miss prediction won't increase cycle time. It won't increase increase cycle time. If there was a specially the front in the front end for the front end dispatch and the instruction memory on decode and the critical path of the processor is usually up there somewhere and it's because of this are critical for the critical little bit too critical Loop, including the next program counter chairs in there so that I could increase of cycle time because it could add some more crushed up here in the front end.  So I will say that one is correct.  No, and this is because of that 70 we fetch instructions for more thread. So we tend to fetch fewer instructions for thread and that means it was less critical that our Branch predictor be good to require the process.  the CPUs require speculation  Yes, right. So we're guessing that the CPU is going to do something and then we speculated that's cracked. Then we proceed accordingly. So I said that A and D are correct. File give you a n d or B on that one.  either of those  All right, last one floating Point operations over and got this pretty much with their most important scientific codes and software. They can totally be pipeline. This is nonsense.  So if you're ever curious and you want to burn 5 minutes go look up.  Turbo in cab you later.  That's a good way to waste 5 minutes on the internet as if we needed more of those. All right, and they're not just as fast as an integer operations. All right, so I reminder a midterm.  in 7 days  in a world where your midterm is in 7 Days much like this one true not true mid-term review.  Superman games  I wish I planned that.  Hello world. I'm like next Thursday. Please come with.  Questions. Yeah, okay come with questions.  Someone asked me today if I could go to another meeting during our class time next Thursday, and I was like, I can't miss class and I was thinking in terms of you. I wouldn't be great. I just put up a video of Leo Daily Reporter doing one of his mid-term review.  I will answer questions and tell them no more questions and then we will get out of class early. So make sure you come with questions right last time.  Alright, so  Ariana Grande this problem we have is we have this case where we spend 1% of our time doing some integer addition inside some Keely for my program. So it wasn't a lot so just 1% I didn't do a lot of it and I decided that I don't know maybe do improve the accuracy of something I decided album. I see if he doesn't have floating-point support. And so then what is a speed up and we saw from some data, I grabbed off the web that a software floating-point add cost 52 cycles and some processor. I found some Intel processor Hardware in the  Yeah in the Elliott turned out to be a .66% speed up, which is a 30.  3% slowed down  and this is because Amanda's law says that even if you have a very small amount of your program and you slow it down by a whole lot. This is actually a 52 x slow down.  That you can actually have a big effect on your overall overall program performance.  So what's up girl that question this one is for you.  So I'll have the same setup. But now the question is what is the speed up with floating Point versus the original program?  With the should say Hardware floating points if we have hardware and the hardware than what is the speed up of Hardware flying Point versus the original programs. And what is the speed up of Hardware floating Point vs. Software playing point on that program?  All right. So where can I fill the bed and then you guys were kind of groups in a look at the answer?  37 seconds or so  5  5 4 3 2 1  descent. All right, so  discuss amongst yourselves  Give me another 15 seconds or so.  5 4 3 2 1 this is exciting.  little more company  All right, let's go through and do the math. It's a bit tricky.  Alright, so so we know the things we know we know the speed up versus.  The speed up soccer playing playing versus the original program, which you already know. So this is versus  both software versus  original equals .66  that's because we took 1% of our program and we slowed it down by a factor of 52.66 need to know is the the speed up of Hardware vs original.  Hey that we can use animals law.  And we will get one over.  .01. So that's the X ride at 1% The Slowdown is 1/4.  Plus .99 that's the one minus X for the for the rest of the program and this thing equals.  as it turns out .97  so that means that the speed up of Hardware vs. Original is .97. What means lose about 3% of performance by going to floating Point instead of integers. That's pretty that's not bad and then to find out the speed up of software support versus harder than for flame point. We just take the ratio of those two and that is a .974 / 0.66.  and this is  Hardware vs software and that equals 1.46. So that is that one and that one?  Renaissance  the questions about that  All right, so that's why we might want to add floating-point support to our processor to let's say heavy heavy duty a floating points program. That number might be 50% might be 60% if it's really floating Point intensive and now I'm going to software it's just going to be a complete disaster in a slow down 60% of my program by 52 x right that's really really bad performance is going to be something like 30 times worse. If I do it in Hardware, then I'll you know, every time a lot of that last performance as a big win. So how do we integrate it challenges is floating Point operations. Take a while. They can take a variable number of Cycles the dead guy found for I think it's until the Halen was 92203 cycles for a floating-point divide the reason to take so long as they do.  As you know from elementary school long division, you know, it takes a variable number of steps depending on what the numbers are like so very long pipeline. This is a rough sketch of Intel Core i7 pipeline. I think the one your textbook and then you sort of stage of a bunch of different values. So you'll maybe have a couple that can do memory you'll have some for energy and you'll have some for floating point and the floating Point operations will take variable numbers of cycles. And if you look at you can get tables of instructional agencies for processors and they'll be kind of all over the place 12416 sine and cosine or like 50 or 60 Cycles or something like that a square root is pretty complicated as well. And so some of these instruction sit there for a really long time. We talked about all this complicated.  Scheduling logic handles all of this pretty much for free, right? Because remember we had these instructions waiting and they were just waiting for their inputs to arrive and if the inputs take an extra 10 Cycles to arrive, that's fine the instructional just sit there for an extra ten Cycles waiting for if they didn't show up this all works together pretty nicely.  Alright some architectures have a separate floating-point register file so that I actually sort of being a separates at least was the separate set of architectural registers or may not be a separate set of physical registers. And remember you register renaming and also sometimes have separate renaming and scheduling logic. So in some processor is it actually could have two pipelines one for doing floating-point one for doing integer? Yes.  How does graphics with gpus with Graphics? So we're going to get there. Hopefully eventually we are going to get a step closer to Graphics in the next set of slides and we'll see this if you slide but when I do something like a vector operation or so. Products, I'm doing a little tight Loop and I'm going in multiple multiplying a bunch of numbers together in an array.  If I run that on out of our superscalar the out-of-order super kills a pretty good at finding that parallelism so you can imagine if I have a loop it goes from 1 to 10 and it fetches going to take the garbage. There's two values for memory will the Flies them together and then writes them back to another spot. Another array has a lot of parallelism because each one of those Loop iteration is actually independent and then we'll do a pretty good job of finding that parallelism. And so this is member I said last time the reason we have smt is because in a lot of for a lot of programs, we can't find you know, the five instructions are in a big out of order superscalar can execute a recycle but one place where you can often find them is in big floating point codes for there's a lots and lots of instructions. I got xq2000 instructions to do some complicated Matrix calculation. The branches are really predictable and everything and so we can get pretty close to Peak.  Instructions per cycle or the lowest possible CP I ended up working really well for floating point and Graphics is off-putting point.  All right instructions had somehow and so right now so far been talking about MEPS and he said, you know, it's a spare that has 3 instruction form beautiful simple instruction sets get more complicated as that happens. Whenever you start having floating points country floating-point register file. That's a separate set of values. Now last time I said, we were talking about registering Amy and I said what why can't we add more registers in the answer was because we only had five bits to name the registers one way around that is to have a completely separate set of floating-point separate set of registers and basically have an implicit in the instructions are executing which of the two big set. The registers are so sexy cute. So floating Point operations, operating floating-point registers, and therefore we get to effectively doubled the number of registers in our processor, so it's kind of  We get some new instruction formats. I'll get to explaining what one of them is down here in a moment actors add and subtract and multiply and so forth one very useful operation for him when it's called a multiply accumulate and hear what we do is this looks very different from previous instructions because it has three inputs and went outside. So we're going to multiply R1 and R2 together and then we are going to add the results are three. So this instruction is purpose-built for doing. Products and convolutions, if you know anything about digital signal processing so you can imagine if I'm doing it. Products. Product I take to raise two vectors and I want to compute the product of each of the paralyzed entries in the array and then I want to take the sum. I want to add up all those results are three in this case would be the the  The sum I would put our three over here as well. And I just run this write down my vector and I will get the dot product for free and I've saved having to execute the extra extra add instructions for slots. It has three inputs and output. I'm not sure if anyone else uses this besides multiply accumulate what their price several flavors a little complicated logic of how you do math on floating Point numbers is very complicated. There's a huge fat called IEEE  1334 I believe that handles things like underflow and infinity and dividing by zero North other stuff. You need to make things, right?  All right, so and then there's also a couple of instructions for moving things between the integer register file and the flame point register file so you can Shuffle step back and forth.  That's how you put the volume.  Here's a quick example. I found this amazing website. I wasn't going I was thinking I meant to tell you about if I don't think we're going to actually do a lot of translating code to  Assembly a homework. It's called compiler.  Explorer  Dot-com on some guy's website, but it's really cool and it will show you the output of the assembly for the compiler, but you can choose between like 40 different compilers. So in a bunch of different instruction sets, you can do nips and x86 and Allen 32 and a bunch of other stuff and you can also pass it all kinds of command line options so you can connect to look and see what you know vectorization looks like or what comments of expression Play She Looks like looks like what the different level optimizations do when they process the code. I highly recommend it. I mean  You know, it's not something I would do about something. I might do on a Saturday night if that's something they recommend for you guys on a Saturday night and we're going to multiply 1 * a * B and put the result in to see The Interlopers happens. This is actually basically a bunch of. Products. You can see it's using our multiply accumulate instruction. So it's using that m a. S. So it's multiply a positive single Precision floating point. There can be 32-bit floating-point and also a 64-bit floating Point values if you want more precision, and then these instructions right here.  Load Word xb1x. Do you want a little bit see one in Star Wars who won these our operations to move things between the integer register clown the flame point register. I was actually stands for his co-processor one because of you. They used to model the floating point you to the start of a separate coprocessor when you can maybe maybe I'm more than one coprocessor, but that's of these guys are doing one of the book you're looking pretty much the same if we had suffered a floating-point. This would become a function call. The compiler would actually insert a function call for you and it would take a long time to execute like Fifty Cycles.  All right. So that's fine point the other thing that modern processors have because it's good for computational intensive code is vectors. So vectors are a little arrays of numbers. So, you know usually for for going to talk about the usually between 4 and 8 inches long what we can do with them as we can specify operations on them. So this could be a vector a  Henrico dr. B  And we could say C equals a plus b and this will do the paralyzed edition of all of those entries, right?  Are everywhere Inns in scientific Elder also everywhere in graphics Graphics fundamentally. Almost all of Graphics is about 4 by 4 matrix multiplication. That's it. So they represent points in three dimensional space of the forest slot nectar for a very clever reason called homogeneous coordinates that makes everything work amazingly well and then the transformation to like rotate something in 3 space or move the camera in 3 space is you can specify that is a four-by-four vector and you just multiply points to do the Leah transformation to move stuff around that's really like all it is and because of that you can go to Intel's website and you can download a library that among other things has the fastest possible way to multiply to four by four vectors on a on a given processor and I'll have a different version free CPU and uses magic piece of code, and it goes really fast.  Today's a very common as lost as if they're worth while speeding outfits also using Graphics, they can use RGB values. So RGB for color are usually start Hazard GPA is a transparency value for slot Vector usually of 4 integers on so they can use the same operations to work on those. They are everywhere and modern applications we're going to do is we're going to take this this Vector here. So the slice of the array we're going to go right there and then we're going to go and then we're going to go to play at times this phone and this one and this one will fill it in over there. This is a vector operation as a sequence of operations on safe for a doctor's I could maybe make them a lot lot faster.  So here is what support for this looks like in the in the instruction set.  So nips nips 32 too. We've been looking at doesn't actually support vectors 64 does this is a 64-bit version of nips. Basically the same except the registers are 64-bit slide and set a 32-bit swine. There is a 32in sure and free Vector register file. So now I have 32 vectors and they have names like v-0 and V1 and so far. It's like we had for the floating point in the end of the registry in a bunch of different way so you can have to 64-bit values or 4:30 to bedias rate 16 desires or Sixteen Acres values. So you can slice it up in a bunch of different ways Sandy. There's a bunch of instructions. There's Vector loads in stores as vectors add. There's some other operations to kind of moved me out with some track elements from doctors and shuffle them around and so forth.  Turn up 64-bit an x86. I believe there's no actually. Product instructions. So again, this is like the X operation but it's for a whole Vector so I can do that paralyzed and the additions and I get lots of value out. That's the you know, the the dot product of those about the portion of the of the of the big array.  What they do, they do a clever trick and this happens actually a lot is they actually overlap part of the floating points the poor part of the vector register file overlaps with a floating-point register file. So this is the boiling point register file right here with 64-bit values. The vector register file is the same except that adds another $64 on to the end. This means it's super easy to move stuff between the flooding in the back to register file because it's already there right if I want to move out of a floating-point value. I can just use slot zero of the vector and it's already there conveniently. I don't have to move it.  Here is basically the code for US Written haven't seen and then here is so this is kind of interesting. So one question right beside you specify this in your programming. You need some like pretty serious black magic for the compiler. At least if you want to guarantee that he gets used. So this is a type of sea called V4 s i  This stands for four slots.  find  Integer, that's just the name. I chose or this example shows. I pulled this off the web. And then this is like a little bit of compiler black magic about this this declaration. I just made and it's going to be a vector of size 16. This is the number of bytes.  Not the number of slots in the vector since integers are 4X long. This gives me 1/4 month doctor. So this is the little array when you can actually index it so I could say a zero and I would get a value but then I can declare some values of that type and I can have them together. And now this is actually equivalent to that Loop.  I thought if I were most mere mortals don't usually write code like this, but we usually happen is that you will use some library right that has a matrix multiply function or. Product function and inside they will do this and they will try to hide it from you. If you're using say something like python, you know, this thing will get Wrapped Up Inside by which is high-speed numeric applicator library for Python. And this is what happens down in the bottom. Someone writes this code to make sure that things get back to Rice. I was also do this on its own but it's a little bit, you know, it relies on the loops being very well-behaved for the compiler can tell what's going on. But nothing is going to go wrong if it tries to do things actors, but the compiler I couldn't actually house using compiler Explorer my new favorite tool trying to get it to admit vectorize code for Mid 64 by giving it looks like this.  It looks like this up here. I couldn't actually get it to produce the doctor outfit I wanted so.  This is one of these places where you know.  If I want to use this if you either after I could like this or you have to go in if he's a library with the devious thing. If you want do it yourself you after I could like this which is a little bit cumbersome or you have to go in and look at the assembly that the compiler is generating for your program to make sure it's doing what you wanted to do. I was in very scalable, right? Cuz if I change one thing right, so if I were to say just right and nocuous Lee plus plus right there I can probably would probably get confused and stop being able to vectorize the code.  All right. So, how do I add this to the pipeline? Right actually pretty easy. Usually we just put it in here.  Reflective actors in at the Flying Point Unit. So number one house. We saw the floating Point register file overlaps with the back to register file so we can just watch everything in their Vector operations are complicated the floating Point operations are complicated. And also we do a lot of floating Point operations on vectors. And so let it kind of happens is that the scalar floating Point operations just become special cases of the vector floating Point operations, and we can just do them over here. So we don't need a big other special piece of hardware.  how to do it  All right. So all different processors have different sorts of extensions for this Intel has their own as a whole sequence of them. I forget if mxr SSC came first, I think maybe SSE came first.  assistance for single  Construction  multiple  data  951 instruction say Vector ad  sing on suction and it works on multiple.  Data, this is something called this from something called Flynn's.  taxonomy  But you can go read about if you're curious. What are some other alternatives to send me there's Mindy and thisbe and Mincey and some other stuff tension do this is Advanced Battery instructions 512 seven this latest version of Intel's they have left a 512-bit Becker's so that means you can have 8 64-bit floats the gold standard for a high-precision say scientific calculations so I can work on 8 slot directors of those big data types.  How much is what's in your phone also has these they're called scalable Vector in extensions or SV PS. Ve but they support a variable length of vector up at 2 and 2048-bit. I'm so the is a actually has a lot of scalability built into it until sort of hard codes and Max were they started I think part of this is an arm came a lot later the game. So they say in the beginning until start out with him is the same as like a 64 right now.  Police are everywhere and they can use a lot so cold you ride may not actually do this. But you know, if you're doing watching movies or doing graphics and stuff this stuff is getting used all over the place. The performance gains are pretty big. This is just a graph from your textbook. So this is for Matrix X. This is an optimized code. So this is  without the  any of the optimization to be talking about this is what they've EXs is the latest version of Intel's instruction set extensions in this is a blue body and instead of Stamp Out multiple copies of it. So I get more static instructions, but then I'm allowed more flexibility and how that those instructions get scheduled and it also makes a lot easier to explore these Vector instructions because I can see you're bigger group of a bigger group of instructions to optimize across and so the end up performance gains are pretty big so you can get you know, something like 14 x with a moderately smart compiler and some Cindy acceleration instructions.  That's way more than you know, you're going to get from basically anything else we've seen.  Pipe lining or I guess if you turned off your branch for more than that, but this is a big big win.  Four vectors is Energy Efficiency or if I execute instructions that I see all the operations that are encoded in my Vector instructions and I execute them individually, then I have to fetch and decode and read the register file for each of those instructions to schedule an order processor. So this is energy breakdown for I didn't order mips processor 7 stages is a processor be used for a research prototype machine that we built. This is just the pure instruction fetch overhead. So I have to read Safari I cash and I have to fetch and decoded that accounts for 42% of the energy that I spend for instruction.  So if I go and do a vector actually more than this because this is an inner core. So if I was doing a modern processor out after registering a minion out of order scheduling on top of that would be even higher so probably more than half at least 4 or 8 or 16 or even 32 operations if I'm using narrow integer types, and so I save all of that fetch energy and so that reduces the total.  Reduces that's over had by twin 75 or 90% or maybe even more than that for small data sizes with pictures of lots of lots and lots of big win from Energy Efficiency perspective. Yes.  This is my operation. So should I should be cooler right? I mean that's what it seems like having your computer is because the places were Vector operations get used most are in these tight highly optimize Loops where you're doing something that took a lot of takes a lot of computation. And so that usually means that the code is very highly optimized. It means it's doing good things in the memory system and it will probably has been scheduled aggressively maybe by the compiler by hand since I'm getting lots of instructions to the pipeline. I'm doing lots of work. And so the more work I do the more energy I burning of the more power dissipated then my processor gets hotter.  Who started this is actually a little I guess. I don't know. Maybe we should name the Swansons Paradox. I like that Swanson paradox.  I'm so often times when you add things to your computer that make it go faster or you answering that you use less energy the computer gets hotter. So it's happened with solid state drives as well. So solid state drives replacement for hard drives these vastly less power than hard drives a hard drive a few of you. Right? So you have to keep this thing spinning they make noise cuz you're spending all the time that burns a lot of power ssds nothing spend as much much less power when they first came out and people start putting them into a laptop service like my laptop got hotter Sophie going Cora and you Google for why do ssds bring my laptop harder? You will find it. I answer the question because I know a lot of please don't you put that away faster faster and so the computers actually doing more work per unit time.  Which means the energy of the power dissipation goes up witches Watts for time and that means that it gets hotter because that's what happens when the power goes out more quickly Paradox or the final maybe for extra credit. Alright gpus are good. Bigger vectors must be better. So gpus take vectors support to the extreme. So gpus are these coprocessors back when I was in college, they were just for graphics and they were what you got if you wanted to have video games look good. I did I went to the University of Iowa for the summer and it's in research and they had an early.  GPU in a machine from a company called silicon Graphics was used to build the coolest Graphics workstations in the world and the computer that had the GPU in it was a size of a refrigerator.  I guess bag and it's sad over there and you're like sign up for time to use it then had one floating-point Pipeline and it could do like what looks like now pretty pretty crazy. Looking Graphics. Like they've gotten smaller and smaller and smaller eventually got programmable. And then after they got programmable if we can do real and then they got done on purpose. So now they're actually called gpgp use for general-purpose graphic processing units, which is a little bit of an oxymoron because of the general purpose, why are the graphics are called and they are basically giant Vector processors and they work on very very large vectors have Peugeot models that are all built around very very large vectors. And so there's no use for graphics. They're really good at graphic. This is what is in your graphics card still obviously.  They look very different from the processors that we talked about so far and we will go through and look at how they work. It's really interesting. It's kind of like so current processors grew more or less directly out of MEPS and some other similar see if he was in the early eighties that is a pretty straight path of evolution between them and where we are today and we've covered most of its see how that plays played out the graphics processing world started out as this fixed function thing and slowly became more general-purpose inside. You still couldn't work on it and it's evolved a completely different kind of passed until the terminology and the architect of the terminology is totally different the Ferrari models. Totally different architectures are beginning to look more similar in some ways, but they really pretty different be so we will talk about that. Hopefully, you're the end of the quarter only have two more time.  All right, but that is where you do. That's where you do really big big floating Point math. If you want to do big big floating-point math and Vector math. All right.  So we have talked about having commitment. I have shown you how to go from the IRS area to a single cycle processor and then all the way through a deeply pipelined out of order vectorized superscalar processor. So  one set of open the question is how do we Implement? X86 because we started with Nets and mint is this simple elegant looking inspection set. So I don't want to do for the rest of today is to explain to you how we got to where we are at Exit 86. Why does a sad sad story and how we deal with it in modern processors?  so in the beginning  back far before the beginning of 141  the IFA was invented in 1964 King and Computing trajectory tables and so forth in 1940-1964 system 360 which has remember and available slot helped invent decided decided they were going to have a family and machines from wimpy Little Machines to big big machines 20 machine was like the size of this lectern baby and the big machine would have made me feel this room. I mean if you wanted to go to a bigger machine near the rear end all your software, which was a real bummer.  So this is a big indention very important and initially they use in Focus was on usability by humans because there weren't good compilers. And so you would actually write a lot of assembly and so there were lots of user-friendly instructions things that could encode pretty complicated operation. So, you know accessing the element from an array, you know, something like this a guy plus plus it's like a single instruction for that right? Because that's something you might want to do. There were instructions that roughly correspond to loops and so forth good thing we wanted was a memory was really expensive stove code. NC was very important, right if you could get by with one bite instructions, that would be it would have been much better cuz it cost less space to a hold them.  And then there's also this implementation technique that people used to use called micro coating reconstruction actually triggered the execution of a sequence of instructions kind of internally instructions to be really complicated, right they could do a bunch of stuff is actually instruction x86 for copying a string from one place to another and it encodes a little Loop and it'll run along and do a bunch of operations. So what that means it's sleepy eyes are super super high instruction count is low because I could write down one instruction to do string copy. And so what all this led to was a whole bunch of different directions with lots of bells and whistles variable and variable length instructions to save space. So the common instructions like add would be one bite The Uncommon instructions will be longer because of who says short instruction should be for common case are the common case should be start instructions.  Wow, that's an excellent point. It is kind of like Huffman encoding but we don't cover instructions that right. If we have lots of them. They should take up a little space as possible. And so their success had some downside the essays of all kind of organically. They got Messier and Messier and more complex. The Pinnacle of complexity was in an essay called backs.  And there was actually the instruction that would evaluate a polynomial so you can give it the coefficients on a polynomial and it would take an invite and it would raise it to a bunch of powers and multiply time to write the things got really out of hand.  Turn the modern era compilers are really good. So we don't need humans humans are right Assembly Language anymore. So you should chew your is a to be good for the compiler rather than good for the human memory is cheap. Soco density is not so important, right? We have terabytes of d'ram now megabytes back when this was on Vinted and we can get low CP I should be possible but only for simple instruction. So these old processors they were actually built from a bunch of discrete chips on a circuit board. So it's not all integrated on a single chip and very long  And things are going to be slow but now we can integrate things tightly enough. This is actually what triggers a lot of this was a good finally fit an entire process or under a single piece of silicon. The first one to do this commercial with the until 40 the Intel 4040, which is followed by the Intel 8080 huge step forward with that man. But if you want to have low CPR instructions, and we also by the time you were like 30 years and now we have learned a lot about how to design is and so we can start over Davis MEPS  I miss is an infant or something called risk, which is reduced instructions that Computing so these previous things.  We're called Sisk.  So this is Sisk for complex instructions that can do this is reduced instructions to Computing. And the goal was fast clock slow CPI in a simple is a dove easy to compile form. So there were two one of them is called risk nipples milk at Stanford. They were designed by  Go to the authors of your textbook there were two competing project Hennessy and Patterson recent winners of the Turing award mostly for this work and they were very similar, right? So the same kinds of ideas.  There a compiler friendly not user-friendly simple regular is a everything is very simple and straightforward a few simple flexible flask operations of the compiler can come by easily on a nail stopping to Steph Curry data access from data manipulation. That's why we have ADD instructions and load instructions and the arithmetic operations never touch memory and vice-versa that makes everything simpler. We've seen the three beautiful instructions to the instructions for Mass. Right how elegant right? Just three things. We saw how this makes everything easy to decode write G-Code is almost free to take the oxycodone. They control you just pull out these register values. The register values are always in the same place. So we just slice them off of the instruction where that we fetch her memory and everything is perfect. And then, you know, we can encode branches and loads and stores and a meat operations in this format arithmetic goes up here and then we need us for some weird stuff like jumps and so forth.  Beautiful girl for a bite song everything is a line. I always know where my next instruction is going to be. This is like a paragon of Elegance, right? So here's a list of all the things that we can say about this there is always one or if your arithmetic operations solo they do an ad to compute the offset, right? They take the address. They had the immediate and that's the address that we used. There's only one arithmetic operation one memory access to register reads one register ride in one branch never more than that. And that means that I I don't have any if I if I provide her to do that, I will never have a structural makes my logic simpler decoding a super easy simple.  We can do this. This is the algorithm for executing any risk Construction in nips instruction. Write the only kind of weird wrinkle. Is it some of them don't do memory operations, but that's a small price to pay for the beautiful Pipeline and we were able to implement compiling is easy all the instructions look the same if I wanted to know where the inputs and the hardware is simple. So when I used to treat 141l still in 10 weeks and you can compiler for it if 33 instructions that we use and you can Target is GCC and you could run programs, right? So I took longer than 10 weeks. We have better tools now, but this is not a hard thing to implement right beautiful. It's all because of these ideas with  So pom pom pom. I wish I could draw Darth Vader's head. I'll have to get a  Darth Vader Darth Vader example of Sisk there many others long ago 36th Ave survived because then tell us the best CP manufacturer in the world and so they can have him get us at the rules. There are many many instruction formats. There are variable length will see some examples of that in a minute there a bunch of complex rules about rich registers can be used and when in mips all the instructions are the same there is a kind of a on as far as the instructions go there is a protocol which you probably look good for something similar forearm or we pass arguments and so forth and certain instructions passing arguments to functions, but from the architectures perspective all of the register the same I can add two things for going to have any to register together and put the result in any register not the case in x86 bunch of complicated instructions stream copy.  I-76 includes instructions for something called binary coded decimal were instead of using two's complement arithmetic use each for bits to represent the digits 0 through 9 and you do math like that the disaster still there. They combine memory and arithmetic operations are special purpose registers and there are many many many many instructions.  Implementing x36 correctly is almost intractable. This is actually a big strategic Advantage for Intel. It's very difficult for someone else to go along and build an x86 processor AMD is. There was another company called spirits that used to build it until clones as well. But giving this ride will take you know, it's a huge effort all of these Corner cases that you need to cover because somewhere there's some piece of code that actually going to rely on binary coded decimal and it's not going to work on your processor.  so  x86 is a poorly designed is a by modern standards and breaks every rule. There's nothing regular predictor of syntax. We don't know if we're not going to teach us to teach you guys to read x86 assembly. We don't do that. I can do that in this class. It's the most widely used as in your phone is probably running on more processors the next 36 is now but a lot of them are pretty deeply embedded but as far as like been complications you do x86 is mostly King. So your laptop's everything that runs in the cloud almost all of it is x86 but your tablets and your phones are running arm and your Apple watch stuff like that. So basically anything smaller than a laptop is running or anything bigger than a laptop is running x86.  Has managed to do in Helena Mt. Have managed to do at a considerable cost is somehow make all of this. Okay, and I have fixed all these problems.  Where are we TimeWise?  10 minutes  All right, we're going to go through and talk a little bit about x86.  Just to show you how terrible it is. We're going to use this world of pain 6 processor whose name. I cannot remember and the lead architect came up with the  Was writing down all the instructions and they were going to have like an 8-bit off code or something and he was raining all the instructions that they were going to need and he wrote them down and the head of implementation team got till like instruction. 240mm is like we're stopping  Refuse to go on right? This is too much work. And so there were some upcodes left unused at the end of the x86 instruction set.  And the way that they have extended x86 is you take some of those unused opcode then use it to say if it's a stop code then go get another bite and I will tell you more of the opcode if that implementation team had just finished their work.  There would have been no room for the extension and they would have had to throw away x86 and build something else.  So whoever that is responsible for all that has come after.  All right. That is the sad story. All right, who is one destination in one source, and all of the operations are destructive. So destination equals destination opsource. This is what's called an accumulator.  Does an accumulator architecture and so this means that if I wanted to keep going to have to copy it somewhere before I use it, right? Cuz I'm about to override it with this result is called destructive right at the register file the Monstrous register file.  This is the original x86 register file. There are 8 registers right here.  Emeril have special uses  sir register a is called the accumulator register bee is called the base. There's a can of this day to register the stack pointer. This is the frame pointer. There are some general purpose registers. Oh, this is actually store in here. Originally. This isn't this is he's already x86 64 which is a 64-bit extension. They had no they added some where they could this is the end X Predator which is used for the string operations. So this actually increments this is where it keeps track of the loop and Excellence copying string. This is the destination in Beck's cuz you're going to copy from one place to another I need to keep track of both of them. There's the instruction pointer with the points to the current construction said  These are different names depending on how you like to use them. So if I say l  then I get eight bits if I say acts like it's 16 bits. If I say E X I get a 32-bit then if I say RX I will get 64-bit to that register want to work on and then this carries over into the  oh actually were going to step outside.  These are what the arguments can be. There's a bunch of different argument in this can go to any instruction more or less. Well, of course not that would imply some sort of regularity. I can have a register which is the name of a register have an immediate value. I could have a label if I want to have a branch have a displacement. This is very similar to what nips has. So this is a register Value Plus Value Plus some offset. So we've seen that already. This is based off set this I take two registers and I add them together. Right and I use that as the address I'm going to access. This is a scaled offset. So this is a base address plus a second read a bass register base. I just registered + x to the end so I can jump in big slices. I see these faces are just like like I ate a bug  Add a sound of the end.  So this is like how many registers do I need to read? How many arithmetic operations do I need to perform ride? If you can't Implement all of this in a nice pipeline equipment that you need way more harder than it actually is another interesting piece. We have to move instruction. This is like I was suggesting you one of my colleagues that we should teach 141 and x86. He was skeptical as you might imagine. Why can I only like 5 instructions? And he said yeah, but they're all moved which is about right. So move if I move icons on the media in to register, that's like try to create a constant smooth is so this move is a load because I'm moving from a register or a memory kind of operon to register. I can move I think I can move from memory location to the memory location or maybe not you know, because it's not very regular and all these are destructive. So this one  So here's a store. I'm going to move this label into this under the stack pointer what I can do with one move operation.  Arithmetic, it's the same thing. I can add and subtract among all of these things. So I can do memory operations and arithmetic in the same piece of code branches. Are we really broken up? So there's a compare and discuss the comparison to record the outcome in the special thing called the condition code register and then I do a branch. I do a conditional jump depending on the contents of the condition. So this is like a jump of greater than jump of greater than equal to jump with less than and that depends on what the last comparison executed is. So now I need to instructions to execute a branch and this thing has put the destination of a condition code.  Best way to go so I got them right into it. And now if I wanted to go out of order something I have to virtualize and rename the condition code because I kind of multiple branches going on at the same time right now. They're big Matt's here is a little flowchart that I found. This is the kind of syntax structure for an x86 instruction. They can be between 15 and 1B long is the shortest ones are one by the longest one you can have is 15x long and there's a bunch of stuff in here. Y'all three producers on the final say, okay. Don't worry. Some people don't realize I'm joking and then I feel bad cuz it like it was on the final. It's not going to be on the final.  not on the final  hybrids, very very complicated. So the decoded stage has to deal with this. Right? Right, and it depends like this is for Vector instructions. I think or maybe it's the only here's the vector instruction prefix. I don't know what the Rex and started the Rex prefixes for take you through and figure out why you should perish you might need some extra operations for the displacement of the base scaling factor and also their stuff with the total mess.  So, how do we do that? Can I get through this in 2 minutes? Maybe they take the x86 code and they converted in the decode unit in to order essentially nips operations. They're called microwatts. They're not actually there's some internal thing that is not documented outside of Intel but it takes them and it converts them and some of them that converts into multiple microwaves and this is what actually gets executed. So an x86 processor is a big complicated thing that understands x86 instructions and translated it into an internal risk. I say and then the rest of the pipeline looks basically like for the rest of the quarter  That's the trick is a big giant mess. It's a huge power sink right energy sink. I have to do all this work just pure overhead. This is a big reason why arm is running on your phone instead of x86 is a bunch of business reasons as well and some poor strategic Decisions by Intel's back in the early 2000, but a lot of it is because they start off with this fundamental energy and disadvantage because I have to deal with x 36 in the scalp. "
}