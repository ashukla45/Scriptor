{
    "Blurbs": {
        "1 is 4 There were 50739 people at the end. So if you subtract a from that you can get this value and you can always add Down The Columns to get the total idea. Now this sum of squares, we haven't talked much about okay, it's really just an intermediary it to get to the main idea which divides the sum of squares column by the degree of Freedom ": [
            3006.7,
            3031.7,
            93
        ],
        "70% already. So here you're just using the you need to make sure you get the right order. That's something a lot of people don't know. It's a group thing is the first spot and they have distribution in the errors II spot. What commands should we write in our freaking an F stat of 4.2? don't you love the temptation of a It seems right in every way right? ": [
            2271.0,
            2330.6,
            75
        ],
        "America. You're not probably familiar with this stupid unit. But all the non American people are probably loving it cuz they know and meet. He's a little better 50739 women. Okay, so here is sort of a summary table of what's going on. So we're going to run in and over here. It is to see if different amounts of coffee leads to different amounts of exercise on average. So ": [
            2565.9,
            2591.6,
            83
        ],
        "And the variation here in MSC is all about variation sort of within a group in terms of the errors that are happening as you look at infielders. How do they vary? And this other one is looking at the variation in groups as they compared to an overall average. And then we have some ratio. So this is all the apps that let me show you how to use ": [
            1273.4,
            1302.5,
            48
        ],
        "Do you think the designated hitters have higher batting average than the others? If you compare them over to infielders or outfielders, you're like a .01 to compare them over to catchers. You like .02. Starting to look bigger. That's what I know. It was trying to tell us if these numbers really are different from each other. Assuming we made all these conditions. Now do you know but does ": [
            1050.0,
            1079.1,
            40
        ],
        "I need you to set up temporary amateur some hypotheses for me. Try to do this in the most efficient way on the page. Now that you've already done something like this previously. Okay. Set up an average idea. It should be the average of a quantitative variable in the problem. So hear the quantitative variables The Meta this is this metabolic equivalents tasks here. Now you need to tell ": [
            2591.6,
            2736.4,
            84
        ],
        "I'm gay. So all of your data points, you can see fall into one of these three bins. Now if the dater are independent within it means you shouldn't be able to use one data point to infer the value of others. Within your same group and across means if I know this datapoint, it shouldn't help me make predictions about ones in the other bins. The easiest way to ": [
            709.1,
            734.1,
            27
        ],
        "Listen to a podcast. Hope you are emotionally recovering after the Game of Thrones finale. I won't spoil it. if you're still waiting to see okay. Today is a beautiful topic because it shows you the future of where you could go with all this. So we're going to talk with something known as Anova analysis of variance. And the right way to think about this straight off. Is this ": [
            1.9,
            43.5,
            0
        ],
        "Okay, so here you can count that. There are six possible tests and that comes from the number for choose two of the four ones choose to that you'd like to compare and the true symbol tells you how many ways you could choose those two out of the group of four? Unfortunately, the two symbol is nasty and gets big very fast. So if you have K different populations ": [
            470.0,
            492.9,
            17
        ],
        "Some people were selected before they read on. Okay are always resist lower Tails by default. So you need to tell it to shade or the right side, which is what he is doing. if a Nova suggest a move to the alternative Then we will have identified the mean that is different than the rest. So I said it a couple times already. The alternative is frustrating. It says ": [
            2330.6,
            2371.4,
            76
        ],
        "a couple of distribution. So you could see what they sort of look like. So their support the possible ways they can take on or on the horizontal axis so starts at 0 and goes to Infinity. And the reason is the f-stop is a ratio to mean squared ideas which always positive so that's why you can't get anything negative. And so I just chose to random Ones based ": [
            1978.4,
            2002.2,
            68
        ],
        "always get this is to fill up your study by choosing people randomly and to not choose to many. This is the randomization 10% condition. So the same thing as true here and that usually will give Independence both within all the different groups and across the different groups. Okay, next sing the data in each group have to be nearly normal. So if you took these dots from say ": [
            734.1,
            757.3,
            28
        ],
        "and was like crazy in there. He won't crazy for that class. Anyhow, so when lava hardens because it's full of lots of magnetic things metal things. You can see the direction of Earth's magnetic field in the hardened lava because of the iron that appears in lava. Okay, so that's kind of cool. So what research is did is there were three different eruptions of Mount Etna in these ": [
            1332.3,
            1361.3,
            50
        ],
        "and you want to study two of them. And you're going to study all possible combinations of two of them. Suddenly. We have K choose to which is about K squared. So now you're running about case we're different tests. How do you feel about running so many tests? Well, there's different emotional things to feel about this. The first is is it possible to run that many tests that ": [
            492.9,
            523.4,
            18
        ],
        "anymore in relation to what you're trying to measure they're just a bunch of populations that happen to look identical across your quantitative variable that you're setting now things get strange here as you can see in the alternative. If you're claiming a bunch of things are all identical the opposite of that is that there's at least one person or one idea one average. Whatever it is. You're studying ": [
            119.8,
            144.1,
            4
        ],
        "are fair. You're going to flip them 10 times. And then you going to tell me what happens know most of you will get 5 out of 10 heads. Nothing exciting happened, but there's one person in the room. The lucky one that was just get 10 heads, right just by someone, you know, it's like a one out of a hundred event or something, but we have a hundred ": [
            567.0,
            589.3,
            21
        ],
        "average. Okay, so it's looking like what how did the outfielders differ from the overall average and how did the infielders differ from the overall in a sort of a team elating? This idea when you measure what's going on between the groups as they compare to the overall average. Okay, and then discenza buys how many people were in a certain group? So how many infielders were in your ": [
            1140.9,
            1165.4,
            43
        ],
        "beyond our data. Can you can do it? Like obviously the columns don't have exactly the same average, but the question is are they so far apart from each other that they must have come from populations that actually differ. So let's eat first thing. Can you define me some variables and write me some hypotheses? Let's start building the infrastructure around this a minute here to do that. Okay, ": [
            1433.5,
            1621.9,
            53
        ],
        "column. So if I subtract I can get this number and then if I divide the sum of squares by the degrees of freedom, I'll get the mean square column, which is what we really care about. So we have to stop but I suggest you keep looking at these things and working on them. Have a great day. I'll see you later. Can you see sandiego.edu? ": [
            3031.7,
            3068.0,
            94
        ],
        "conditions. First of all, you can't have garbage data. So you need your data to be independent within and across groups. It was a tough raise the first time you see it within means if here's a picture of our data. So we have three different populations. One, two, three, and we're measuring something quantitative about him. I would come whatever that is. Okay doesn't really matter in this problem. ": [
            685.8,
            709.1,
            26
        ],
        "data set? 120 parents are people breaking out the degree of Freedom related to the error formula and minus K and back driving 110 was from that. collect salary info on a hundred 1012 200 people five different states What curve is the p-value found on for this study? His number should be subscripts here. But so proud of his so limited. Can't really show it nicely. Look at that ": [
            2200.3,
            2271.0,
            74
        ],
        "deep it's not visible. So this is a very counterintuitive results. In fact the 181b class. They just had their homework on a Nova and they had to write code in our that would generate such a data set. So it's not obvious. Like how one would even have such a dataset you have to like let the computer try to create such a thing. So it's so it shows ": [
            2457.7,
            2478.6,
            79
        ],
        "differ that would be even more exciting. It would show continual change. Not just a one-year change. Okay now. The calculations around an oval are horrible in complicated and they usually must be done with the computer. Guess I'm not going to ask you to like. Find an F stat by hand for from the original data. This would be nightmarish. Okay, so let me show you what it looks ": [
            1671.5,
            1697.6,
            56
        ],
        "different measurements. They did from the 1669 eruption hardened lava. now let's try something with your soul. Hey average together the numbers in each column. And then ask yourself if you feel like they're different from each other. Does your soul have an answer? So we're going to run into Nova now and figure it out. And with the Anova is going to do is going to try to look ": [
            1397.4,
            1433.5,
            52
        ],
        "don't have it except for Taylor polynomials and horrible transformations in another class. Okay? So then there's a set up there so I can check into payments through the two conditions. We always do that. You can see the normality and the equal spread newest quite easily in one of the following two things. You can make me a box plot that has side-by-side grouping. Okay. Now in the box ": [
            861.7,
            887.6,
            33
        ],
        "don't know something it turns out it doesn't really matter about the normality though. So in general we just need normality unless we have large sample sizes and we have huge sample sizes. So these numbers 6017 thousand etcetera. Now, how am I ever going to give you a problem like this on an exam? It mostly involves computer stuff, right? Well, here's how you can give a problem like ": [
            2951.3,
            2980.0,
            91
        ],
        "down here the embassy this is the mean squared error. Or the sum of squares related to are terms that you've taken an average of okay. So this looks within each group and it says okay within each group. You'll see a lot of variability not all infielders. Look the same. Okay, and it's Tessa by is the standard deviation going on or the spread out news within a given ": [
            1214.7,
            1239.7,
            46
        ],
        "drawback of running an Omnibus test as we said earlier is the alternative is to satisfying something differs. What how many things differ I don't know but at least you have a starting point that you could then go off. And maybe run some tests never going to use this infrastructure. There are a lot of assumptions. This is one of the frustrating things about it cuz he's all the ": [
            659.4,
            685.8,
            25
        ],
        "final thing. This is the P value. This helps you decide between the two hypotheses. You just wrote it and it's as it normally is if this is really small you'll move to the alternative we've seen incredibly weird. Aprecio is with this number the fact that it's so small. This is what it means. Okay. Now if you want to think about what a p-value always means it's always ": [
            1925.5,
            1953.1,
            66
        ],
        "find it using a bunch of pairwise tests. None of them can find it. Okay. So here's this analogy corruption present in the government, but you can't exactly identify where it is. Sometimes the totality of the experience flowing at you says excitement. If you can't pinpoint it for some reason maybe it's generated across the totality of lots of different comparisons are leading to that. But when you dig ": [
            2423.5,
            2457.7,
            78
        ],
        "get a value. In 1669 that value would be a new one. And so on for the others so, you know, it'd be super boring if they were all the same number. We wasted our life going back in time. And I would be super excited if one of them different from the rest because this would show a change in the magnetic field. In fact, lots of them could ": [
            1648.4,
            1671.5,
            55
        ],
        "get introduction and get a good feel for what's going on in queso language in terms of how you heard. All these things on. The very left won't use the phrase one sample T Test. Okay, the one refers to the fact that you drew from one population to a sampling from it. The teen tells you what distribution the analysis eventually happened on. So obviously this will be two ": [
            354.0,
            377.5,
            13
        ],
        "group. Okay, so our infielders really spread out or catchers really spread out. And what you're going to do is sort of again wait, he's and then averaged together everything going on. Now it's not obvious. We're doing any of these calculations is helpful. It's not obvious. Why taking their average gives you anything important at all. Right now it's just sort of we're analyzing the variation that can happen. ": [
            1239.7,
            1271.7,
            47
        ],
        "guessing. Okay, we don't know about this condition because we don't know how the data were collected. Volunteer bias is most likely what's going on. Although I don't know if that affects anything about what's going on in this. We also don't know anything about the normality condition here because I haven't shown you a picture of the day Tiny Tim the five groups. So just tell me if you ": [
            2929.5,
            2951.3,
            90
        ],
        "here. We use standard errors the name for this occasion. This was Sigma / \u221a and where Sigma was being taken on by the roll of standard deviation cuz you don't know Sigma. So that was a nice thing we did there when we have two populations. We would do the same idea take the difference that's going on in your two samples and their averages subtract away what you ": [
            195.7,
            218.7,
            7
        ],
        "in this list of numbers here, unfortunately. But what is cool about the sister numbers? You can start to see you're studying whether you think the means are different, right? That's what this is all about. So if you look at these numbers or so, we'll have a feeling immediately. But the point of the Anova is to rigorously decide on what your soul is feeling there. I don't know. ": [
            1027.0,
            1050.0,
            39
        ],
        "is a very dangerous thing because you're going to start seeing some type 1 error showing up. Just a random chance. So yours the metaphor suppose we're trying to decide if a coin is fair and I have a big bunch of coins and they're all really fair. Okay, and I give everyone in the room a coin and I say here's how we're going to decide if these coins ": [
            545.3,
            567.0,
            20
        ],
        "it does it runs a single test. And it's check to see if all the means are equal to this known as an Omnibus test Omni for everything. Now it's not the best kind of test ever. Is everything equal in life. Seems like a silly question, right? You almost know the answer is no immediately but there's some cases where you want to ask such a thing. Unfortunately the ": [
            631.6,
            659.4,
            24
        ],
        "it will hurt some horrible transformation Taylor polynomials. Remember these 20 be cuz I cant infrastructure in case it is a rough thing to lose and finally the spread in each group have to be roughly equal. So you look at how spread out the data are in group one at some number using the standard deviation use the variance. Whatever you want to me roughly the same ideas. So ": [
            784.1,
            811.1,
            30
        ],
        "just have to get through 180 and 181 a first so K. They're quite doable and enjoyable. If you're not one of those people in your fine, like give me a tool I love using tools. Is a great day for you and you enjoy using things like a black box. We don't really truly understand. What's going on underneath. Okay. How about we do some Socrative? I wonder how ": [
            2089.8,
            2135.7,
            72
        ],
        "like when we go put this into our especially if the structure correctly. Now you don't actually put it into our is a matrix. Like I gave him the previous thing. You put it into dataframe where all the angles are in one column and then the other column has the grouping variable. Will you showing me how does data split by year? That usually people will use some letter ": [
            1697.6,
            1723.9,
            57
        ],
        "lots of different populations. Just run tons of two sample T Test. Turn natural question, right? Why do I need to sort of compare everything at once with a single Hammer hitting all of the nails? I can I just look at them in pairs. Well tonight idea except it. You'll see if we have four different populations. You can already see that the number of comparisons is growing large. ": [
            439.1,
            467.8,
            16
        ],
        "me told me what you need and you're not willing to even commit on this one day. Someone else might be totally fine. What are the other conditions? Go ahead. I'm sorry. 10 so 10% and randomization are trying to create the independence idea. How do you feel about I agree there more than 50,000 women in the US drink coffee. But we need more than 500,000 right when we ": [
            2850.1,
            2888.7,
            88
        ],
        "me what the different groups are. So this is the different coffee categories. It helps to tell what population or measuring here. It's women probably American women. Okay, and then Dino should be that all them eyes are the same. If there's a lot of nuso buys you can write them all out and put equal sign between them or you can choose words. So it's totally fine to use ": [
            2736.4,
            2761.9,
            85
        ],
        "mean look what's going on. It was already a hot mess with one population with two we had to analyze differences and then suddenly the distribution you do everything on it's already getting complicated. So when it comes to three or more populations, we actually have to leave the T distribution behind and we're going to move to something called an F distribution. Named after sir, Ronald Fisher. That's where ": [
            244.2,
            269.3,
            9
        ],
        "middle, it means that there's a nice balance between the two halves. No doesn't mean it's normal. But that usually the way people look for normality. Okay. So this I'd say this is like Roughly normal and roughly the same spread. That's just what my emotional soul is telling me the boxes look like the same height and the lines are sort of in the middle. Basically. I want to ": [
            919.5,
            943.2,
            35
        ],
        "most pieces of software there actually is some uniformity in what people in the world have decided to do despite the fact that we have different softwares. Okay. So when they have to be able to do is read this print out regardless of the software came from the F statistic, it's taking the ratio of MSG in mac, and it just happened to get the number 15. Hey, so ": [
            1800.0,
            1824.4,
            61
        ],
        "need more than 50 thousand times 10. So I also think there's more than 500,000 Starbucks exists. What about the randomization? How you feel about that? They already why'd you say the randomly-selected? Where'd you get that? Hoping and dreaming in your soul. So you have to go back and look and see what's going on up here. I don't see anything about random attack. This was probably volunteer. I'm ": [
            2888.7,
            2929.5,
            89
        ],
        "notice here that the particular F distribution you do the analysis on suddenly has two indexing parameters. Where's the T distribution? Just had a single one. I'm gay. So here we have to tell me two different things to figure out the curve is bf1 and df2. If you take 181b which is what I teach right after this you learn what's going on Wheelie with a Nova and it ": [
            300.5,
            329.1,
            11
        ],
        "of coffee and they made it a categorical variable it bend it pay less than or equal to one cup of week Etc. So they have these different bins so you can see where you are. And then they measure people's exercise level per week. Now that could have been in lots of different things number of calories burned. So there's a continuous random variable that used to Mets. Sorry ": [
            2538.8,
            2565.9,
            82
        ],
        "of covariance. Then I can go to manova or manova, depending on how do you pronounce add multiple analysis of variance in in man? Kova multiple analysis of covariance. So then you started classes, but the acronym Train goes for a long time. I'll tell you that. Okay. Now one thing you immediately asked the same. Why do I need your fancy new framework? Why don't we if we have ": [
            406.7,
            439.1,
            15
        ],
        "of thinking about averages and eventually we moved to this the T statistic. You saw me doing all the tats all the time. So the stars is it takes your ex bar and it standardized is it and moved it to an ice distribution the T distribution with n -1 degrees of freedom to subtract the meaning of what's going on in you / the Spread out in this switch ": [
            172.9,
            195.7,
            6
        ],
        "on these different crazy degrees of freedom, and you can see in general. They sort of go up have a hump and then come back down. Is what these look like? So when you're calculating A P value for an anova all you do is draw the appropriate F distribution, and then you plop down your fstat or your value and then you just say to the right. Now one ": [
            2002.2,
            2026.9,
            69
        ],
        "or something for the grouping variable that you can use numbers here. If you really know what you're doing, but it's very dangerous to put numbers in this column because our needs to know that it's a categorical variable a bidding way of dividing things up in our can get very confused if you put numbers in the year column. Okay, so then what are you do so then you ": [
            1723.9,
            1746.7,
            58
        ],
        "people someone's going to see it. So the problem is when you start doing stuff a lot. Randomness crazy random events just start happening. Because you have a large multiplier until you start to think something weird is going on with nothing inside weird is going on. So if you run lots of tests and we're going to start showing the significant difference. when really maybe nothing's going on now, ": [
            589.3,
            614.2,
            22
        ],
        "plot, the spread is just the height of this box. That's the IQR actually. So if all the IQR is look sort of similar now, that is an emotional judgment. I need you to just make emotional judgment sometimes. It's like I don't want to do that in a math class. Give me some rules. I can apply sorry. No, no malady. If the central line is roughly in the ": [
            887.6,
            919.5,
            34
        ],
        "researchers. Never check anything. We have three conditions. Go ahead. Which one do you want? Okay, Sweeney the spreads to be roughly equal just called homoscedasticity almost the same speed as tissy T4 spread out in this. You're worried about the 25.5 right here. So if I were you I would write we need the spreads to be roughly equal. I'm somewhat concerned about the 25.5. So there you told ": [
            2804.7,
            2850.1,
            87
        ],
        "right. So the infrastructure is built around that happening. How you feeling so far? Some people will hate today. Because I said here's the tool. You don't really get to understand why anything works. It's a super complicated tool with all these weird intricacies. And people get bothered why they when they don't know things are true. So if you're that kind of person 181b is waiting for you. You ": [
            2054.4,
            2089.8,
            71
        ],
        "run aov analysis of variance? And the first thing it wants is a This formula here. So on the left side and wants to know the quantitative variable and this Tilda you're going to tell it how to break up a partition the data into different groups. So use the year variable to split the angle data into different groups. And then everything comes from the volcano data set. So ": [
            1746.7,
            1775.1,
            59
        ],
        "sample T Test and over here people just use the phrase a Nova for an analysis of variation for variance. Okay, it's part of a huge framework where you study things by analyzing how they vary. And it's in the variation of an object that you come to truly understand it. So slick name now and has lots of extensions. The first extension people usually learn this and Cova analysis ": [
            377.5,
            406.7,
            14
        ],
        "see hard numbers. You can always create a table like this now, there's a table of people playing baseball. Where they've been grouped or put into populations by their position, so Outfield and infield designated hitter catcher. Okay. Now you have to be measuring something about them some quantitative idea. So here you can see that it's on base percentage, which is some sort of measure for batting average if ": [
            943.2,
            970.9,
            36
        ],
        "see what's for missing a bar here. There should be a bar over this xr80r know why it's not there. That's weird Okay, so that's some number. And then it subtracting that from X bar, which is the overall average if you take all the players and don't think about them based on any group in just the average amount together. This is sort of what happens in baseball on ": [
            1112.1,
            1140.9,
            42
        ],
        "seen this in the past with the central limit theorem. I always say I need to do to be nearly normal. Except it's okay. If they're hot mess as long as there's enough of them. This isn't balanced, right they're always talking about to the same thing will turn out to be true here in a Nova. Also. The last one frustratingly. There's no nice way to fix if you ": [
            840.4,
            861.7,
            32
        ],
        "something exciting is going on. That's all it says nothing more doesn't tell you what? How that's good. I love to ended on one that the majority gets wrong. Now you got very little. Hope of getting this right. I'll tell you that right now. Okay, and you probably song the explanation? It's possible for the Anova to say something interesting is going on. But when you go try to ": [
            2371.4,
            2423.5,
            77
        ],
        "sort of a logistical question? Now that your computer's you have no problem with telling the computer to do tons of work, right? So maybe it's not so bad anymore the wave technology to help us. The other question is is it bad to run? A lot of tests? This is sort of a theoretical question and it turns out that if you do something a lot of times this ": [
            523.4,
            545.3,
            19
        ],
        "study how many outfielders so it's a weighting factor in case I'm very exciting to get a huge difference between two ideas. If it weren't many people that went into making the difference. But if there's a huge difference in there's a million infielders that used to build that that's important. So this isn't a Sensei waited. Squared idea. So there's a square that has to do with groups. And ": [
            1165.4,
            1190.6,
            44
        ],
        "takes about a week to have to cover it and it takes about a month build up. So if you want to know why we don't really do this and Technical detail it's because it requires that much work in addition to two entire quarters before that of lead in material. So that's the amount of meat on the bone here if you like, okay, but we can at least ": [
            329.1,
            354.0,
            12
        ],
        "that there's like not a perfect structure to all of our statistical testing here that we can figure out everything in life, basically. Okay, should we try some more? But try some more. True tried smoking problem. No nose to have coffee problems. I like the coffee one more than the smoking one. Okay. So here's an actual study. This is one of the freely accessible jumbo data sets on ": [
            2478.6,
            2511.3,
            80
        ],
        "that's not hard because of the name is exactly above it. Now. The mean squared call him is telling you what MSG and empathy are. So the way this is set up is one of the Rose will be about grouping ideas. So here's the year variables how you group things? Okay. So this entire first row is group related things. That's why this number is MSG. You can see ": [
            1824.4,
            1849.9,
            62
        ],
        "the F comes from. One of the founders of Statistics did most of the important work in building the entire field between say 1920 and 1960. Now that's a statistic is a complicated object that we won't fully study. But I'll tell you how to use this. Basically. It's going to be the ratio of two other quantities MSG taste good, doesn't it? And MSC whatever those are and you'll ": [
            269.3,
            300.5,
            10
        ],
        "the MS from the column in the G comes from the year idea. The other row will be the error residual row. Depending on what language they use here because of this was all about error ideas. So you can see Ms. Andy coming together 2.95 is the MSC. Okay, and you can even chat 45 / 3 is about 15. Is it ain't doing any magic here if you ": [
            1849.9,
            1876.3,
            63
        ],
        "the area under some curve. Based on your data or something more extreme. assuming the null hypothesis So here that curve happens to be an F distribution in the particular F distribution you on the Stick Page by the two degree of Freedom parameters that you see right here. Okay, so I can actually draw a picture of what's going on if you want. Okay know we probably should draw ": [
            1953.1,
            1978.4,
            67
        ],
        "the differs from the group. This is a frustrating alternative because it doesn't tell you which one differs. Okay, so as you moved to more than two populations, you can continue the infrastructure, but maybe the alternative is less satisfying than it used to be. Okay. How do you test all these things? Well in one population? We said okay, let's study X bar that seems like a good way ": [
            144.1,
            172.9,
            5
        ],
        "the first group those all represent numbers, you can make a histogram out of them and that histogram shouldn't look roughly normal and the same thing is true in all the groups. So that is a very strong Criterion all of your different population of studying have to be nice dish. Just on Friday next door. We figured out how to fix that condition. It's rough. I'll tell you that ": [
            757.3,
            784.1,
            29
        ],
        "the generalization of things you've already seen before? That's one thing you should always say like what are the limitations of my current tools? Will see that all our testing around averages right now is limited to one or two Okay, so we've worked with populations. For the rest of single one and we learned how to form hypotheses around these may be the average in your population is equal ": [
            43.5,
            70.6,
            1
        ],
        "the internet that you can go get the women's health study. Okay, so they asked women a bunch of questions. And then they the Tuesday to set and then people could just go study whatever they wanted. So this is study that came out about this to people who date drink different amounts of coffee each day. 10 to exercise different amounts It's what they did is in 2 cups ": [
            2511.3,
            2538.8,
            81
        ],
        "the thing I named this object the state of for him here. Okay, so this will run the Anova and it gives you back a very complicated object with tons of information in it. So then I just say give me the summary of this big complicated object. So this prints out a table and this would be the Anova print out or read out and this is common to ": [
            1775.1,
            1800.0,
            60
        ],
        "then in the end you take an average across all of those different ideas. Okay, and you don't / que / K - 1 we seen the sort of like weirdness before. That you don't really need to truly understand this at some point. You should have some emotional feeling that is trying to measure some idea that has to do with how groups differ. From the overall average. Okay ": [
            1190.6,
            1214.7,
            45
        ],
        "there's a way to correct for this at the end of the slide so we won't get too cuz bonferroni correction if you want to read about but anyhow, that's why you don't want to run a bunch of tests. So so you just want to write run one that does it all at once. This gets rid of What's called the multiple testing problem. So I wouldn't know if ": [
            614.2,
            631.6,
            23
        ],
        "these would be the most natural source of null and alternative hypotheses. So if you continue that you could imagine how one would set up a situation around more than two, you know, they really boring if they're all these populations and I go try to study what's going on and then all the average is come out to be the same. And sometimes they're not even different populations really ": [
            96.9,
            119.8,
            3
        ],
        "thing is strange about these. You're always going to shade to the right. And there is no two-sided alternative. There is no symmetric shading both directions. There's none of that. The hypotheses for an anova always look the same the null is all the means of the same feel ternative is something is different. And when you do the shading you always popped on your f-stop and Shay to the ": [
            2026.9,
            2054.4,
            70
        ],
        "think the difference should be which is almost always zero for us because of what the no looks like the null says Hey are two things are the same and then divide how to spread out things were and here we were just on a more complicated distribution. Now if your soul has good intuition here, it should tell you that we're about to make a big hot mess. I ": [
            218.7,
            244.2,
            8
        ],
        "this by calculating the F statistic? So this is the ratio of 2 weird Expressions that are both trying to analyze variance. It's going on in queso on top MSG the stands for the mean squares across from Now this is defined in terms of a horrible formula. 1 / K - 1 * some summation I'm so what this summation is doing. It's taking XII bar. Okay, you can ": [
            1079.1,
            1112.1,
            41
        ],
        "this in a nice example. This is one of my favorite ones that I've ever seen done with a Nova because it's a really cool result actually. So if you know anything about volcanology, have you taken the volcanoes course here? Has anyone taken volcanoes? Apparently it's like one of the most popular courses at UCSD. I used to teach in the room right before the guy went on volcanoes ": [
            1302.5,
            1332.3,
            49
        ],
        "this is like a very well controlled. Structured cookie pay the flower and the salt and the chocolate chips are in perfect balance in order to use this. Okay, now some of these conditions are more and less important. Did the last one actually turned out to be really important? You've never seen anything like this before the no malady one is easier to a road. And in fact, we've ": [
            811.1,
            840.4,
            31
        ],
        "this on an exam. You can give the print out and say there's missing pieces. Coffee stains are blurring things whatever. Okay and between this description of what's going on in this table, you should be able to fill in everything that's going on here. So for example, you can get the DF column by just knowing how the study of structure there were five groups, right? So 5 - ": [
            2980.0,
            3006.7,
            92
        ],
        "this. So here we three different years. And so that's why they're getting to Now this residuals degree of freedom is always going to be in the total number of data points. You were studying. / - K the number of groups that you broke things into so here we get 9-3 or 6. Okay, so that's that column. Now. The last thing you're going to care about is the ": [
            1898.6,
            1925.5,
            65
        ],
        "three different populations. 1669 1780 1865 implies three different averages you can call them whatever you want. Make sure they've Greek letters you can put the years of subscripts or you can just put numbers and then tell me what they relate to so if we were able to truly go back in time. And set up a device that would measure the direction of the magnetic field. We would ": [
            1621.9,
            1648.4,
            54
        ],
        "to some value you not do your specifying so think about this constant and maybe you set up an alternative is it's not that value. And we realized that we could continue this idea into more than one population and we could do this as long as we started studying differences in case on two populations said there's two things. We need to study. Maybe they're equal. Maybe they're not ": [
            70.6,
            96.9,
            2
        ],
        "understand what those calculations are now over here the degree of Freedom call him. It's helping keep track of what kind of car were eventually going to be on. Okay. Now the first one since that this is the year is a grouping idea. This is known as DFG the degree of Freedom related to the group's. Okay. So it's the number of groups - 1 that's how you find ": [
            1876.3,
            1898.6,
            64
        ],
        "we are somewhat concerned about the equal spread condition. They don't have to choose a side if you don't want to. But I would recommend choosing a side if it's reasonably clear now. I'm I don't know. I mean with the pictures over here. I believed it more the numbers. I'm starting to get worried cuz numbers you look at them differently, right? Now you're not going to see normality ": [
            1001.0,
            1027.0,
            38
        ],
        "words infected easiest to use words for the Anova null and alternative hypotheses. All them you eyes are equal some at least one is different. Okay, next thing let's talk about inference conditions. There were a bunch of these. So if someone talked to me about any of these that you'd like to mention and whether you think it's met. Help us who checks conditions anyway. Let's be stopping garbage ": [
            2761.9,
            2804.7,
            86
        ],
        "years and you can go back in the fossil record record and you can see the direction of Earth's magnetic field by looking and just measuring from some like true north or whatever. It's so how's the direction of Earth's magnetic field changed? Overtime this is a nice way to check it because we have data that span for 400 years basically and you can see so these are three ": [
            1361.3,
            1397.4,
            51
        ],
        "you know that. It's probably needs something to think about. She might wonder if the different players when you group them based on where they play. If that idea differs in terms of their batting average. Okay. So here are the numbers that come out so sample standard deviation. Do these numbers look roughly the same? something emotional in your soul Okay, if you're worried, you can always tell me ": [
            970.9,
            1001.0,
            37
        ],
        "you're going to do. Let's go see. Swans doing an F test that is an anova. How many groups? Are the researchers dataset? Hey, there we go. So this is just seeing if you have learned this expression for the degree of Freedom related to the grouping idea K - 1 so you can back derive the number of groups that way. How many total data observations were in the ": [
            2135.7,
            2200.3,
            73
        ]
    },
    "File Name": "Statistical Methods - A00 - Quarfoot, David James - Spring 2019-lecture_22.flac",
    "Full Transcript": "Listen to a podcast.  Hope you are emotionally recovering after the Game of Thrones finale.  I won't spoil it.  if you're still waiting to see  okay.  Today is a beautiful topic because it shows you the future of where you could go with all this.  So we're going to talk with something known as Anova analysis of variance.  And the right way to think about this straight off. Is this the generalization of things you've already seen before?  That's one thing you should always say like what are the limitations of my current tools?  Will see that all our testing around averages right now is limited to one or two Okay, so we've worked with populations.  For the rest of single one and we learned how to form hypotheses around these may be the average in your population is equal to some value you not do your specifying so think about this constant and maybe you set up an alternative is it's not that value.  And we realized that we could continue this idea into more than one population and we could do this as long as we started studying differences in case on two populations said there's two things. We need to study. Maybe they're equal. Maybe they're not these would be the most natural source of null and alternative hypotheses. So if you continue that you could imagine how one would set up a situation around more than two, you know, they really boring if they're all these populations and I go try to study what's going on and then all the average is come out to be the same.  And sometimes they're not even different populations really anymore in relation to what you're trying to measure they're just a bunch of populations that happen to look identical across your quantitative variable that you're setting now things get strange here as you can see in the alternative.  If you're claiming a bunch of things are all identical the opposite of that is that there's at least one person or one idea one average. Whatever it is. You're studying the differs from the group.  This is a frustrating alternative because it doesn't tell you which one differs.  Okay, so as you moved to more than two populations, you can continue the infrastructure, but maybe the alternative is less satisfying than it used to be.  Okay. How do you test all these things? Well in one population?  We said okay, let's study X bar that seems like a good way of thinking about averages and eventually we moved to this the T statistic. You saw me doing all the tats all the time. So the stars is it takes your ex bar and it standardized is it and moved it to an ice distribution the T distribution with n -1 degrees of freedom to subtract the meaning of what's going on in you / the  Spread out in this switch here. We use standard errors the name for this occasion. This was Sigma / \u221a and where Sigma was being taken on by the roll of standard deviation cuz you don't know Sigma.  So that was a nice thing we did there when we have two populations. We would do the same idea take the difference that's going on in your two samples and their averages subtract away what you think the difference should be which is almost always zero for us because of what the no looks like the null says Hey are two things are the same and then divide how to spread out things were and here we were just on a more complicated distribution.  Now if your soul has good intuition here, it should tell you that we're about to make a big hot mess.  I mean look what's going on. It was already a hot mess with one population with two we had to analyze differences and then suddenly the distribution you do everything on it's already getting complicated.  So when it comes to three or more populations, we actually have to leave the T distribution behind and we're going to move to something called an F distribution.  Named after sir, Ronald Fisher. That's where the F comes from. One of the founders of Statistics did most of the important work in building the entire field between say 1920 and 1960.  Now that's a statistic is a complicated object that we won't fully study. But I'll tell you how to use this. Basically. It's going to be the ratio of two other quantities MSG taste good, doesn't it?  And MSC whatever those are and you'll notice here that the particular F distribution you do the analysis on suddenly has two indexing parameters. Where's the T distribution? Just had a single one. I'm gay. So here we have to tell me two different things to figure out the curve is bf1 and df2.  If you take 181b which is what I teach right after this you learn what's going on Wheelie with a Nova and it takes about a week to have to cover it and it takes about a month build up.  So if you want to know why we don't really do this and Technical detail it's because it requires that much work in addition to two entire quarters before that of lead in material. So that's the amount of meat on the bone here if you like, okay, but we can at least get introduction and get a good feel for what's going on in queso language in terms of how you heard. All these things on. The very left won't use the phrase one sample T Test. Okay, the one refers to the fact that you drew from one population to a sampling from it. The teen tells you what distribution the analysis eventually happened on. So obviously this will be two sample T Test and over here people just use the phrase a Nova for an analysis of variation for variance.  Okay, it's part of a huge framework where you study things by analyzing how they vary.  And it's in the variation of an object that you come to truly understand it. So slick name now and has lots of extensions.  The first extension people usually learn this and Cova analysis of covariance.  Then I can go to manova or manova, depending on how do you pronounce add multiple analysis of variance in in man? Kova multiple analysis of covariance. So then you started classes, but the acronym  Train goes for a long time. I'll tell you that. Okay. Now one thing you immediately asked the same. Why do I need your fancy new framework? Why don't we if we have lots of different populations. Just run tons of two sample T Test.  Turn natural question, right?  Why do I need to sort of compare everything at once with a single Hammer hitting all of the nails? I can I just look at them in pairs. Well tonight idea except it. You'll see if we have four different populations. You can already see that the number of comparisons is growing large.  Okay, so here you can count that. There are six possible tests and that comes from the number for choose two of the four ones choose to that you'd like to compare and the true symbol tells you how many ways you could choose those two out of the group of four?  Unfortunately, the two symbol is nasty and gets big very fast. So if you have K different populations and you want to study two of them.  And you're going to study all possible combinations of two of them. Suddenly. We have K choose to which is about K squared.  So now you're running about case we're different tests.  How do you feel about running so many tests?  Well, there's different emotional things to feel about this. The first is is it possible to run that many tests that sort of a logistical question?  Now that your computer's you have no problem with telling the computer to do tons of work, right?  So maybe it's not so bad anymore the wave technology to help us. The other question is is it bad to run? A lot of tests? This is sort of a theoretical question and it turns out that if you do something a lot of times this is a very dangerous thing because you're going to start seeing some type 1 error showing up.  Just a random chance. So yours the metaphor suppose we're trying to decide if a coin is fair and I have a big bunch of coins and they're all really fair. Okay, and I give everyone in the room a coin and I say here's how we're going to decide if these coins are fair. You're going to flip them 10 times.  And then you going to tell me what happens know most of you will get 5 out of 10 heads. Nothing exciting happened, but there's one person in the room.  The lucky one that was just get 10 heads, right just by someone, you know, it's like a one out of a hundred event or something, but we have a hundred people someone's going to see it. So the problem is when you start doing stuff a lot.  Randomness crazy random events just start happening.  Because you have a large multiplier until you start to think something weird is going on with nothing inside weird is going on. So if you run lots of tests and we're going to start showing the significant difference.  when really maybe nothing's going on now, there's a way to correct for this at the end of the slide so we won't get too cuz bonferroni correction if you want to read about  but anyhow, that's why you don't want to run a bunch of tests. So so you just want to write run one that does it all at once.  This gets rid of What's called the multiple testing problem. So I wouldn't know if it does it runs a single test.  And it's check to see if all the means are equal to this known as an Omnibus test Omni for everything.  Now it's not the best kind of test ever. Is everything equal in life.  Seems like a silly question, right? You almost know the answer is no immediately but there's some cases where you want to ask such a thing. Unfortunately the drawback of running an Omnibus test as we said earlier is the alternative is to satisfying something differs.  What how many things differ I don't know but at least you have a starting point that you could then go off.  And maybe run some tests never going to use this infrastructure. There are a lot of assumptions. This is one of the frustrating things about it cuz he's all the conditions. First of all, you can't have garbage data.  So you need your data to be independent within and across groups. It was a tough raise the first time you see it within means if here's a picture of our data. So we have three different populations. One, two, three, and we're measuring something quantitative about him. I would come whatever that is. Okay doesn't really matter in this problem. I'm gay. So all of your data points, you can see fall into one of these three bins. Now if the dater are independent within it means you shouldn't be able to use one data point to infer the value of others.  Within your same group and across means if I know this datapoint, it shouldn't help me make predictions about ones in the other bins.  The easiest way to always get this is to fill up your study by choosing people randomly and to not choose to many. This is the randomization 10% condition. So the same thing as true here and that usually will give Independence both within all the different groups and across the different groups.  Okay, next sing the data in each group have to be nearly normal. So if you took these dots from say the first group those all represent numbers, you can make a histogram out of them and that histogram shouldn't look roughly normal and the same thing is true in all the groups. So that is a very strong Criterion all of your different population of studying have to be nice dish.  Just on Friday next door. We figured out how to fix that condition. It's rough. I'll tell you that it will hurt some horrible transformation Taylor polynomials. Remember these 20 be cuz I cant infrastructure in case it is a rough thing to lose and finally the spread in each group have to be roughly equal. So you look at how spread out the data are in group one at some number using the standard deviation use the variance. Whatever you want to me roughly the same ideas.  So this is like a very well controlled.  Structured cookie pay the flower and the salt and the chocolate chips are in perfect balance in order to use this.  Okay, now some of these conditions are more and less important.  Did the last one actually turned out to be really important? You've never seen anything like this before the no malady one is easier to a road. And in fact, we've seen this in the past with the central limit theorem. I always say I need to do to be nearly normal.  Except it's okay. If they're hot mess as long as there's enough of them. This isn't balanced, right they're always talking about to the same thing will turn out to be true here in a Nova. Also. The last one frustratingly. There's no nice way to fix if you don't have it except for Taylor polynomials and horrible transformations in another class. Okay?  So then there's a set up there so I can check into payments through the two conditions. We always do that. You can see the normality and the equal spread newest quite easily in one of the following two things. You can make me a box plot that has side-by-side grouping.  Okay. Now in the box plot, the spread is just the height of this box. That's the IQR actually. So if all the IQR is look sort of similar now, that is an emotional judgment.  I need you to just make emotional judgment sometimes.  It's like I don't want to do that in a math class. Give me some rules. I can apply sorry.  No, no malady. If the central line is roughly in the middle, it means that there's a nice balance between the two halves. No doesn't mean it's normal. But that usually the way people look for normality. Okay. So this I'd say this is like  Roughly normal and roughly the same spread. That's just what my emotional soul is telling me the boxes look like the same height and the lines are sort of in the middle. Basically. I want to see hard numbers. You can always create a table like this now, there's a table of people playing baseball.  Where they've been grouped or put into populations by their position, so Outfield and infield designated hitter catcher.  Okay. Now you have to be measuring something about them some quantitative idea. So here you can see that it's on base percentage, which is some sort of measure for batting average if you know that.  It's probably needs something to think about. She might wonder if the different players when you group them based on where they play.  If that idea differs in terms of their batting average.  Okay. So here are the numbers that come out so sample standard deviation. Do these numbers look roughly the same?  something emotional in your soul  Okay, if you're worried, you can always tell me we are somewhat concerned about the equal spread condition. They don't have to choose a side if you don't want to.  But I would recommend choosing a side if it's reasonably clear now. I'm I don't know.  I mean with the pictures over here. I believed it more the numbers. I'm starting to get worried cuz numbers you look at them differently, right?  Now you're not going to see normality in this list of numbers here, unfortunately.  But what is cool about the sister numbers? You can start to see you're studying whether you think the means are different, right? That's what this is all about. So if you look at these numbers or so, we'll have a feeling immediately.  But the point of the Anova is to rigorously decide on what your soul is feeling there. I don't know. Do you think the designated hitters have higher batting average than the others?  If you compare them over to infielders or outfielders, you're like a .01 to compare them over to catchers. You like .02.  Starting to look bigger. That's what I know. It was trying to tell us if these numbers really are different from each other. Assuming we made all these conditions.  Now do you know but does this by calculating the F statistic?  So this is the ratio of 2 weird Expressions that are both trying to analyze variance. It's going on in queso on top MSG the stands for the mean squares across from  Now this is defined in terms of a horrible formula.  1 / K - 1 * some summation  I'm so what this summation is doing.  It's taking XII bar.  Okay, you can see what's for missing a bar here.  There should be a bar over this xr80r know why it's not there. That's weird Okay, so that's some number.  And then it subtracting that from X bar, which is the overall average if you take all the players and don't think about them based on any group in just the average amount together. This is sort of what happens in baseball on average.  Okay, so it's looking like what how did the outfielders differ from the overall average and how did the infielders differ from the overall in a sort of a team elating?  This idea when you measure what's going on between the groups as they compare to the overall average.  Okay, and then discenza buys how many people were in a certain group? So how many infielders were in your study how many outfielders so it's a weighting factor in case I'm very exciting to get a huge difference between two ideas. If it weren't many people that went into making the difference.  But if there's a huge difference in there's a million infielders that used to build that that's important. So this isn't a Sensei waited.  Squared idea. So there's a square that has to do with groups. And then in the end you take an average across all of those different ideas.  Okay, and you don't / que / K - 1 we seen the sort of like weirdness before.  That you don't really need to truly understand this at some point. You should have some emotional feeling that is trying to measure some idea that has to do with how groups differ.  From the overall average. Okay down here the embassy this is the mean squared error.  Or the sum of squares related to are terms that you've taken an average of okay. So this looks within each group and it says okay within each group. You'll see a lot of variability not all infielders. Look the same.  Okay, and it's Tessa by is the standard deviation going on or the spread out news within a given group. Okay, so our infielders really spread out or catchers really spread out.  And what you're going to do is sort of again wait, he's and then averaged together everything going on.  Now it's not obvious. We're doing any of these calculations is helpful. It's not obvious. Why taking their average gives you anything important at all.  Right now it's just sort of we're analyzing the variation that can happen.  And the variation here in MSC is all about variation sort of within a group in terms of the errors that are happening as you look at infielders. How do they vary?  And this other one is looking at the variation in groups as they compared to an overall average.  And then we have some ratio.  So this is all the apps that let me show you how to use this in a nice example. This is one of my favorite ones that I've ever seen done with a Nova because it's a really cool result actually.  So if you know anything about volcanology, have you taken the volcanoes course here?  Has anyone taken volcanoes?  Apparently it's like one of the most popular courses at UCSD.  I used to teach in the room right before the guy went on volcanoes and was like crazy in there. He won't crazy for that class.  Anyhow, so when lava hardens because it's full of lots of magnetic things metal things. You can see the direction of Earth's magnetic field in the hardened lava because of the iron that appears in lava.  Okay, so that's kind of cool. So what research is did is there were three different eruptions of Mount Etna in these years and you can go back in the fossil record record and you can see the direction of Earth's magnetic field by looking and just measuring from some like true north or whatever. It's  so how's the direction of Earth's magnetic field changed?  Overtime this is a nice way to check it because we have data that span for 400 years basically and you can see so these are three different measurements. They did from the 1669 eruption hardened lava.  now  let's try something with your soul.  Hey average together the numbers in each column.  And then ask yourself if you feel like they're different from each other.  Does your soul have an answer?  So we're going to run into Nova now and figure it out. And with the Anova is going to do is going to try to look beyond our data.  Can you can do it? Like obviously the columns don't have exactly the same average, but the question is are they so far apart from each other that they must have come from populations that actually differ.  So let's eat first thing. Can you define me some variables and write me some hypotheses? Let's start building the infrastructure around this a minute here to do that.  Okay, three different populations.  1669 1780 1865 implies three different averages you can call them whatever you want. Make sure they've Greek letters you can put the years of subscripts or you can just put numbers and then tell me what they relate to so if we were able to truly go back in time.  And set up a device that would measure the direction of the magnetic field. We would get a value.  In 1669 that value would be a new one.  And so on for the others so, you know, it'd be super boring if they were all the same number. We wasted our life going back in time.  And I would be super excited if one of them different from the rest because this would show a change in the magnetic field.  In fact, lots of them could differ that would be even more exciting. It would show continual change. Not just a one-year change.  Okay now.  The calculations around an oval are horrible in complicated and they usually must be done with the computer.  Guess I'm not going to ask you to like.  Find an F stat by hand for from the original data. This would be nightmarish. Okay, so let me show you what it looks like when we go put this into our especially if the structure correctly.  Now you don't actually put it into our is a matrix. Like I gave him the previous thing. You put it into dataframe where all the angles are in one column and then the other column has the grouping variable. Will you showing me how does data split by year?  That usually people will use some letter or something for the grouping variable that you can use numbers here. If you really know what you're doing, but it's very dangerous to put numbers in this column because our needs to know that it's a categorical variable a bidding way of dividing things up in our can get very confused if you put numbers in the year column.  Okay, so then what are you do so then you run aov analysis of variance?  And the first thing it wants is a  This formula here. So on the left side and wants to know the quantitative variable and this Tilda you're going to tell it how to break up a partition the data into different groups. So use the year variable to split the angle data into different groups.  And then everything comes from the volcano data set. So the thing I named this object the state of for him here.  Okay, so this will run the Anova and it gives you back a very complicated object with tons of information in it. So then I just say give me the summary of this big complicated object.  So this prints out a table and this would be the Anova print out or read out and this is common to most pieces of software there actually is some uniformity in what people in the world have decided to do despite the fact that we have different softwares. Okay. So when they have to be able to do is read this print out regardless of the software came from the F statistic, it's taking the ratio of MSG in mac, and it just happened to get the number 15.  Hey, so that's not hard because of the name is exactly above it. Now. The mean squared call him is telling you what MSG and empathy are. So the way this is set up is one of the Rose will be about grouping ideas. So here's the year variables how you group things? Okay. So this entire first row is group related things. That's why this number is MSG. You can see the MS from the column in the G comes from the year idea. The other row will be the error residual row.  Depending on what language they use here because of this was all about error ideas. So you can see Ms. Andy coming together 2.95 is the MSC. Okay, and you can even chat 45 / 3 is about 15.  Is it ain't doing any magic here if you understand what those calculations are now over here the degree of Freedom call him. It's helping keep track of what kind of car were eventually going to be on.  Okay. Now the first one since that this is the year is a grouping idea. This is known as DFG the degree of Freedom related to the group's. Okay. So it's the number of groups - 1 that's how you find this. So here we three different years. And so that's why they're getting to  Now this residuals degree of freedom is always going to be in the total number of data points. You were studying.  / - K the number of groups that you broke things into so here we get 9-3 or 6.  Okay, so that's that column. Now. The last thing you're going to care about is the final thing. This is the P value.  This helps you decide between the two hypotheses. You just wrote it and it's as it normally is if this is really small you'll move to the alternative we've seen incredibly weird. Aprecio is with this number the fact that it's so small. This is what it means.  Okay.  Now if you want to think about what a p-value always means it's always the area under some curve.  Based on your data or something more extreme.  assuming the null hypothesis  So here that curve happens to be an F distribution in the particular F distribution you on the Stick Page by the two degree of Freedom parameters that you see right here.  Okay, so I can actually draw a picture of what's going on if you want. Okay know we probably should draw a couple of distribution. So you could see what they sort of look like. So their support the possible ways they can take on or on the horizontal axis so starts at 0 and goes to Infinity.  And the reason is the f-stop is a ratio to mean squared ideas which always positive so that's why you can't get anything negative. And so I just chose to random Ones based on these different crazy degrees of freedom, and you can see in general. They sort of go up have a hump and then come back down.  Is what these look like? So when you're calculating A P value for an anova all you do is draw the appropriate F distribution, and then you plop down your fstat or your value and then you just say to the right.  Now one thing is strange about these.  You're always going to shade to the right.  And there is no two-sided alternative. There is no symmetric shading both directions. There's none of that.  The hypotheses for an anova always look the same the null is all the means of the same feel ternative is something is different.  And when you do the shading you always popped on your f-stop and Shay to the right. So the infrastructure is built around that happening.  How you feeling so far?  Some people will hate today.  Because I said here's the tool. You don't really get to understand why anything works. It's a super complicated tool with all these weird intricacies.  And people get bothered why they when they don't know things are true.  So if you're that kind of person 181b is waiting for you.  You just have to get through 180 and 181 a first so K. They're quite doable and enjoyable.  If you're not one of those people in your fine, like give me a tool I love using tools.  Is a great day for you and you enjoy using things like a black box. We don't really truly understand. What's going on underneath. Okay. How about we do some Socrative?  I wonder how you're going to do. Let's go see.  Swans doing an F test that is an anova.  How many groups?  Are the researchers dataset?  Hey, there we go.  So this is just seeing if you have learned this expression for the degree of Freedom related to the grouping idea K - 1 so you can back derive the number of groups that way.  How many total data observations were in the data set?  120  parents are people breaking out the degree of Freedom related to the error formula and minus K and back driving 110 was from that.  collect salary info on a hundred 1012 200 people five different states  What curve is the p-value found on for this study?  His number should be subscripts here. But so proud of his so limited.  Can't really show it nicely.  Look at that 70% already. So here you're just using the you need to make sure you get the right order.  That's something a lot of people don't know. It's a group thing is the first spot and they have distribution in the errors II spot.  What commands should we write in our freaking an F stat of 4.2?  don't you love the temptation of a  It seems right in every way right? Some people were selected before they read on.  Okay are always resist lower Tails by default.  So you need to tell it to shade or the right side, which is what he is doing.  if a Nova suggest a move to the alternative  Then we will have identified the mean that is different than the rest.  So I said it a couple times already. The alternative is frustrating. It says something exciting is going on. That's all it says nothing more doesn't tell you what?  How that's good. I love to ended on one that the majority gets wrong.  Now you got very little. Hope of getting this right. I'll tell you that right now.  Okay, and you probably song the explanation?  It's possible for the Anova to say something interesting is going on.  But when you go try to find it using a bunch of pairwise tests.  None of them can find it.  Okay. So here's this analogy corruption present in the government, but you can't exactly identify where it is.  Sometimes the totality of the experience flowing at you says excitement. If you can't pinpoint it for some reason maybe it's generated across the totality of lots of different comparisons are leading to that.  But when you dig deep it's not visible. So this is a very counterintuitive results.  In fact the 181b class. They just had their homework on a Nova and they had to write code in our that would generate such a data set. So it's not obvious. Like how one would even have such a dataset you have to like let the computer try to create such a thing. So it's so it shows that there's like not a perfect structure to all of our statistical testing here that we can figure out everything in life, basically.  Okay, should we try some more?  But try some more.  True tried smoking problem. No nose to have coffee problems. I like the coffee one more than the smoking one.  Okay. So here's an actual study. This is one of the freely accessible jumbo data sets on the internet that you can go get the women's health study.  Okay, so they asked women a bunch of questions.  And then they the Tuesday to set and then people could just go study whatever they wanted. So this is study that came out about this to people who date drink different amounts of coffee each day.  10 to exercise different amounts  It's what they did is in 2 cups of coffee and they made it a categorical variable it bend it pay less than or equal to one cup of week Etc. So they have these different bins so you can see where you are. And then they measure people's exercise level per week. Now that could have been in lots of different things number of calories burned. So there's a continuous random variable that used to Mets. Sorry America. You're not probably familiar with this stupid unit.  But all the non American people are probably loving it cuz they know and meet. He's a little better 50739 women.  Okay, so here is sort of a summary table of what's going on. So we're going to run in and over here. It is to see if different amounts of coffee leads to different amounts of exercise on average. So I need you to set up temporary amateur some hypotheses for me. Try to do this in the most efficient way on the page. Now that you've already done something like this previously.  Okay.  Set up an average idea. It should be the average of a quantitative variable in the problem. So hear the quantitative variables The Meta this is this metabolic equivalents tasks here.  Now you need to tell me what the different groups are. So this is the different coffee categories. It helps to tell what population or measuring here. It's women probably American women.  Okay, and then Dino should be that all them eyes are the same. If there's a lot of nuso buys you can write them all out and put equal sign between them or you can choose words. So it's totally fine to use words infected easiest to use words for the Anova null and alternative hypotheses. All them you eyes are equal some at least one is different.  Okay, next thing let's talk about inference conditions. There were a bunch of these.  So if someone talked to me about any of these that you'd like to mention and whether you think it's met.  Help us who checks conditions anyway.  Let's be stopping garbage researchers. Never check anything.  We have three conditions.  Go ahead. Which one do you want?  Okay, Sweeney the spreads to be roughly equal just called homoscedasticity almost the same speed as tissy T4 spread out in this.  You're worried about the 25.5 right here.  So if I were you I would write we need the spreads to be roughly equal. I'm somewhat concerned about the 25.5.  So there you told me told me what you need and you're not willing to even commit on this one day. Someone else might be totally fine.  What are the other conditions?  Go ahead.  I'm sorry.  10 so 10% and randomization are trying to create the independence idea. How do you feel about  I agree there more than 50,000 women in the US drink coffee.  But we need more than 500,000 right when we need more than 50 thousand times 10. So I also think there's more than 500,000 Starbucks exists. What about the randomization?  How you feel about that?  They already why'd you say the randomly-selected? Where'd you get that?  Hoping and dreaming in your soul. So you have to go back and look and see what's going on up here.  I don't see anything about random attack. This was probably volunteer. I'm guessing.  Okay, we don't know about this condition because we don't know how the data were collected.  Volunteer bias is most likely what's going on. Although I don't know if that affects anything about what's going on in this. We also don't know anything about the normality condition here because I haven't shown you a picture of the day Tiny Tim the five groups.  So just tell me if you don't know something it turns out it doesn't really matter about the normality though.  So in general we just need normality unless we have large sample sizes and we have huge sample sizes. So these numbers 6017 thousand etcetera.  Now, how am I ever going to give you a problem like this on an exam?  It mostly involves computer stuff, right? Well, here's how you can give a problem like this on an exam. You can give the print out and say there's missing pieces.  Coffee stains are blurring things whatever. Okay and between this description of what's going on in this table, you should be able to fill in everything that's going on here.  So for example, you can get the DF column by just knowing how the study of structure there were five groups, right? So 5 - 1 is 4  There were 50739 people at the end. So if you subtract a from that you can get this value and you can always add Down The Columns to get the total idea.  Now this sum of squares, we haven't talked much about okay, it's really just an intermediary it to get to the main idea which divides the sum of squares column by the degree of Freedom column. So if I subtract I can get this number and then if I divide the sum of squares by the degrees of freedom, I'll get the mean square column, which is what we really care about. So we have to stop but I suggest you keep looking at these things and working on them.  Have a great day. I'll see you later.  Can you see sandiego.edu? "
}