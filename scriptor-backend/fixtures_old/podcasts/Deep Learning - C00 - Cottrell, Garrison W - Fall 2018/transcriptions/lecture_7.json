{
  "Blurbs": {
    "2 weight. So assuming the inputs and the weights are independently given in the waiter initialize randomly. So they're going to be independent of the inputs and hiyori means expectation. Send the variance of AJ, which is the variance of x x w if you just you know variance is X W Squared. So this is just a and identity and look this up on the web. Okay, its variants ": [
      3092.8,
      3125.4,
      66
    ],
    "80 images. Now, we've got data sets with a gazillion images. And so the dimensionality maybe the the guy that you use instead, okay. alright, so This is a picture you should have in your mind. Here's the data and instead of moving the accies. I'm going to move the data the same idea the first I put the origin at the center of the data. And PCA is also ": [
      1690.5,
      1728.1,
      29
    ],
    "And so that's why dimensionality reduction and if you look at the if you take some real data that has correlations in it and you plot The eigenvalues which tell you what the variances. They usually go like this and so you can cut it off where you can explain 95% of the variance with you know, this many eigenvectors say that maintains most of the information. right Keith Haring ": [
      2171.1,
      2211.0,
      41
    ],
    "And this is the weight of x y z z y. But if I projecting onto this axis, I'll get a slightly different answer. For his height and his weight and I get a small amount of error here. And a small amount of error here and if I'm willing to tolerate that I can just send one number for everybody. So what principal component analysis does is it finds ": [
      1410.9,
      1447.5,
      22
    ],
    "I but if you know if you have two variables to Binary variables, how many numbers do you have to have to fill in the joint probability table? Give two binary variables. How many numbers do you need for the joint probability table? 45 what? What did you say? PSR variable one variable to true-false true-false 4 No. 3 play some to one. I only need three numbers then I ": [
      1933.0,
      1984.2,
      35
    ],
    "I call funny can age because it's so funny that it's 1.715 910h two-thirds X. That's not 2/3 x 2/3 x So if ex has unit variance than the output will have unit variance turns out so the output is not formatted for the next layer up. So again, you get zero in you get zero out. If you poop turns out if you put one in you got one ": [
      2612.9,
      2651.2,
      52
    ],
    "I'm following this Direction with a little bit of that and it got a little bit that way that I make the correction. That's my new way chain. Yeah, yeah, we're still doing momentum. It's just like when do we add in the gradient? So it's really just a slight change in the code. Yeah, well. These days we just use the Adam Optimizer and forget about it, which probably ": [
      4612.2,
      4657.3,
      107
    ],
    "I'm going to ask again. What do you think? What happens when? They're all positive inputs. Remember, that's the way change rule. Say that again. What will always be positive? Okay, so the weight change rule for w i j so here's Iwi Jay, okay all those guys. X the input on that line in Delta J. So what's going to happen there all all the way to going to ": [
      1042.3,
      1099.3,
      13
    ],
    "Listen to a podcast. Rihanna work Okay. Let's go. Let's get started. Okay. So last time we started talking about tricks of the trade and we talked about stochastic gradient descent versus patch shuffling the examples. And how wonderful that was and then perform PCA of the inputs. But I serve it a little bit of introductory information here. So the Earth's surface of a linear neuron. With a horizontal ": [
      1.9,
      205.8,
      0
    ],
    "M. So we have M inputs coming in call that the Fannin. If we set the standard deviation of the weights to be won over the fan in square root. You got the standard deviation of AJ is the square root of the sum of 1 / M square root squared which is just one over n and there's end of them. So the sum of 1 over m m ": [
      3204.1,
      3230.3,
      69
    ],
    "Okay. So that's a kind of analysis you can do. It's not exactly correct, but it gives you an idea of the effect of momentum. Yeah. Is the alphas constant usually? Well, Jeff whole give you some different advice in a moment. Yeah. Yeah, yeah, if you have small many batches then your gradient is going to be less consistent, right? Cuz a small mini batch is a bad sample ": [
      4195.3,
      4249.8,
      96
    ],
    "So let's and then Jeff hasn't had this rmsprop idea. that normalizes learning right and we'll kind of get to that but okay, so just to reminder we've got this. quadratic Bowl that's usually a pretty good approximation and the steepest descent here doesn't go to their which is where we want to go to the bottom of the bowl. The slope is going to be that way. Here it ": [
      3701.6,
      3738.9,
      83
    ],
    "Spell but it turns out if it keeps on coming keep getting more results out of that surprisingly. Who knew? And they seem to apply to real deep networks with I mean if he is relo your kind of linear, right? Okay. Okay, so now some slides from Jeff hinton's course. About mini Bachelor ending ways to speed it up in the first ones going to be momentum. So Instead ": [
      3630.4,
      3669.4,
      81
    ],
    "Squared terms. Which is going to make it a quadratic. That's the reason. Okay, if you multiply this out. Okay any questions? For the 13th of you who thought it was 10 of you thought it was C. and three of you thought it was a okay. already, OK Google So what again this preliminary kind of data? Before we get to the normal stuff the PCA stuff. So let's ": [
      580.4,
      630.8,
      6
    ],
    "Yeah her size and she said yeah. Oh. Yeah, when you do this, so this data before I did that to it was highly correlated. Right. Taller, you are the heavier you are. Right. When I do this, I've got a new access size and adiposity and those aren't correlated with each other. So a d correlates the data. Which is pretty close to Independence. It's not Independence, but you ": [
      1847.0,
      1889.9,
      33
    ],
    "You can actually these are smooth functions. You can differentiate these. So and you can differentiate this so you can learn these guys. Do gradient descent on these parameters? Okay. However, there has been some discussion on the web and I don't know if it's resolved yet. But some people have found that if you apply batch normalization to the input to the layer instead of the output of the ": [
      3558.1,
      3595.5,
      79
    ],
    "adaptive learning rates. So the gradients are could be very small and it really layers and there's going to be a different fan into different units depending on you know, like in your guys last assignment you had like a huge fan in from the pixels and then a tiny fan in Well, I'm at smaller fan in like 50 to the outputs, right? Oh wait. No. That was the ": [
      4692.5,
      4725.2,
      109
    ],
    "and Mario Advance ER okay Okay, I was looking good there for a while. I guess the people are pretty sure answer first. Answer is naughty. No, it's not. Come on there, either it's not okay. run Okay, we're hovering around 68 70% I'm going to close it out going going going gone. Just spend a couple minutes talking to your neighbor about this. You have a 71% chance that ": [
      306.4,
      365.9,
      3
    ],
    "and had a little something to avoid / 0 just in case this gets really small and that's a z scoring the variable. Okay, then. Then what are you do you take that nicely z-score thing and you multiply it by this parameter and add that parameter. So these are learned by backpropagation as well. And this is specific to this particular hidden unit as as this and if you ": [
      3484.7,
      3520.6,
      77
    ],
    "and now I go that way the gradients that way but it's been pushing me this way. So I'll go a little bit that way and in the end. Those oscillations will average out and I'll head straight down. If I keep track of my previous gradients and average them in. The momentum keeps it going in the in the previous Direction. So instead of changing the weights according to ": [
      3836.2,
      3871.6,
      87
    ],
    "and standard deviation whenever is current event. Okay. So the point of this is that after normalizing the input by combining a particular weight initialization with the right sigmoid, we get normalized inputs the next layer up so 0 and 0 out standard deviation 1 in standard deviation one out. And it makes some intuitive sense. Get a room Wally has big fan in small change incoming weights can make ": [
      3265.8,
      3311.7,
      71
    ],
    "and the weight change for guys coming into the unit depend on the input on those lines and dealt with the inputs are all positive Delta. See they're going to be positive or negative. They're all going to change positively or I'll change negatively. So where can the weights go? They could go this way or this way or this way or that way or that way or that way ": [
      1142.0,
      1171.7,
      15
    ],
    "axis for each weight and one vertical axis for the error. For a linear neuron with squared error. It's a quadratic fall. So vertical cross-sections are parabolas. horizontal cross-sections are ellipses and Fur multi layer neural networks. The earth's surface is much more complicated but locally aguanta quadratic ball is usually a pretty good approximation. So here's a quicker question. so This is in the case of of sum squared ": [
      205.8,
      265.8,
      1
    ],
    "be 0 mean in unit standard deviation with the recommended sigmoid the honey 10h. So the weight initialization needs to be coordinated with the input normalization and the choice of sigmoid. So we achieved zero mean and unit standard deviation with the inputs using PCA. We also want that to be true of the outputs of the first hidden layer. And so AJ is the product of the inputs x ": [
      3057.4,
      3092.8,
      65
    ],
    "be for your problem. It would be like taking every pixel through all the images. And making a 0 mean in unit standard deviation. Which is different than I think what I told you to do, which was take the images and make them 0 mean in the unit standard deviation. So they're slightly different ideas. Okay. So now we get this nicely formatted data snow 0 mean it's unit ": [
      2493.3,
      2524.9,
      49
    ],
    "be the eigenvectors of the covariance Matrix of the data. Okay, and you don't have to know that it's just fun to say so now if I send back to Earth the coordinates of this new coordinate system. I could just send them the coordinate along this axis. And here's xyzzy. He's now his coordinate on that new axis is right there. And so this is the height of xyzzy. ": [
      1372.9,
      1408.8,
      21
    ],
    "be? So when you're on some Square there. turn some Knights T minus y quantity squared, but why is in for a linear neuron w i x 20 squared so this is going to have a t squared term A- Duty some wi, there's going to be a lot of w, so, this is constant. That's constant. It's W's are the variables you're going to have a lot of W ": [
      520.3,
      580.4,
      5
    ],
    "called the car Herman and Luther's transform and now I got the ax he's lined up with the variance. And that's what PCA does. Stop here. That's PCA. But it turns out that the eigenvalue on the eigenvector is the variance along that Dimension and if you divided by the square root of the variance then You squish the data down. So it's all equally variant. So it's like Z ": [
      1728.1,
      1762.2,
      30
    ],
    "can fill in the forest. Okay, but if I have three variables, I need seven numbers five four variables. I need 15 with numbers. So if I wanted to represent the probability distribution of the world. In my brain, which I would like to do because that will tell me what to expect right then. I would need in my features of the world are not independent. Then I would ": [
      1984.2,
      2018.0,
      36
    ],
    "can't start at 0. Okay. So let's assume that we have logistic kidney in a tear for this demonstration. Okay, if all the way it started zero then these guys are going to have an activation of .5 if there is a logistic cert NH. They're just going to have activation 0 and nothing happens 45 out. Okay, and the outputs if they were Logistics lb point five. Also and ": [
      2821.1,
      2863.4,
      58
    ],
    "change which way? War negatively cuz Delta could be positive or negative. So Delta is the same sign for all of these weights. It's the one guy that you know, most place times those guys and changes these wait. So that means that either all the weights go up or all the way to go down. Okay. So, all right. That makes sense. Because Delta is associated with the unit ": [
      1099.3,
      1142.0,
      14
    ],
    "cuz he make a small change to a large amount of weight is going to change your weight did some of your inputs much more than that. Same Small Change 250 weights. All right, so that implies maybe we should have adaptive learning rates. And then the remaining minute. Which we start with a Global Learning right? And then we have a little multiplier on that called the gain. That's ": [
      4750.9,
      4787.3,
      111
    ],
    "direction and you're not allowed to go this way because that would make this one positive in that one- Okay, so you can only change. In this direction or that direction roughly, right? Okay. So that's bad. That means the the weights. Can't move an arbitrary directions specially if they want to get to that green. Okay. So this is why it's a good idea to have. positive and negative ": [
      1207.7,
      1246.8,
      17
    ],
    "error. Why is the Earth's surface of a linear Neuron a quadratic? Is it because quadratics are good approximation to most curves is it because of some Square there? It has to be quite erratic or is it because cross entropy are leads to quadratic or is it because linear neurons you squared inputs? hand I seem okay. I've started it now. I seem to have misplaced my clicker again ": [
      265.8,
      306.4,
      2
    ],
    "first principal component is the one that captures most of the variance and it's basically just the line through the space that's closest to all the data. And then the next Direction you find because of the rotation of the axis. You're basically finding something that's at right angles to this and it's going to capture the next most variance. And if there is a third dimension here and it's ": [
      2368.7,
      2398.7,
      46
    ],
    "fodder great fodder for a midterm question. Okay. Okay. So any questions about that, so this was assuming that we're using logistic unit. So they had an activation even though the input wasn't was Europe what happens if they're tan H. The input is 0 the output of zero the weights never change nothing happens. Okay, can you have no weights? Be very lightweight Network. Okay. So again, we want ": [
      2988.6,
      3032.8,
      63
    ],
    "get really small and if this is important, I need to waste to get really big to have the same effect. So the way I are going to have to take a long time for the weights to this one to get really small in the waist of this one to get really big. So if I scale them if I whiten them so that each one varies about the ": [
      2105.6,
      2130.4,
      39
    ],
    "go a small distance is small in the direction. Where in which we want to travel a large distance? And even for most popular nuts, the earth's surface is off and locally quadratic. So the same issues apply. Okay, so that's that's the beginning. Now the question is what's wrong with all positive influence. What happens if all the incoming Waits 21 hidden unit say these are the inputs acts ": [
      668.3,
      706.6,
      8
    ],
    "going to go like this. Right and so those gradients have different signs when I jump across this funnel. I get a different I get a opposite sign gradient. so the intuition behind the momentum method is I go in the direction of the gradient, but then my next one which is going to be that way. I add in some of this time averaging the gradient is a go ": [
      3800.8,
      3836.2,
      86
    ],
    "gradient and I'm adding a little Vector to it, which is the gradient right here. So this the running average of the gradient. This is my gradient at this point. So I'm making a little correction and then I'm taking a big jump wouldn't it be better? To go that way and then compute which way is downhill from there to make a little correction. That's the difference between regular ": [
      4425.3,
      4457.4,
      102
    ],
    "gradient going this way. It's going to get bigger and bigger say average in all these guys going the same way. if if you've been getting opposite signs So are you going to stop doing that? And then I'm going to start doing that. Yeah, yeah. So here's the equation is right, so you take the hidden calls it the bossy cuz it's really the velocity not the momentum if ": [
      3944.6,
      3994.2,
      90
    ],
    "guy depending on what his Delta is cuz all the temperature the same they're all point five. For this guy they're all going to change the same amount. And for this guy they're all going to change the same amount. Okay. and but what happens here I get The black the green and the red I get the black and green the red I get the black the green the ": [
      2896.8,
      2923.8,
      60
    ],
    "here on top of the saddle on you. Does that mean small your mother? The less you care about your previous average. Yeah. Yeah, exactly. You got it. He's got it asked him if you have a question. Right when the when the learning slows down. Would it be that are less about the previous average because we've arrived at the correct rating. Yeah, but we want to go faster ": [
      4325.2,
      4364.8,
      99
    ],
    "how you change the weight. Epsilon oh, that's a learning rate. Yeah, sorry. Yeah, I use different characters. Alpha is the momentum rate. so the weight change is equal to the current average she take okay. So how fast should be less than 1 cuz you're taking some fraction of how much you've been moving and this has to this should be small Epsilon here should be small. So usually ": [
      4030.1,
      4074.7,
      92
    ],
    "in the direction of the previously accumulated gradient the momentum term and then you measure the gradient where you end up. And make a correction. So it's better to correct a mistake after you've made it to see idea instead of before e So here's a picture of the nest Rock method. I take a big jump because I'm using the momentum term which is a big factor to make ": [
      4500.2,
      4535.9,
      104
    ],
    "information along each axis. It's the variance variances Poor Man's information and not you know, it's like a cheap way to measure entropy. It's not quite entropy but close enough for government work so I can I can reduce the dimensionality of the data also. So in your programming assignment last one you gave it all the images, but if I did PCA of those images You could have turned ": [
      1558.1,
      1594.5,
      26
    ],
    "inputs So why PCA the inputs? So I usually give a little. demonstration here of what PCA does so suppose I'm up on Mars. I'm the Martian. And I'm meeting all these martians up there. And I'm recording their height and their weight and it turns out that height and weight are highly correlated with one another. Bright, so this is the height and weight of xxyzz. Why? Martians with ": [
      1246.8,
      1298.8,
      18
    ],
    "is a side note linear autoencoder essentially does PCA and I showed that a long time ago. Maybe before your parents were born. so Actually, somebody else showed that I started empirically they said it. Baldy showed it, theoretically. Okay, so if I'm doing Auto encoding that means what I'm doing is I'm taking some input. I'm running it through a narrow channel of hidden units in trying to reproduce ": [
      2212.4,
      2250.2,
      42
    ],
    "is. So cool. Be good in here. It'll be good. But over here, it's going to go hoops. Okay, and if we go too far then what happens we we end up like increasing the air if we have too high of a learning rate. Okay, so that's why it's good to lower. The learning rate generally is Hugo. So what we want to do is move quickly and directions ": [
      3738.9,
      3769.4,
      84
    ],
    "it again on the output. Okay, and because everybody's linear my objective function is squared error. So I'm trying to minimize the squared error or what is PCA do it provably minimizes the square there and what happens is that these guys don't end up being the principal components, but they spend the principal Subspace. So there's no one guy hero becomes principal component one and another guy that becomes ": [
      2250.2,
      2284.6,
      43
    ],
    "it it D noises the data to so you get rid of high frequency things that don't don't do much. Okay. So wet last week when I show you those eigenfaces those were principal components of images. Relatively. So yeah. backprop custoza Okay. Okay change the sigmoid, okay. So now we've got these nicely formatted data. We've got it zero mean unit standard deviation for all the variables. That would ": [
      2433.4,
      2493.3,
      48
    ],
    "it over shoot right fit as a lot of weight. So we want them to be very small. And if so, we want smaller incoming weights if the fan is big and vice versa. So do what I say. okay, but Okay, it's all great. But then what happens we learn? We change the weight. So things will no longer be zero mean and unit standard deviation fact, we don't ": [
      3311.7,
      3345.3,
      72
    ],
    "know, it's like using variance for entropy. It's close enough. So like x and x squared are uncorrelated, but they're not independent. What we would like is to have independent variables to represent the world. If we could because A lot of pixels out there in the world are highly correlated with one another all these pixels are pretty correlated with one another I'll get to you and so if ": [
      1889.9,
      1933.0,
      34
    ],
    "last time he didn't have hidden units last time nevermind. This time you have hidden unit. So you got like nine hundred inputs to the hidden units. So that's a fan of 900 and then you got 50 hidden unit. So the fan into the outputs is 50. Vs900 that's least an order of magnitude different. So you want the gradients are the learning rate should be different for those ": [
      4725.2,
      4750.9,
      110
    ],
    "layer it works better. Who knows. So we're still in empirical mode with deep learning. We don't we have some theories about deep learning and surprisingly one of them just assumes the whole network is linear. You know, maybe you can figure you can get something out of assuming the whole network is linear because then you can just multiply all the weights together. You have a single layer Network. ": [
      3595.5,
      3628.3,
      80
    ],
    "learn relatively independent features. Yeah. I'm sorry. What? Oh, so why is this a good idea? Is that what you're asking? Well suppose I have two inputs coming in. So now they're not positive anymore. Once you know, they very positively and negatively but one of them is going like this. And the other one is going like this. Then if this is important and I need the weights to ": [
      2055.3,
      2105.6,
      38
    ],
    "linear range to start out with and it can learn the linear part of whatever it is. You're learning first and then is the weights grow. It'll get into the nonlinear regime and start to take care of things that require nonlinear representations. Okay. so I noticed that brother units don't have all these properties and you need a different analysis for those in people have done that. That's why ": [
      2685.2,
      2722.7,
      54
    ],
    "looks like right. flat There's a little peek at the right answer but most of it's flat. Okay, meaning you guys have no idea must be okay. I don't know if it'd be okay Delta J is associated with what when you compute it. Okay. I'm going to give you some time to talk about it. Hey talk about it. You have a 34% chance that the person next to ": [
      764.6,
      802.5,
      10
    ],
    "mainly going to Mars spring vowels. Okay, nevermind. So that's that's what their names look like. And this it this Martian has this height and that way and I have to every time I went to The Story Goes every time I want to send a data back to Earth. I have to shoot off a little rocket. And so I would like to send just one number instead of ": [
      1298.8,
      1336.0,
      19
    ],
    "make these properties true and we need them to be random not zero. And we don't want them to small or there won't be a gradient because we're propagating the Deltas back through these very small weights and if they're too large. The teenage girl gets hit to the rails in the slope will be near zero and will have very small gradients. But that's quick ask why the weights ": [
      2789.3,
      2821.1,
      57
    ],
    "makes sense. No, okay. Not at all. Yeah. I keep track of a running average does a gradient so if I started here and it's a positive gradient and then I end up over here. It's negative to positive and a negative is going to be smaller than both of them. So I'm going to going to add less going this direction. If I keep a running average of the ": [
      3906.5,
      3944.6,
      89
    ],
    "momentum and nesteroff momentum. So the standard method Starts Here, we know we're going to add that big momentum term in and it computes the gradient right here and adds it to it. That seems wrong instead. We should go there and then make a correction. That makes sense. You're not shaking your head. No this time. Okay. heliostats Guyver thought of this so you first make a big jump ": [
      4457.4,
      4500.2,
      103
    ],
    "n 0 out one in one out -1 in minus one out, but That is if the weighted sum of the inputs, you know, that's nice if the weighted sum of the inputs is 0 mean and unit standard deviation, but I didn't do that when I did was make the input 0 mean and standard deviation not the weighted sum of the inputs. and so we need initializing to ": [
      2754.0,
      2789.3,
      56
    ],
    "need a brain the size of Manhattan in order to represent that probability distribution. But if all my variables are independent, then I just need a linear number of numbers because I just multiply them together, right? So this is why you want to learn independent features of the world. And I don't know if anybody's proved it or shown it or empirically tested it but generally deep networks will ": [
      2018.0,
      2055.3,
      37
    ],
    "network over the mini batch. So you run the mini batch into the first hidden lair. Then you z-score those who this hitting unit has a bunch of different values over the input in the mini batch use z-score that naked 0 mean in unit standard deviation. Then you propagate it up do the same thing again, etc, etc. And then well suppose that's not a good idea. It adds ": [
      3416.0,
      3453.9,
      75
    ],
    "now that we're on this shallow thing. Okay, so that's momentum, but now forget everything I said because there's a better version of momentum called nesterov momentum. So what the momentum does is say? Okay. I'm going this way. I want to go a lot that way. But before I do that, I'm going to figure out which way the slope is from here and add that to it. Hey, ": [
      4364.8,
      4402.7,
      100
    ],
    "of actions various Square. Okay, which turns into this because x and double you were mean 0 so this is me and zero that's means your own. This is the mean squared. So it's 0 sqrt of 0. And X has unit standard deviation. So the variance of X is 1 Right, cuz we made it that way. And so the variance of the weighted sum of the inputs is ": [
      3125.4,
      3166.3,
      67
    ],
    "of all your data, right? It's it's more going to be more idiosyncratic. And so the the direction of the grating is going to change War. So this is Jeff's advice. These are Jeff slides that I've made small changes to So He suggests starting at the beginning of Learning. There's going to be big gradients because you're beginning to learn and so start with a small momentum and once ": [
      4249.8,
      4286.8,
      97
    ],
    "of using the gradient to change the weights. You keep a running average of the weight changes. And use a second, so that's one thing. And you can use separate adaptive learning rates for every parameter. So for every weight in your network, you can have a learning rate. Dudududu and then you can slowly adjust those things the learning rates using the consistency of the gradient for that. Okay. ": [
      3669.4,
      3701.6,
      82
    ],
    "on a little bit of stuff to give the network a chance to undo that. Okay, cuz maybe that's not a great thing. Okay, so maybe it needs to be nunzi scored. So, okay. So you take every variable like this might be the activation of the third head unit in the first layer over the mini batch you compute the mean of that and divided by the standard deviation ": [
      3453.9,
      3484.7,
      76
    ],
    "only have one principal component. If I have three pixels, then I'll have two principal component. So if I have 80 images I can only have 79 principal component and I'll convert all these images to 79 numbers preserving all of the information. Yeah, I just can't dry 91000 dimensions. Maybe you can anyway. So this was really great back in the day when we had data sets for the ": [
      1644.0,
      1690.5,
      28
    ],
    "or that way. They can't go that way. Because so you can only change in this quadrant or that quadrant direction, right? You have to tack for those of you that know how to sail you're essentially going up when try to get here. And so you're not allowed to change. This way because that would be changing this way in a negative Direction and that one in a positive ": [
      1171.7,
      1207.7,
      16
    ],
    "out and if he put -1 and you get minus one out that seems like a good property to have and it's mostly linear in that range. So and the second derivative is maximum at x equal one. So the slope is changing fastest there, which is going to change the Deltas. And so one good thing about this is for example, if you're doing regression, it's everything's in the ": [
      2651.2,
      2685.2,
      53
    ],
    "out given the network like 80 numbers instead of $90,000. Why because the number of principal components is the maximum for sorry. the minimum of the dimensionality and the number of points 4/8 - 1 so imagine I have this huge and pixels but I only have two images. So here is pixel 1 years pixel 2 if I only have two images in that high dimensional space. I can ": [
      1594.5,
      1644.0,
      27
    ],
    "practically projection. You just have to divide by the length of one of the vectors each one of these guys has a weight Vector that corresponds to one of these directions. Right, but it won't perfectly be it'll just span the space. And then they won't have one guys doing all the work. They're they're not going to tone principal component analysis. There's an ordering to the principal components. The ": [
      2326.9,
      2368.7,
      45
    ],
    "principal component to what happens is they kind of equally because of the randomness of training and up relatively equally having similar variabilities. But they're for that number of numbers. They represent as much information as possible in a squared error sense, which is what PCA is trying to do with linear projections again. You're projecting the data under this axis. That's how you got to coordinate inner product is ": [
      2284.6,
      2326.9,
      44
    ],
    "really want that because eventually we want to learn on when your features. So then enter batch normalization with just came out like I don't know year or two ago at this point. Batch normalization uses all these ideas, but dynamically as the network is being trained. So after the after the activation of a of a hidden unit layer has been computed through forward propagation. Then batch normalization makes ": [
      3345.3,
      3383.9,
      73
    ],
    "red. I'm going to get the same Delta at the hidden Lair and then I can start to change my weight. So the Deltas are the headings are all going to be the same. But the inputs are going to be different. Right. So I'm going to diesel how I'll have the same Delta but the inputs are different and the weight changes according to the input X adult x ": [
      2923.8,
      2955.6,
      61
    ],
    "same amount then they're all equal in how much they affect the hidden units are connected to Okay. So, you know, I wouldn't if I was in Iran, I wouldn't want to be the first principal component. Going like that a lot. And this one's like Wall-E. Doesn't do much. right So, okay. And so I can throw away like this Dimension and I'll get a small amount of error. ": [
      2130.4,
      2171.1,
      40
    ],
    "scoring. The difference between PCA and see scoring is PCA finds the directions of Maximum variability first. And then you see score. And you can't use he's going to reduce dimensionality, but you can use PCA for that. Okay. Yep. You throw away some of the axes so back here when I had the Martians and I threw away the saxes. So, you know where this data? I've got basically ": [
      1762.2,
      1807.3,
      31
    ],
    "shoe size and I get all the data perfectly reconstructed covariance in their shoe size. So this PCA is it does two things or several things it whiten centers the data. So the mean is here to put in now, if you look at what the projections are the new coordinates of these guys, they're all positive and negative. And you have some measure of importance or the amount of ": [
      1520.3,
      1558.1,
      25
    ],
    "standard deviation their positive and negative and then we put it into the sigmoid and then what happens I put it into the logistic heading unit. Now what? It's all positive. Okay, so that's bad. right and okay, so we need a different activation function. So, you know back in the day. We all use logistic hidden units and we suffered through. But you don't have to do that. I've ": [
      2524.9,
      2573.4,
      50
    ],
    "start them out around 1 and 0 then you get z-score and variables. But their learnable so you can replace you know, you can change it to not be scored if you want to in fact. This transformation a lot. If you figure it out, you can actually invert the scoring completely if he'll earn the right gamma and beta. Okay. And the cool thing is well, look at this. ": [
      3520.6,
      3558.1,
      78
    ],
    "steps the velocity or momentum. Comes out to be one over 1 minus alpha x this this is 9. So this is 10. So you're essentially speeding up your learning by a factor of 10. Okay, it's a recurrent. It's a recurrence relation, which maybe you did in 21? so Just saw that recurrence relation. That's assuming that. This thing is always the same. So you're on a tilted plane? ": [
      4153.4,
      4195.3,
      95
    ],
    "suffered for your for you. Okay. So bipolar sigmoid would be better. Okay one that symmetric around 0 and so what you should see is if you have it use 10h. That's symmetric around 0 and now you'll send positive and negative inputs up to the next layer. Okay, but yeah, I'm Lagoon did some work and he came out with it came up with an even better idea which ": [
      2573.4,
      2612.9,
      51
    ],
    "that seems a little weird because I know I'm going to go that far but I'm going to compute the slope up here and add that to it. So I'll go that way a little bit but down there is going to be different than it is up here probably. Write him in a different part of the space. So I'm adding and taking this big Vector which is the ": [
      4402.7,
      4425.3,
      101
    ],
    "the directions of highest variance in the data? So if I move the access hear the variance of the data on this axis has highest and the variance on this axis is much smaller until I can neglect a taxes if I want to if I am willing to tolerate that much are and PCA for a linear system provably minimizes the squared error for the number of numbers that ": [
      1447.5,
      1479.9,
      23
    ],
    "the gradient you change the way it's according to the average roughly of the gradient overtime. It's an exponentially decaying average of the gradient. So that damps the oscillations from going this way and because I have a lot of little guys pointing me this way after while they add up and I go even faster in that direction, so that does exactly what I want. What's happening? Okay. That ": [
      3871.6,
      3906.5,
      88
    ],
    "the momentum rate is about 2.9 or something like that and this can be like .01 or whatever. And so you're taking a big chunk of the way, you've been moving and a little chunk of which way is downhill from here. Okay. And so this is the sum fraction of the previous weight change. Minus the current gradient if we want to keep going downhill in the gradient. That's ": [
      4074.7,
      4111.6,
      93
    ],
    "the outputs will have but they're going to have different targets. Right, so they're going to have different deltas. So the weights are going to change because there's actually errors up here and there is activations down here. So even though the weights are zero. You got a Delta up there and you have an input here so that the weights changed but they're all going to change into this ": [
      2863.4,
      2896.8,
      59
    ],
    "the remember I said 0 and 0 at 1 and went out - 1 in - 1 out. We want the weighted sum of the inputs to be in the linear range of the sigmoid because the gradients will be big cuz it's where the slope is biggest. And the network can learn whatever linear part of the mapping that it needs right away. So we want the AJ's to ": [
      3032.8,
      3057.4,
      64
    ],
    "the variance of w in this case. So the variance the standard deviation of AJ is the square root of the variance of w. which is the square root of the sum of the squares of the W's because these are all 0 mean right? So it's approximately just the sum of the squares. Okay, so we figured that out. So suppose the number of inputs to unit J is ": [
      3166.3,
      3204.1,
      68
    ],
    "them all 0 mean in unit standard deviation and then it's going to do the same thing the next layer up in the next layer up in the next layer. so it normalizes all the activation sin the network the inputs and each in unit throughout the network on a per-unit basis overreach mini batch. So this is the cliff notes version z-scores every variable. At every layer of the ": [
      3383.9,
      3416.0,
      74
    ],
    "then add set big factor to it. Okay, so that seems better, right? Yeah. What? Directions to make the big jump is the momentum term. It's the weighted sum of all your previous gradients. And then I do that and now I take these two and average them together this the momentum. This is the correction. And that's going to tell down a little bit from the green Vector because ": [
      4570.1,
      4612.2,
      106
    ],
    "they've disappeared in the weights are stuck in one of these Ravines then crank the momentum up. And you can learn at a rate that would that would oscillate without the momentum. You don't know. In fact recent more recent analysis has suggested that a lot of times were in a saddle point, which means you know, what a horse saddle looks like it's like this answer your like right ": [
      4286.8,
      4325.2,
      98
    ],
    "think about the case when the the error surface is like this. Okay, which way is straight downhill. It's that way. But where do you want to go that way? So unless the ellipse is a circle then all directions Point towards where you want to go. So it'd be great if we could make it more of a circle. So, it's big in the Direction Where We want to ": [
      630.8,
      668.3,
      7
    ],
    "this big job. Then I compute the gradient there. So in the end my weight changes is some of those too. Now I take a big jump which is like the average of those two. So it's shifted a little bit down compute the gradient there. That's my new gradient. On the other hand what momentum standard momentum does is it figures out which way the gradient is here and ": [
      4535.9,
      4570.1,
      105
    ],
    "this guy. So here's our initial learning writing. Again. Jeff uses Epsilon gij is the Adaptive guy. San Diego ": [
      4787.3,
      4801.4,
      112
    ],
    "this question later. Nobody's getting it. Okay. Community Chevrolet Okay. why why is it so stay with the unit J Okay, so What happens for this guy? It takes the Deltas propagated back from everybody else. It's some some up and multiplies * the slope and that's the Delta of this guy. Okay, so it's associated with the unit. Okay C Unit J Okay. All right. So what happens now? ": [
      981.5,
      1042.3,
      12
    ],
    "to axies this one you could call size. Right and this one you could call adiposity. You can look that up. So adiposity is fatness. So, you know, this guy over here is a little tall. Where is wait wait now, he's about right this guy is short and fat. Yeah. Okay, so this is height and that's wait. So the second is how heavy you are for your weight. ": [
      1807.3,
      1846.0,
      32
    ],
    "to the delta. so the waves to these guys will be the same the weights here will be the same cuz this input is the same in the input in the Deltas are the same in the weights to this guys are all the same. So in the end. Every hidden unit computes exactly the same feature posts not like having more than one hidden unit. Hey. So that's that's ": [
      2955.6,
      2988.6,
      62
    ],
    "to this hidden unit. Call it Jay and all the excise are positive. What's going to happen? In order to answer this. I'm going to ask you another clicker question. So during learning Delta J is associated with the input on the line. Wjjk the activation function of y j the unit J the weight JK, which is it. High entropy answers here You know what a high entropy distribution ": [
      706.6,
      764.6,
      9
    ],
    "two numbers. Height and weight are highly correlated. if I could we move the Oxys rotate these actually so the center is in the center of the data that the mean of the data and the other access is this way. This is called principal component 1 this is principal component to what principal components does. It's a linear technique and it linear algebra technique. These guys turn out to ": [
      1336.0,
      1372.9,
      20
    ],
    "very tiny variants. Like they also have size 13 shoes. Then you get all of the information with two numbers. Because the third number is redundant. but if you want to get rid of If you want to reduce the dimensionality I could. I could take a hit for this much error, and I'll be relatively happy. So what that does? Another way of thinking about what that does. Is ": [
      2398.7,
      2433.4,
      47
    ],
    "was small that consistent gradients. And slowly and directions with big but inconsistent gradients and what what do I mean by that to mean by consistent? Okay this way the gradient is shallow, but it's consistent. It keeps going that way this way. The gradient is high cuz it's a steep slope. But if I jump across there then it's going to be pointing in the opposite direction. So I'm ": [
      3769.4,
      3800.8,
      85
    ],
    "why it's negative. Okay. So if the air surface is a tilted plane. one way to think about this is what if the What if the gradient is always the same? And then you can kind of apply this formula and saw. So it turns out if the gradient is always the same. You can solve this and get this and what it does is this is at Infinity time ": [
      4111.6,
      4153.4,
      94
    ],
    "wondering if that has I think so. Your next assignment is going to be using a package. Ye that does the gradient for you no more back prop. And so it's got you know, one from column A to from column B and I like switch on nesteroff momentum switch on the Adam Optimizer. You don't have to worry about any of this stuff. Okay. in the remaining two minutes ": [
      4657.3,
      4691.4,
      108
    ],
    "x as one. So the standard deviation is one. So this says using funny can h0me and younes standard Aviation then puts then we should initialize the weights to be one over the square root of the Fanning. So assuming the inputs have been normalized. The unit 0 many units are deviation. The sigmoid is funny can age then the way should be drawn from a distribution with zero mean ": [
      3230.3,
      3265.8,
      70
    ],
    "you has the right answer. I'm sorry. Let me let me correct that. It's 25% chance. Okay. Are we ready to try again? Okay last time 16 if you got it, right. Let's try this time. Okay this time 14 of You Got a Friend. Peer instruction fails again. Okay. Wow. Okay Lots. alrighty Maybe the questions to ambiguous. All right. I'm going to close it out and throw away ": [
      802.5,
      981.5,
      11
    ],
    "you sent back. So for example, how could I minimizing squared error? It is like minimizing distance. This first principal component is that is the access that's closest all the data. and if the Martians all had like size 13 shoes, let's say so they're all out here and I could send just two numbers back this axis and x axis and the center would be a 13 for the ": [
      1479.9,
      1520.3,
      24
    ],
    "you use Xavier initialization that tells you. how to initialize the weights to have this be the case which we're going to talk about Xavier was the first name of one of Yoshua bengio students. Okay, so we want to initialize the weights also so that these properties are achieved at the next level up. So I said the great thing about this funny 10h. Is it c r o ": [
      2722.7,
      2754.0,
      55
    ],
    "you're a physicist and I think he's start out life. Maybe he's a physicist. So here's the gradient. That's what we've been Computing and we take the running average of the previous one and we want to go downhill in the air. So we subtract that off and that's our new weight change. sohvi, here is the Is how we change the ways? Sohvi becomes big Delta W, which is ": [
      3994.2,
      4030.1,
      91
    ],
    "your neighbor has the right answer. Okay. The hobbits not die down, but I think you've had enough time. Let's try again. Who? shoots up with the right answer are there still? 11 of you twelve and go 13 where did 80% come on Stick there? Okay. Okay going. going going John okay. I think we convinced maybe six of you. So what's the answer class? Yeah. Why is it ": [
      365.9,
      520.3,
      4
    ]
  },
  "File Name": "Deep Learning - C00 - Cottrell, Garrison W - Fall 2018-lecture_7.flac",
  "Full Transcript": "Listen to a podcast. Rihanna work Okay.  Let's go. Let's get started.  Okay. So last time we started talking about tricks of the trade and  we talked about stochastic gradient descent versus patch shuffling the examples.  And how wonderful that was and then perform PCA of the inputs.  But I serve it a little bit of introductory information here.  So the Earth's surface of a linear neuron.  With a horizontal axis for each weight and one vertical axis for the error.  For a linear neuron with squared error. It's a quadratic fall.  So vertical cross-sections are parabolas.  horizontal cross-sections are ellipses  and  Fur multi layer neural networks. The earth's surface is much more complicated but locally aguanta quadratic ball is usually a pretty good approximation.  So here's a quicker question.  so  This is in the case of of sum squared error.  Why is the Earth's surface of a linear Neuron a quadratic?  Is it because quadratics are good approximation to most curves is it because of some Square there? It has to be quite erratic or is it because cross entropy are leads to quadratic or is it because linear neurons you squared inputs?  hand  I seem okay. I've started it now.  I seem to have misplaced my clicker again and  Mario Advance ER okay  Okay, I was looking good there for a while.  I guess the  people are pretty sure answer first.  Answer is naughty. No, it's not.  Come on there, either it's not okay.  run  Okay, we're hovering around 68 70% I'm going to close it out going going going gone. Just spend a couple minutes talking to your neighbor about this. You have a 71% chance that your neighbor has the right answer.  Okay.  The hobbits not die down, but I think you've had enough time. Let's try again.  Who?  shoots up with the right answer  are there still?  11 of you twelve and go  13  where did 80% come on Stick there?  Okay.  Okay going.  going going  John okay. I think we convinced maybe six of you. So what's the answer class?  Yeah.  Why is it be?  So when you're on some Square there.  turn some  Knights  T minus y quantity squared, but why is in for a linear neuron w i x  20 squared so this is going to have a t squared term A- Duty some wi, there's going to be a lot of w, so, this is constant. That's constant. It's W's are the variables you're going to have a lot of W Squared terms.  Which is going to make it a quadratic.  That's the reason.  Okay, if you multiply this out.  Okay any questions?  For the 13th of you who thought it was 10 of you thought it was C.  and three of you thought it was a  okay.  already, OK Google  So what again this preliminary kind of data?  Before we get to the normal stuff the PCA stuff. So let's think about the case when the the error surface is like this. Okay, which way is straight downhill. It's that way.  But where do you want to go that way? So unless the ellipse is a circle then all directions Point towards where you want to go.  So it'd be great if we could make it more of a circle.  So, it's big in the Direction Where We want to go a small distance is small in the direction.  Where in which we want to travel a large distance?  And even for most popular nuts, the earth's surface is off and locally quadratic. So the same issues apply.  Okay, so that's that's the beginning.  Now the question is what's wrong with all positive influence.  What happens if all the incoming Waits 21 hidden unit say these are the inputs acts to this hidden unit.  Call it Jay and all the excise are positive. What's going to happen?  In order to answer this. I'm going to ask you another clicker question.  So during learning Delta J is associated with the input on the line. Wjjk the activation function of y j the unit J the weight JK, which is it.  High entropy answers here  You know what a high entropy distribution looks like right.  flat  There's a little peek at the right answer but most of it's flat. Okay, meaning you guys have no idea must be okay. I don't know if it'd be okay Delta J is associated with what when you compute it. Okay. I'm going to give you some time to talk about it.  Hey talk about it. You have a 34% chance that the person next to you has the right answer.  I'm sorry. Let me let me correct that. It's 25% chance.  Okay.  Are we ready to try again?  Okay last time 16 if you got it, right. Let's try this time.  Okay this time 14 of You Got a Friend.  Peer instruction fails again. Okay. Wow. Okay Lots.  alrighty  Maybe the questions to ambiguous.  All right. I'm going to close it out and throw away this question later.  Nobody's getting it. Okay.  Community Chevrolet  Okay.  why why is it so stay with the unit J Okay, so  What happens for this guy? It takes the Deltas propagated back from everybody else. It's some some up and multiplies * the slope and that's the Delta of this guy.  Okay, so it's associated with the unit.  Okay C Unit J  Okay.  All right. So what happens now? I'm going to ask again. What do you think? What happens when?  They're all positive inputs.  Remember, that's the way change rule.  Say that again.  What will always be positive?  Okay, so the weight change rule for w i j so here's  Iwi Jay, okay all those guys.  X  the input on that line in Delta J. So what's going to happen there all all the way to going to change which way?  War  negatively cuz Delta could be positive or negative.  So  Delta is the same sign for all of these weights. It's the one guy that you know, most place times those guys and changes these wait. So that means that either all the weights go up or all the way to go down.  Okay.  So, all right. That makes sense.  Because Delta is associated with the unit and the weight change for guys coming into the unit depend on the input on those lines and dealt with the inputs are all positive Delta. See they're going to be positive or negative. They're all going to change positively or I'll change negatively.  So  where can the weights go?  They could go this way or this way or this way or that way or that way or that way or that way.  They can't go that way.  Because so you can only change in this quadrant or that quadrant direction, right? You have to tack for those of you that know how to sail you're essentially going up when try to get here.  And so you're not allowed to change.  This way because that would be changing this way in a negative Direction and that one in a positive direction and you're not allowed to go this way because that would make this one positive in that one- Okay, so you can only change.  In this direction or that direction roughly, right?  Okay.  So that's bad.  That means the the weights.  Can't move an arbitrary directions specially if they want to get to that green.  Okay.  So this is why it's a good idea to have.  positive and negative inputs  So why PCA the inputs?  So I usually give a little.  demonstration here of what PCA does  so suppose  I'm up on Mars. I'm the Martian.  And I'm meeting all these martians up there.  And I'm recording their height and their weight and it turns out that height and weight are highly correlated with one another.  Bright, so this is the height and weight of xxyzz. Why?  Martians with mainly going to Mars spring vowels.  Okay, nevermind.  So that's that's what their names look like. And this it this Martian has this height and that way and I have to every time I went to The Story Goes every time I want to send a data back to Earth. I have to shoot off a little rocket.  And so I would like to send just one number instead of two numbers.  Height and weight are highly correlated.  if I could  we move the Oxys rotate these actually so the center is in the center of the data that the mean of the data and the other access is this way.  This is called principal component 1 this is principal component to what principal components does. It's a linear technique and it linear algebra technique.  These guys turn out to be the eigenvectors of the covariance Matrix of the data. Okay, and you don't have to know that it's just fun to say so now if I send back to Earth the coordinates of this new coordinate system.  I could just send them the coordinate along this axis. And here's xyzzy. He's now his coordinate on that new axis is right there.  And so this is the height of xyzzy.  And this is the weight of x y z z y.  But if I projecting onto this axis, I'll get a slightly different answer.  For his height and his weight and I get a small amount of error here.  And a small amount of error here and if I'm willing to tolerate that I can just send one number for everybody.  So what principal component analysis does is it finds the directions of highest variance in the data? So if I move the access hear the variance of the data on this axis has highest and the variance on this axis is much smaller until I can neglect a taxes if I want to if I am willing to tolerate that much are and PCA for a linear system provably minimizes the squared error for the number of numbers that you sent back.  So for example, how could I minimizing squared error? It is like minimizing distance. This first principal component is that is the access that's closest all the data.  and if the Martians all had  like size 13 shoes, let's say so they're all out here and I could send just two numbers back this axis and x axis and the center would be a 13 for the shoe size and I get all the data perfectly reconstructed covariance in their shoe size. So this PCA is it does two things or several things it whiten centers the data. So the mean is here to put in now, if you look at what the projections are the new coordinates of these guys, they're all positive and negative.  And you have some measure of importance or the amount of information along each axis. It's the variance variances Poor Man's information and not you know, it's like a cheap way to measure entropy. It's not quite entropy but close enough for government work so I can I can reduce the dimensionality of the data also. So in your programming assignment last one you gave it all the images, but if I did PCA of those images  You could have turned out given the network like 80 numbers instead of $90,000. Why because the number of principal components is the maximum for sorry.  the minimum of the dimensionality  and the number of points  4/8 - 1 so imagine I have this huge and pixels but I only have two images. So here is pixel 1 years pixel 2 if I only have two images in that high dimensional space. I can only have one principal component.  If I have three pixels, then I'll have two principal component. So if I have 80 images  I can only have 79 principal component and I'll convert all these images to 79 numbers preserving all of the information.  Yeah, I just can't dry 91000 dimensions.  Maybe you can anyway.  So this was really great back in the day when we had data sets for the 80 images. Now, we've got data sets with a gazillion images. And so the dimensionality maybe the the guy that you  use instead, okay.  alright, so  This is a picture you should have in your mind. Here's the data and instead of moving the accies. I'm going to move the data the same idea the first I put the origin at the center of the data.  And PCA is also called the car Herman and Luther's transform and now I got the ax he's lined up with the variance.  And that's what PCA does.  Stop here. That's PCA. But it turns out that the eigenvalue on the eigenvector is the variance along that Dimension and if you divided by the square root of the variance then  You squish the data down. So it's all equally variant. So it's like Z scoring.  The difference between PCA and see scoring is PCA finds the directions of Maximum variability first.  And then you see score.  And you can't use he's going to reduce dimensionality, but you can use PCA for that.  Okay.  Yep.  You throw away some of the axes so back here when I had the Martians and I threw away the saxes.  So, you know where this data?  I've got basically to axies this one you could call size. Right and this one you could call adiposity.  You can look that up. So adiposity is fatness. So, you know, this guy over here is a little tall.  Where is wait wait now, he's about right this guy is short and fat. Yeah. Okay, so this is height and that's wait. So the second is how heavy you are for your weight.  Yeah her size and she said yeah.  Oh.  Yeah, when you do this, so this data before I did that to it was highly correlated.  Right. Taller, you are the heavier you are.  Right. When I do this, I've got a new access size and adiposity and those aren't correlated with each other.  So a d correlates the data.  Which is pretty close to Independence. It's not Independence, but you know, it's like using variance for entropy. It's close enough. So  like  x and x squared are uncorrelated, but they're not independent.  What we would like is to have independent variables to represent the world. If we could because  A lot of pixels out there in the world are highly correlated with one another all these pixels are pretty correlated with one another I'll get to you and so if I but if you know if you have two variables to Binary variables, how many numbers do you have to have to fill in the joint probability table?  Give two binary variables. How many numbers do you need for the joint probability table?  45 what? What did you say?  PSR variable one variable to true-false true-false  4  No.  3  play some to one.  I only need three numbers then I can fill in the forest.  Okay, but if I have three variables, I need seven numbers five four variables. I need 15 with numbers. So if I wanted to represent the probability distribution of the world.  In my brain, which I would like to do because that will tell me what to expect right then.  I would need in my features of the world are not independent. Then I would need a brain the size of Manhattan in order to represent that probability distribution.  But if all my variables are independent, then I just need a linear number of numbers because I just multiply them together, right? So this is why you want to learn independent features of the world.  And I don't know if anybody's proved it or shown it or empirically tested it but generally deep networks will learn relatively independent features.  Yeah.  I'm sorry. What?  Oh, so why is this a good idea? Is that what you're asking?  Well suppose I have two inputs coming in. So now they're not positive anymore. Once you know, they very positively and negatively but one of them is going like this.  And the other one is going like this.  Then if this is important and I need the weights to get really small and if this is important, I need to waste to get really big to have the same effect. So the way I are going to have to take a long time for the weights to this one to get really small in the waist of this one to get really big.  So if I scale them if I whiten them so that each one varies about the same amount then they're all equal in how much they affect the hidden units are connected to  Okay.  So, you know, I wouldn't if I was in Iran, I wouldn't want to be the first principal component.  Going like that a lot. And this one's like Wall-E.  Doesn't do much.  right  So, okay.  And so I can throw away like this Dimension and I'll get a small amount of error. And so that's why dimensionality reduction and if you look at the if you take some real data that has correlations in it and you plot  The eigenvalues which tell you what the variances.  They usually go like this and so you can cut it off where you can explain 95% of the variance with you know, this many eigenvectors say that maintains most of the information.  right  Keith Haring  is a side note linear autoencoder essentially does PCA and I showed that a long time ago.  Maybe before your parents were born.  so  Actually, somebody else showed that I started empirically they said it. Baldy showed it, theoretically.  Okay, so if I'm doing Auto encoding that means what I'm doing is I'm taking some input. I'm running it through a narrow channel of hidden units in trying to reproduce it again on the output.  Okay, and because everybody's linear my objective function is squared error. So I'm trying to minimize the squared error or what is PCA do it provably minimizes the square there and what happens is that these guys don't end up being the principal components, but they spend the principal Subspace. So there's no one guy hero becomes principal component one and another guy that becomes principal component to what happens is they kind of equally because of the randomness of training and up relatively equally having similar variabilities.  But they're for that number of numbers. They represent as much information as possible in a squared error sense, which is what PCA is trying to do with linear projections again.  You're projecting the data under this axis. That's how you got to coordinate inner product is practically projection. You just have to divide by the length of one of the vectors each one of these guys has a weight Vector that corresponds to one of these directions.  Right, but it won't perfectly be it'll just span the space.  And then they won't have one guys doing all the work.  They're they're not going to tone principal component analysis. There's an ordering to the principal components. The first principal component is the one that captures most of the variance and it's basically just the line through the space that's closest to all the data.  And then the next Direction you find because of the rotation of the axis.  You're basically finding something that's at right angles to this and it's going to capture the next most variance. And if there is a third dimension here and it's very tiny variants. Like they also have size 13 shoes. Then you get all of the information with two numbers.  Because the third number is redundant.  but if you want to get rid of  If you want to reduce the dimensionality I could.  I could take a hit for this much error, and I'll be relatively happy. So what that does?  Another way of thinking about what that does. Is it it D noises the data to so you get rid of high frequency things that don't don't do much.  Okay. So wet last week when I show you those eigenfaces those were principal components of images.  Relatively. So yeah.  backprop custoza  Okay.  Okay change the sigmoid, okay.  So now we've got these nicely formatted data. We've got it zero mean unit standard deviation for all the variables.  That would be for your problem. It would be like taking every pixel through all the images.  And making a 0 mean in unit standard deviation.  Which is different than I think what I told you to do, which was take the images and make them 0 mean in the unit standard deviation.  So they're slightly different ideas. Okay. So now we get this nicely formatted data snow 0 mean it's unit standard deviation their positive and negative and then we put it into the sigmoid and then what happens  I put it into the logistic heading unit.  Now what?  It's all positive.  Okay, so that's bad.  right  and  okay, so we need a different activation function. So, you know back in the day. We all use logistic hidden units and we suffered through.  But you don't have to do that.  I've suffered for your for you.  Okay. So bipolar sigmoid would be better. Okay one that symmetric around 0 and so what you should see is if you have it use 10h.  That's symmetric around 0 and now you'll send positive and negative inputs up to the next layer.  Okay, but yeah, I'm Lagoon did some work and he came out with it came up with an even better idea which I call funny can age because it's so funny that it's 1.715 910h two-thirds X. That's not 2/3 x 2/3 x  So if ex has unit variance than the output will have unit variance turns out so the output is not formatted for the next layer up. So again, you get zero in you get zero out.  If you poop turns out if you put one in you got one out and if he put -1 and you get minus one out that seems like a good property to have and it's mostly linear in that range. So and the second derivative is maximum at x equal one. So the slope is changing fastest there, which is going to change the Deltas.  And so one good thing about this is for example, if you're doing regression, it's everything's in the linear range to start out with and it can learn the linear part of whatever it is. You're learning first and then is the weights grow. It'll get into the nonlinear regime and start to take care of things that require nonlinear representations.  Okay.  so  I noticed that brother units don't have all these properties and you need a different analysis for those in people have done that. That's why you use Xavier initialization that tells you.  how to initialize the weights to have this be the case  which we're going to talk about Xavier was the first name of one of Yoshua bengio students.  Okay, so we want to initialize the weights also so that these properties are achieved at the next level up. So I said the great thing about this funny 10h. Is it c r o n 0 out one in one out -1 in minus one out, but  That is if the weighted sum of the inputs, you know, that's nice if the weighted sum of the inputs is 0 mean and unit standard deviation, but I didn't do that when I did was make the input 0 mean and standard deviation not the weighted sum of the inputs.  and  so we need initializing to make these properties true and we need them to be random not zero.  And we don't want them to small or there won't be a gradient because we're propagating the Deltas back through these very small weights and if they're too large.  The teenage girl gets hit to the rails in the slope will be near zero and will have very small gradients.  But that's quick ask why the weights can't start at 0.  Okay.  So let's assume that we have logistic kidney in a tear for this demonstration. Okay, if all the way it started zero then these guys are going to have an activation of .5 if there is a logistic cert NH. They're just going to have activation 0 and nothing happens 45 out.  Okay, and the outputs if they were Logistics lb point five.  Also and the outputs will have but they're going to have different targets.  Right, so they're going to have different deltas.  So  the weights are going to change because there's actually errors up here and there is activations down here. So even though the weights are zero.  You got a Delta up there and you have an input here so that the weights changed but they're all going to change into this guy depending on what his Delta is cuz all the temperature the same they're all point five.  For this guy they're all going to change the same amount. And for this guy they're all going to change the same amount.  Okay.  and but what happens here I get  The black the green and the red I get the black and green the red I get the black the green the red. I'm going to get the same Delta at the hidden Lair and then I can start to change my weight.  So the Deltas are the headings are all going to be the same.  But the inputs are going to be different.  Right. So I'm going to diesel how I'll have the same Delta but the inputs are different and the weight changes according to the input X adult x to the delta.  so  the waves to these guys will be the same the weights here will be the same cuz this input is the same in the input in the Deltas are the same in the weights to this guys are all the same.  So in the end.  Every hidden unit computes exactly the same feature posts not like having more than one hidden unit.  Hey.  So that's that's fodder great fodder for a midterm question.  Okay. Okay. So any questions about that, so this was assuming that we're using logistic unit. So they had an activation even though the input wasn't was Europe what happens if they're tan H. The input is 0 the output of zero the weights never change nothing happens.  Okay, can you have no weights?  Be very lightweight Network.  Okay.  So again, we want the remember I said 0 and 0 at 1 and went out - 1 in - 1 out. We want the weighted sum of the inputs to be in the linear range of the sigmoid because the gradients will be big cuz it's where the slope is biggest.  And the network can learn whatever linear part of the mapping that it needs right away.  So we want the AJ's to be 0 mean in unit standard deviation with the recommended sigmoid the honey 10h.  So the weight initialization needs to be coordinated with the input normalization and the choice of sigmoid.  So we achieved zero mean and unit standard deviation with the inputs using PCA. We also want that to be true of the outputs of the first hidden layer.  And so AJ is the product of the inputs x 2 weight.  So assuming the inputs and the weights are independently given in the waiter initialize randomly. So they're going to be independent of the inputs and hiyori means expectation.  Send the variance of AJ, which is the variance of x x w if you just you know variance is X W Squared. So this is just a and identity and look this up on the web. Okay, its variants of actions various Square.  Okay, which turns into this because  x and double you were mean 0 so this is me and zero that's means your own. This is the mean squared.  So it's 0 sqrt of 0.  And X has unit standard deviation. So the variance of X is 1  Right, cuz we made it that way.  And so the variance of the weighted sum of the inputs is the variance of w in this case.  So the variance the standard deviation of AJ is the square root of the variance of w.  which is  the square root of the sum of the squares of the W's because these are all 0 mean right? So it's approximately just the sum of the squares.  Okay, so we figured that out.  So suppose the number of inputs to unit J is M. So we have M inputs coming in call that the Fannin.  If we set the standard deviation of the weights to be won over the fan in square root.  You got the standard deviation of AJ is the square root of the sum of 1 / M square root squared which is just one over n and there's end of them. So the sum of 1 over m m x as one.  So the standard deviation is one. So this says using funny can h0me and younes standard Aviation then puts then we should initialize the weights to be one over the square root of the Fanning.  So assuming the inputs have been normalized.  The unit 0 many units are deviation. The sigmoid is funny can age then the way should be drawn from a distribution with zero mean and standard deviation whenever is current event.  Okay.  So the point of this is that after normalizing the input by combining a particular weight initialization with the right sigmoid, we get normalized inputs the next layer up so 0 and 0 out standard deviation 1 in standard deviation one out.  And it makes some intuitive sense.  Get a room Wally has big fan in small change incoming weights can make it over shoot right fit as a lot of weight. So we want them to be very small.  And if so, we want smaller incoming weights if the fan is big and vice versa. So do what I say.  okay, but  Okay, it's all great. But then what happens we learn?  We change the weight.  So things will no longer be zero mean and unit standard deviation fact, we don't really want that because eventually we want to learn on when your features.  So then enter batch normalization with just came out like I don't know year or two ago at this point.  Batch normalization uses all these ideas, but dynamically as the network is being trained.  So after the  after the activation of a of a hidden unit layer has been computed through forward propagation.  Then batch normalization makes them all 0 mean in unit standard deviation and then it's going to do the same thing the next layer up in the next layer up in the next layer.  so  it normalizes all the activation sin the network the inputs and each in unit throughout the network on a per-unit basis overreach mini batch.  So this is the cliff notes version z-scores every variable.  At every layer of the network over the mini batch. So you run the mini batch into the first hidden lair.  Then you z-score those who this hitting unit has a bunch of different values over the input in the mini batch use z-score that naked 0 mean in unit standard deviation. Then you propagate it up do the same thing again, etc, etc.  And then well suppose that's not a good idea.  It adds on a little bit of stuff to give the network a chance to undo that.  Okay, cuz maybe that's not a great thing. Okay, so maybe it needs to be nunzi scored. So, okay. So you take every variable like this might be the activation of the third head unit in the first layer over the mini batch you compute the mean of that and divided by the standard deviation and had a little something to avoid / 0 just in case this gets really small and that's a z scoring the variable.  Okay, then.  Then what are you do you take that nicely z-score thing and you multiply it by this parameter and add that parameter. So these are learned by backpropagation as well.  And this is specific to this particular hidden unit as as this and if you start them out around 1 and 0 then you get z-score and variables.  But their learnable so you can replace you know, you can change it to not be scored if you want to in fact.  This transformation a lot. If you figure it out, you can actually invert the scoring completely if he'll earn the right gamma and beta.  Okay.  And the cool thing is well, look at this. You can actually these are smooth functions. You can differentiate these.  So and you can differentiate this so you can learn these guys.  Do gradient descent on these parameters?  Okay.  However, there has been some discussion on the web and I don't know if it's resolved yet. But some people have found that if you apply batch normalization to the input to the layer instead of the output of the layer it works better. Who knows.  So we're still in empirical mode with deep learning.  We don't we have some theories about deep learning and surprisingly one of them just assumes the whole network is linear.  You know, maybe you can figure you can get something out of assuming the whole network is linear because then you can just multiply all the weights together. You have a single layer Network.  Spell but it turns out if it keeps on coming keep getting more results out of that surprisingly.  Who knew?  And they seem to apply to real deep networks with I mean if he is relo your kind of linear, right?  Okay.  Okay, so now some slides from Jeff hinton's course.  About mini Bachelor ending ways to speed it up in the first ones going to be momentum.  So  Instead of using the gradient to change the weights. You keep a running average of the weight changes.  And use a second, so that's one thing.  And you can use separate adaptive learning rates for every parameter. So for every weight in your network, you can have a learning rate.  Dudududu and then you can slowly adjust those things the learning rates using the consistency of the gradient for that. Okay. So let's  and then Jeff hasn't had this rmsprop idea.  that normalizes learning right and we'll kind of get to that but  okay, so just to reminder we've got this.  quadratic Bowl that's usually a pretty good approximation and  the steepest descent here doesn't go to their which is where we want to go to the bottom of the bowl. The slope is going to be that way.  Here it is. So cool. Be good in here. It'll be good. But over here, it's going to go hoops.  Okay, and if we go too far  then what happens we we end up like increasing the air if we have too high of a learning rate.  Okay, so that's why it's good to lower. The learning rate generally is Hugo.  So what we want to do is move quickly and directions was small that consistent gradients.  And slowly and directions with big but inconsistent gradients and what what do I mean by that to mean by consistent? Okay this way the gradient is shallow, but it's consistent. It keeps going that way this way.  The gradient is high cuz it's a steep slope. But if I jump across there then it's going to be pointing in the opposite direction. So I'm going to go like this. Right and so those gradients have different signs when I jump across this funnel. I get a different I get a opposite sign gradient.  so the intuition behind the momentum method is  I go in the direction of the gradient, but then my next one which is going to be that way.  I add in some of this time averaging the gradient is a go and now I go that way the gradients that way but it's been pushing me this way. So I'll go a little bit that way and in the end.  Those oscillations will average out and I'll head straight down.  If I keep track of my previous gradients and average them in.  The momentum keeps it going in the in the previous Direction. So instead of changing the weights according to the gradient you change the way it's according to the average roughly of the gradient overtime. It's an exponentially decaying average of the gradient.  So that damps the oscillations from going this way and because I have a lot of little guys pointing me this way after while they add up and I go even faster in that direction, so that does exactly what I want.  What's happening? Okay.  That makes sense.  No, okay.  Not at all.  Yeah.  I keep track of a running average does a gradient so  if I started here and it's a positive gradient and then I end up over here. It's negative to positive and a negative is going to be smaller than both of them.  So I'm going to going to add less going this direction. If I keep a running average of the gradient going this way. It's going to get bigger and bigger say average in all these guys going the same way.  if if you've been getting opposite signs  So are you going to stop doing that?  And then I'm going to start doing that.  Yeah, yeah.  So here's the equation is right, so  you take the  hidden calls it the bossy cuz it's really the velocity not the momentum if you're a physicist and I think he's start out life. Maybe he's a physicist. So here's the gradient. That's what we've been Computing and we take the running average of the previous one and we want to go downhill in the air. So we subtract that off and that's our new weight change.  sohvi, here is the  Is how we change the ways?  Sohvi becomes big Delta W, which is how you change the weight.  Epsilon oh, that's a learning rate.  Yeah, sorry.  Yeah, I use different characters.  Alpha is the momentum rate.  so  the weight change is equal to the current average she take okay. So how fast should be less than 1  cuz you're taking some fraction of how much you've been moving and this has to this should be small Epsilon here should be small. So usually the momentum rate is about 2.9 or something like that and this can be like .01 or whatever.  And so you're taking a big chunk of the way, you've been moving and a little chunk of which way is downhill from here.  Okay.  And so this is the sum fraction of the previous weight change.  Minus the current gradient if we want to keep going downhill in the gradient. That's why it's negative.  Okay.  So if the air surface is a tilted plane.  one way to think about this is what if the  What if the gradient is always the same?  And then you can kind of apply this formula and saw.  So it turns out if the gradient is always the same.  You can solve this and get this and what it does is this is at Infinity time steps the velocity or momentum.  Comes out to be one over 1 minus alpha x this this is 9. So this is 10. So you're essentially speeding up your learning by a factor of 10.  Okay, it's a recurrent. It's a recurrence relation, which maybe you did in 21?  so  Just saw that recurrence relation. That's assuming that.  This thing is always the same.  So you're on a tilted plane? Okay.  So that's a kind of analysis you can do. It's not exactly correct, but it gives you an idea of the effect of momentum. Yeah.  Is the alphas constant usually?  Well, Jeff whole give you some different advice in a moment. Yeah.  Yeah, yeah, if you have small many batches then your gradient is going to be less consistent, right? Cuz a small mini batch is a bad sample of all your data, right? It's it's more going to be more idiosyncratic. And so the the direction of the grating is going to change War.  So this is Jeff's advice.  These are Jeff slides that I've made small changes to So He suggests starting at the beginning of Learning. There's going to be big gradients because you're beginning to learn and so start with a small momentum and once they've disappeared in the weights are stuck in one of these Ravines then crank the momentum up.  And you can learn at a rate that would that would oscillate without the momentum.  You don't know.  In fact recent more recent analysis has suggested that a lot of times were in a saddle point, which means you know, what a horse saddle looks like it's like this answer your like right here on top of the saddle on you.  Does that mean small your mother?  The less you care about your previous average. Yeah.  Yeah, exactly.  You got it. He's got it asked him if you have a question.  Right when the when the learning slows down.  Would it be that are less about the previous average because we've arrived at the correct rating.  Yeah, but we want to go faster now that we're on this shallow thing.  Okay, so that's momentum, but now forget everything I said because there's a better version of momentum called nesterov momentum.  So what the momentum does is say? Okay. I'm going this way. I want to go a lot that way. But before I do that, I'm going to figure out which way the slope is from here and add that to it.  Hey, that seems a little weird because I know I'm going to go that far but I'm going to compute the slope up here and add that to it. So I'll go that way a little bit but down there is going to be different than it is up here probably.  Write him in a different part of the space. So I'm adding and taking this big Vector which is the gradient and I'm adding a little Vector to it, which is the gradient right here. So this the running average of the gradient. This is my gradient at this point. So I'm making a little correction and then I'm taking a big jump wouldn't it be better?  To go that way and then compute which way is downhill from there to make a little correction.  That's the difference between regular momentum and nesteroff momentum.  So the standard method Starts Here, we know we're going to add that big momentum term in and it computes the gradient right here and adds it to it. That seems wrong instead. We should go there and then make a correction.  That makes sense.  You're not shaking your head. No this time.  Okay.  heliostats Guyver  thought of this  so you first make a big jump in the direction of the previously accumulated gradient the momentum term and then you measure the gradient where you end up.  And make a correction. So it's better to correct a mistake after you've made it to see idea instead of before e  So here's a picture of the nest Rock method.  I take a big jump because I'm using the momentum term which is a big factor to make this big job. Then I compute the gradient there. So in the end my weight changes is some of those too.  Now I take a big jump which is like the average of those two. So it's shifted a little bit down compute the gradient there. That's my new gradient.  On the other hand what momentum standard momentum does is it figures out which way the gradient is here and then add set big factor to it.  Okay, so that seems better, right?  Yeah.  What?  Directions to make the big jump is the momentum term. It's the weighted sum of all your previous gradients.  And then I do that and now I take these two and average them together this the momentum. This is the correction.  And that's going to tell down a little bit from the green Vector because I'm following this Direction with a little bit of that and it got a little bit that way that I make the correction. That's my new way chain.  Yeah, yeah, we're still doing momentum. It's just like when do we add in the gradient?  So it's really just a slight change in the code.  Yeah, well.  These days we just use the Adam Optimizer and forget about it, which probably wondering if that has I think so. Your next assignment is going to be using a package.  Ye that does the gradient for you no more back prop. And so it's got you know, one from column A to from column B and I like switch on nesteroff momentum switch on the Adam Optimizer. You don't have to worry about any of this stuff.  Okay.  in the remaining two minutes  adaptive learning rates. So the gradients are could be very small and it really layers and there's going to be a different fan into different units depending on you know, like in your guys last assignment you had like a huge fan in from the pixels and then a tiny fan in  Well, I'm at smaller fan in like 50 to the outputs, right?  Oh wait. No. That was the last time he didn't have hidden units last time nevermind.  This time you have hidden unit. So you got like nine hundred inputs to the hidden units. So that's a fan of 900 and then you got 50 hidden unit. So the fan into the outputs is 50.  Vs900 that's least an order of magnitude different.  So you want the gradients are the learning rate should be different for those cuz he make a small change to a large amount of weight is going to change your weight did some of your inputs much more than that. Same Small Change 250 weights.  All right, so that implies maybe we should have adaptive learning rates.  And then the remaining minute.  Which we start with a Global Learning right? And then we have a little multiplier on that called the gain.  That's this guy. So here's our initial learning writing. Again. Jeff uses Epsilon gij is the Adaptive guy.  San Diego "
}