{
  "Blurbs": {
    "1 pics out that one picks out that one picks out that And so that narrows it down to Penelope and Christopher. Which was you'll see that's enough to solve the problem. So it's going to pick out pairs of people. And other combinations of these units will pick out different parts of the tree that makes a lot of sense, I think. And then these are the relationship unit. ": [
      2827.4,
      2862.9,
      70
    ],
    "40 principal components which are kind of a a underlying like dimension for each face. So this is like how much of this slider here has how much of this is in that one? Her face is dark. This is light. So it's kind of stand up the bottom Etc sewed this face. Is a weighted some of these eigenfaces they're called. Okay, because they're the eigenvectors of the covariance ": [
      3097.8,
      3139.4,
      76
    ],
    "87. So it took a while to train and just had one hidden layer. But remember the thing about the bias it learns the bias on the outputs is what's going to learn first because it's always got a 1 on the other end of the line until what does it learn the way you can minimize the are the best is to learn the average phoneme in English, which ": [
      1604.1,
      1627.3,
      40
    ],
    "And tests that are goes down to zero very quickly in this case. And now it happens the internal representation. It's exactly the same for this guy. As well as that guy. Doesn't change. Until we hit a woman. But it's the same for all women. Let's see Universal women in the Universal Man. So, okay. So again, it's learned a representation in the service of the task here. It ": [
      3726.8,
      3777.3,
      91
    ],
    "And the other cool thing is it generalized has to recurrent Networks. Okay, so last time. We started talking about some of the things back prop has learned. And again, your Mantra is back propped learns representations in the service of the tasks. So when you're meditating, that's your Mantra, okay? So we talked about xor this simple Network that does X or we talked about. Symmetry this is more ": [
      1133.0,
      1174.8,
      28
    ],
    "Delta in the same minute. Well, it could be the same activation function to be a different one. But usually they're the same in in the hidden layers of teeth Network. Okay, so you just keep doing that. Now as I said in my last lecture in the first iteration of Russell, norvig the best AI book on the planet, which is known space. They got this wrong in that ": [
      812.2,
      854.4,
      20
    ],
    "Delta rule for the perceptron or logistic regression or softmax regression that part becomes the input on that line. And then by analogy this becomes the Delta were the negative of it does depending on Whose book you're reading you get it both ways sew-in in the Delta rule. We had T T minus y which turns out to be this and then we have the input on that line, ": [
      233.4,
      273.1,
      4
    ],
    "English that's supposed to turn on the cuffs on now the smart thing they did with it and then they shipped it over one produce the next next sat in the smart thing. They did was hook this up to a deck talk speech synthesizer so you could actually hear it. Which is what we're going to do. So this produces the sound in the context of the other letters ": [
      1368.6,
      1396.2,
      34
    ],
    "From this guy to that guy. Okay. Okay. And there is the weight. There they are. And for some reason I don't put G Prime there. And so, okay. Okay, so this through all that we get to this which is the Delta for any unit. That's not an output unit. This doesn't depend at all on how the Deltas for the output sir are computed. They could be computed ": [
      496.4,
      543.7,
      11
    ],
    "I can also. Word the identities instead as you guys did. And now it knows there's 10 outputs. We got 10 hidden units. So we can train it to do that. and now notice what's going on in the right hand side here. What's happening? they're very much very similar to one another because it's trying to learn his identity now, so it's not changing very much from one picture ": [
      3508.4,
      3560.9,
      86
    ],
    "I get that far today. I will be taught or Thursday. I'll be talkin about some some ideas for how you should initialize this weights. Okay, some tricks of the trade. So because you start out with initially random weights, and now the earth's surface is no longer of it, you know, depending on how many hidden layers you have. It can be some arbitrary landscape. And so if I ": [
      938.9,
      974.9,
      23
    ],
    "I was supposed to. have to load the PCA projections again, but there she is again. And here's somebody else. That's my that's a meow I should say. She was my PhD students. girlfriend now wife and so you can do these kind of poor man's more swear. You're just like this is like 40% of these pixels plus 60% of those pixels. So it's not a real morph. But ": [
      3282.0,
      3334.4,
      81
    ],
    "Matrix of the data. And so there are orthogonal meeting each one of these has an inner product of 0 and so face is a point in this 40 dimensional space and it's gotten this much of this one that much of that one that much of that one Etc. She can see the sliders very well. if I move these You know, it changes her face. Write in a ": [
      3139.4,
      3176.6,
      77
    ],
    "Maybe a little different than what you guys found discussed surprise cetera. k Do to do so that's that's kind of fun. and now So, okay, so I did emotions and we all did this so, it should look familiar, but it learned these representations in the service of the tasks. so that's what the network Thanks. This guy looks like in order to get surprised at right. Okay. But ": [
      3469.4,
      3508.4,
      85
    ],
    "Okay. and the next thing is PCA the input so we'll save that for next time cuz Okay. UC San Diego podcast for more visit podcast. Ucf.edu ": [
      4663.3,
      4802.9,
      116
    ],
    "Recording represents the first 5 minutes of learning with a network starting from zero waste. The second recording gives the performance of the network after 20 passes through a corpus of 500 Words. The third recording shows how the network generalizes to Fresh text. First Recording denovo learning I don't know. No, no, no, no, no, no, no. So remember what these that this is old right, this is from ": [
      1549.9,
      1604.1,
      39
    ],
    "So Okay, honey. I'm going to close this out. Okay, so all right, so that's learning internal representations in the service of the tasks. Okay. Are we all happy? I'm happy. Okay, any any questions about back crop learning internal representation, so they yeah. Yeah, so it's all of the Hidden units basically run the network backwards. So this input comes in it activates the hidden units different amounts and ": [
      3813.7,
      3869.8,
      93
    ],
    "So now I've got a Delta for these guys. And if there's guys below here, they're also neural network units and not the input then I propagate these back. And then if there's more I keep propagating. Those back so in a deep Network for propagating the Deltas a long way from the output back to nearly the input. And so this makes sense intuitively believe it or not. So ": [
      676.6,
      712.7,
      16
    ],
    "So what is this one in code? Gender turns on for Aunt niece sister and wife daughter mother. Okay, so so the gender is going to be picked out by that unit. So you don't need the gender here you just need Christopher and Penelope, and then you could say son and you'll get Arthur. Okay. Okay any questions? So the network learns his features in the service of the ": [
      2862.9,
      2907.5,
      71
    ],
    "Tell again these are this is like an axis and this is where on that access the saws basically project that face under this and you find out what its principal component is. Okay, so this so you can have fun with this, you know. There's a good that's somebody a happy guy. Okay anyway, and and then you can do kind of poor man's Morse. these These are Oops, ": [
      3241.4,
      3282.0,
      80
    ],
    "a great idea again if you have a lot of noise, but that's that's what we did and it seems like you would want to pick the verse examples. So maybe you want to have high entropy in your batch so that you get a lot of different in a lot of information for me to example, I don't know. I don't know who's done work on this, but you ": [
      4509.8,
      4534.3,
      111
    ],
    "a sense of classification problem because it has to turn on the right phone the correct phoneme at the output out of the 40 sofo names in English. So that's it. Yeah, and again, once they get that phone team, they slide this over get the next one sided over get the next one and that's called napping time into space to pay cuz I've laid it out. This is ": [
      1985.4,
      2010.1,
      48
    ],
    "a small way to him than I had less to do with it. I don't I'm not as worried or if he has a small air and I have a big way to him. I don't care as much but it changes my Delta a little bit if he has a small air and I have a small waist and I don't give a shit. Okay? Okay, so that makes ": [
      738.7,
      761.1,
      18
    ],
    "again. Here's all the letters of the alphabet plus space, etc. Etc. Etc. So time that is the passing of this text is mapped into this space. Okay, and then the the job of the network is to take the central letter here. Which is C and it's in this context it sounds like or and that's the outputs are the all the phone ER unit for every phoneme and ": [
      1331.2,
      1368.6,
      33
    ],
    "and Penelope and her corresponding Italian have negative weights into it. Okay. That's what a hen diagram shows you it's a way to visualize the weights. Two were heading unit. Any questions about what that showing? Hey, so we have six hidden units shown here. and what is unit one? That is the upper right one in coding. So here's the family trees and what? What feature is that one ": [
      2598.3,
      2636.0,
      65
    ],
    "and consonants and puts, you know hard consonants together. Yeah. This guy is hard to read but I think it's a stiiizy which is very infrequent. We don't have a lot of in a wizard sir bones. There's an acid the end, but it becomes a z. right Okay. Okay, fine. Hey questions. One more. Yeah. What the unit? Oh, this is distance in some space. So I guess you ": [
      2264.1,
      2319.0,
      57
    ],
    "answer? Okay, we did this one already. What was he answer? I think he just remembered that. I don't think you're red it. Okay. Okay. And again, this is wonderful because it learns internal representations and I don't have any clicker questions today so you can you can leave now. Okay, so it's wonderful because it learns internal representations and that's basically what we're going to talk about today some ": [
      1043.6,
      1088.8,
      26
    ],
    "around it. So I'm this contact see is cuh. But if this was perceive then it would be so right so that context is enough for many letters to get the right pronunciation, but it doesn't work for all cases. Yeah. I think it went to 7. His mine mine goes up to 11. You had to see this is spinal tap for that hits a mockumentary about a heavy ": [
      1396.2,
      1438.1,
      35
    ],
    "as strong. Now this says, okay. It's going to turn on for Christopher Andrew and Penelope and Christine. And turn way off for Colin. And way off for Charlotte, so it's height in the tree. 2 pics out those guys Okay. How about unit 6? What's unit 6 that's this guy? turns on for Chris Zimmer ideas children Christopher is a father grandfather actually. What? I'm sorry. I really am ": [
      2719.4,
      2786.9,
      68
    ],
    "average the hidden unit activations for that together. So I have one hidden unit Vector of activations for C to K. And I do that for you know O2 who and stuff like that. And then I do the cluster analysis and it tells me kind of how the network breaks up the world. And it breaks it up and do naturally things to a linguist would appreciate a vowels ": [
      2227.4,
      2264.1,
      56
    ],
    "be usually very efficient a GPU so When on Computing the gradient, I have to take these hundred and twenty-eight examples and go Funk with all of them at once with exactly the same weights in the network. And then I go backwards getting the Deltas and those all use the same weight cuz I haven't changed the weights yet. So I can do that in parallel date of parallelism ": [
      4333.2,
      4363.2,
      106
    ],
    "because you're trying to optimize a softmax or your got a logistic or their various other things we could be doing and all I need is the Delta for the if I'm the layer just before the outputs. I just need the Delta for those guys and I don't care what I'm optimizing. Except I need how that thing. I'm trying to optimize is how that changes as being put ": [
      543.7,
      579.0,
      12
    ],
    "can turn in my primary. My baby. He won't stop jumping around New England cricket match Live Match in the horse. It will just take two minutes to get us. We'd just about 12 minutes. If you'll just tell me why don't you get some ice on it, right? Where what does my baby? Don't get the garbage on color theory? Okay, so it's not great. But you know, we've ": [
      1872.9,
      1929.2,
      46
    ],
    "chapter in a book called tricks to the trade the name of the chapters efficient back prop and it's from a while ago. And so some of these tricks are a bit outdated at this point, but They're good. It's going to be good for you and they illustrate some points that later led to things like batch normalization. Which we're going to learn about later while later in this ": [
      3944.5,
      3980.3,
      96
    ],
    "comb for example, but they figured those out as well as they could. And so that's the training Corpus and then they turn it on this and then they turn the page and test it on the next page. okay, so this is what it sounds like when it's just learning by Terry sinofsky and Charles Rosenberg learning Corpus taken from Carteret and Jones informal speech first-rate transcription. The First ": [
      1503.7,
      1549.9,
      38
    ],
    "come a long way since 1987 or so and it's a six-year-old. So that's why the baby comes out of the crib and crawls in your bed and takes out your ear plugs when you're trying to sleep. Okay, any questions about that? Okay. Oh, there's one. Yeah. What? Yes. Yeah a classification problem. So again Here's the network. So again, you're you're the job of the network is in ": [
      1929.2,
      1985.4,
      47
    ],
    "complicated and we talked about Edition. So now we're going to have more more fun and talk about Natok. So this means I need to. iTunes and yeah. okay, most of its Rock but there is where is technical traditional now? Natok demonstration of network learning so that was Terry Sandusky talking but I could barely hear him. I hope so. I have to be quiet but let's come back ": [
      1174.8,
      1229.7,
      29
    ],
    "could try it look and see. Okay, I mean the main thing is you want to shuffle the examples and you know, if you have a big enough if you have 60,000 examples, you probably just need to shuffle once right, He just want to make sure you're not getting yours. The one one one one one one one one one and you got a hundred and twenty-eight ones in ": [
      4534.3,
      4561.3,
      112
    ],
    "days. I bet I can do gender with one hidden unit. Okay, so this is basically going to be a perceptron. Okay. Now it knows it's should have two outputs because we had two outputs we could have just one but we had to Okay, and boom it learned it really fast. I could probably also hold out some. Some guys here. Let's hold out say 10. See what happens. ": [
      3690.2,
      3724.9,
      90
    ],
    "different internal representations that backprop has learned. This reminds me I need this. Okay. Hope it works. Okay. And it's very efficient. And you're as you're going to see in your programming assignment checking the weight numerically is very expensive and Computing the derivative of the error with respect to the weight numerically is very Computationally intensive. Where is this does one seek forward One Sweep back and you're done. ": [
      1088.8,
      1132.0,
      27
    ],
    "do Cafe 40. That's better. Okay. So the first thing so just said demo of face processing we're going to train a neural network to recognize facial expressions and identities and gender. And the first thing we did though was something called principal components analysis, which is a way to reduce dimensionality. So these images are like a hundred by 130. and this particular data set is got the first ": [
      3055.7,
      3097.8,
      75
    ],
    "don't know what that blue line is. This is the test that it's all 0 cousins empty, but what you can do with us now is kind of do a gradient backwards from each hidden unit and see what the representation kind of get the internal representation of the face, so She is angry. And that's the internal representation of anger by the network. So this should look somewhat familiar. ": [
      3428.6,
      3468.1,
      84
    ],
    "example and Bash learning where you change the weights after seeing all the example instead you get a good size shot of examples say 128 to pick a random number and now I've got in that batch, you know, probably all the digits and 10 examples are 12 examples of each and so I've got a kind of Representative subset of the data and I'll get a pretty good estimate ": [
      4261.1,
      4293.4,
      104
    ],
    "example with the greatest error and trained on that. Then we'd pick the next example of the greatest Aaron train on those two examples. And so we just pick the examples with the biggest Terror. This would be a terrible thing to do if you have noise because she picked noisy examples, but we picked example set for Mackie glass. We know what the you know is deterministic so we ": [
      4450.9,
      4480.1,
      109
    ],
    "few zero all of them. You get the average face. In this state of set and then you can if you do that, then you can have fun kind of nerd find by trying to find that face find this space pray, you know. Well, I know this one's down and this one's up cuz I just saw it. So and then like how do I how do I set ": [
      3176.6,
      3204.5,
      78
    ],
    "fix representations of everybody because it doesn't have Any resources to reflect these changes in the input that really doesn't need to reflect anyway. and it's now I think you might have noticed before that when that guy had his mouth wide open. The representation was different. this guy and now it's not different because it doesn't have the resources to represent those differences and it doesn't need to to ": [
      3609.2,
      3647.2,
      88
    ],
    "for all the hidden units? One paragraph we take average overall team for is just a weighted sum of all of them. exactly Okay. Alright, so tricks of the trade in the remaining 15 minutes will start doing some of these so there is at the like V paper from the bottom of the resources page under readings is laocoon tricks of the trade. So this is from a a ": [
      3901.7,
      3944.5,
      95
    ],
    "from 25 examples plus or minus 2 and the idea of America glasses a Time series that models your heart beats. And so it's one of these chaotic time series and so chaotic time series are deterministic. But if you make a little change at one point, it makes a big change later on that's what chaos is and So we have the system where we would actually pick the ": [
      4415.7,
      4450.9,
      108
    ],
    "from this family tree. Okay. How about hitting unit 2? so here is in the unit to it corresponds to Christopher Andrew being on and Etc. What what do you think? That one's encoding. Wow, that was fast generation. It turns on for Christopher and Penelope notice. It's symmetric between the Italians and the British once I figured out who's who this one's kind of an Italian unit. Not quite ": [
      2675.3,
      2719.4,
      67
    ],
    "function, which could be softmax or the logistic or linear. If you're doing regression you want linear outputs, you're doing classification. You want logistic or softmax and then it was there any questions about that? I hope not at this point and then we started talking about gradient descent. Are we just plugged in played? so so the first thing we always do here is break this into the gradient ": [
      144.2,
      188.6,
      2
    ],
    "given the Deltas up there. They would use that to change these weights because all you need is the Delta and the input on that line. And then they would propagate the Deltas back and then they would change these weights and then they would propagate the Deltas back if that's wrong. Okay, because now I'm using different weights than I computed the gradient with. So I need to propagate ": [
      854.4,
      886.3,
      21
    ],
    "go back through here. You're going to have to go through a derivative of this guy to get to him. So that's why you can't get rid of the slope and the hidden units. and so I just replace that with Delta k That's the definition of delta. It's got a minus sign in front of it. and then how does the How does I'm doing a chain rule, so ": [
      429.0,
      462.7,
      9
    ],
    "going to get a hearing aid by the end of this quarter. But what? siblings in law Side of the family hits the side of the turrets which side of the aisle you sit on at the wedding. Right, you know this is once it's turning on for these guys and turning off for these guys. Yeah. Okay, so between those three features and their opposites which are also there. ": [
      2786.9,
      2826.4,
      69
    ],
    "have exactly and and so we had a lot of examples of yours the input, you know, maybe you take five or six points in the previous part of the time series and then you're trying to predict the next point. And so if you keep picking examples based on that and we could learn the whole thing from 25 examples. And so that was a you know, that's not ": [
      4480.1,
      4509.8,
      110
    ],
    "have to think about how it changes the guys that's connected to how they are changes compared to the for the guys that's connected to and how their input. How the air changes is there input changes and how their input changes as my outfit changes notice. This will be zero for anybody, you know upstream or whatever you want to call it of this unit cuz my activation my ": [
      324.3,
      356.3,
      6
    ],
    "hidden units above the relationship units here. So this is called a hidden diagram. After Jeff Hinton surprisingly enough along the top or all the British and the second row corresponds to the corresponding Italian. A square is white if it's positive black if it's negative. So these are the weights into this hidden unit. So Charles and its corresponding Italian have big waves into this hidden unit positive ones ": [
      2559.5,
      2598.3,
      64
    ],
    "if You have a big error, and I have a big weight to you that I probably had something to do with it, and I need to change what I'm doing. So I'll get a big Delta. My Delta will be increased a lot if I have a big weight to the next guy up and he has a big Delta. If he has a big Delta and I have ": [
      712.7,
      738.7,
      17
    ],
    "in coding? Yeah, so this is the British unit it turns on if they're British. That's a useful feature. That means you only want to turn on the British at the output because British and Italian Stone intermarry in the story. Okay, so that's the the nationality unit or the British unit. So it picks out this family tree. Hey, which is useful because the answer is going to come ": [
      2636.0,
      2675.3,
      66
    ],
    "intuitive sense. Does that make intuitive sense to you? Okay. If this guy is a hidden unit. So what do you want to know about him if he's a hidden unit? Yeah, okay. So it's it's exactly the same as this. Except you know, I've got J's up here instead of case. I mean Yeah, so the Delta here will be most five times this and added into this guy's ": [
      761.1,
      812.2,
      19
    ],
    "is just T minus y if James and output unit, we have the right combination of objective function and activation function that doesn't have to be team run this way as we saw if you use the wrong objective function squared error with logistic output to get a slope term in here, which is too bad. But later we figured out we should talk to my something else. And then ": [
      615.0,
      641.8,
      14
    ],
    "is okay. Oh, he's wrong. That's the difference between the person who actually did the work Charlie Rosenberg and the person who supervised to work and got their name as first author. Terry Terry is at the Salk Institute here, by the way, and he's also got associated with a bunch of departments on campus. Yeah. Why are you look at the error? Right and you you have the right ": [
      1627.3,
      1672.0,
      41
    ],
    "is over now, right but Anyway, okay. Right on Brothers & Sisters of the Revolution, okay. And you can use this for very large datasets that you can't keep in memory cuz it's online essentially. Why bachelor in the mathematicians over in the math department that do the Sioux function optimization have a very good understanding of how these things converge when you take the true gradient. There are lots ": [
      4142.7,
      4191.4,
      101
    ],
    "it's it's a little bit fun. Okay. Sorry. That was the fun part. Okay. so now we're going to do back prop and and I'm going to use I guess the same data set. So I just read in the 40 principal components for all the images in this little data set. Wait this the days that you guys had right? Yeah, okay, so it's got it figured out. I ": [
      3334.4,
      3380.9,
      82
    ],
    "just a bowl anymore. Now, we've got local Minima because the error surface is no longer just a bowl. It's it's all weirdly shaped because of these hidden units. and another thing is if the training examples start to change like you guys have new things to say compared to what I used to say write and say cool far out and you guys say something else like having SEC ": [
      4102.2,
      4142.7,
      100
    ],
    "just has to representations 1/4 male to 1/4 female. Okay. All right. I knew questions comments funny stories. So why is why can I do this for their perceptron? I mean, how do I know that from what I just did I had one hidden unit, right? So it's just going to have positive connection to one of the outputs in the negative connection to the other. And that's it. ": [
      3777.3,
      3813.7,
      92
    ],
    "know I've got 40 input units got any patterns sounds familiar and 10 hidden units now and now I'm going to load the targets and then go for Cafe I have to go there and there's list expressions. and got 10 hidden units now I can train it is you and I don't know what the blue I didn't. I guess. I have zero validations and zero test so I ": [
      3380.9,
      3428.6,
      83
    ],
    "know probably normalized in some way. So things up here far apart things down here really close. So here is B2B and P2P for example? BNP are very similar to one another the only difference between them is voicing. So if you hold your throat and you go bbbbbbb. try b b b b b b b a p p p p p the only difference is your voice and ": [
      2319.0,
      2359.9,
      58
    ],
    "listening to a podcast Okay, let's get started. Just a reminder. Yo, just a reminder of where we got to last time we were talking about. Multiple layer neural networks. And how which is what your programming assignment is about and how you give it an input from you got an output and then you back propagate the error through the network and then update the weights. And I briefly ": [
      1.9,
      102.6,
      0
    ],
    "metal band's called spinal tap and their drummer always gets electrocuted or whatever and every concert but the guy has a nap and he says they're really dumb guys and that guy has a nap and he said most people stamps just go up to 10, but mine goes up to 11. Okay, so, okay. So anyway, so Terry and Charlie Rosenberg got this book that has a transcript of ": [
      1438.1,
      1475.4,
      36
    ],
    "might be an error in the training set the mislabeled example, and that's why it's hard that crap is really good at finding mislabeled examples in the training set by putting a lot of hair on them. Maybe you need to learn something about the easy examples before he learned the hard examples, but it can improve performance on infrequent examples like and now we get back to that talk. ": [
      4633.7,
      4663.3,
      115
    ],
    "net input doesn't affect anybody else except people. I'm directly connected to okay, and the cool thing about this is if we Define delta is The derivative of Jay with respect to the net inputs of the units and this is just Delta for all those guys. I'm connected to Sweetie, find it that way. And again, this is the picture to have in your mind. Here. I am I'm ": [
      356.3,
      394.3,
      7
    ],
    "not as powerful as your phone now and business weeks at at learn the abilities of a six-year-old child overnights are there was a lot of hype but this is the only neural-net so far that's been on the Today Show twice. Cuz Terry knew somebody who was a producer on that and what it does is map time into space. So let's let's see what I mean by that ": [
      1300.4,
      1331.2,
      32
    ],
    "of good optimization techniques conjugate gradient bfgs, etcetera. These taken to conjugate gradient is kind of a one and a half order technique bfgs is a second-order technique and what I mean by that The gradient is first-order. It's like which way is downhill. Conjugate sorry bfgs takes into account the curvature around the slope so that the curvature is second-order. And so these things can find the minimum very ": [
      4191.4,
      4228.0,
      102
    ],
    "of which way is downhill. It's still called stochastic gradient descent because you're not going down the true gradient depending on which examples you picked. You'll go a little bit different directions down hill. Okay. So so using a mini batches, what is done by everyone now? and again, you take these examples and you change the weights once and then you get another batch of 128 say this can ": [
      4293.4,
      4333.2,
      105
    ],
    "one is the British the bottom one is the Italians. Okay. Is there a request did you if you ask me already? Okay, here's the network. It was a deep Network took a very long time to train but the idea was here. You had one hot encoding of each person. It over here you had relationships so you can turn on Emilio father and it should turn on at ": [
      2454.1,
      2489.2,
      61
    ],
    "one of these inputs a zero. So there's no similarity structure in the input. It has to learn it. Learn a representation of their features that solves the task. Okay. And so that's it's learning representation in the service of the tasks. So what we're going to look at next is 6 hidden units that sat above these 24 people down here and later. We're going to look at 6 ": [
      2525.9,
      2559.5,
      63
    ],
    "organizes that internal representational space That makes sense. Yeah, is that a question? No. What's our Mantra back propped learns representations in the service of the tasks? So this is not the end, but it's not the outdoor. It's the hidden Lair where the representations are. And what we did was we took all went through all the Corpus took all the instances where she was going to K. And ": [
      2189.1,
      2227.4,
      55
    ],
    "other sea going to cuz you take the activation of the Hidden units in the average all those together. So there's just a finite number of these mappings from letters to sound right? That's what we do when we read aloud from text. And so you get you know, Kay different ones of these three have 70 say activations a vector of length 70 for every input output mapping and ": [
      2070.9,
      2103.5,
      51
    ],
    "overnight there sometime when I go to my go to my cousin I get updates Off Taylor play that mean turn on all that sounds like your tears and doubly bed. This was you know our first shot at it and it was it was you know years ago anyway for my undergrads my freshman that I would say, it was a religious experience listening to this. Okay, so now ": [
      1801.0,
      1837.8,
      44
    ],
    "phoneme and you have the you can also look at the percent, correct? It's that simple. There's nothing more fancy that goes on the speech like nothing more fancy. Okay, you look disappointed. Okay, so it goes on like that for a little while? No, no, no, no, no, no, no, no, no, no, no tears and EE. Okay. Okay. synonym for win and Emmett Okay second recording after 10,000 ": [
      1672.0,
      1762.4,
      42
    ],
    "proceedings of the cognitive science Society in 1986 big thick book. And this is the first paper in it. These are two family trees. They're isomorphic. So everything up here first find something down here Roberto married Maria and had Emilio and Luchia Emilio Mary Jean Elysium. Married Marco, etcetera Okay, so Every every every family has a boy and a girl in this weird family tree. So the top ": [
      2412.3,
      2454.1,
      60
    ],
    "quickly. They tend to get very large weights, but you have to go through the whole training set which is Impractical like Facebook has billions of pictures of faces. It would be impractical to do that on those. Okay. So why many bachelor in which is part of your homework, you're going to be doing many batches. So what's a compromise between online learning where you change the wait forever ": [
      4228.0,
      4261.1,
      103
    ],
    "redundancies in the training set. It's not completely true. Now that we have imagenet for example there a wide variety of Scottish deerhounds in the training set, but there's still some redundancy their attempts to get better Solutions because of the stochastic City you're changing weights based on some examples and your you can get around some of these local Minima that way to remember we have we don't have ": [
      4066.8,
      4102.2,
      99
    ],
    "see from where you are but really easy for me to see if it it divides the world into vowels and consonants. And the hard consonants are in some places in the soft consonants and others and over here. There's a e i o u and sometimes y and so you get a similarity structure over the inputs or over the mappings in this case. how the network kind of ": [
      2156.0,
      2189.1,
      54
    ],
    "seems like a huge waste of time to go through 60000 examples and add up all the way changes and after you've process 60,000 example you change the weights once what a waste because by the time you process say 10000 examples, you probably almost already solved the problem because you've seen a thousand examples of every category by then right then so Real world data sets tend to have ": [
      4027.6,
      4066.8,
      98
    ],
    "solve the problem. So what did it do? Yes. learned representations in the service of the tasks Okay. Any questions about that? So there's a little bit of mathemagic going on here where I get this internal representation from the five hidden units, but other than that, okay. Now I'm going to try. gender and I apologize to the non-binary among you but there we only had two in those ": [
      3647.2,
      3690.2,
      89
    ],
    "start over here and there's a minimum over there. I might get to that one by start over here and there's a minimum over here. I might get to that one. So depending on where you start you'll get different answers. And different local minimum and so this is one solution. If you set the initial weights, just right it'll find this excellent at work with where one does or ": [
      974.9,
      1005.8,
      24
    ],
    "t t + 1 t + 2 t - 20 - 2 etcetera instead of for example having the network State be recurrent and change with inputs over time. We'll talk more about that. So one of the cool things they did and Steve Hansen is still pissed off that they didn't reference him for it. Cuz he gave him the idea is they took how do you look at ": [
      2010.1,
      2043.1,
      49
    ],
    "take the next to closest ones and do the same thing in the same thing. And eventually I get a binder I get down to one guy. That's the average of all of them and that's the root of the tree. So you create a binary tree this way and things down here are closer together than things up here and what you see basic it's hard for you to ": [
      2131.2,
      2156.0,
      53
    ],
    "talk. Okay, so these are the tricks. this particular chapter comes down on the side of stochastic gradient descent vs. Batch So this tends to learn faster because of redundancy in the training set. So Mnist, what you're working on now has 60,000 training examples of 10 digits. So you're going to see every digit lots and lots of times before you get through the whole training set. so it ": [
      3980.3,
      4027.6,
      97
    ],
    "talked about forward propagation. Cuz sometimes that confuses people. I don't know. I guess I've forgotten are confusing things can be and in this example, we had a some nonlinear softmax that are non-linear activation function. That can be whatever we wanted to be as long as you can take a derivative of it. And then at the output level we had some other nonlinear activation function or linear activation ": [
      102.6,
      144.2,
      1
    ],
    "task and learn some on its own. We didn't have to think those up the network figured in there and show that's useful if we don't know what they ought to be like. When I trained in that work to recognize facial expressions, I didn't know what features it should learn it had to figure it out. Which we're going to see next. so I'm going to give a demo ": [
      2907.5,
      2933.6,
      72
    ],
    "that's not what I want. Hello, okay. All right. Let's quit on a Matlab and start over is currently the screens been screwed up by somebody else. Okay. Sorry. Computer, okay. Define my desktop there is. Jay Cooke by the way is like 80 11 years old oldies lives on my street and he owned a car that Grace Hopper Road in But she recently sold. Austin-Healey Okay. Okay, let's ": [
      2988.3,
      3055.7,
      74
    ],
    "the Deltas all the way back to however far they go and then change the weight. Okay. any questions Okay. So again one thing that's different about this than your previous assignments is now I need random initial way. It's not zero weights and we'll talk a little later about why 0/8 doesn't work. We need Randomness to break up some symmetry problem, but And in the next lecture assuming ": [
      886.3,
      938.9,
      22
    ],
    "the input on this line and here's everybody connected to and the question is what that breaks down to his. How does the error change? Okay. I want to know how the air changes is my temperature changes. That's the son of how the air changes is all these guys. Input changes and how all these guys input changes as my input change. Okay, and you can see when you ": [
      394.3,
      429.0,
      8
    ],
    "the output. The one person is Emilio's father. Okay, and the thing that the idea that hidden wanted to explain motivate here was these are localist. We call them one hot encoding and then it would learn a distributed representation up here of different features of these people and it learned it all through me and there's there's no similarities between any of these guys the inner product of any ": [
      2489.2,
      2525.9,
      62
    ],
    "then use that amount to multiply X the weights and had that up to get an image out of it. So it's like each unit is going to have some features. It's responding to and if it's .1 you multiply 1 * awaits get pixels out of that and then welcome principal components with you then. and turn into one of these things and then can you add those up ": [
      3869.8,
      3901.7,
      94
    ],
    "then what you do a hierarchical clustering analysis, so who knows what that is? Good, you're going to learn something today, you know where he's so you take the two closest ones usually in euclidean distance and you average them together and you draw a little bit of tree. So you put that one there in that one there and you put a little line between him and now I ": [
      2103.5,
      2131.2,
      52
    ],
    "these to get her? You know? So anyway, the point is these numbers that is where on the slider you are can there be input to my network? And I've only got 40 inputs instead of 100 x 130 and you found out in your programming assignment how bad that could be right you have 90,000 some pixel inputs with PCA. I could have given you much smaller dimensional inputs. ": [
      3204.5,
      3240.6,
      79
    ],
    "this and figure out what it's doing? I mean, you've got like 70 hidden units. I think I don't remember how many they had. And it's not and you can look at the weights and try and figure out what it's doing. But another idea is to take the activations. All these hidden units for every input output mapping like this is C going to cook and if there are ": [
      2043.1,
      2070.9,
      50
    ],
    "this is always the Delta for the hidden unit. So people on Piazza ask this question like why is it always that well, that's what it is. I don't know what else to tell you. That's the way the math comes out. And so this is a recursive definition of delta you computer that the output units and then you propagated backwards through the network and you keep doing that. ": [
      641.8,
      676.6,
      15
    ],
    "this is downloadable from my website I showed you this one before I think right we're just learns to separate these two things. You saw that one and when I had their learning rate too high, the green line went away and Etc that sound familiar. Hey buddy, Knox Versi. That's okay. So let's go here. and so the first thing we did let's use California facial expressions. Oh wait, ": [
      2933.6,
      2988.3,
      73
    ],
    "to figure out that one I'm figuring out how this changes as this changes and then how this changes. Is that changes. using the chain rule So there's using the chain rule for the Zack. and this guy is how the output of this unit changes has its input changes. So that's just the slope of that unit. And this is going to turn out to be just the weight. ": [
      462.7,
      495.5,
      10
    ],
    "to that momentarily. So what is Natok Natok a neural network that learns to read aloud from text now? It doesn't really read from texts. What we do is we have localist inputs. That is one year one hot encoding of a through z and space and probably some other punctuation and to have the network read a cat you would have, you know, like 27 units here with a ": [
      1229.7,
      1267.5,
      30
    ],
    "to that out put me in a changes. So if this is the next layer up as an output, this is just T minus y usually and then X to wait I'm connected to them by. Okay. And let me see if I've got. And so that the final learning will for the hidden units. Well the learning room for everybody. Is that right? But for the output units Delta ": [
      579.0,
      615.0,
      13
    ],
    "to the next until You got this somebody else. Okay. You guys didn't do this. I don't think of Step through these bed. This is it's got hidden unit. So it's the hidden unit representation. We can try and do this with fewer heading units. So I just reduced them to 5 and still does really well and now they will with fewer resources the network will learn even more. ": [
      3560.9,
      3607.1,
      87
    ],
    "training words. Black and blonde or something when we walk home from school. I want them with my friends and sometimes we can't run down from school. Now. I'm still on the run. She gets the recalls on stuff very well and she get sick. That's why we can't run. I like to go to my grandmother's will it cause she gave us will be there sometime. Sometimes we sleep ": [
      1762.4,
      1801.0,
      43
    ],
    "turn that bias down in the to buy a ShopRite and just need to learn anything. So you really need different examples in sequences. Sorry, so you really want examples from different classes in your mini batch and one possible heuristic is as I just mentioned pick ones with more air or more often but used with caution because there's a lot of reasons why that example might be hard ": [
      4600.5,
      4633.7,
      114
    ],
    "turned on. 27 units here with blank turned on 27 units here with C turned on a and t okay, so it's not like actually looking at the image. We're just giving it an input where it's already seen the the input and this I learned overnight on a vax which is an old super mini supercomputer. We used to call it. We were happy to have it, It's probably ": [
      1267.5,
      1300.4,
      31
    ],
    "we're going to turn the page. And listen to it. Reading the next page. I'm not going to show you that page right away and see if you can figure out what it's saying and then I'll show it to you after a few words. In the third recording the previous network is tested on a new Corpus, which is a continuation of the training Corpus. There's just something I ": [
      1837.8,
      1872.9,
      45
    ],
    "what some six-year-old child is saying. And on the page, you can't see your opposite. This is transcription by linguist into phonemes. I guess that's what linguists do for fun. And I don't know but that was a training set. So they had to go to the words and the sounds and there are some alignment issues cuz they're a lot of you know silent letters like a yacht or ": [
      1475.4,
      1503.7,
      37
    ],
    "where I can do that as a big Matrix of here's some inputs pump I get, you know, 128 * 10 outputs right for each example and I can do my back prop using Matrix X so I don't know if what's the latest on that back in the day Mark blue towsky my PhD student in Hell white and I had a couple of papers on learning Maki glass ": [
      4363.2,
      4415.7,
      107
    ],
    "which is three out of the four 4X door and this guy catches the case where Xor is different from or that is when both inputs are on and turns off the output and just sat case. Okay. But because you have different initial random weights, you can get lots of different solutions this problem some of which might surprise you. Okay. and we did this already. What was the ": [
      1005.8,
      1043.6,
      25
    ],
    "which turns out to be that. okay, and so from there we we drive back prop. So that that one is just CI. Thank you those either cut that. and then we have that other one. okay, and okay. So this is the derivation of backdrop for the hidden units. So we're trying to compute what Delta is and for some arbitrary unit in the middle of the network. We ": [
      273.1,
      324.3,
      5
    ],
    "with respect to the weights break that into the gradient with respect to the activation of unit J. So this the weight from I to Jay and then how the activation input waited some of the inputs to Jay change as the weight changes and this just turns into what Okay, how many people make W how many people think ziti? It's the input on that line. Just like the ": [
      188.6,
      233.4,
      3
    ],
    "you're vibrating your phone before you open your mouth and with Pierre not. So they're very similar sounds. Yeah those over here and consonants over here. Yeah, and there is like soft consonants like hard consonants like sure where so you don't release are you're just vibrating and but with the big stunt glottal stop there. Okay, next example sentence family trees. So this was the first paper in the ": [
      2359.9,
      2412.3,
      59
    ],
    "your mini batch. What's going to happen? What's it going to do? It's going to be vs120. Sorry. Not exactly. I mean, it's telling you selling Network turn on the one unit. Turn on the one unit. Turn on the Atkins solve that very quickly by cranking up the bias on the one output. Not learn anything about the input then supposed to get all too. So I was just ": [
      4561.3,
      4600.5,
      113
    ]
  },
  "File Name": "Deep Learning - C00 - Cottrell, Garrison W - Fall 2018-lecture_6.flac",
  "Full Transcript": "listening to a podcast Okay, let's get started.  Just a reminder.  Yo, just a reminder of where we got to last time we were talking about.  Multiple layer neural networks. And how which is what your programming assignment is about and how you give it an input from you got an output and then you back propagate the error through the network and then update the weights.  And I briefly talked about forward propagation. Cuz sometimes that confuses people. I don't know.  I guess I've  forgotten are confusing things can be  and in this example, we had a  some nonlinear softmax that are non-linear activation function.  That can be whatever we wanted to be as long as you can take a derivative of it. And then at the output level we had some other nonlinear activation function or linear activation function, which could be softmax or the logistic or linear. If you're doing regression you want linear outputs, you're doing classification. You want logistic or softmax and then it was there any questions about that?  I hope not at this point and then we started talking about gradient descent.  Are we just plugged in played?  so  so the first thing we always do here is break this into the gradient with respect to the weights break that into the gradient with respect to the activation of unit J. So this the weight from I to Jay and then how the activation input waited some of the inputs to Jay change as the weight changes and this just turns into what  Okay, how many people make W how many people think ziti? It's the input on that line. Just like the Delta rule for the perceptron or logistic regression or softmax regression that part becomes the input on that line. And then by analogy this becomes the Delta were the negative of it does depending on Whose book you're reading you get it both ways sew-in in the Delta rule.  We had T T minus y which turns out to be this and then we have the input on that line, which turns out to be that.  okay, and so from there we  we drive back prop.  So that that one is just CI.  Thank you those either cut that.  and then  we have that other one.  okay, and  okay.  So this is the derivation of backdrop for the hidden units. So we're trying to compute what Delta is and for some arbitrary unit in the middle of the network.  We have to think about how it changes the guys that's connected to how they are changes compared to the for the guys that's connected to and how their input. How the air changes is there input changes and how their input changes as my outfit changes notice. This will be zero for anybody, you know upstream or whatever you want to call it of this unit cuz my activation my net input doesn't affect anybody else except people. I'm directly connected to  okay, and the cool thing about this is if we Define delta is  The derivative of Jay with respect to the net inputs of the units and this is just Delta for all those guys. I'm connected to  Sweetie, find it that way.  And again, this is the picture to have in your mind. Here. I am I'm the input on this line and here's everybody connected to and the question is what that breaks down to his. How does the error change? Okay. I want to know how the air changes is my temperature changes.  That's the son of how the air changes is all these guys.  Input changes and how all these guys input changes as my input change.  Okay, and you can see when you go back through here. You're going to have to go through a derivative of this guy to get to him.  So that's why you can't get rid of the slope and the hidden units.  and so I just replace that with Delta k  That's the definition of delta.  It's got a minus sign in front of it.  and then  how does the  How does I'm doing a chain rule, so to figure out that one I'm figuring out how this changes as this changes and then how this changes. Is that changes.  using the chain rule  So there's using the chain rule for the Zack.  and  this guy is how the output of this unit changes has its input changes. So that's just the slope of that unit.  And this is going to turn out to be just the weight.  From this guy to that guy.  Okay.  Okay.  And there is the weight.  There they are.  And for some reason I don't put G Prime there.  And so, okay.  Okay, so this through all that we get to this which is the Delta for any unit. That's not an output unit.  This doesn't depend at all on how the Deltas for the output sir are computed. They could be computed because you're trying to optimize a softmax or your got a logistic or their various other things we could be doing and all I need is the Delta for the if I'm the layer just before the outputs. I just need the Delta for those guys and I don't care what I'm optimizing.  Except I need how that thing. I'm trying to optimize is how that changes as being put to that out put me in a changes. So if this is the next layer up as an output, this is just T minus y usually and then X to wait I'm connected to them by.  Okay.  And let me see if I've got.  And so that the final learning will for the hidden units.  Well the learning room for everybody. Is that right?  But for the output units Delta is just T minus y if James and output unit, we have the right combination of objective function and activation function that doesn't have to be team run this way as we saw if you use the wrong objective function squared error with logistic output to get a slope term in here, which is too bad. But later we figured out we should talk to my something else.  And then this is always the Delta for the hidden unit. So people on Piazza ask this question like why is it always that well, that's what it is. I don't know what else to tell you. That's the way the math comes out.  And so this is a recursive definition of delta you computer that the output units and then  you propagated backwards through the network and you keep doing that. So now I've got a Delta for these guys.  And if there's guys below here, they're also neural network units and not the input then I propagate these back.  And then if there's more I keep propagating.  Those back so in a deep Network for propagating the Deltas a long way from the output back to nearly the input.  And so this makes sense intuitively believe it or not. So if  You have a big error, and I have a big weight to you that I probably had something to do with it, and I need to change what I'm doing.  So I'll get a big Delta. My Delta will be increased a lot if I have a big weight to the next guy up and he has a big Delta.  If he has a big Delta and I have a small way to him than I had less to do with it. I don't I'm not as worried or if he has a small air and I have a big way to him. I don't care as much but it changes my Delta a little bit if he has a small air and I have a small waist and I don't give a shit. Okay? Okay, so that makes intuitive sense. Does that make intuitive sense to you?  Okay.  If this guy is a hidden unit.  So what do you want to know about him if he's a hidden unit?  Yeah, okay. So it's it's exactly the same as this.  Except you know, I've got J's up here instead of case. I mean  Yeah, so the Delta here will be most five times this and added into this guy's Delta in the same minute.  Well, it could be the same activation function to be a different one. But usually they're the same in in the hidden layers of teeth Network.  Okay, so you just keep doing that. Now as I said in my last lecture in the first iteration of Russell, norvig the best AI book on the planet, which is known space. They got this wrong in that given the Deltas up there. They would use that to change these weights because all you need is the Delta and the input on that line.  And then they would propagate the Deltas back and then they would change these weights and then they would propagate the Deltas back if that's wrong.  Okay, because now I'm using different weights than I computed the gradient with.  So I need to propagate the Deltas all the way back to however far they go and then change the weight.  Okay.  any questions  Okay. So again one thing that's different about this than your previous assignments is now I need random initial way. It's not zero weights and we'll talk a little later about why 0/8 doesn't work. We need Randomness to break up some symmetry problem, but  And in the next lecture assuming I get that far today.  I will be taught or Thursday. I'll be talkin about some some ideas for how you should initialize this weights.  Okay, some tricks of the trade. So because you start out with initially random weights, and now the earth's surface is no longer of it, you know, depending on how many hidden layers you have. It can be some arbitrary landscape.  And so if I start over here and there's a minimum over there. I might get to that one by start over here and there's a minimum over here. I might get to that one. So depending on where you start you'll get different answers.  And different local minimum and so this is one solution. If you set the initial weights, just right it'll find this excellent at work with where one does or which is three out of the four 4X door and this guy catches the case where  Xor is different from or that is when both inputs are on and turns off the output and just sat case.  Okay.  But because you have different initial random weights, you can get lots of different solutions this problem some of which might surprise you.  Okay.  and  we did this already.  What was the answer?  Okay, we did this one already. What was he answer?  I think he just remembered that. I don't think you're red it. Okay.  Okay. And again, this is wonderful because it learns internal representations and I don't have any clicker questions today so you can you can leave now.  Okay, so it's wonderful because it learns internal representations and that's basically what we're going to talk about today some different internal representations that backprop has learned.  This reminds me I need this.  Okay.  Hope it works.  Okay.  And it's very efficient. And you're as you're going to see in your programming assignment checking the weight numerically is very expensive and Computing the derivative of the error with respect to the weight numerically is very  Computationally intensive. Where is this does one seek forward One Sweep back and you're done.  And the other cool thing is it generalized has to recurrent Networks.  Okay, so last time.  We started talking about some of the things back prop has learned. And again, your Mantra is back propped learns representations in the service of the tasks. So when you're meditating, that's your Mantra, okay?  So we talked about xor this simple Network that does X or we talked about.  Symmetry this is more complicated and we talked about Edition.  So now we're going to have more more fun and talk about Natok.  So this means I need to.  iTunes  and  yeah.  okay, most of its Rock but there is  where is technical traditional now?  Natok  demonstration of network learning so that was Terry Sandusky talking but I could barely hear him. I hope so. I have to be quiet but let's come back to that momentarily.  So what is Natok Natok a neural network that learns to read aloud from text now? It doesn't really read from texts. What we do is we have localist inputs. That is one year one hot encoding of a through z and space and probably some other punctuation and to have the network read a cat you would have, you know, like 27 units here with a turned on.  27 units here with blank turned on 27 units here with C turned on a and t okay, so it's not like actually looking at the image. We're just giving it an input where it's already seen the  the input and this I learned overnight on a vax which is an old super mini supercomputer. We used to call it. We were happy to have it, It's probably not as powerful as your phone now and business weeks at at learn the abilities of a six-year-old child overnights are there was a lot of hype but this is the only neural-net so far that's been on the Today Show twice.  Cuz Terry knew somebody who was a producer on that and what it does is map time into space. So let's let's see what I mean by that again. Here's all the letters of the alphabet plus space, etc. Etc. Etc. So time that is the passing of this text is mapped into this space.  Okay, and then the the job of the network is to take the central letter here.  Which is C and it's in this context it sounds like or and that's the outputs are the all the phone ER unit for every phoneme and English that's supposed to turn on the cuffs on now the smart thing they did with it and then they shipped it over one produce the next next sat in the smart thing. They did was hook this up to a deck talk speech synthesizer so you could actually hear it.  Which is what we're going to do.  So this produces the sound in the context of the other letters around it. So I'm this contact see is cuh. But if this was perceive then it would be so right so that context is enough for many letters to get the right pronunciation, but it doesn't work for all cases.  Yeah.  I think it went to 7.  His mine mine goes up to 11.  You had to see this is spinal tap for that hits a mockumentary about a heavy metal band's called spinal tap and their drummer always gets electrocuted or whatever and every concert but the guy has a nap and he says they're really dumb guys and that guy has a nap and he said most people stamps just go up to 10, but mine goes up to 11.  Okay, so, okay. So anyway, so Terry and Charlie Rosenberg got this book that has a transcript of what some six-year-old child is saying.  And on the page, you can't see your opposite. This is transcription by linguist into phonemes. I guess that's what linguists do for fun. And I don't know but that was a training set. So they had to go to the words and the sounds and there are some alignment issues cuz they're a lot of you know silent letters like a yacht or comb for example, but they figured those out as well as they could.  And so that's the training Corpus and then they turn it on this and then they turn the page and test it on the next page.  okay, so this is what it  sounds like when it's just  learning  by Terry sinofsky and Charles Rosenberg learning Corpus taken from Carteret and Jones informal speech first-rate transcription.  The First Recording represents the first 5 minutes of learning with a network starting from zero waste.  The second recording gives the performance of the network after 20 passes through a corpus of 500 Words.  The third recording shows how the network generalizes to Fresh text.  First Recording denovo learning  I don't know. No, no, no, no, no, no, no.  So remember what these that this is old right, this is from 87. So it took a while to train and just had one hidden layer. But remember the thing about the bias it learns the bias on the outputs is what's going to learn first because it's always got a 1 on the other end of the line until what does it learn the way you can minimize the are the best is to learn the average phoneme in English, which is okay.  Oh, he's wrong. That's the difference between the person who actually did the work Charlie Rosenberg and the person who supervised to work and got their name as first author. Terry Terry is at the Salk Institute here, by the way, and he's also  got associated with a bunch of departments on campus. Yeah.  Why are you look at the error? Right and you you have the right phoneme and you have the you can also look at the percent, correct?  It's that simple. There's nothing more fancy that goes on the speech like nothing more fancy.  Okay, you look disappointed.  Okay, so it goes on like that for a little while?  No, no, no, no, no, no, no, no, no, no, no tears and EE.  Okay.  Okay.  synonym for win and Emmett  Okay second recording after 10,000 training words.  Black and blonde or something when we walk home from school. I want them with my friends and sometimes we can't run down from school. Now. I'm still on the run. She gets the recalls on stuff very well and she get sick. That's why we can't run. I like to go to my grandmother's will it cause she gave us will be there sometime. Sometimes we sleep overnight there sometime when I go to my go to my cousin I get updates Off Taylor play that mean turn on all that sounds like your tears and doubly bed.  This was you know our first shot at it and it was it was you know years ago anyway for my undergrads my freshman that I would say, it was a religious experience listening to this.  Okay, so now we're going to turn the page.  And listen to it.  Reading the next page. I'm not going to show you that page right away and see if you can figure out what it's saying and then I'll show it to you after a few words.  In the third recording the previous network is tested on a new Corpus, which is a continuation of the training Corpus.  There's just something I can turn in my primary. My baby. He won't stop jumping around New England cricket match Live Match in the horse. It will just take two minutes to get us. We'd just about 12 minutes. If you'll just tell me why don't you get some ice on it, right? Where what does my baby? Don't get the garbage on color theory?  Okay, so it's not great. But you know, we've come a long way since 1987 or so and it's a six-year-old. So that's why the baby comes out of the crib and crawls in your bed and takes out your ear plugs when you're trying to sleep.  Okay, any questions about that?  Okay.  Oh, there's one. Yeah.  What?  Yes. Yeah a classification problem. So again  Here's the network.  So again, you're you're the job of the network is in a sense of classification problem because it has to turn on the right phone the correct phoneme at the output out of the 40 sofo names in English.  So that's it. Yeah, and again, once they get that phone team, they slide this over get the next one sided over get the next one and that's called napping time into space to pay cuz I've laid it out. This is t t + 1 t + 2 t - 20 - 2 etcetera instead of for example having the network State be recurrent and change with inputs over time.  We'll talk more about that. So one of the cool things they did and Steve Hansen is still pissed off that they didn't reference him for it. Cuz he gave him the idea is they took how do you look at this and figure out what it's doing? I mean, you've got like 70 hidden units. I think I don't remember how many they had.  And it's not and you can look at the weights and try and figure out what it's doing. But another idea is to take the activations.  All these hidden units for every input output mapping like this is C going to cook and if there are other sea going to cuz you take the activation of the Hidden units in the average all those together. So there's just a finite number of these mappings from letters to sound right? That's what we do when we read aloud from text. And so you get you know, Kay different ones of these three have  70 say activations a vector of length 70 for every input output mapping and then what you do a hierarchical clustering analysis, so who knows what that is?  Good, you're going to learn something today, you know where he's so you take the two closest ones usually in euclidean distance and you average them together and you draw a little bit of tree.  So you put that one there in that one there and you put a little line between him and now I take the next to closest ones and do the same thing in the same thing. And eventually I get a binder I get down to one guy. That's the average of all of them and that's the root of the tree. So you create a binary tree this way and things down here are closer together than things up here and what you see basic it's hard for you to see from where you are but really easy for me to see if it it divides the world into vowels and consonants.  And the hard consonants are in some places in the soft consonants and others and over here. There's a e i o u and sometimes y and so you get a similarity structure over the inputs or over the mappings in this case.  how the network kind of organizes that internal representational space  That makes sense. Yeah, is that a question?  No.  What's our Mantra back propped learns representations in the service of the tasks? So this is not the end, but it's not the outdoor. It's the hidden Lair where the representations are.  And what we did was we took all went through all the Corpus took all the instances where she was going to K.  And average the hidden unit activations for that together. So I have one hidden unit Vector of activations for C to K.  And I do that for you know O2 who and stuff like that.  And then I do the cluster analysis and it tells me kind of how the network breaks up the world.  And it breaks it up and do naturally things to a linguist would appreciate a vowels and consonants and puts, you know hard consonants together. Yeah.  This guy is hard to read but I think it's a stiiizy which is very infrequent.  We don't have a lot of in a wizard sir bones.  There's an acid the end, but it becomes a z.  right  Okay.  Okay, fine.  Hey questions.  One more. Yeah.  What the unit?  Oh, this is distance in some space. So I guess you know probably normalized in some way. So things up here far apart things down here really close.  So here is B2B and P2P for example?  BNP are very similar to one another the only difference between them is voicing. So if you hold your throat and you go bbbbbbb.  try  b b b b b b b a p p p p p the only difference is your voice and you're vibrating your phone before you open your mouth and with Pierre not.  So they're very similar sounds.  Yeah those over here and consonants over here.  Yeah, and there is like soft consonants like hard consonants like  sure where so you don't release are you're just vibrating and but with the big stunt glottal stop there.  Okay, next example sentence family trees. So this was the first paper in the proceedings of the cognitive science Society in 1986 big thick book. And this is the first paper in it. These are two family trees. They're isomorphic. So everything up here first find something down here Roberto married Maria and had Emilio and Luchia Emilio Mary Jean Elysium. Married Marco, etcetera Okay, so  Every every every family has a boy and a girl in this weird family tree. So the top one is the British the bottom one is the Italians.  Okay. Is there a request did you if you ask me already? Okay, here's the network. It was a deep Network took a very long time to train but the idea was here. You had one hot encoding of each person.  It over here you had relationships so you can turn on Emilio father and it should turn on at the output. The one person is Emilio's father.  Okay, and the thing that the idea that hidden wanted to explain motivate here was these are localist. We call them one hot encoding and then it would learn a distributed representation up here of different features of these people and it learned it all through me and there's there's no similarities between any of these guys the inner product of any one of these inputs a zero.  So there's no similarity structure in the input. It has to learn it. Learn a representation of their features that solves the task. Okay. And so that's it's learning representation in the service of the tasks. So what we're going to look at next is 6 hidden units that sat above these 24 people down here and later. We're going to look at 6 hidden units above the relationship units here.  So this is called a hidden diagram.  After Jeff Hinton surprisingly enough along the top or all the British and the second row corresponds to the corresponding Italian.  A square is white if it's positive black if it's negative. So these are the weights into this hidden unit. So Charles and its corresponding Italian have big waves into this hidden unit positive ones and Penelope and her corresponding Italian have negative weights into it. Okay. That's what a hen diagram shows you it's a way to visualize the weights.  Two were heading unit.  Any questions about what that showing?  Hey, so we have six hidden units shown here.  and  what is unit one? That is the upper right one in coding.  So here's the family trees and what?  What feature is that one in coding?  Yeah, so this is the British unit it turns on if they're British. That's a useful feature. That means you only want to turn on the British at the output because British and Italian Stone intermarry in the story. Okay, so that's the the nationality unit or the British unit. So it picks out this family tree.  Hey, which is useful because the answer is going to come from this family tree.  Okay.  How about hitting unit 2?  so  here is in the unit to it corresponds to Christopher Andrew being on and Etc. What what do you think?  That one's encoding.  Wow, that was fast generation. It turns on for Christopher and Penelope notice. It's symmetric between the Italians and the British once I figured out who's who this one's kind of an Italian unit. Not quite as strong. Now this says, okay. It's going to turn on for Christopher Andrew and Penelope and Christine.  And turn way off for Colin.  And way off for Charlotte, so it's height in the tree.  2 pics out those guys  Okay.  How about unit 6?  What's unit 6 that's this guy?  turns on for Chris Zimmer  ideas  children  Christopher is a father grandfather actually.  What?  I'm sorry. I really am going to get a hearing aid by the end of this quarter. But what?  siblings in law  Side of the family hits the side of the turrets which side of the aisle you sit on at the wedding.  Right, you know this is once it's turning on for these guys and turning off for these guys.  Yeah.  Okay, so between those three features and their opposites which are also there.  1 pics out that one picks out that one picks out that  And so that narrows it down to Penelope and Christopher.  Which was you'll see that's enough to solve the problem.  So it's going to pick out pairs of people.  And other combinations of these units will pick out different parts of the tree that makes a lot of sense, I think.  And then these are the relationship unit. So what is this one in code?  Gender turns on for Aunt niece sister and wife daughter mother.  Okay, so so the gender is going to be picked out by that unit. So you don't need the gender here you just need  Christopher and Penelope, and then you could say son and you'll get Arthur.  Okay.  Okay any questions?  So the network learns his features in the service of the task and learn some on its own. We didn't have to think those up the network figured in there and show that's useful if we don't know what they ought to be like.  When I trained in that work to recognize facial expressions, I didn't know what features it should learn it had to figure it out.  Which we're going to see next.  so I'm going to give a demo this is  downloadable from my website  I showed you this one before I think right we're just learns to separate these two things. You saw that one and when I had their learning rate too high, the green line went away and Etc that sound familiar.  Hey buddy, Knox Versi. That's okay. So let's go here.  and  so  the first thing we did  let's use California facial expressions.  Oh wait, that's not what I want.  Hello, okay.  All right. Let's quit on a Matlab and start over is currently the screens been screwed up by somebody else.  Okay.  Sorry.  Computer, okay.  Define my desktop there is.  Jay Cooke by the way is like 80 11 years old oldies lives on my street and he  owned a car that Grace Hopper Road in  But she recently sold.  Austin-Healey  Okay.  Okay, let's do Cafe 40.  That's better. Okay. So the first thing so just said demo of face processing we're going to train a neural network to recognize facial expressions and identities and gender. And the first thing we did though was something called principal components analysis, which is a way to reduce dimensionality. So these images are like a hundred by 130.  and this particular data set is got the first 40 principal components which are kind of a  a underlying like dimension for each face. So this is like how much of this slider here has how much of this is in that one? Her face is dark. This is light. So it's kind of stand up the bottom Etc sewed this face.  Is a weighted some of these eigenfaces they're called. Okay, because they're the eigenvectors of the covariance Matrix of the data. And so there are orthogonal meeting each one of these has an inner product of 0 and so face is a point in this 40 dimensional space and it's gotten this much of this one that much of that one that much of that one Etc. She can see the sliders very well.  if I move these  You know, it changes her face.  Write in a few zero all of them. You get the average face.  In this state of set and then you can if you do that, then you can have fun kind of nerd find by trying to find that face find this space pray, you know. Well, I know this one's down and this one's up cuz I just saw it.  So and then like how do I how do I set these to get her? You know? So anyway, the point is these numbers that is where on the slider you are can there be input to my network? And I've only got 40 inputs instead of 100 x 130 and you found out in your programming assignment how bad that could be right you have 90,000 some pixel inputs with PCA. I could have given you much smaller dimensional inputs.  Tell again these are this is like an axis and this is where on that access the saws basically project that face under this and you find out what its principal component is.  Okay, so this so you can have fun with this, you know.  There's a good that's somebody a happy guy. Okay anyway, and and then you can do kind of poor man's Morse.  these These are  Oops, I was supposed to.  have to load the PCA projections  again, but there she is again.  And here's somebody else. That's my that's a  meow I should say.  She was my PhD students.  girlfriend now wife  and  so you can do these kind of poor man's more swear. You're just like this is like 40% of these pixels plus 60% of those pixels. So it's not a real morph.  But it's it's a little bit fun.  Okay. Sorry. That was the fun part.  Okay.  so now we're going to do back prop and and I'm going to use  I guess the same data set.  So I just read in the 40 principal components for all the images in this little data set. Wait this the days that you guys had right?  Yeah, okay, so it's got it figured out. I know I've got 40 input units got any patterns sounds familiar and 10 hidden units now and now  I'm going to load the targets and then go for Cafe I have to go there and there's list expressions.  and got 10 hidden units now I can train it is you and  I don't know what the blue I didn't.  I guess.  I have zero validations and zero test so I don't know what that blue line is. This is the test that it's all 0 cousins empty, but what you can do with us now is kind of do a gradient backwards from each hidden unit and see what the representation kind of get the internal representation of the face, so  She is angry.  And that's the internal representation of anger by the network. So this should look somewhat familiar.  Maybe a little different than what you guys found discussed surprise cetera.  k  Do to do so that's that's kind of fun.  and now  So, okay, so I did emotions and we all did this so, it should look familiar, but it learned these representations in the service of the tasks.  so that's what the network  Thanks. This guy looks like in order to get surprised at right.  Okay.  But I can also.  Word the identities instead as you guys did.  And now it knows there's 10 outputs.  We got 10 hidden units.  So we can train it to do that.  and  now  notice what's going on in the right hand side here.  What's happening?  they're very much very similar to one another because it's trying to learn his identity now, so it's not changing very much from one picture to the next until  You got this somebody else.  Okay.  You guys didn't do this. I don't think of Step through these bed. This is it's got hidden unit. So it's the hidden unit representation. We can try and do this with fewer heading units.  So I just reduced them to 5 and still does really well and now they will with fewer resources the network will learn even more.  fix representations of everybody because it doesn't have  Any resources to reflect these changes in the input that really doesn't need to reflect anyway.  and it's now  I think you might have noticed before that when that guy had his mouth wide open. The representation was different.  this guy  and now it's not different because it doesn't have the resources to represent those differences and it doesn't need to to solve the problem.  So what did it do?  Yes.  learned representations in the service of the tasks  Okay.  Any questions about that?  So there's a little bit of mathemagic going on here where I get this internal representation from the five hidden units, but other than that,  okay.  Now I'm going to try.  gender and I apologize to the non-binary among you but  there we only had two in those days.  I bet I can do gender with one hidden unit.  Okay, so this is basically going to be a perceptron.  Okay. Now it knows it's should have two outputs because we had two outputs we could have just one but we had to  Okay, and boom it learned it really fast. I could probably also hold out some.  Some guys here. Let's hold out say 10. See what happens.  And tests that are goes down to zero very quickly in this case.  And now it happens the internal representation.  It's exactly the same for this guy.  As well as that guy.  Doesn't change.  Until we hit a woman.  But it's the same for all women.  Let's see Universal women in the Universal Man.  So, okay. So again, it's learned a representation in the service of the task here. It just has to representations 1/4 male to 1/4 female.  Okay.  All right. I knew questions comments funny stories.  So why is why can I do this for their perceptron? I mean, how do I know that from what I just did I had one hidden unit, right? So it's just going to have positive connection to one of the outputs in the negative connection to the other.  And that's it. So  Okay, honey. I'm going to close this out.  Okay, so  all right, so that's  learning internal representations in the service of the tasks.  Okay.  Are we all happy?  I'm happy.  Okay, any any questions about back crop learning internal representation, so they yeah.  Yeah, so it's all of the Hidden units basically run the network backwards. So this input comes in it activates the hidden units different amounts and then use that amount to multiply X the weights and had that up to get an image out of it. So it's like each unit is going to have some features. It's responding to and if it's .1 you multiply 1 * awaits get pixels out of that and then welcome principal components with you then.  and turn into one of these things and then  can you add those up for all the hidden units?  One paragraph we take average overall team for is just a weighted sum of all of them.  exactly  Okay. Alright, so tricks of the trade in the remaining 15 minutes will start doing some of these so there is at the like V paper from the bottom of the resources page under readings is laocoon tricks of the trade. So this is from a a chapter in a book called tricks to the trade the name of the chapters efficient back prop and it's from a while ago. And so some of these tricks are a bit outdated at this point, but  They're good. It's going to be good for you and they illustrate some points that later led to things like batch normalization.  Which we're going to learn about later while later in this talk.  Okay, so these are the tricks.  this particular chapter comes down on the side of  stochastic gradient descent vs. Batch  So this tends to learn faster because of redundancy in the training set. So  Mnist, what you're working on now has 60,000 training examples of 10 digits. So you're going to see every digit lots and lots of times before you get through the whole training set.  so it seems like a huge waste of time to go through 60000 examples and add up all the way changes and after you've process 60,000 example you change the weights once  what a waste because by the time you process say 10000 examples, you probably almost already solved the problem because you've seen a thousand examples of every category by then right then so  Real world data sets tend to have redundancies in the training set. It's not completely true. Now that we have imagenet for example there a wide variety of Scottish deerhounds in the training set, but there's still some redundancy their attempts to get better Solutions because of the stochastic City you're changing weights based on some examples and your you can get around some of these local Minima that way to remember we have we don't have just a bowl anymore. Now, we've got local Minima because the error surface is no longer just a bowl. It's it's all weirdly shaped because of these hidden units.  and another thing is if the training examples start to change like you guys have new things to say compared to what I used to say write and say cool far out and you guys say something else like having SEC is over now, right but  Anyway, okay.  Right on Brothers & Sisters of the Revolution, okay.  And you can use this for very large datasets that you can't keep in memory cuz it's online essentially.  Why bachelor in the mathematicians over in the math department that do the Sioux function optimization have a very good understanding of how these things converge when you take the true gradient.  There are lots of good optimization techniques conjugate gradient bfgs, etcetera. These taken to conjugate gradient is kind of a one and a half order technique bfgs is a second-order technique and what I mean by that  The gradient is first-order. It's like which way is downhill.  Conjugate sorry bfgs takes into account the curvature around the slope so that the curvature is second-order. And so these things can find the minimum very quickly. They tend to get very large weights, but you have to go through the whole training set which is Impractical like Facebook has billions of pictures of faces. It would be impractical to do that on those.  Okay. So why many bachelor in which is part of your homework, you're going to be doing many batches. So what's a compromise between online learning where you change the wait forever example and Bash learning where you change the weights after seeing all the example instead you get a good size shot of examples say 128 to pick a random number and now I've got in that batch, you know, probably all the digits and 10 examples are 12 examples of each and so I've got a kind of Representative subset of the data and I'll get a pretty good estimate of which way is downhill. It's still called stochastic gradient descent because you're not going down the true gradient depending on which examples you picked.  You'll go a little bit different directions down hill.  Okay.  So so using a mini batches, what is done by everyone now?  and again, you take these examples and you change the weights once and then you get another batch of 128 say this can be usually very efficient a GPU so  When on Computing the gradient, I have to take these hundred and twenty-eight examples and go Funk with all of them at once with exactly the same weights in the network. And then I go backwards getting the Deltas and those all use the same weight cuz I haven't changed the weights yet. So I can do that in parallel date of parallelism where I can do that as a big Matrix of here's some inputs pump I get, you know,  128 * 10 outputs right for each example and I can do my back prop using Matrix X  so  I don't know if what's the latest on that back in the day Mark blue towsky my PhD student in Hell white and I had a couple of papers on learning Maki glass from 25 examples plus or minus 2 and the idea of America glasses a Time series that models your heart beats. And so it's one of these chaotic time series and so chaotic time series are deterministic. But if you make a little change at one point, it makes a big change later on that's what chaos is and  So we have the system where we would actually pick the example with the greatest error and trained on that.  Then we'd pick the next example of the greatest Aaron train on those two examples. And so we just pick the examples with the biggest Terror. This would be a terrible thing to do if you have noise because she picked noisy examples, but we picked example set for Mackie glass. We know what the you know is deterministic so we have exactly and and so we had a lot of examples of yours the input, you know, maybe you take five or six points in the previous part of the time series and then you're trying to predict the next point. And so if you keep picking examples based on that and we could learn the whole thing from 25 examples.  And so that was a you know, that's not a great idea again if you have a lot of noise, but that's that's what we did and it seems like you would want to pick the verse examples. So maybe you want to have high entropy in your batch so that you get a lot of different in a lot of information for me to example, I don't know. I don't know who's done work on this, but you could try it look and see.  Okay, I mean the main thing is you want to shuffle the examples and you know, if you have a big enough if you have 60,000 examples, you probably just need to shuffle once right, He just want to make sure you're not getting yours. The one one one one one one one one one and you got a hundred and twenty-eight ones in your mini batch. What's going to happen?  What's it going to do?  It's going to be vs120. Sorry.  Not exactly. I mean, it's telling you selling Network turn on the one unit. Turn on the one unit. Turn on the Atkins solve that very quickly by cranking up the bias on the one output.  Not learn anything about the input then supposed to get all too. So I was just turn that bias down in the to buy a ShopRite and just need to learn anything. So you really need different examples in sequences.  Sorry, so you really want examples from different classes in your mini batch and one possible heuristic is as I just mentioned pick ones with more air or more often but used with caution because there's a lot of reasons why that example might be hard might be an error in the training set the mislabeled example, and that's why it's hard that crap is really good at finding mislabeled examples in the training set by putting a lot of hair on them. Maybe you need to learn something about the easy examples before he learned the hard examples, but it can improve performance on infrequent examples like and now we get back to that talk. Okay.  and the next thing is PCA the input so we'll save that for next time cuz  Okay.  UC San Diego podcast for more visit podcast. Ucf.edu "
}