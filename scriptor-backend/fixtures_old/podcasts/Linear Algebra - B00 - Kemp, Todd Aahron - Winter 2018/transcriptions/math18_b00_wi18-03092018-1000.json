{
  "Blurbs": {
    "/ the length of - 211 squared x that factor - 211 Okay, and then from there you just compute those numbers and add them up and you'll get some Factor so mechanically it's a very easy, but it's important to understand what the orthogonal projection is. It's the closest point to the Subspace. Have a nice weekend. UC San Diego podcast ": [
      3048.3,
      3299.6,
      113
    ],
    "Find an orthonormal basis of vee-vee is one-dimensional. So the ortho part is vacuous. We don't need to find to find orthogonal vectors. There's only one vector. There doesn't need to be anything. But we do need to have the normal parts. We do need to have a unit length of vector. So that means we should replace. The vector here, which I'll call you. with It's normalization which means ": [
      2498.4,
      2528.0,
      93
    ],
    "I take two vectors, okay and add them up. And project that'll be the same thing as projecting each of the vectors and adding them up. Why is that just look actually at the formula that's right here everything in that formula is linear that formula is linear and why I think that's because the dot product is linear on matrix multiplication is linear. Orthogonal projection is a linear transformation. ": [
      2058.3,
      2089.1,
      77
    ],
    "I want to orthogonally project it down onto the table what that means is I'm taking a shine a light straight over head straight over the table. I look at the shadow underneath there. That's what an orthogonal projection is. So let's do an example of that on the on the graph here. So let me draw vector. There's a vector v. Okay, and I want to compute its orthogonal ": [
      984.3,
      1014.0,
      37
    ],
    "It has components 2 and 3 what I'm saying is I projected onto the first basis vector and see how long it is there. That's too and I project it onto the second they suspect you're here and that's Street and so. Vector is 2 * 81 + 3 * e2e to okay, that's exactly what this tells me exactly the same idea. So now let me make a few ": [
      1850.4,
      1878.2,
      69
    ],
    "It's worth talking to ex. But the first condition tells us that why is some multiple of x? So therefore B- Lambda X dotted with X is equal to 0. Now, let me use the properties of the. Product to expand that out. That's a v. It with x minus Lambda X. It with x and really all we need to do here is figure out what land that is ": [
      1157.2,
      1183.7,
      43
    ],
    "Listening to a podcast Friday. Good morning. It is a happy Friday. Sun is shining and feels like spring out there. And we got some linear algebra going on in here. It's a good time. Two administrative reminders. And once again, please fill in your capes your student about your professor and and course evaluations. Okay, you have until the end of classes until next Friday at 11:59 to fill ": [
      2.0,
      34.1,
      0
    ],
    "Now one of them is immediate but it's Envy because look that thing there is a linear combination of the basis vectors of the so it is Envy. So all you need to do is verify that it's orthogonal to be I'm going to leave that to you as an exercise to work out. You can also read it in the textbook. You just need to take the dog product ": [
      1991.9,
      2009.8,
      74
    ],
    "Okay, so I'm going to leave you now in a few minutes at the end of the lecture. slightly confused unfortunately so why is that because What I said is if we want to find the orthogonal projection. Into a Subspace. The first thing we need to do is find an orthonormal basis for that Subspace. How do you find an orthonormal basis for a Subspace? We haven't learned how ": [
      2635.2,
      2668.7,
      98
    ],
    "RNA is called orthogonal if each pair of the vectors are orthogonal that is if I take any two vectors from that list, their perpendicular to each other there. Product is zero and then we have this additional where we stayed there orthonormal. If in addition to being orthogonal they are also all of the unit length are all of length 1 normalized vectors. Okay, so that rules out the ": [
      143.5,
      170.5,
      5
    ],
    "So let's figure out what those three number with those two numbers. Are you. It with v is equal to 1 + 0 + 0 and the length of you squared is 1 squared + 2 squared + -2 squared Okay, * the vector you which is 1 to -2. So this is 1/9 of the vector 1 2-2, that's the projection. It's shorter than you. It's only one night ": [
      1297.4,
      1330.5,
      48
    ],
    "So that the function that takes a vector and returns its orthogonal projection into a Subspace that isn't that is a linear transformation, which means that it has a matrix and what is the Matrix of that linear transformation? We can compute the standard Matrix of a linear transformation that one way we can do that is to apply the linear transformation to the standard basis vectors and we get ": [
      2089.1,
      2113.4,
      78
    ],
    "Then you are orthogonal projection is just given by that's like the simple looking formula. You take the dot product of your vector with each basis Vector multiply that number by the basis vector and add those guys up. Okay. Now we'll just some examples to compute these things in a moment. But first I want to talk a little bit more of the properties of orthogonal projection. So if ": [
      2036.7,
      2058.3,
      76
    ],
    "V what that really is is it's the vector envy that is closest to Hawaii. So that is let's draw. Let's do this dropping down a vertical line again. Just like we did on the side where we introduced this. so here in this example Is the the retinal projection of Y into V? There are lots of vectors Envy. Let's draw some other ones. Here's one. Here's one. What ": [
      2764.4,
      2799.7,
      103
    ],
    "Vector down here. So this examples to her. I've got a vector 1 2 3 I'm asking you find the closest Vector to that one in the Subspace spanned by those two vectors. Hey, if you see a question like that, all it's asking you to do is to find. the orthogonal projection into that Subspace of the vector 1 2 3 and we use the formula that we had ": [
      2994.5,
      3020.5,
      111
    ],
    "Vector it which I could write as y 1 - 2y 2 + 2y 3. Time is the vector. one- to two so there's a perfectly good formula for the projection of any Vector. Why into the Subspace spanned by that vector? But alternatively we could find the Matrix for that thing by following what we just did what we have to do. We have to find an orthonormal basis. ": [
      2469.5,
      2497.0,
      92
    ],
    "a basis. I can uniquely decompose it. As a sum of two vectors or one of them. is envy The other is in the park. Okay, that's exactly what we did on this last slide here. We had our vector. V K and what we did was we wanted to write it we wanted to project it onto x what we did was we wrote that v as the sum ": [
      1512.5,
      1544.5,
      56
    ],
    "a list of numbers. It might be a long list of numbers like the entries in the S&P 500 505 numbers. Okay, geometrically, it's hard to understand what you would mean. We've you were talking about such a vector and projecting it except what you can say as well. Maybe what I've got is maybe I've got a model that predicts how the stocks are supposed to evolve like all ": [
      2905.0,
      2928.3,
      107
    ],
    "a vector Y into V by doing this you just add up the projections into Subspace spanned by those vectors individually. Okay. Let me write this again. This is what we've been up there. This is the projection onto the span of you want or I wrote it is just you won before of Y plus the projection onto you to of why. And so on Down the Line. That's ": [
      1794.5,
      1824.7,
      67
    ],
    "an orthogonal basis and take the Matrix of those this fact will not be true anymore. Okay. All right. Let me tell you something else about these orthogonal matrices. Okay, that's that's pretty great. So an orthogonal Matrix, well, it's a matrix to square Matrix so we can think of it as a linear transformation the linear transformation T of X is equal to a u x x Now what ": [
      676.0,
      706.0,
      26
    ],
    "and orthodontist basis just means a single standing that you're with his life that you can do you find a spanning vector and you normalize it but if you have a two-dimensional or three-dimensional higher dimensional space finding an orthonormal basis that requires a new technique and we're not going to get that take me till next day. So that's going to be the topic for Monday's lecture the orthogonalisation ": [
      2687.9,
      2708.6,
      100
    ],
    "and we can do that now because I can divide through by X. ID with X. It's important the vector ex here wasn't the zero vector or it doesn't make sense to project onto it. But if I have any nonzero Vector ex that X. It was X that's the length of X, right. So this tells me hear that VDOT it with x - Lambda times. The length of ": [
      1183.7,
      1204.9,
      44
    ],
    "are exactly the linear Transformations that preserve all geometry in the represented by these orthogonal matrices, which are exactly the matrices that have an Arsenal cases as the columns or the rose. Alright, so now I want to talk about orthogonal projections. So what is an orthogonal projection? Well, orthogonal projection really just means a shadow. Okay. It means a direct Shadow. So if I take my pencil here and ": [
      949.3,
      984.3,
      36
    ],
    "as you one dotted with. Why is you one transpose x y And this one is you to transpose x y. so on Down the Line Okay. Now the first thing I want to know about this is let's remember the definition of matrix multiplication, right? So if I have some Vector X let's say let's just say it has three components. And I have some Matrix a witch as ": [
      2156.3,
      2189.5,
      81
    ],
    "by the length of x squared x the vector X and there's a formula for the projection. That's how I project any Vector onto another one just with that simple formula. Okay. So for example got a couple down here, here's a vector U of A curvy and a vector W. So let's project. the vector v onto the vector you hear it with these two vectors in R3. Well, ": [
      1232.8,
      1269.0,
      46
    ],
    "called least-squares approximation. Okay, this is very important stuff. That's what orthogonal projection actually is. Okay, so let me finish just by so I won't prove that them that it's the shortest but it certainly is very believable from the picture that that line there. The black one in the middle is the shortest line. So let's finish just by finding the closest points. To a Subspace of a certain ": [
      2968.9,
      2994.5,
      110
    ],
    "can we say about this linear transformation? What kind of properties does it have? Let me show you owing to this fact that you transpose USD identity. It has a pretty great property. So if I take that linear transformation that I get by multiplying a vector by this orthogonal Matrix And I take two vectors and transform them. I want to look at the dot product of those to ": [
      706.0,
      730.3,
      27
    ],
    "did on the last slide so noticed that this means that you transpose you is the identity each one of these is and Mayan But in this special case with square. You transpose USD identity what that tells you is? Okay. Well, let's just call this thing you transpose is equal to a and what we see, is that a x you as the identity. But if you have two ": [
      520.4,
      545.5,
      20
    ],
    "down here. So this Vector I'm going to give it a name. I'm going to call it. Why? How do I compute the vector y well I just noticed two things about the vector y. Okay. The first is that why is parallel to the vector you are to the vector ex that we started with right? So why is equal to some constant time is the vector ex is ": [
      1076.9,
      1102.6,
      40
    ],
    "end of the lecture that there's a sort of a Nifty way or a very concise way to express this orthogonally condition as you went through Ups training together with The Columns of a matrix as usual so that Matrix is going to be typically taller than it is wide. It will have more rows than columns or at worst equal number of rows and columns. and what we saw ": [
      215.9,
      238.2,
      8
    ],
    "from before you're told that this is a north and or mocha that this is an orthogonal basis. So what I do is I take the dot product of Y with that first base inspector. Divided by the length of that first base inspector squared times that first base inspector. And then I add to that that projection onto the second base inspector. One two, three dotted with - 211 ": [
      3020.5,
      3048.3,
      112
    ],
    "from why is minimal? so what this really is saying is that the claim is that for any vector v Envy let's give this a thug in a projection named. Let's call it a z like before. Find a vector V in V. the distance from V 2z Is the distance from y to V? Is bigger than the distance from y to Z? What is z is the thing ": [
      2824.8,
      2879.9,
      105
    ],
    "geometrically to find the orthogonal projection onto a single Vector. Well, the first property is that it must be in the Subspace. We're projecting into the Subspace and then the second property is that if I take the difference between that vector and why? But that's supposed to be orthogonal to the Subspace. That was the definition. Remember we're uniquely decomposing a vector has something in the Subspace plus something ": [
      1942.8,
      1967.8,
      72
    ],
    "guess it's not that abstract. But what it means is that you take a vertical line of a perpendicular line to your Subspace down and Slide the vector along there. Okay shortening flanks until it's into Subspace. Okay. What does that actually mean? Well this theorem tells us what orthogonal projection actually is. So if I give you a Subspace V and a vector why The projection of Y into ": [
      2735.9,
      2764.4,
      102
    ],
    "had to happen. You. It was wa0 means that you and W are orthogonal to each other. So if I have a light straight over his bottle here. It doesn't have a shadow. Okay, the shadow is right underneath. It's just the zero Vector is orthogonal projection. If you are already orthogonal to the vector you your projection onto the vector you will be zero. That's what's all going on ": [
      1382.6,
      1408.8,
      51
    ],
    "how you do it. Can you fix a Northland or an orthogonal basis and you project your vector onto each one of the basis vectors and then you add those up and if you think about How we calculate the coordinates of a vector in euclidean space thinking of the standard basis as an orphan Armel basis. That's exactly what we're doing. When I say OK this Vector over here. ": [
      1824.7,
      1850.4,
      68
    ],
    "identity matrix. It's not what it is is is the orthogonal projection onto the Subspace spanned by those vectors. If you started with an orthonormal basis for RN, then the Subspace spanned by those vectors is all of our in which means that the orthogonal projection is just the identity. If I asked you to orthogonally project this Vector, which is already in the table orthogonally projected into the table ": [
      2337.4,
      2366.3,
      87
    ],
    "in a V whose distance to why is smallest and that's really important because what it says is that this projection thing this Vector Z. That is the orthogonal projection of Y into V. It's the best approximation of the vector. Why in the Subspace V? So if you think about these things in terms of how we talked about vectors on the first day of class adventure is just ": [
      2879.9,
      2905.0,
      106
    ],
    "in the set. Okay. So that's that's a concise way of saying that you have orthonormal vectors if the Matrix you with those mattresses Collins satisfies you transpose you is the identity. That's what it means to be worth in normal want to make an important comment here, which is in the typical situation where we have fewer than n vectors. Are we have our p is less than an ": [
      370.8,
      392.3,
      14
    ],
    "in the time slot that you normally have your section on Thursday. If you have a conflict than there are conflict X to set up and you should be in touch with them at love ta about that. Okay, and I didn't post it on here. But I will mention once again your final exam is one week from tomorrow at the beginning of exam week. 11:30 a m the ": [
      65.3,
      86.3,
      2
    ],
    "independent vectors in RN, you can't have more than any of them. So P orthogonal vectors and rnp must be less than or equal to n it could be equal to end which case you have and linearly independent vectors an RN they form a basis in which case they would form an orthogonal basis or or the normal if they're North normal set. Now we saw last time the ": [
      192.8,
      215.9,
      7
    ],
    "is a square Matrix. now in that case those vectors you once were you and they form a North enorma basis? Either orthogonal vectors their normalized that what's the normal vectors and there's any of them in our end that means that they form a basis. So if I have a matrix who is columns are an orthonormal basis of RN we call that Matrix you and orthogonal Matrix. Okay, ": [
      468.5,
      496.1,
      18
    ],
    "is an nyp Matrix. And so that means that you transpose is p by n and so this is the P by P. Identity Matrix, okay. Do the veterans themselves they could live in our n where N is a billion and could be very large this Matrix that encodes there. Products is p by P its size is a square Matrix of size equal to the number of vectors ": [
      345.8,
      370.8,
      13
    ],
    "is but you you transpose is not we've seen examples like this before when we first start talking about invertible matrices. If you have a rectangular Matrix, it can well be the case that it has a left inverse but not a right at first that will be the case here too. Now. What is you you transpose? Do you transpose USD identity when there was a normal vectors turns ": [
      429.0,
      449.4,
      16
    ],
    "is the Matrix of the orthogonal projection into that Subspace. So that is how you compute orthogonal projection. If you want to think about them in terms of the Matrix that represents the linear transformation to Let's do an example of here before we get too far ahead of ourselves. So here is a vector 1-2 2 And it's fans of Subspace. What's called a Subspace V? And I want ": [
      2392.4,
      2419.6,
      89
    ],
    "is the distance from each of those vectors to watch I will hear the distance from this first green vectorizer to why is the length of that line right there and the distance from this other one is that line right there? Those lines are longer than this one. That's the statement here the statement is that the orthogonal projection of Y into V. That's the vector Envy whose distance ": [
      2799.7,
      2824.8,
      104
    ],
    "is this also equal to Y dotted with nine you / the length of a 9 you squared x 9 you Well, actually yes because I've got this 9 in the numerator here. And this 9 in the numerator there and I've got two nines in the denominator down there because it's inside the square so that all those nines cancel out. Okay, so it doesn't that make sense to ": [
      1699.3,
      1726.5,
      63
    ],
    "it. So I guess I've written the same formula again. If he is one dimension of a single Vector in the projection onto V is given by this for me like your projection of why on TV is why daughter with you / you. It with you times you where you is any basis vector? Any basis Vector of the Subspace V? That is how we're going to generalize I ": [
      1749.3,
      1773.5,
      65
    ],
    "just an orthogonal basis. Let me be clear here. This is a theorem. I haven't proved it. It's not hard to prove though and let me give you an idea of why it's true. Okay, this thing the orthogonal projection. And why interview, let's go. Let's give it a name with z. What are its properties so let's go through basically the same idea that we went through when we'd ": [
      1916.2,
      1942.8,
      71
    ],
    "know that this is X transpose X you transpose you x y But you transpose you is the identity. So this is just X transpose X the identity x y. Which is X transpose y but that's the definition of the. Product of X and Y. So what we see from here is that if we have an orthogonal Matrix you then as a linear transformation, it has the special ": [
      786.8,
      817.4,
      30
    ],
    "left slide. So at least we know how it works when the V is one-dimensional. Let me point out. There's a tiny bit of a disconnect at least notational here here. I said the projection onto a Subspace V on the last flight. I said the projection onto a vector you Okay know if the Subspace V is one-dimensional there's not so much difference between those but actually we should ": [
      1653.9,
      1675.3,
      61
    ],
    "matrices. If you have in vectors in RN that aren't worth a normal if you have an orthonormal basis of our end then that Matrix that square orthogonal Matrix it satisfies both you transpose USD identity and you you transpose is the identity. Only in the Square cases that work. Let's think about this for a second. Actually. This is kind of remarkable this you your transpose is the identity ": [
      575.0,
      600.5,
      22
    ],
    "of that thing minus y with each of the basis vectors you once were you pee and compute and you will see that it's exactly or organized so that everything will cancel out and you'll get 0 But anyway, that is how you compute orthogonal projections. and here I've just restated what I said on the on the last five there that if you have an orthodontist basis already then ": [
      2009.8,
      2034.7,
      75
    ],
    "of the trading firms on Wall Street 2 And my model tells me how things are supposed to evolve and then I actually look at what the numbers are. They don't quite agree with my model. So what I'm going to do is I'm going to take the real numbers and I'm going to find the best approximation that my model gives of those numbers to adjust my model for ": [
      2928.3,
      2948.0,
      108
    ],
    "of transformation. These are orthogonal matrices are rotations like the kind that we studied in the two by two case. This is the generalization that is actually one other kind of transformation that is rigid like this The Preserves angles and lengths Reflections. I can't do a reflection in this room. I don't have a mirror. Okay, but that's that's what is it is a rotation or reflection. so those ": [
      919.2,
      949.3,
      35
    ],
    "of two vectors the one we call the Y on the one we called V - Y where Y is in the Subspace spanned by X and V - why is perpendicular to the Subspace spanned by X? Okay. That is exactly what we're doing when we're doing orthogonal projection. So what we want to do is take our Vector any Factory start with only want to decompose it as ": [
      1544.5,
      1568.7,
      57
    ],
    "okay. That does not tell us anything about you you transpose. I throw you you transpose. Well you is and by P you transpose is p by n. Okay. So the size of this Matrix is and buy an but this is typically false affect. Italy always be false. if p is less than an okay that the product you you transpose. It's not the identity Matrix you transpose you ": [
      392.3,
      429.0,
      15
    ],
    "orthodontic basis of RN the kind of linear transformation, it represents is one that preserves all geometry. Okay, in other words if I'd have some vectors and I transform them. They must transform rigidly their lengths don't change and the angles between them don't change. Okay, that's the kind of transmission of talking about which is not a 1980s Jazzercise routine. I'm just representing here a rotation. That's the kind ": [
      892.8,
      919.2,
      34
    ],
    "orthonormal vectors which has by the way only one Elena tubes. Then those lights are all one. So this is another concise way of saying that a collection of vectors are 4th. And normal is to say if you take those vectors and put them together in this Matrix you with The Columns of as those vectors, then you transpose you is the identity Matrix of what size. Well you ": [
      314.1,
      345.8,
      12
    ],
    "out that 1/9. So this is a 3 by 3 Matrix and we get its entries by for example, not playing one by one and that's it. cave the next one we get by multiplying 1 by -2 And one by 2 then next row. We have to multiply -2 by 1 - 2 by -2 and -2 by 2 and then to -4 and 4 and that's it. That's ": [
      2582.9,
      2610.1,
      96
    ],
    "out that you you transpose mean something as well as a very important geometric meaning and we're going to discuss that in this lecture. But before we get there, I do want to talk about the case when PT is equal to a so if p is equal to end. That means that this Matrix which has the columns equal to those vectors you want you to up to un ": [
      449.4,
      468.5,
      17
    ],
    "p of the Subspace V. And there is a r formula for the orthogonal projection. I want to write the Matrix of that which means I want to write that as a x y for some Matrix a okay. Well first thing I'm going to do to get there is use the definition of the. Product which says that why I thought it with you one, which is the same ": [
      2133.5,
      2156.3,
      80
    ],
    "p transpose why But now let's remember that there's serve another way the common way that we actually compute matrix multiplication. If I'm taking a matrix and multiplying it by a column Vector. What I do is I multiply the row by the column right row by column to get the first entry then second row by column to come the second entry and so on. But that's what this ": [
      2249.4,
      2279.4,
      84
    ],
    "parallel to X, but the other thing is, how did we arrive at this why we dropped a vertical line down? Perpendicular to the vector X from the tip of V. Okay, and that where that line intersect of the lines 4X is what we called the vector why so that means we're using this Vector hear. What is that Vector? Well, that's the vector which when I add it ": [
      1102.6,
      1124.8,
      41
    ],
    "pee? so if I as a as before if I Write you as the Matrix whose columns. Are you want you to through you pee? Then just from the definition of matrix multiplication. This thing is equal to That Matrix you want you to talk to you pee with those columns times the column Vector whose entries are you 1 transpose? Why you to transpose why I'm down till u ": [
      2219.5,
      2249.4,
      83
    ],
    "perpendicular the Subspace. So just like with the one-dimensional span example, these two properties uniquely specify what the orthogonal projection is and it can now be computed all you need to do to show that this thing. I've told you hear this some of one-dimensional projections to prove that. That's actually they're throwing a projection. All you need to do is verify that that thing there satisfies these two properties. ": [
      1967.8,
      1991.0,
      73
    ],
    "procedure by which you can always find an orthonormal basis. For Subspace. So for now you're just going to have to trust me that every Subspace has or thermal basis. But before we get there. There's one thing I want to tell you one further thing. I want to tell you which is what is orthogonal projection with described it mechanically and we described it abstractly. It's a shadow. I ": [
      2708.6,
      2735.9,
      101
    ],
    "projected into V. What I have to do is take while I dotted with that Vector 1 - 2 2 / the length of that factor 1-2 2 squared time is the vector 1-2 2 and so we already computed that one that length of that Vector squared is 9 so it would I get is 1/9 of why dotted with that that Matrix there. That's a that's a column ": [
      2440.1,
      2469.5,
      91
    ],
    "projection onto the line through X here. Cancel here is the line through the vector ex down here. Well what that means is that I drop a line perpendicular to X straight down and it won't let me change the vector v. So it's not quite as close to vertical. That could be a little easier to draw so here is a vertical line. Straight down at a right angle. ": [
      1014.0,
      1049.5,
      38
    ],
    "projection. On TV up some Vector Y is equal to Y dotted with x / X dotted with x. Exit let me use the vector you instead of excess or not confused with the notation for the last slide if he is the span of some Vector you that this is why daughter with you / the length of you squared times the factory that's what we computed on the ": [
      1626.7,
      1653.9,
      60
    ],
    "projections. Okay. Well, that's no projections onto a single Vector but When I talked about a second ago, the projection of this bottle the line was out of his bottle or by my pencil under the table. I wasn't talking about the projection onto a particular Vector. I was protecting about the projection to the table onto a two dimensional Subspace. Hey, so that's a little different. Well, we'll see ": [
      1408.8,
      1435.5,
      52
    ],
    "property that it preserves. Products if I transform to vectors by you. The dot product between those vectors doesn't change. Now the dump I don't remember it didn't codes both lengths and angles. So what we see is there for that the length of T of x squared. Well, that's the inner product of T of X with itself. But taking x equals y in the calculation we just made ": [
      817.4,
      841.8,
      31
    ],
    "remarks. if the basis is orthonormal That means that those links of the use are all one. And therefore we have the slightly simpler formula. But we just get why dotted with you one X you one plus y. It with you two times you too. Etc why. It with you pee. I'm sleepy. So that's one way that it's nicer to work with a north and arm avesis than ": [
      1878.2,
      1916.2,
      70
    ],
    "right there. I'm here at the projection is really The linear transformation whose Matrix is you you transpose remember at the beginning of the lecture? I said if you've got an orthonormal set of vectors, that means that you transpose you is the identity but you you transpose may not be the identity. If the vectors are there fewer vectors than an RN then you you transpose won't be the ": [
      2313.5,
      2337.4,
      86
    ],
    "rooms have been sets that's posted on the course webpage their rooms that we've used for the midterms Galbraith 242 Peterson 108 and York 27220 have a seat assignment in one of those rooms selected randomly, which will be posted on the course webpage early next week. I have posted on the course webpage to practice final exams. So you can take a look at those and start working on ": [
      86.3,
      109.7,
      3
    ],
    "same sort of thing I did before I'm going to draw a line vertically down to that Subspace perpendicular to that Subspace. Okay. Well perpendicular Subspace. What is that mean? We know what that means cuz every Subspace has an orthogonal complement as we saw last lecture. So what's really going on here is we take our Subspace. Now it has an orthogonal complement we call that V perp and ": [
      1460.3,
      1487.7,
      54
    ],
    "says here. I'm taking you one transpose, which is the first column of you written as a row and dotting it with Y and so on down the line, so what that thing actually is. Is the vector whose Rose? Are you one transpose you to transpose down till u p transpose? Time is the vector y. but look this is you times you transpose x y And that is ": [
      2279.4,
      2313.5,
      85
    ],
    "second vector. Okay. Now remember how the transpose works. The transpose of a product is the product of the transpose is but in the opposite order, these are good things to remember as we're heading into the final exam where you'll be tested on all these sort of things. Okay, so this ux transpose the same as X transpose you transpose. and now we'll use the associative matrix multiplication to ": [
      755.3,
      786.8,
      29
    ],
    "so that's a new word for you to put in your your flashcards. An orthogonal Matrix is a matrix with columns are orthonormal vectors form an orthodontist basis. That's a square Matrix orthogonal Matrix. It might make more sense to call that a North enormo Matrix, but that's not the way that you could call it an orthogonal Matrix. Now in this case it let's do the same thing. We ": [
      496.1,
      520.4,
      19
    ],
    "some new pictures that are The Columns of the Saturn Matrix. Just reminding you how those ideas work from the beginning of the course before the final exam. But actually in this case is going to be easier to work from the definitions to see what Matrix we get here. So let's work with an orthonormal basis. Language start with an Arsenal basis you want you to up to u ": [
      2113.4,
      2133.5,
      79
    ],
    "square matrices a x USD identity that tells you that a is the inverse of you. But we know that for square matrices if I have an inverse on the left, it's also the inverse on the right and so we get there for also you a is the identity that is you you transpose is the identity. No caution. Once again, this only works for square matrices for orthogonal ": [
      545.5,
      575.0,
      21
    ],
    "talk about the projection onto a Subspace this formula make sense. Okay, but still that doesn't answer the question. How do we compute the orthogonal projection onto a Subspace, you know it exists and we can do this kind of unique decomposition. It sounded like maybe we need to do some row reduction or something like that. I don't know we don't need to okay. So here's how we do ": [
      1726.5,
      1749.3,
      64
    ],
    "that they're very closely related. So how do we talk about projections on two subspaces in general? But let's think about what we were really doing in the last example with projections on two lines. So if I've got an RN, I'm going to Vector an RN. I've got a Subspace V an RN and I want to project into that Subspace orthogonally. What I'm going to do is the ": [
      1435.5,
      1460.3,
      53
    ],
    "that this happens here. So let's think about this. We said that you transpose you record the. Products between The Columns of you, right? That's what we saw last time. So you transpose you because the identity means that the columns are orthonormal. So what does you you transpose equal the identity say, well you transpose is the transpose of you. So what this says is that The Columns of ": [
      600.5,
      627.2,
      23
    ],
    "that turns into the dot product of X with itself. Which is the length of x squared so this thing preserves lengths. And also therefore if I take the dot product of T of X with t of Y and divide that by the product of the lengths of T of x and t of Y. Everything is preserved there. That'll be the inner product of X with Y / ": [
      841.8,
      869.4,
      32
    ],
    "the Matrix of the orthogonal projection onto this one-dimensional Subspace. And by the way, you can go ahead and compute now that if you multiply that matrix by the vector y 1 Y 2 y 3 you'll get exactly the formula that's written up there. Search I'll leave that to you to verify the matrix multiplication agrees with that formula we got but that's how you compute orthogonal projection. Okay? ": [
      2610.1,
      2635.2,
      97
    ],
    "the dot product of T of X with t of Y. And actually to make the notation easier to read I'm going to use this inner product notation instead. Well, okay, so that's just ux inner product you why. I'll let you know just remember the definition of the inner product of the dot product. It just means that I take the transpose of the first vector and X the ": [
      730.3,
      755.3,
      28
    ],
    "the length of x times the length of Y. But we know what this says. This says that the cosine of the angle between X and Y is equal to the cosine of the angle between t of x and t f y so another words this linear transformation T. It also preserves angles. So if I take out an orthogonal Matrix and Matrix whose columns or rows form an ": [
      869.4,
      892.8,
      33
    ],
    "the length of you because the vectors u and v they're not that far away from orthogonal just like the example we stopped here if the vectors X and V up there are pretty close to orthogonal then if we project V down on two exits going to be pretty close to zero. In fact, let's look at the second example here to bolster that point. Let's project the vector ": [
      1330.5,
      1349.7,
      49
    ],
    "the next iteration another words. I need to know how to find the best approximation in my models parameters that I can find to the actual numbers. I need to be able to do this closest point to a Subspace thing. This is the single most important application of linear algebra that you will find out there. If you took a statistics class we're just talking about right now is ": [
      2948.0,
      2968.9,
      109
    ],
    "the orthogonal projection of that vector onto so that vector v onto the line through X. Is this Vector down here? Contactor down there. So it's the one that is parallel to X that I get by just dropping that vertical line straight down. So the picture is very easy to understand question is how do we calculate that thing? Thanks. I want to figure out what that Vector is ": [
      1050.6,
      1076.9,
      39
    ],
    "the sum of a vector in Subspace V and a vector that is perpendicular to the Subspace be a vector in the orthogonal complement because those two Subspaces are complementary to each other by the rank theorem their Dimensions add up to the whole space and because they are linearly independent from each other we can always do this uniquely. So we Define the projection into V of the vector ": [
      1568.7,
      1598.5,
      58
    ],
    "the the picture that I drew to drive this formula that was in the plane that was two dimensional. But that's the definition of the projection in any Dimension Dimension to 376. Whatever you want. So the projection of the vector v onto the vector you is simply equal to we take the dog product you. It with v / the length of you squared X the doctor are you? ": [
      1269.0,
      1296.3,
      47
    ],
    "them together in a matrix that Matrix if you look at it throws now the first entries of all of the columns and then the second and trees of all the calls and so on those two will form a North enormo basis of our at that's a very useful fact. Okay, it's very important that you normalize the factors for that to be true. If you just a court ": [
      659.2,
      676.0,
      25
    ],
    "think in terms of a basis. So if you are going to find the orthogonal projection of a vector into a Subspace, the first thing you need to do is find a basis but not just any basis an orthogonal basis. So the first thing you do is you find some orthogonal basis you one through you pee for your Subspace V. And then what you do is you project ": [
      1773.5,
      1794.5,
      66
    ],
    "those are two complementary subspaces as we've seen. Okay, which means in particular that every Vector in V is perpendicular to every Vector Envy purple. And so that means that they are also linearly independent from each other. So what that tells me is that if I take any Vector in the big space in RN, I can uniquely decompose that factor kind of like the decomposition in terms of ": [
      1487.7,
      1512.5,
      55
    ],
    "those know we will go through some solutions of those problems in the review session next Friday, which is what next Friday's lecture will be about okay. so today we are going to go to section 6.3 which is on orthogonal projections and next time we'll move to orthogonalisation section 6.4. So let's review a little bit and catch up where we were before. So a collection of vectors in ": [
      109.7,
      143.5,
      4
    ],
    "those out online. Your final MyMathLab homework sets has been assigned supposed to last night at 8 you have until next Thursday at 11:59 p.m. That covers the remaining material recovery in this class from chapters 5 6 and 7 and 7x. They already did five. Your last Matlab assignment is due tomorrow night at 11:59 and you're not love quiz. Just as a reminder is next week on Tuesday ": [
      34.1,
      65.3,
      1
    ],
    "to Y gives me V. That's the vector v - y. And that Vector is drawn perpendicularly to X. So the other condition here is that V - why is perpendicular to X? Now those two conditions they actually uniquely specify what the orthogonal projection is because the second condition that tells us that V - y dotted with X is equal to 0 is what it means to say. ": [
      1124.8,
      1157.2,
      42
    ],
    "to do that. We know how to find a basis for a Subspace typically or we have a generic procedure where you have a Subspace, you know, it's spanned by sum of vectors and then you can start throwing away vectors until you find a Spanish sentence linearly independent but doing that will typically not producing or the normal basis. Now, if I if you have a Subspace as one-dimensional ": [
      2668.7,
      2687.9,
      99
    ],
    "to find the orthogonal projection onto V by which I mean I want to find. The linear transformation. I want to find the Matrix that represents this linear transformation. Well first things first, we have our formula and here we don't need to do is some of things we just go back to the original thing we'd arrive. So if I have some Vector why and I want to orthogonally ": [
      2419.6,
      2440.1,
      90
    ],
    "usual right in terms of its columns, but when I multiply these it better have three columns then by definition a x x is equal to the linear combination of the columns. given by the vector X right Well, look at what I've written here for the orthogonal projection the orthogonal projection. Is a why is a certain linear combination of those columns you want you to up to you ": [
      2189.5,
      2219.5,
      82
    ],
    "verify that things are making sense here because the span of V is also the span of 9V. I could have used some of you with the span of 9u I could have used a scalar multiple of that Vector you. Okay, so well, I have this formula here in terms of you. That means I should also make sure that they make sense if I replace you with 9u ": [
      1675.3,
      1699.3,
      62
    ],
    "w. On to the vector you well, that means we need to compute you. It with W / the length of you squared times the vector you so what is you daughter with W. Let's see here you daughter with w? Is equal to 1 * 0 + 2 * 1 + -2 * 1 which is 2-2, which is 0 to the zero vector. And of course that's what ": [
      1349.7,
      1382.6,
      50
    ],
    "was that the dot product of you wanted you to we can calculate that by taking you transpose times you the one to entry of you transpose times you is the duff out of you wanting you to the seven six entry of that Matrix you transpose you is the dot product review 7 and U6 the duck products of all the vectors. That's great. And also it means that ": [
      238.2,
      268.3,
      9
    ],
    "we can simply write what it means for a bunch of vectors to be or to normal if they're worth the normal. That means that all the vectors are orthogonal to each other. So that means that if I take you wanted you to its. Product of 0 and same with you too and you one I take it you wanted use three there. Product of 0 and same with ": [
      268.3,
      292.2,
      10
    ],
    "we divide it by its length and it's length. It's length is \u221a 9 it's length is 3 1 - 2 to like that and then what we know that thing is an orthodontist places that one vector is an orthodontic basis of V. and what we just saw the last night is that the projection onto V The Matrix of the projection of that Subspace is you you transpose ": [
      2528.0,
      2552.9,
      94
    ],
    "what happens to it? It stays the same. It's already there. Okay, if you're already in the Subspace, then you are throwing a projection in Subspace is yourself. This is all consistent you you transpose is the identity if your vectors are an orthonormal basis of our end if your pictures are normal basis of some Subspace of R and you you transpose isn't the identity Matrix. What it is ": [
      2366.3,
      2392.4,
      88
    ],
    "where you is the Single column Matrix that is that normalize Vector. So that is we have to take 1/3 of 1-2 2 * 1/3 of 1-2, 2 as a row. Now you're used to writing row times column and we're doing matrix multiplication. But here we have to do call him X row that's gives us the orthogonal projection. So that might look a little funny but let's take ": [
      2552.9,
      2582.9,
      95
    ],
    "why to be that why I want to be the one that's in Veep. So we do this orthogonal decomposition and the part. That's envy that is the orthogonal projection of the vector y on TV. That's the definition but how do you actually compute it? Well, we just saw that you computed in one-dimensional case. If V is the span of some Vector X. Then we saw that the ": [
      1598.5,
      1626.7,
      59
    ],
    "x squared is equal to 0 I can solve for Lambda there. Lambda is able to be dotted with X over the length of x squared. And so back from property one that gives us a formula for this Vector. Why can't watch all right down here. why which will call the projection onto X of the vector v is equal to the dot product of V with x divided ": [
      1204.9,
      1232.8,
      45
    ],
    "you three and you want you too and you three there. Product is zero and so on we find that all of the entries of this Matrix except on the diagonal cut on the diagonal. We have you won. You want and you too. You too and you three. You three Those are all the length squared of the vectors you want you to and you three if these are ": [
      292.2,
      314.1,
      11
    ],
    "you transpose are orthonormal. The Columns of you transpose are the rows of you. So this says That the rose of you also form. an orthonormal basis of RN That's amazing. Why should that be? I don't actually have a good explanation for this other than using the linear algebra tools were developed in this course, you start out with a bunch of column vectors that are orthonormal you string ": [
      627.2,
      659.2,
      24
    ],
    "zero Vector which is good. We don't really want to include the zero Factor. One thing we noted last day is that if you have a collection of orthogonal vectors, they are linearly independent. That means that if you have an invective GOP vectors an RN that are orthogonal P cannot be bigger than n right. If you have a bunch of orthogonal if you have a bunch of linearly ": [
      170.5,
      192.8,
      6
    ]
  },
  "Full Transcript": "Listening to a podcast Friday. Good morning.  It is a happy Friday.  Sun is shining and feels like spring out there.  And we got some linear algebra going on in here. It's a good time.  Two administrative reminders. And once again, please fill in your capes your student about your professor and and course evaluations. Okay, you have until the end of classes until next Friday at 11:59 to fill those out online.  Your final MyMathLab homework sets has been assigned supposed to last night at 8 you have until next Thursday at 11:59 p.m. That covers the remaining material recovery in this class from chapters 5 6 and 7 and 7x. They already did five.  Your last Matlab assignment is due tomorrow night at 11:59 and you're not love quiz. Just as a reminder is next week on Tuesday in the time slot that you normally have your section on Thursday. If you have a conflict than there are conflict X to set up and you should be in touch with them at love ta about that. Okay, and I didn't post it on here. But I will mention once again your final exam is one week from tomorrow at the beginning of exam week. 11:30 a m the rooms have been sets that's posted on the course webpage their rooms that we've used for the midterms Galbraith 242 Peterson 108 and York 27220 have a seat assignment in one of those rooms selected randomly, which will be posted on the course webpage early next week.  I have posted on the course webpage to practice final exams. So you can take a look at those and start working on those know we will go through some solutions of those problems in the review session next Friday, which is what next Friday's lecture will be about  okay.  so today  we are going to go to section 6.3 which is on orthogonal projections and next time we'll move to orthogonalisation section 6.4.  So let's review a little bit and catch up where we were before.  So a collection of vectors in RNA is called orthogonal if each pair of the vectors are orthogonal that is if I take any two vectors from that list, their perpendicular to each other there. Product is zero and then we have this additional where we stayed there orthonormal. If in addition to being orthogonal they are also all of the unit length are all of length 1 normalized vectors. Okay, so that rules out the zero Vector which is good. We don't really want to include the zero Factor. One thing we noted last day is that if you have a collection of orthogonal vectors, they are linearly independent. That means that if you have an invective GOP vectors an RN that are orthogonal P cannot be bigger than n right. If you have a bunch of orthogonal if you have a bunch of linearly independent vectors in RN, you can't have more than any of them.  So P orthogonal vectors and rnp must be less than or equal to n it could be equal to end which case you have and linearly independent vectors an RN they form a basis in which case they would form an orthogonal basis or or the normal if they're North normal set.  Now we saw last time the end of the lecture that there's a sort of a Nifty way or a very concise way to express this orthogonally condition as you went through Ups training together with The Columns of a matrix as usual so that Matrix is going to be typically taller than it is wide. It will have more rows than columns or at worst equal number of rows and columns.  and what we saw was that the dot product of you wanted you to  we can calculate that by taking you transpose times you the one to entry of you transpose times you is the duff out of you wanting you to the seven six entry of that Matrix you transpose you is the dot product review 7 and U6 the duck products of all the vectors.  That's great. And also it means that we can simply write what it means for a bunch of vectors to be or to normal if they're worth the normal. That means that all the vectors are orthogonal to each other. So that means that if I take you wanted you to its. Product of 0 and same with you too and you one I take it you wanted use three there. Product of 0 and same with you three and you want you too and you three there. Product is zero and so on we find that all of the entries of this Matrix except on the diagonal cut on the diagonal. We have you won. You want and you too. You too and you three. You three  Those are all the length squared of the vectors you want you to and you three if these are orthonormal vectors which has by the way only one Elena tubes.  Then those lights are all one.  So this is another concise way of saying that a collection of vectors are 4th. And normal is to say if you take those vectors and put them together in this Matrix you with The Columns of as those vectors, then you transpose you is the identity Matrix of what size. Well you is an nyp Matrix. And so that means that you transpose is p by n and so this is the P by P. Identity Matrix, okay.  Do the veterans themselves they could live in our n where N is a billion and could be very large this Matrix that encodes there. Products is p by P its size is a square Matrix of size equal to the number of vectors in the set. Okay. So that's that's a concise way of saying that you have orthonormal vectors if the Matrix you with those mattresses Collins satisfies you transpose you is the identity. That's what it means to be worth in normal want to make an important comment here, which is in the typical situation where we have fewer than n vectors. Are we have our p is less than an okay.  That does not tell us anything about you you transpose.  I throw you you transpose. Well you is and by P you transpose is p by n. Okay. So the size of this Matrix is and buy an  but this is typically false affect. Italy always be false.  if p is less than an  okay that the product you you transpose. It's not the identity Matrix you transpose you is but you you transpose is not we've seen examples like this before when we first start talking about invertible matrices.  If you have a rectangular Matrix, it can well be the case that it has a left inverse but not a right at first that will be the case here too. Now. What is you you transpose? Do you transpose USD identity when there was a normal vectors turns out that you you transpose mean something as well as a very important geometric meaning and we're going to discuss that in this lecture. But before we get there, I do want to talk about the case when PT is equal to a so if p is equal to end. That means that this Matrix which has the columns equal to those vectors you want you to up to un is a square Matrix.  now in that case  those vectors you once were you and they form a North enorma basis?  Either orthogonal vectors their normalized that what's the normal vectors and there's any of them in our end that means that they form a basis. So if I have a matrix who is columns are an orthonormal basis of RN we call that Matrix you and orthogonal Matrix.  Okay, so that's a new word for you to put in your your flashcards. An orthogonal Matrix is a matrix with columns are orthonormal vectors form an orthodontist basis. That's a square Matrix orthogonal Matrix. It might make more sense to call that a North enormo Matrix, but that's not the way that you could call it an orthogonal Matrix. Now in this case it let's do the same thing. We did on the last slide so noticed that this means that you transpose you is the identity each one of these is and Mayan  But in this special case with square.  You transpose USD identity what that tells you is? Okay. Well, let's just call this thing you transpose is equal to a and what we see, is that a x you as the identity.  But if you have two square matrices a x USD identity that tells you that a is the inverse of you.  But we know that for square matrices if I have an inverse on the left, it's also the inverse on the right and so we get there for also you a is the identity that is you you transpose is the identity.  No caution. Once again, this only works for square matrices for orthogonal matrices. If you have in vectors in RN that aren't worth a normal if you have an orthonormal basis of our end then that Matrix that square orthogonal Matrix it satisfies both you transpose USD identity and you you transpose is the identity.  Only in the Square cases that work.  Let's think about this for a second. Actually. This is kind of remarkable this you your transpose is the identity that this happens here. So let's think about this. We said that you transpose you record the. Products between The Columns of you, right? That's what we saw last time. So you transpose you because the identity means that the columns are orthonormal.  So what does you you transpose equal the identity say, well you transpose is the transpose of you. So what this says is that The Columns of you transpose are orthonormal.  The Columns of you transpose are the rows of you. So this says  That the rose of you also form.  an orthonormal basis of RN  That's amazing. Why should that be? I don't actually have a good explanation for this other than using the linear algebra tools were developed in this course, you start out with a bunch of column vectors that are orthonormal you string them together in a matrix that Matrix if you look at it throws now the first entries of all of the columns and then the second and trees of all the calls and so on those two will form a North enormo basis of our at that's a very useful fact. Okay, it's very important that you normalize the factors for that to be true. If you just a court an orthogonal basis and take the Matrix of those this fact will not be true anymore.  Okay.  All right. Let me tell you something else about these orthogonal matrices. Okay, that's that's pretty great.  So an orthogonal Matrix, well, it's a matrix to square Matrix so we can think of it as a linear transformation the linear transformation T of X is equal to a u x x  Now what can we say about this linear transformation? What kind of properties does it have? Let me show you owing to this fact that you transpose USD identity. It has a pretty great property. So if I take that linear transformation that I get by multiplying a vector by this orthogonal Matrix  And I take two vectors and transform them.  I want to look at the dot product of those to the dot product of T of X with t of Y.  And actually to make the notation easier to read I'm going to use this inner product notation instead.  Well, okay, so that's just ux inner product you why.  I'll let you know just remember the definition of the inner product of the dot product. It just means that I take the transpose of the first vector and X the second vector.  Okay. Now remember how the transpose works.  The transpose of a product is the product of the transpose is but in the opposite order, these are good things to remember as we're heading into the final exam where you'll be tested on all these sort of things.  Okay, so this ux transpose the same as X transpose you transpose.  and now we'll use the associative matrix multiplication to know that this is X transpose X you transpose you x y  But you transpose you is the identity.  So this is just X transpose X the identity x y.  Which is X transpose y but that's the definition of the. Product of X and Y.  So what we see from here is that if we have an orthogonal Matrix you then as a linear transformation, it has the special property that it preserves. Products if I transform to vectors by you.  The dot product between those vectors doesn't change.  Now the dump I don't remember it didn't codes both lengths and angles. So what we see is there for that the length of T of x squared. Well, that's the inner product of T of X with itself. But taking x equals y in the calculation we just made that turns into the dot product of X with itself.  Which is the length of x squared so this thing preserves lengths.  And also therefore if I take the dot product of T of X with t of Y and divide that by the product of the lengths of T of x and t of Y.  Everything is preserved there. That'll be the inner product of X with Y / the length of x times the length of Y.  But we know what this says. This says that the cosine of the angle between X and Y is equal to the cosine of the angle between t of x and t f y  so another words this linear transformation T. It also preserves angles.  So if I take out an orthogonal Matrix and Matrix whose columns or rows form an orthodontic basis of RN the kind of linear transformation, it represents is one that preserves all geometry.  Okay, in other words if I'd have some vectors and I transform them. They must transform rigidly their lengths don't change and the angles between them don't change. Okay, that's the kind of transmission of talking about which is not a 1980s Jazzercise routine. I'm just representing here a rotation.  That's the kind of transformation. These are orthogonal matrices are rotations like the kind that we studied in the two by two case. This is the generalization that is actually one other kind of transformation that is rigid like this The Preserves angles and lengths Reflections. I can't do a reflection in this room. I don't have a mirror. Okay, but that's that's what is it is a rotation or reflection.  so those are exactly the linear Transformations that preserve all geometry in the represented by these orthogonal matrices, which are exactly the matrices that  have an Arsenal cases as the columns or the rose. Alright, so now I want to talk about orthogonal projections. So what is an orthogonal projection? Well, orthogonal projection really just means a shadow. Okay. It means a direct Shadow. So if I take  my pencil here  and I want to orthogonally project it down onto the table what that means is I'm taking a shine a light straight over head straight over the table. I look at the shadow underneath there. That's what an orthogonal projection is.  So let's do an example of that on the on the graph here. So let me draw vector.  There's a vector v.  Okay, and I want to compute its orthogonal projection onto the line through X here.  Cancel here is the line through the vector ex down here.  Well what that means is that I drop a line perpendicular to X straight down and it won't let me change the vector v. So it's not quite as close to vertical.  That could be a little easier to draw so here is a vertical line.  Straight down at a right angle.  the orthogonal projection of that vector  onto so that vector v onto the line through X. Is this Vector down here?  Contactor down there. So it's the one that is parallel to X that I get by just dropping that vertical line straight down. So the picture is very easy to understand question is how do we calculate that thing?  Thanks. I want to figure out what that Vector is down here. So this Vector I'm going to give it a name. I'm going to call it.  Why?  How do I compute the vector y well I just noticed two things about the vector y. Okay. The first is  that why is parallel to the vector you are to the vector ex that we started with right? So why is equal to some constant time is the vector ex is parallel to X, but the other thing is, how did we arrive at this why we dropped a vertical line down?  Perpendicular to the vector X from the tip of V.  Okay, and that where that line intersect of the lines 4X is what we called the vector why so that means we're using this Vector hear. What is that Vector? Well, that's the vector which when I add it to Y gives me V. That's the vector v - y.  And that Vector is drawn perpendicularly to X. So the other condition here is that V - why is perpendicular to X?  Now those two conditions they actually uniquely specify what the orthogonal projection is because the second condition that tells us that V - y dotted with X is equal to 0 is what it means to say. It's worth talking to ex.  But the first condition tells us that why is some multiple of x?  So therefore B- Lambda X dotted with X is equal to 0.  Now, let me use the properties of the. Product to expand that out. That's a v. It with x minus Lambda X. It with x  and really all we need to do here is figure out what land that is and we can do that now because I can divide through by X. ID with X. It's important the vector ex here wasn't the zero vector or it doesn't make sense to project onto it. But if I have any nonzero Vector ex that X. It was X that's the length of X, right. So this tells me hear that VDOT it with x - Lambda times. The length of x squared is equal to 0 I can solve for Lambda there. Lambda is able to be dotted with X over the length of x squared.  And so back from property one that gives us a formula for this Vector. Why can't watch all right down here.  why which will call the projection onto X of the vector v is equal to the dot product of V with x divided by the length of x squared x  the vector X  and there's a formula for the projection. That's how I project any Vector onto another one just with that simple formula.  Okay. So for example got a couple down here, here's a vector U of A curvy and a vector W. So let's project.  the vector v  onto the vector you hear it with these two vectors in R3.  Well, the the picture that I drew to drive this formula that was in the plane that was two dimensional. But that's the definition of the projection in any Dimension Dimension to 376. Whatever you want. So the projection of the vector v onto the vector you is simply equal to we take the dog product you. It with v  /  the length of you squared  X the doctor are you?  So let's figure out what those three number with those two numbers. Are you. It with v is equal to 1 + 0 + 0  and the length of you squared is 1 squared + 2 squared + -2 squared  Okay, * the vector you which is 1 to -2.  So this is 1/9 of the vector 1 2-2, that's the projection. It's shorter than you. It's only one night the length of you because the vectors u and v they're not that far away from orthogonal just like the example we stopped here if the vectors X and V up there are pretty close to orthogonal then if we project V down on two exits going to be pretty close to zero. In fact, let's look at the second example here to bolster that point. Let's project the vector w.  On to the vector you well, that means we need to compute you. It with W / the length of you squared times the vector you so what is you daughter with W. Let's see here you daughter with w?  Is equal to 1 * 0 + 2 * 1 + -2 * 1 which is 2-2, which is 0 to the zero vector. And of course that's what had to happen. You. It was wa0 means that you and W are orthogonal to each other. So if I have a light straight over his bottle here.  It doesn't have a shadow.  Okay, the shadow is right underneath. It's just the zero Vector is orthogonal projection. If you are already orthogonal to the vector you your projection onto the vector you will be zero.  That's what's all going on projections. Okay. Well, that's no projections onto a single Vector but  When I talked about a second ago, the projection of this bottle the line was out of his bottle or by my pencil under the table. I wasn't talking about the projection onto a particular Vector. I was protecting about the projection to the table onto a two dimensional Subspace.  Hey, so that's a little different.  Well, we'll see that they're very closely related. So how do we talk about projections on two subspaces in general? But let's think about what we were really doing in the last example with projections on two lines. So if I've got an RN, I'm going to Vector an RN. I've got a Subspace V an RN and I want to project into that Subspace orthogonally. What I'm going to do is the same sort of thing I did before I'm going to draw a line vertically down to that Subspace perpendicular to that Subspace.  Okay. Well perpendicular Subspace. What is that mean? We know what that means cuz every Subspace has an orthogonal complement as we saw last lecture. So what's really going on here is we take our Subspace.  Now it has an orthogonal complement we call that V perp and those are two complementary subspaces as we've seen. Okay, which means in particular that every Vector in V is perpendicular to every Vector Envy purple. And so that means that they are also linearly independent from each other.  So what that tells me is that if I take any Vector in the big space in RN, I can uniquely decompose that factor kind of like the decomposition in terms of a basis. I can uniquely decompose it.  As a sum of two vectors or one of them.  is envy  The other is in the park.  Okay, that's exactly what we did on this last slide here. We had our vector.  V K and what we did was we wanted to write it we wanted to project it onto x what we did was we wrote that v as the sum of two vectors the one we call the Y on the one we called V - Y where Y is in the Subspace spanned by X and V - why is perpendicular to the Subspace spanned by X? Okay. That is exactly what we're doing when we're doing orthogonal projection. So what we want to do is take our Vector any Factory start with only want to decompose it as the sum of a vector in Subspace V and a vector that is perpendicular to the Subspace be a vector in the orthogonal complement because those two  Subspaces are complementary to each other by the rank theorem their Dimensions add up to the whole space and because they are linearly independent from each other we can always do this uniquely.  So we Define the projection into V of the vector why to be that why I want to be the one that's in Veep. So we do this orthogonal decomposition and the part. That's envy that is the orthogonal projection of the vector y on TV.  That's the definition but how do you actually compute it? Well, we just saw that you computed in one-dimensional case.  If V is the span of some Vector X.  Then we saw that the projection.  On TV up some Vector Y is equal to Y dotted with x / X dotted with x.  Exit let me use the vector you instead of excess or not confused with the notation for the last slide if he is the span of some Vector you that this is why daughter with you / the length of you squared times the factory that's what we computed on the left slide. So at least we know how it works when the V is one-dimensional. Let me point out. There's a tiny bit of a disconnect at least notational here here. I said the projection onto a Subspace V on the last flight. I said the projection onto a vector you  Okay know if the Subspace V is one-dimensional there's not so much difference between those but actually we should verify that things are making sense here because the span of V is also the span of 9V.  I could have used some of you with the span of 9u I could have used a scalar multiple of that Vector you.  Okay, so well, I have this formula here in terms of you. That means I should also make sure that they make sense if I replace you with 9u is this also equal to Y dotted with nine you / the length of a 9 you squared x 9 you  Well, actually yes because I've got this 9 in the numerator here.  And this 9 in the numerator there and I've got two nines in the denominator down there because it's inside the square so that all those nines cancel out.  Okay, so it doesn't that make sense to talk about the projection onto a Subspace this formula make sense.  Okay, but still that doesn't answer the question. How do we compute the orthogonal projection onto a Subspace, you know it exists and we can do this kind of unique decomposition. It sounded like maybe we need to do some row reduction or something like that. I don't know we don't need to okay. So here's how we do it. So I guess I've written the same formula again. If he is one dimension of a single Vector in the projection onto V is given by this for me like your projection of why on TV is why daughter with you / you. It with you times you where you is any basis vector?  Any basis Vector of the Subspace V?  That is how we're going to generalize I think in terms of a basis. So if you are going to find the orthogonal projection of a vector into a Subspace, the first thing you need to do is find a basis but not just any basis an orthogonal basis. So the first thing you do is you find some orthogonal basis you one through you pee for your Subspace V. And then what you do is you project a vector Y into V by doing this you just add up the projections into Subspace spanned by those vectors individually.  Okay. Let me write this again. This is what we've been up there. This is the projection onto the span of you want or I wrote it is just you won before of Y plus the projection onto you to of why.  And so on Down the Line.  That's how you do it.  Can you fix a Northland or an orthogonal basis and you project your vector onto each one of the basis vectors and then you add those up and if you think about  How we calculate the coordinates of a vector in euclidean space thinking of the standard basis as an orphan Armel basis. That's exactly what we're doing. When I say OK this Vector over here. It has components 2 and 3 what I'm saying is I projected onto the first basis vector and see how long it is there. That's too and I project it onto the second they suspect you're here and that's Street and so. Vector is 2 * 81 + 3 * e2e to okay, that's exactly what this tells me exactly the same idea.  So now let me make a few remarks.  if the basis  is orthonormal  That means that those links of the use are all one.  And therefore we have the slightly simpler formula.  But we just get why dotted with you one X you one plus y. It with you two times you too.  Etc why. It with you pee. I'm sleepy. So that's one way that it's nicer to work with a north and arm avesis than just an orthogonal basis. Let me be clear here. This is a theorem. I haven't proved it. It's not hard to prove though and let me give you an idea of why it's true.  Okay, this thing the orthogonal projection.  And why interview, let's go. Let's give it a name with z.  What are its properties so let's go through basically the same idea that we went through when we'd geometrically to find the orthogonal projection onto a single Vector. Well, the first property is that it must be in the Subspace. We're projecting into the Subspace and then the second property is that if I take the difference between that vector and why?  But that's supposed to be orthogonal to the Subspace. That was the definition. Remember we're uniquely decomposing a vector has something in the Subspace plus something perpendicular the Subspace. So just like with the one-dimensional span example, these two properties uniquely specify what the orthogonal projection is and it can now be computed all you need to do to show that this thing. I've told you hear this some of one-dimensional projections to prove that. That's actually they're throwing a projection. All you need to do is verify that that thing there satisfies these two properties.  Now one of them is immediate but it's Envy because look that thing there is a linear combination of the basis vectors of the so it is Envy. So all you need to do is verify that it's orthogonal to be I'm going to leave that to you as an exercise to work out. You can also read it in the textbook. You just need to take the dog product of that thing minus y with each of the basis vectors you once were you pee and compute and you will see that it's exactly or organized so that everything will cancel out and you'll get 0  But anyway, that is how you compute orthogonal projections.  and here I've just restated what I said on the on the last five there that if you have an orthodontist basis already then  Then you are orthogonal projection is just given by that's like the simple looking formula. You take the dot product of your vector with each basis Vector multiply that number by the basis vector and add those guys up.  Okay.  Now we'll just some examples to compute these things in a moment. But first I want to talk a little bit more of the properties of orthogonal projection. So if I take two vectors, okay and add them up.  And project that'll be the same thing as projecting each of the vectors and adding them up. Why is that just look actually at the formula that's right here everything in that formula is linear that formula is linear and why I think that's because the dot product is linear on matrix multiplication is linear.  Orthogonal projection is a linear transformation. So that the function that takes a vector and returns its orthogonal projection into a Subspace that isn't that is a linear transformation, which means that it has a matrix and what is the Matrix of that linear transformation? We can compute the standard Matrix of a linear transformation that one way we can do that is to apply the linear transformation to the standard basis vectors and we get some new pictures that are The Columns of the Saturn Matrix. Just reminding you how those ideas work from the beginning of the course before the final exam. But actually in this case is going to be easier to work from the definitions to see what Matrix we get here. So let's work with an orthonormal basis.  Language start with an Arsenal basis you want you to up to u p of the Subspace V. And there is a r formula for the orthogonal projection. I want to write the Matrix of that which means I want to write that as  a x y for some Matrix a  okay. Well first thing I'm going to do to get there is use the definition of the. Product which says that why I thought it with you one, which is the same as you one dotted with. Why is you one transpose x y  And this one is you to transpose x y.  so on Down the Line  Okay. Now the first thing I want to know about this is let's remember the definition of matrix multiplication, right? So if I have some Vector X let's say let's just say it has three components.  And I have some Matrix a witch as usual right in terms of its columns, but when I multiply these it better have three columns then by definition a x x is equal to the linear combination of the columns.  given by the vector X  right  Well, look at what I've written here for the orthogonal projection the orthogonal projection.  Is a why is a certain linear combination of those columns you want you to up to you pee?  so if I as a as before if I  Write you as the Matrix whose columns. Are you want you to through you pee?  Then just from the definition of matrix multiplication. This thing is equal to  That Matrix you want you to talk to you pee with those columns times the column Vector whose entries are you 1 transpose? Why you to transpose why I'm down till u p transpose why  But now let's remember that there's serve another way the common way that we actually compute matrix multiplication. If I'm taking a matrix and multiplying it by a column Vector. What I do is I multiply the row by the column right row by column to get the first entry then second row by column to come the second entry and so on.  But that's what this says here. I'm taking you one transpose, which is the first column of you written as a row and dotting it with Y and so on down the line, so what that thing actually is.  Is the vector whose Rose? Are you one transpose you to transpose down till u p transpose?  Time is the vector y.  but look this is you times you transpose x y  And that is right there. I'm here at the projection is really  The linear transformation whose Matrix is you you transpose remember at the beginning of the lecture? I said if you've got an orthonormal set of vectors, that means that you transpose you is the identity but you you transpose may not be the identity. If the vectors are there fewer vectors than an RN then you you transpose won't be the identity matrix. It's not what it is is is the orthogonal projection onto the Subspace spanned by those vectors.  If you started with an orthonormal basis for RN, then the Subspace spanned by those vectors is all of our in which means that the orthogonal projection is just the identity. If I asked you to orthogonally project this Vector, which is already in the table orthogonally projected into the table what happens to it?  It stays the same. It's already there.  Okay, if you're already in the Subspace, then you are throwing a projection in Subspace is yourself.  This is all consistent you you transpose is the identity if your vectors are an orthonormal basis of our end if your pictures are normal basis of some Subspace of R and you you transpose isn't the identity Matrix. What it is is the Matrix of the orthogonal projection into that Subspace. So that is how you compute orthogonal projection. If you want to think about them in terms of the Matrix that represents the linear transformation to Let's do an example of here before we get too far ahead of ourselves.  So here is a vector 1-2 2  And it's fans of Subspace. What's called a Subspace V?  And I want to find the orthogonal projection onto V by which I mean I want to find.  The linear transformation. I want to find the Matrix that represents this linear transformation. Well first things first, we have our formula and here we don't need to do is some of things we just go back to the original thing we'd arrive. So if I have some Vector why and I want to orthogonally projected into V. What I have to do is take while I dotted with that Vector 1 - 2 2 / the length of that factor 1-2 2 squared  time is the vector 1-2 2  and so we already computed that one that length of that Vector squared is 9 so it would I get is 1/9 of why dotted with that that Matrix there. That's a that's a column Vector it which I could write as y 1 - 2y 2 + 2y 3.  Time is the vector.  one- to two  so there's a perfectly good formula for the projection of any Vector. Why into the Subspace spanned by that vector?  But alternatively we could find the Matrix for that thing by following what we just did what we have to do. We have to find an orthonormal basis.  Find an orthonormal basis of vee-vee is one-dimensional. So the ortho part is vacuous. We don't need to find to find orthogonal vectors. There's only one vector. There doesn't need to be anything. But we do need to have the normal parts. We do need to have a unit length of vector. So that means we should replace.  The vector here, which I'll call you.  with  It's normalization which means we divide it by its length and it's length. It's length is \u221a 9 it's length is 3  1 - 2 to like that and then what we know that thing is an orthodontist places that one vector is an orthodontic basis of V.  and what we just saw the last night is that the projection onto V The Matrix of the projection of that Subspace is you you transpose where you  is the  Single column Matrix that is that normalize Vector. So that is we have to take 1/3 of 1-2 2 * 1/3 of 1-2, 2 as a row. Now you're used to writing row times column and we're doing matrix multiplication. But here we have to do call him X row that's gives us the orthogonal projection. So that might look a little funny but let's take out that 1/9. So this is a 3 by 3 Matrix and we get its entries by for example, not playing one by one and that's it.  cave the next one we get by multiplying 1 by -2  And one by 2 then next row. We have to multiply -2 by 1 - 2 by -2 and -2 by 2  and then to  -4 and 4 and that's it. That's the Matrix of the orthogonal projection onto this one-dimensional Subspace. And by the way, you can go ahead and compute now that if you multiply that matrix by the vector y 1 Y 2 y 3 you'll get exactly the formula that's written up there.  Search I'll leave that to you to verify the matrix multiplication agrees with that formula we got but that's how you compute orthogonal projection. Okay? Okay, so I'm going to leave you now in a few minutes at the end of the lecture.  slightly confused unfortunately  so why is that because  What I said is if we want to find the orthogonal projection.  Into a Subspace. The first thing we need to do is find an orthonormal basis for that Subspace.  How do you find an orthonormal basis for a Subspace?  We haven't learned how to do that. We know how to find a basis for a Subspace typically or we have a generic procedure where you have a Subspace, you know, it's spanned by sum of vectors and then you can start throwing away vectors until you find a Spanish sentence linearly independent but doing that will typically not producing or the normal basis. Now, if I if you have a Subspace as one-dimensional and orthodontist basis just means a single standing that you're with his life that you can do you find a spanning vector and you normalize it but if you have a two-dimensional or three-dimensional higher dimensional space finding an orthonormal basis that requires a new technique and we're not going to get that take me till next day. So that's going to be the topic for Monday's lecture the orthogonalisation procedure by which you can always find an orthonormal basis.  For Subspace. So for now you're just going to have to trust me that every Subspace has or thermal basis.  But before we get there.  There's one thing I want to tell you one further thing. I want to tell you which is what is orthogonal projection with described it mechanically and we described it abstractly. It's a shadow. I guess it's not that abstract. But what it means is  that you take a vertical line of a perpendicular line to your Subspace down and Slide the vector along there. Okay shortening flanks until it's into Subspace. Okay. What does that actually mean? Well this theorem tells us what orthogonal projection actually is. So if I give you a Subspace V and a vector why  The projection of Y into V what that really is is it's the vector envy that is closest to Hawaii.  So that is let's draw. Let's do this dropping down a vertical line again. Just like we did on the side where we introduced this.  so here  in this example  Is the the retinal projection of Y into V?  There are lots of vectors Envy. Let's draw some other ones.  Here's one. Here's one.  What is the distance from each of those vectors to watch I will hear the distance from this first green vectorizer to why is the length of that line right there and the distance from this other one is that line right there? Those lines are longer than this one.  That's the statement here the statement is that the orthogonal projection of Y into V. That's the vector Envy whose distance from why is minimal?  so what this really is saying is  that the claim is  that  for any vector v  Envy  let's give this a thug in a projection named. Let's call it a z like before.  Find a vector V in V.  the distance  from V  2z  Is the distance from y to V?  Is bigger than the distance from y to Z?  What is z is the thing in a V whose distance to why is smallest and that's really important because what it says is that this projection thing this Vector Z. That is the orthogonal projection of Y into V. It's the best approximation of the vector. Why in the Subspace V?  So if you think about these things in terms of how we talked about vectors on the first day of class adventure is just a list of numbers. It might be a long list of numbers like the entries in the S&P 500 505 numbers. Okay, geometrically, it's hard to understand what you would mean. We've you were talking about such a vector and projecting it except what you can say as well. Maybe what I've got is maybe I've got a model that predicts how the stocks are supposed to evolve like all of the trading firms on Wall Street 2  And my model tells me how things are supposed to evolve and then I actually look at what the numbers are. They don't quite agree with my model. So what I'm going to do is I'm going to take the real numbers and I'm going to find the best approximation that my model gives of those numbers to adjust my model for the next iteration another words. I need to know how to find the best approximation in my models parameters that I can find to the actual numbers. I need to be able to do this closest point to a Subspace thing.  This is the single most important application of linear algebra that you will find out there. If you took a statistics class we're just talking about right now is called least-squares approximation. Okay, this is very important stuff. That's what orthogonal projection actually is.  Okay, so let me finish just by so I won't prove that them that it's the shortest but it certainly is very believable from the picture that that line there. The black one in the middle is the shortest line. So let's finish just by finding the closest points.  To a Subspace of a certain Vector down here. So this examples to her. I've got a vector 1 2 3 I'm asking you find the closest Vector to that one in the Subspace spanned by those two vectors.  Hey, if you see a question like that, all it's asking you to do is to find.  the orthogonal projection into that Subspace of the vector 1 2 3  and we use the formula that we had from before you're told that this is a north and or mocha that this is an orthogonal basis. So what I do is I take the dot product of Y with that first base inspector.  Divided by the length of that first base inspector squared times that first base inspector.  And then I add to that that projection onto the second base inspector. One two, three dotted with - 211 / the length of - 211 squared x that factor - 211  Okay, and then from there you just compute those numbers and add them up and you'll get some Factor so mechanically it's a very easy, but it's important to understand what the orthogonal projection is. It's the closest point to the Subspace. Have a nice weekend.  UC San Diego podcast ",
  "Name": "math18_b00_wi18-03092018-1000",
  "File Name": "lecture_25.flac"
}