{
  "Blurbs": {
    "100 and I can see that I'm not going to get a basis for all of our three here because the second row and the third row are parallel to each other right when I now subtract when I subtract 5 times the second row from the third I'm going to get zeros there. I know I'm in row Echelon form not quite reduced row Echelon form, but I'm in ": [
      1455.3,
      1482.5,
      56
    ],
    "And then I interspersed them with the standard basis vectors in the three variables slots. That's what this algorithm does. And here's the point if I take a linearly independent set like a standard basis vectors and then I have pain some more entries in other places a pending more entries will not make them linearly dependent because if you have a non-trivial linear combination of the bigger vectors Vanishing ": [
      2170.1,
      2194.9,
      83
    ],
    "Do I listen to a podcast section 4.3 in the first 20 minutes of today's lecture so homework for is now due tomorrow by 11:59 p.m. Those last four or five questions on that homework set are things that that we will do in today's lecture related to the On your next MyMathLab homework that is already posted and it is due next Tuesday. So 1 week after that. Okay. ": [
      2.0,
      34.7,
      0
    ],
    "H is defined to be the span of those vectors. Now if those three vectors were linearly independent, then they would spend all of our 3H would actually be all of our three are these guys linearly independent. No, what do you think? What why not? Someone said no. And then I heard indiscernible mumbling. What time I'd like to raise the head. Yes. Correct. This first Spectrum on a ": [
      306.1,
      341.2,
      12
    ],
    "Hey, I ended up deciding that we could use the first two vectors there as a basis which was true, but we could have also used the second two vectors or the first in the third any in this example. Any pair of those vectors is a perfectly fine basis. And that would also be true in this example here that we are just working through. Because no two of ": [
      1695.9,
      1717.5,
      65
    ],
    "I can ask the question. Is it a basis if it's not I know what to do. I just start pruning it if it's not a basis, there's some Vector in there that the linear combination of the others. I find one of them I remove it and I keep going until I get down to a basis. Is this a bassist the ones that I get from this procedure ": [
      2005.9,
      2023.6,
      78
    ],
    "I get four. + 2 V 3 so4 + - So I'm supposed to take. Minus V1 which gives me a 3 in the first thought + 2 x V3 which gives me a 4 so minus. Positive 3 - 4 is equal to one. Is that true? Yeah, I apparently need to actually write it down to see I'm caught in the Blackboard feel even though I'm not using ": [
      1176.8,
      1213.6,
      45
    ],
    "I had swapped and made the third column the First Column of this Matrix, then I would find in that case that the first two columns were still pivotal which columns are pivotal depends on which order you put them in or which but the point is that there are some of the nonpareil columns in The Matrix the way we do our algorithm. We could have chosen those to ": [
      1766.1,
      1789.8,
      68
    ],
    "Is there a one basis that's better than another can I always find what what's what's the minimum number of elements of a basis? Maybe I'll have a basis with 76 vectors, but another one with only three vectors so clearly I want to use the one with only three vectors. Well, sorry to disappoint you but all the bases will be equally efficient or inefficient. You have any to ": [
      2243.6,
      2265.2,
      86
    ],
    "Love you, too. So, let's recombine them in terms of you wanting you to that says X1 A14 the you want in the first term + X2 a24 the you want in the second term + x 383 Those are all the things that multiply to 1. And for the second terms that multiply you to I get X1 B1. X2 V2 and x-33 okay times you too. But hey, ": [
      2676.9,
      2712.0,
      102
    ],
    "Maybe the column space is all of our three, maybe not we just have to check if I doing row reduction. So let's do some production here. Okay, when we need to get this to reduced row Echelon form, it's not so bad. It's pretty close already actually are we don't actually need to go all the way to reduce trash one for right? We only need to check which ": [
      1411.4,
      1429.7,
      54
    ],
    "So hold on the answer is yes, but not only is there a minimal there's also a maximal and they're the same every two bases. Any two faces have exactly the same number of vectors and we'll get to that in a minute. But before we get there, let's talk about the other. What about the other kind of Subspace that we've mostly been focusing on in the last couple ": [
      1830.8,
      1850.9,
      71
    ],
    "So what we find is the dimension Of the space of all polynomials is actually Infinity. Does Vector space of all polynomials is an infinite dimensional Vector space which means that a lot of the stuff we talked about today doesn't work for it. We're going to be concerned almost exclusively with finite dimensional Vector spaces in this room, so don't worry. See you again. UC San Diego podcast ": [
      3080.1,
      3108.1,
      117
    ],
    "The third is 001 the third Center basis Vector. This is what always going to happen. In fact, I don't recommend you do this, but you could memorize here is how I find the null space what once I take get the reduced row Echelon form of the Matrix. All I do is I take the coefficients read across in the Rose. I put negative signs in front of them. ": [
      2147.8,
      2170.1,
      82
    ],
    "They're both planes with the similarities and they're so the span of the original column is not the same as the span of The Columns of the wrist restaurant. Okay, that's critical here. You want to find a basis for the for the column space? You do row operations just in order to tell which Columns of the Matrix are pivotal and you use those columns in the original Matrix. ": [
      1646.0,
      1670.8,
      63
    ],
    "To the fact that V1 V2 and V3 as a basis. That's the core idea here if I have two sets of basis vectors. Then I can write down a matrix that expresses one set as linear combinations of the other. and vice versa If I have two bases with different numbers of vectors, that means that Matrix will be rectangular and a rectangular Matrix must have a non pivotal ": [
      2743.0,
      2772.5,
      104
    ],
    "Today we are going to be continuing with the stuff from section 4.2 and 4.3 basis and basis of the importance of space the common space in the null-space of a matrix. And then on to section 4.5 and 4.6 Dimension and rank. Okay, and so we aren't doing change of basis next time we're doing section 4.4 only. Which is coordinates? So that's the plan. And let's get to ": [
      36.7,
      76.4,
      1
    ],
    "U2 you want their Plus? x 3 x a3u 1 + B 3 YouTube I know this is a linear combination. There's six terms there. But if I multiply it out Andre Shuffle things I can express this as a linear combination of u and v are of you wanted you to write each. One of these terms is a scalar multiple of you one plus a scale in math. ": [
      2648.9,
      2676.9,
      101
    ],
    "What we do is we take a Spanish that we start with and we start pruning it by removing factors that are linear combinations of the others. Well, the reduced row Echelon form tells me that the non pivotal columns are linear combinations of the pivotal ones. So we'll just throw out all the non non committal columns and we won't change the column space in that context. That is ": [
      1344.7,
      1366.2,
      51
    ],
    "You don't use the columns in the registration form. Those are not related to the Subspace General. So that's the first caveat how much is written down here? Typically the column space of a on the column space if it's reduced row Echelon form are different things different substances. The other thing is there are zillions of bases like we saw on the first slide here. In this example hear. ": [
      1670.8,
      1695.9,
      64
    ],
    "a 1 x 1 plus a 2 x 2 + 83 x 3. I'm going to move by the second column by X as well. And it gives me b1x 1 + a b 2 x 2 + B 3 x 3. And what I'm told is that that is the zero Vector. So what I have is there's some non-trivial coefficients X1 X2 and X3. So that a 1 ": [
      2559.0,
      2585.9,
      98
    ],
    "a basis so you don't need to do any pruning at all. Okay, so there you go. That's how you find a basis for the Subspace on the basis for the null space again. There are zillions of they see these are not the only ones but these will be basis. So it's it's one way you can find All right, great. So now let's get back to your question. ": [
      2222.4,
      2242.1,
      85
    ],
    "a new bassist. There's zillions of base's but no matter what you choose for a basis. It's always going to have exactly two factors in it. That's the dimension of the column space. Great, let's quickly. Look at a couple more examples here. So here was a Subspace. We saw when we introduced abstract Vector spaces P3 the space of polynomials of degree less than or equal to 3 which ": [
      2937.9,
      2960.6,
      112
    ],
    "and show that that leads to a contradiction same idea here. I'm going to assume I have two bases that have different numbers of vectors in them. And just to make things concrete. I'm going to assume that I have one that has two elements and one that has three elements, but you'll see that this idea of this proof when we do it in a second is going to ": [
      2342.0,
      2359.5,
      90
    ],
    "are pivotal and therefore that's also true of the original Matrix and the other columns are linear combinations of those. Now from the context of what we're interested in right now, which is finding a basis. okay, what that tells us is that V1 V1 and V3 form a basis for the column space of a Okay, that's this procedure. We want to find a basis from a spanning set. ": [
      1308.9,
      1344.7,
      50
    ],
    "are. Okay, that's good. How many people think they are linearly dependent? One or two hands, okay with somebody who thinks they're linearly independent like to explain why they hit their linearly independent. Yes. Fantastic, perfect explanation each one of the vectors has a one in a place where the other two have a zero and that means you're just not going to be able to make a non-trivial linear ": [
      2085.6,
      2115.9,
      80
    ],
    "arms in my head in this plane. Take that plane is found by any two of those factors. So it's inefficient to include all three in a minimal description. So a basis is a set of vectors that span the space and it's a minimal set that's been to space in the sense that it's also linearly independent. No, one of the basis vectors is already in the span of ": [
      194.6,
      216.6,
      7
    ],
    "basis for a Subspace? They must always have the same number of vectors in them. In other words. I said there are zillions of basis for any space. But there's one thing that's common among them. All of them have the same number of vectors in it. That's up there. I'm here and I want to actually prove that there are more. I want to prove a special case of ": [
      2265.2,
      2284.5,
      87
    ],
    "be a Subspace of a bigger Vector space but one way or another by a vector space the dimension of vector space is the number of vectors in any given basis for it. We just saw that if you have two different bases. They will have the same number of actors in them. So this is a well-defined quantity the dimension of a Subspace or a vector space is the ": [
      2844.4,
      2867.5,
      108
    ],
    "be pivotal if we pre-ordered things around so it could well happen that we could have chosen other vectors from Among The Columns to find a basis. I'm not recommending you do that. I just want you to understand that it's not the case that we can say. This is the basis. This is just one basis. But this is a good basis. You can always use this one. So ": [
      1789.8,
      1809.6,
      69
    ],
    "by checking that it is closed under those operations and that the sum of two vectors in h, which is a vector in the largest space V is actually back in Action same with the other multiples. typically We found that. Subspaces are presented to us as the span of some set of vectors every Subspace we've seen actually is of that form either presented to us that way or ": [
      103.9,
      129.7,
      3
    ],
    "calling them next to a next one. Know that will change how we describe the solution set. But in terms of questions, like are these vectors linearly independent are they expanding set doesn't address those questions. It doesn't change those questions at all. So we could have reordered these vectors just as well and gotten the same answer insurance with the column space is and you can check that if ": [
      1742.6,
      1766.1,
      67
    ],
    "came from the two elements and and the three elements in the two bases. What do we know about rectangular matrices with a lots of things but in particular we know that if I have more columns than Rose than those columns can't be linearly independent. Right, we know that if I take if I compute the registration form of this Matrix, one of those columns, at least one of ": [
      2475.8,
      2498.7,
      95
    ],
    "check if those remaining four columns are linearly independent and it's no longer true that you could obviously see that one of them is a multiple of another one. That's not true anymore. But that just means that no pair of parallel but doesn't mean that you couldn't have three of them that are linearly in linearly dependent even though no two of them are right, but we have a ": [
      939.7,
      960.8,
      36
    ],
    "column actress where those are the colors of the Matrix. But that might not be the most efficient description. Because there can be repetitions. Hey are they can be redundancy in that collection of information. For example, if I tell you the span of the vectors 111 + 111 + 111, that's kind of silly. Right? Well, that's just a span of the vector 111 I could also said the ": [
      147.0,
      174.2,
      5
    ],
    "columns are pivotal to do that. We only have to go to any row Echelon form. That's not going to take too much work here because I've already got a zero in the second row below the pivotal one. So I'm going to subtract twice the first row from the Third. Can you give me a zero there a -5 there? and -5 there looks like 0 - 1 - ": [
      1429.7,
      1455.3,
      55
    ],
    "combination of them equal to 0 Let's look at that more closely. so let's look at these entries these entries NBA centuries Okay, if I look at the entries that correspond to the three variables to four and five in this case and I look at only those entries than the first Vector is 100. It's the first time the second letter is 010. It's the second Saturday suspect Iran. ": [
      2115.9,
      2147.8,
      81
    ],
    "couldn't have less than two either for the same reasoning because if you had less than two than that first when you started with with two vectors, that would be linearly dependent by the same argument in Reverse. Okay. So this is the key factor here at the key fact of linear algebra for that medicine probably the most important there in one ear out for any two bases have ": [
      2798.0,
      2818.6,
      106
    ],
    "definition means is something a linear combinations of three vectors and we found that any such arbitrary a is actually in the span of u and v so in fact each is also equal to the span of just you and me. So we can eliminate one of those vectors. Okay know by the way, we had some choices here. We didn't have to eliminate W. Cuz I could have ": [
      518.2,
      546.5,
      19
    ],
    "down the solutions that which will do on the next flight when we talked about the null space, but they tell us something critical in the column space setting as well. So we already noted from the original Matrix that V2 is -2 times we want. But notice that we can see that immediately in the reduced row Echelon form, right? So if you look at it this -2 in ": [
      1009.7,
      1034.6,
      39
    ],
    "entries there. Actually, I should use different color so that the second one is to first one. I'm going to color orange as well because what that first entry there is telling us is that I can find a 4 as a linear combination of the previous columns. That's what the reduced row Echelon form tells us. It tells us look for phone to get a for I can build ": [
      1060.1,
      1082.3,
      41
    ],
    "find a linearly independent Spanish that how can we do that? Well, we need to yes. You were just raising your arm. Just had a muscle cramp in. Okay, that's really cool. Hope it gets better. So. Well, we we don't have to start over and throw these away we're going to find there is a bassist hiding in there hiding in plain sight and here's here's the idea. What ": [
      368.0,
      395.6,
      14
    ],
    "find some spanning set for it. Typically that's done for you. By the way. The Subspace is presented or will be easy to infer from some techniques. We've already learned as we'll see in the coming slides. Once you have a spanning set doesn't thing you need to do is to prune it. Okay, you need to say okay now go to spanning set. Is it a basis check? As ": [
      687.6,
      708.6,
      26
    ],
    "for the column space you throw away. The non total columns the pivotal Columns of the Matrix a form a basis for the column space of a okay. So that's a restatement of what we just said, so let's do another example here just to cement that so here is a 3 by 3 Matrix 3 columns in R3. So maybe maybe these call him spend all of our three. ": [
      1385.7,
      1411.4,
      53
    ],
    "form a basis. Why do they form a basis? Because they are linearly independent and they are a spanning set. So by definition he rages the sound of those vectors and those two those two are linearly independent. Normally, we would have to check that by doing row reduction. But this is another case where we have two vectors. It's easy to check is one a scale multiple. The other ": [
      637.1,
      662.0,
      24
    ],
    "form as the coefficient Matrix of the system. Tell us that x 1 is equal to 2 x 2. + x 4 - 3 x 5 I'm at x 3 is equal to -2 x 4 + 2 x 5. It's a homogeneous system. So we imagine we have zeros on the other side and we subtract in order to get that. And then the standard thing we do know ": [
      1923.2,
      1948.2,
      75
    ],
    "generalize immediately to if I have 76 + 1 + 74 in the other. Okay doesn't matter if those two numbers are unequal the same idea here is going to show that that So here's the here's the idea here suppose. I have two bases for the space you want you to is 1 basis and you V1 V2 V3 is another now, what does it mean that you wanted ": [
      2359.5,
      2380.6,
      91
    ],
    "guys names. Let's call him V1 V2 V3 V4 and V5 answer the proposal is to now use V1 V3 V4 and V5 to those form a basis for H. Know why not? Okay, so you're at not talking about pivot. So you're jumping ahead that which is good. That's what we're going to say next. But the key Point here is what we need to check. We need to ": [
      908.6,
      939.7,
      35
    ],
    "have a Subspace, there's not all of our three. Then there's a trick that it seems trickier. How do we even find a basis it? What's the generic principle for finding a base as well? We can always just revert back to the definition and say a basis is a linearly independent spanning set. So the first thing I need to do is find a spanning set find a set ": [
      260.3,
      283.7,
      10
    ],
    "if you're asked to find a basis for the column space, this is what you should do. Yes. What a great question. I swear I didn't plant them in the audience because we're going to go into that on the next slide question was is there a minimum number of vectors that you need to form a basis? And that is a phenomenal question actually one more slide after that. ": [
      1809.6,
      1830.8,
      70
    ],
    "in every row and every column we saw that had to be the case so you can see from that. But if you're looking for a basis of our three, it has to have three vectors you have to have three columns and not just any three columns will do they have to be The Columns of an invertible. Matrix would be another way to put it but if we ": [
      244.7,
      260.3,
      9
    ],
    "in you have to check if it's linearly independent. If not by definition. That means at least one of those vectors is in the span of the others find which one it is or which one of the ones it is and remove it. And now you have a smaller set of vectors. Is it a basis check check if that's plenty of the independent if it's not find one ": [
      708.6,
      728.2,
      27
    ],
    "is definitely linearly dependent. That's not linearly independent. So following our nose following are pruning procedure that we talked about in the last life. What we should do is throw away that doctor if we throw away that Vector it's not going to change the Subspace H. The Subspace H's going to be spanned by the other four vectors just as well. Okay, great. So how about let's give these ": [
      886.3,
      908.6,
      34
    ],
    "is that set linearly independent or not going to give you Thirty Seconds to Talk Amongst yourselves and decide go ahead. All right. So sounds like you're having an active discussion. I see some people have a really pained expression on their face. So let me end your misery and let's just take a vote how many people think that these are linearly independent. Even the paint guy thinks they ": [
      2023.6,
      2085.6,
      79
    ],
    "it out of the previous columns A1 A2 and A3 by taking - 1 * a 1 + 2 x A3 because they wanted it a three they they're standard basis vectors and that's just how vectors work if you want to decompose the vector ex why you think positive x x 1 0 + y x 0 1 so that second that the equation there there's not equation. There's ": [
      1082.3,
      1107.0,
      42
    ],
    "it really doesn't matter which factors you use you could use the standard basis vectors, but I could just as well have chosen to of those. Okay, I could take a 2 by 3 Matrix. maybe it's something like this one here. So look at the the first two vectors are 102 and 3 - 1 1. now if I do row operations on just those two vectors Okay, are ": [
      1566.7,
      1596.7,
      60
    ],
    "it's just a factor out the free variables and express this as x 2 x a vector and that doctor will be to 1000. plus x 4 times a vector which is in this case 1 0 - 2 1 0 plus X 5 X Factor. which is -3 / 0 201 okay, so there's a perfectly good description of the null space. It's the set of linear combinations of ": [
      1948.2,
      1981.4,
      76
    ],
    "it. So just a reminder from last lecture, we gave this definition this new definition of basis of a Subspace. What's the idea here? a Subspace is a subset of a vector space that is itself a vector space in its own right meaning that in the same operations addition and scalar multiplication using the same as zero. It forms a vector space. Are you going to confirm that just ": [
      76.4,
      103.9,
      2
    ],
    "know we have a zero in one spot in the one in the other that's you're never going to have linear dependence in that case. Unless one of us is actually the zero vector. So this is a linearly independent spanning set. And therefore it is a basis. And that's exactly how you find a basis in general gave you given a Subspace. First thing you need to do is ": [
      662.0,
      687.6,
      25
    ],
    "linearly independent because I cannot find a numerical linear combination of 1 and X that is equal to 0 I can't say Oak 2 x + 3 that's equal to 0 I mean it is equal to 041 choice of X, but these are abstract Vector. These are the basic object. There's no numbers A&B for which a x + B is always equal to zero those are linearly independent ": [
      2999.8,
      3024.5,
      114
    ],
    "look at x 1 V 1 + x 2 V 2 + x 3 V 3. What's this compute with that thing is well. I have some equations up here. But can that I can use to express the V's in terms of the use. So let's do that here. So this is x 1 x A1 you one plus B1 U2 plus x 2 x A2 YouTube + B2 ": [
      2613.4,
      2648.9,
      100
    ],
    "may as well only look at the reduced row Echelon form. We have are pivotal columns there. That means that x 3 X1 and X3 are going to be expressed in terms of the other columns, right? We're going to have three free variables x 2 x 4 and x 5 are free variables. And then the two non-trivial equations that we get when we interpret. They reduced row Echelon ": [
      1896.4,
      1923.2,
      74
    ],
    "no equation for the fourth column there manifestly shows us that a 4 is equal to - A1. + 2 a 3 Okay, and similarly? We see that the fifth column. a 5 is equal to 3 A 1 - 2 a 3 Okay, so that's what we see from the registration form of the Matrix. But here's the thing. Look over at the original Matrix now a we already ": [
      1107.0,
      1143.1,
      43
    ],
    "noted. That V2 is equal to -2 V 1. I know I claim that we can also see and now check if you like that the V4 is equal to - the OnePlus 2 V3 and V5. Is equal to 3 V 1 - 2 B 3. Let's check that second one before equation V4 is supposed to be minus V1 plus to V3 case if I take - V1 ": [
      1143.1,
      1176.8,
      44
    ],
    "number of vectors in any basis for that space. So for example here is a vector space. It's the span of these three vectors in R3. This is the exact an example. We saw a few size ago, and we saw that these two columns in the reduced row Echelon form are the pivotal columns and the third one is non pivotal so has to Total columns. Now, we saw ": [
      2867.5,
      2895.5,
      109
    ],
    "of lectures, space and null space? So if I give you a matrix it's no space is also a Subspace. Its a Subspace not of the codomain but if the domain so here are my same Matrix a the 3 by 5 Matrix, that means that it's no space is a Subspace of R5. It's a set of vectors which you can multiply 8 by which means that they have ": [
      1850.9,
      1872.7,
      72
    ],
    "of the vectors is linearly dependent on the others remove it and it'll write this procedure and this procedure will eventually stopped right because those vectors there spam the Subspace which is nonempty So eventually you're going to get down to a set where there are no more vectors that are already in the span of the others. You will get down to a linearly independent set. I should add ": [
      728.2,
      749.0,
      28
    ],
    "of those 5 vectors in R3 the column space of H H is equal to the column space of the Matrix a definition of the column space of a matrix is defined to be the span of its columns. So by definition there is a spanning set. No, I'd like to find a basis for the column space. So the first question is maybe that's the basis right there. Maybe ": [
      821.1,
      854.6,
      32
    ],
    "of vectors that spans the Subspace and once I do that check if it's linearly independent. Well, okay. So the first part is often done for us where the Subspace is presented at the span of vectors. But what about the second part I can check if they're linearly independent, but what do I do if they're not so let's look here's an example. Here are three vectors in R3 ": [
      283.7,
      306.1,
      11
    ],
    "one plus be to you too. and V3 is equal to A3 you 1 + B3 you two, okay each one of the vectors V1 V2 and V3 because they're in the Subspace. They must be expressible as linear combinations of the basis vectors you want in YouTube. No. consider the Matrix A1 A2 A3 B1 B2 B3 Hey, this is the moment not much to do with the original ": [
      2411.5,
      2452.8,
      93
    ],
    "or highlight two important caveats? The first is that and I already said this a moment ago. It's really important that we use. The Columns of the original Matrix a row operations, they they preserve the linear relationships between the columns, but they mess up the columns. I think about it this way. If I had a matrix whose columns were all pivotal 3 by 3. Matrix is called. Then ": [
      1516.2,
      1543.8,
      58
    ],
    "rearranged this to write that you is equal to W + V and I could have done the same thing removing the you from consideration and found that this is also equal to the span of via W and similar be a sequel to the fan of you and W I could actually chosen any through any two of these factors, but we have some choices to make that highlights ": [
      546.5,
      566.3,
      20
    ],
    "reduced row Echelon form and you are the five columns of the original Matrix. Now in the reduced row Echelon form we can see that there are only two pivotal calls A1 and A3 are the pivotal columns and the others correspondent free variable. The others are non pivot. What does it really mean that the course 1 2 3 variables will we know what that means if we're riding ": [
      985.6,
      1009.7,
      38
    ],
    "reduced row Echelon form. You get the standard basis Vector now those two vectors 100 and 010. Those are the standard. Those are the standard basis vectors in the X and Y directions In the three dimensional space so the span of those vectors is the XY plane. But the original vectors 102 and 3-1 one, right? They spend a totally different plant. Those two planes kind of look alike. ": [
      1617.9,
      1646.0,
      62
    ],
    "row Echelon form and I can see. That these two columns are pivotal, but the third one is not which means the same holds true in the original Matrix, which means if we want to find a basis of the column space. those first two columns 1 0 2 and 3 - 1 1 form a basis for the column space today. zz's. Now what am I to important caveats ": [
      1482.5,
      1516.2,
      57
    ],
    "row or column. But that's going to mean that that the larger basis will be linearly dependent because of this calculation. That's the core idea here. You can't if you have one basis with two vectors then any set of three or more vectors will automatically be linearly dependent. That's what this shows. So you can never have more than two more than two vectors in a basis, but you ": [
      2772.5,
      2798.0,
      105
    ],
    "same Subspace know, what about those two vectors? the vectors you envy Okay, do those to form a basis? I heard someone say no or maybe they said yes, it was too quiet to hear. So how about by a show of hands how many people think those to form a basis? Must be funny people think they don't form a basis. Nobody, okay. So that's correct. These two vectors ": [
      604.3,
      637.1,
      23
    ],
    "second Vector equals 2/3. So these guys are not linearly independent, right? So if I call this you this V and this W, then what we see is that you minus V equals W. So there is a non-trivial linear combination of those equaling 0 u - b - W equals 0 so it's not linearly independent the spanning set, but it's not linearly independent. So, okay. We wanted to ": [
      341.2,
      368.0,
      13
    ],
    "span of the vector 1-1-1 in the vector 2 2 2 again pretty silly because the second Vector is already in the span of the first stuff about the span of those two vectors. It's really just the stand of either one of them. So it's inefficient to have those redundancies. There might not be as obvious stuff. I have three factors. For example, the three factors that are my ": [
      174.2,
      194.6,
      6
    ],
    "technique or checking. This means we do. Great, and we don't have to do the reduction that love that it for us. So there's the reduced row Echelon form of the Matrix. Let me highlight this now that we've already discussed this idea and used it before but let me make this a little more precise in this example to cement it. So here are the five columns of the ": [
      960.8,
      985.6,
      37
    ],
    "that V1 V2 V3. Is another Subspace for it? So here's the idea. I'm going to give a proof by contradiction already discussed this a lot. How do you how are you going to prove something like linear Independence for some abstract vectors linear Independence is a negative property. So typically he's going to prove by contradiction saying I want to prove their linearly independent. Let's assume that they're not ": [
      2315.9,
      2342.0,
      89
    ],
    "that if you want to find a basis a basis for the column space of that Matrix, you can choose the pivotal columns as a basis. There are two pivotal Collins there that tells you that this Subspace the column space of that Matrix has Dimension 2 is a two-dimensional Subspace. It's a plane in R3 Any other basis you find you could in this case you could use any ": [
      2895.5,
      2919.4,
      110
    ],
    "that it will also be an auntie really a combination of the the components that correspond to the standard basis vectors here and we know that can't happen. So we get that these guys are linearly independent. So that's always going to happen following this procedure. For finding the null-space you're going to get a you're going to get us panning set and that's benefit is always going to be ": [
      2194.9,
      2222.4,
      84
    ],
    "that set of 5 vectors is a basis. What do you think? Is it a basis? Y'all know why not someone in the back. I heard some voices from the back so You're not in the back for that shirt you're in the back or is that your arm cramping? Perfect. The second Vector is manifestly twice - 2 * the first Factor, so it's definitely the set of vectors ": [
      854.6,
      886.3,
      33
    ],
    "that them that will contain the whole idea of the proof and why it's true so suppose. Let's do it with where I finally have a basis with two elements. Okay, I should really do this for a basis with n elements, but I'm going to stick tattoo to make the notation easier suppose that you won you too. Is a basis. for a Subspace pitch and let's also suppose ": [
      2284.5,
      2315.9,
      88
    ],
    "that's what we see here. So you can pick off from the reduced row Echelon form what their linear dependencies between the three columns on the pivotal columns are and those are exactly the same relationships we have over on the left. So that's the reason why we can just go ahead and say well hey in the reduced row Echelon form. I see that the first and third columns ": [
      1288.7,
      1308.9,
      49
    ],
    "the caveat that this only works if you're talkin about finally many factors, but in our in this room were talking mostly about Vector spaces that have a basis of finitely many vectors. Okay. We'll see in a few minutes. That's not always the case. Okay, so I called that a corollary, but I should really call it a theorem cuz we didn't stayed up there on yet that says ": [
      749.0,
      773.1,
      29
    ],
    "the column space to expand just by the pivotal columns. And the pivotal columns those are linearly independent because as we know if we're going to check their linearly independent, what would we do we would do row reduction and see if they were pivotal by definition their pivotal or Not by definition by that theorem their pivotal. So that's how we do this. You want to find a basis ": [
      1366.2,
      1385.7,
      52
    ],
    "the fact that basis is not going to be unique by any means, but the point is that I can throw away one of those vectors. And still have a spanic set. That's what's always going to happen. What we just seen here is the key idea that if you have a linearly dependent set you can always throw one of them away without changing the Span. In fact, if ": [
      566.3,
      586.5,
      21
    ],
    "the others. So it's a linearly independent spanning set. So that's a definition and we saw several examples last day of things that were bases and things that weren't they see as we saw for example, the theorem prove the theorem that he looking for a basis of our three say they are three then what you're looking for are three columns who's Matrix of those columns has a pivot ": [
      216.6,
      244.7,
      8
    ],
    "the reduced row Echelon form of a the span of those columns on the right is totally different from the stand of the columns on the left. But what chant what doesn't change is the relationship between the Cults? The three row operations that we can use in sequence to get from a 2. It's reduced row Echelon form the row operations. They preserve linear relationships between the columns and ": [
      1268.2,
      1288.7,
      48
    ],
    "the same number of elements and that gives us a key fact hear a key new word and to add to your list of vocabulary to let you know this word before you've been you you heard it a lot. We've used it already. But now we're making it precise. The word is dimension. So if I have a vector space it could be an abstract Vector space. It could ": [
      2818.6,
      2844.4,
      107
    ],
    "the the Blackboard. This is a field that emanates from the black boy that makes you stupid. Okay, so I may just come over here with you guys at. write -5 - 3 + 2 * -1 Is -2 + 3 is 1 - 1 + 2 * 2 is 4 minus 1 is 3 + - 2 + 10 is 8. Okay. So there we go. Yes was just ": [
      1213.6,
      1244.9,
      46
    ],
    "there are free variables there because there aren't enough columns to go around Rose to go around to make all the columns pivotal. So let's write out explicitly what this says it. What's 8 * x + 8 * X Is equal to okay. Well, I know how to multiply matrices. Let's remind ourselves. So I'm going to buy that First Column by x + 3 by entry and get ": [
      2532.3,
      2559.0,
      97
    ],
    "there x 1 + x 3 + x 2 - x 3 they're not the same coefficients we had before but they're just too real coefficients in the point. Is that this thing? Is in the span? I've just the vectors u and v it's a linear combination of the vectors u and v. So what we started here with his take an arbitrary Vector A and H. which by ": [
      497.0,
      518.2,
      18
    ],
    "this so this is a theorem. If I have a Subspace H Subspace of some bigger Vector space. Okay, so we got some bigger is defined to be the span of some collection of vectors. He wants space. then some collection Among Us vectors is a basis a subset of that set of vectors is a basis. Just what I just said, you just proved it until you get down ": [
      773.1,
      799.4,
      30
    ],
    "those columns is going to be non pivotal. But we know then is that. So there is some. non-trivial Vector X1 X2 X3 such that if I take this Matrix here a and this is the vector X such that a x x equals 0. But that's what we know we know that there is a non-trivial solution to the homogeneous system with that Matrix as it's coefficient Matrix because ": [
      2498.7,
      2532.3,
      96
    ],
    "those three vectors in other words. It is the span. Of those three vectors. Let me call them A B and C. So I have to rewrite them over here. So the the algorithm that we use the road action algorithm that we used to solve systems of equations it automatically produces for us a spanning set for the null space. So I've now got a spanning set. And so ": [
      1981.4,
      2005.9,
      77
    ],
    "those two vectors linearly independent? Yeah, we know they are because they are pivotal in this Matrix here. Also, they're not parallel. We can just see that if I were to carry the Matrix consisting of just those first two columns to reduced row Echelon form. I would get one zero zero zero one zero that's what you always get. If you have linearly independent vectors you get in the ": [
      1596.7,
      1617.9,
      61
    ],
    "those vectors are parallel. So think about it this way. We don't do column operations when we're doing real operations. We don't swap columns, but would swap column is me swapping columns means that we're renaming the variables like if we're thinking of the Matrix if the coefficient Matrix of a system of linear equations, then swapping two columns means instead of calling a few variables X1 and X2. I'm ": [
      1717.5,
      1742.6,
      66
    ],
    "to a basis. So let's look at a concrete example now relevant to your homework into our understanding of the class. So the most important kind of subspaces we deal with here are so far are the column space in the null-space of a matrix structure the same one we had last lecture and use it several times this lecture. This is a 3 by 5 Matrix its column consists ": [
      799.4,
      821.1,
      31
    ],
    "to have number of components equal to the number of columns of a okay. So and we know how to find this. This is what we've been doing since literally lecture one. So the null-space of a Can be described as a set of vectors X1 X2 X3 X4 and X5. Such that. Okay. We look at the registration form. We have the same thing. Now we can now we ": [
      1872.7,
      1896.4,
      73
    ],
    "to some linear combination of those three vectors was called x1u + x 2 V + x 3 w. for some X1 X2 X3 In a scalar or combination that's what it means to say that age is the span of those vectors everything in h is a linear combination of vectors, but we've also got this relationship down here. So let's substitute that in and say well actually, let's ": [
      424.5,
      460.4,
      16
    ],
    "tree right there. That -2 entry tells you that a 2. Is equal to -2 times I want because a one is just the first standard basis vectors just 100. So if you want to see if something is a multiple of that you just have to check the first entry and indeed. That's true. Now. Let's look at a for a Forza non-committal column and it has two non-zero ": [
      1034.6,
      1060.1,
      40
    ],
    "turns out we can figure out it looks like that in a few minutes. We will be able to prove that that is a theorem. Every Subspace is the span of some set of vectors. Stephen was not presented. That way you can always find some set of vectors this planet. For example, the column space of a matrix is presented to you as the span of this set of ": [
      129.7,
      147.0,
      4
    ],
    "two of The Columns that Matrix and that will be a basis you could also do all sorts of other things. You could take three times the First Column and 76 times the second column that's not going to change the fact that there are basis. You could also mix them up and add two of them together and subtract the other two that will also work and give you ": [
      2919.4,
      2937.9,
      111
    ],
    "vectors. Remember those vectors, they might be abstract vectors and some abstract Vector spaces might be polynomials or something like that. They're not columns typically has no Matrix associated with them, but I can take those coefficients that I just found and I can make a matrix out of this rectangular Matrix on the key thing about this rectangular is a 2 by 3 Matrix the two in the three ": [
      2452.8,
      2475.8,
      94
    ],
    "vectors. So that means that the dimension of P3 Is 4 because there is a basis with four vectors in it. So last quick example here. How about that Vector space of all polynomials? What's the dimension of the vector space of all polynomials? Well, let's see if we can write down a basis everything in there. Looks like you know, there could be a constant a next term and ": [
      3024.5,
      3053.6,
      115
    ],
    "verify that that same relationship holds but we don't need to do that ever again thankfully because that's that's exactly how Ro operations work. That's the point row operations. They mess up the columns. Can you change the columns and you change the column space to be clear and we'll highlight this in a moment. The column space of a is a totally different Subspace in the column space of ": [
      1244.9,
      1268.2,
      47
    ],
    "wait a minute. Look at the columns or look at the Rose. Over here and look at these coefficients. Those coefficients that came up there. Those are the rose. Those are the entries of this Matrix ax but those are zero. And so this is the zero Factor. So what if I got I've got a non-trivial linear combination of the bees that equal 0 and that is a contradiction. ": [
      2712.0,
      2741.6,
      103
    ],
    "we know that that forms the basis of our three we saw that last time okay, but the reduced row Echelon form of any such Matrix is the identity Matrix. Okay, so that means that those vectors are the standard basis vectors on the right. Those are very different from the original vectors. Now, this is actually a poor example because when you have a basis for the whole Space, ": [
      1543.8,
      1566.7,
      59
    ],
    "we want to do is reduce the set of vectors by throwing away some that are in the span of the others. So let's look at this again. So if any Vector ex is in HEX is a portrait cuz when he was asked for the coefficients, so let's call it a if any Vector a is in H. Then by definition of span that means that a is equal ": [
      395.6,
      424.5,
      15
    ],
    "were polynomials of the form a x cubed. + b x squared + CX + d Hey, we're a b c and d. We're arbitrary real number that are free variables here. So what that means? Is that this is the span? of the vectors x cubed x squared x + 1 There is a spanning set for that Subspace consisting of those for polynomials and those for polynomials are ": [
      2960.6,
      2999.8,
      113
    ],
    "write this as x1u + x 2 V + x 3 x w is equal to you minus V. I know let's recombined terms is actually only two vectors present their they're just repeated. So the you appears in two of those terms, let's collect them. So this is x 1 x. Well 1 + x 3 * you + x 2 - x 3 x v those new coefficients ": [
      460.4,
      497.0,
      17
    ],
    "x 1 + 8 2 x 2 + 83 x 3 is 0 and simile be 1 x 1 + b 2 x 2 + B 3 x 3 is 0 Now, how does that help me? What does that have to do with these original vectors? Well, let me just sort of. Pull out of thin air the following. Let's look at those coefficient for Sig those X's. And ": [
      2585.9,
      2613.4,
      99
    ],
    "x squared term in X cubed term eventually it terminates. But if you want to have all polynomials you want to have all possible polynomials, then you have to have terms of arbitrary degree. Any polynomial will be a linear combination of only finitely many of those but if you want to have an arbitrary polynomial, then you have to have all of those Powers there. They're all linearly independent. ": [
      3053.6,
      3080.1,
      116
    ],
    "you remember the definition of linear dependence, one of the two equivalent definitions was that at least one of the vectors is in the span of the other two. So indeed here W was in the span of the other two what that means and what this calculation shows is that if we throw that one away the other two still stand the same set that was there before the ": [
      586.5,
      604.3,
      22
    ],
    "you to as a basis it in particular means that they are expanding set. So that means that V1. Can be written as some A1 you one plus B1 YouTube. Okay, everyone is in the span of you want and you too if you want is a vector in the Subspace and therefore it's in the span of the basis vectors. also V2 is equal to sum a to you ": [
      2380.6,
      2411.5,
      92
    ]
  },
  "Full Transcript": "Do I listen to a podcast section 4.3 in the first 20 minutes of today's lecture so homework for is now due tomorrow by 11:59 p.m. Those last four or five questions on that homework set are things that that we will do in today's lecture related to the  On your next MyMathLab homework that is already posted and it is due next Tuesday. So 1 week after that. Okay.  Today we are going to be continuing with the stuff from section 4.2 and 4.3 basis and basis of the importance of space the common space in the null-space of a matrix.  And then on to section 4.5 and 4.6 Dimension and rank.  Okay, and  so we aren't doing change of basis next time we're doing section 4.4 only.  Which is coordinates?  So that's the plan.  And let's get to it.  So just a reminder from last lecture, we gave this definition this new definition of basis of a Subspace. What's the idea here?  a Subspace  is a subset of a vector space that is itself a vector space in its own right meaning that in the same operations addition and scalar multiplication using the same as zero. It forms a vector space. Are you going to confirm that just by checking that it is closed under those operations and that the sum of two vectors in h, which is a vector in the largest space V is actually back in Action same with the other multiples.  typically  We found that.  Subspaces are presented to us as the span of some set of vectors every Subspace we've seen actually is of that form either presented to us that way or turns out we can figure out it looks like that in a few minutes. We will be able to prove that that is a theorem. Every Subspace is the span of some set of vectors. Stephen was not presented. That way you can always find some set of vectors this planet. For example, the column space of a matrix is presented to you as the span of this set of column actress where those are the colors of the Matrix.  But that might not be the most efficient description.  Because there can be repetitions.  Hey are they can be redundancy in that collection of information. For example, if I tell you the span of the vectors 111 + 111 + 111, that's kind of silly. Right? Well, that's just a span of the vector 111 I could also said the span of the vector 1-1-1 in the vector 2 2 2 again pretty silly because the second Vector is already in the span of the first stuff about the span of those two vectors. It's really just the stand of either one of them. So it's inefficient to have those redundancies. There might not be as obvious stuff. I have three factors. For example, the three factors that are my arms in my head in this plane.  Take that plane is found by any two of those factors. So it's inefficient to include all three in a minimal description. So a basis is a set of vectors that span the space and it's a minimal set that's been to space in the sense that it's also linearly independent. No, one of the basis vectors is already in the span of the others.  So it's a linearly independent spanning set.  So that's a definition and we saw several examples last day of things that were bases and things that weren't they see as we saw for example, the theorem prove the theorem that he looking for a basis of our three say they are three then what you're looking for are three columns who's Matrix of those columns has a pivot in every row and every column we saw that had to be the case so you can see from that. But if you're looking for a basis of our three, it has to have three vectors you have to have three columns and not just any three columns will do they have to be The Columns of an invertible. Matrix would be another way to put it but if we have a Subspace, there's not all of our three.  Then there's a trick that it seems trickier. How do we even find a basis it? What's the generic principle for finding a base as well? We can always just revert back to the definition and say a basis is a linearly independent spanning set. So the first thing I need to do is find a spanning set find a set of vectors that spans the Subspace and once I do that check if it's linearly independent. Well, okay. So the first part is often done for us where the Subspace is presented at the span of vectors. But what about the second part I can check if they're linearly independent, but what do I do if they're not so let's look here's an example. Here are three vectors in R3 H is defined to be the span of those vectors.  Now if those three vectors were linearly independent, then they would spend all of our 3H would actually be all of our three are these guys linearly independent.  No, what do you think? What why not? Someone said no.  And then I heard indiscernible mumbling.  What time I'd like to raise the head. Yes.  Correct. This first Spectrum on a second Vector equals 2/3. So these guys are not linearly independent, right? So if I call this you this V and this W, then what we see is that you minus V equals W. So there is a non-trivial linear combination of those equaling 0 u - b - W equals 0 so it's not linearly independent the spanning set, but it's not linearly independent.  So, okay. We wanted to find a linearly independent Spanish that how can we do that? Well, we need to yes.  You were just raising your arm. Just had a muscle cramp in. Okay, that's really cool. Hope it gets better. So.  Well, we we don't have to start over and throw these away we're going to find there is a bassist hiding in there hiding in plain sight and here's here's the idea. What we want to do is reduce the set of vectors by throwing away some that are in the span of the others. So let's look at this again. So if  any Vector ex is in HEX is a portrait cuz when he was asked for the coefficients, so let's call it a  if any Vector a is in H.  Then by definition of span that means that a is equal to some linear combination of those three vectors was called x1u + x 2 V + x 3 w.  for some X1 X2 X3  In a scalar or combination that's what it means to say that age is the span of those vectors everything in h is a linear combination of vectors, but we've also got this relationship down here.  So let's substitute that in and say well actually, let's write this as x1u + x 2 V + x 3 x w is equal to you minus V.  I know let's recombined terms is actually only two vectors present their they're just repeated. So the you appears in two of those terms, let's collect them. So this is x 1 x. Well 1 + x 3 * you  + x 2 - x 3 x v  those new coefficients there x 1 + x 3 + x 2 - x 3 they're not the same coefficients we had before but they're just too real coefficients in the point. Is that this thing?  Is in the span?  I've just the vectors u and v it's a linear combination of the vectors u and v. So what we started here with his take an arbitrary Vector A and H.  which by definition means is something a linear combinations of three vectors and we found that any such arbitrary a is actually in the span of u and v  so in fact  each is also equal to the span of just you and me.  So we can eliminate one of those vectors.  Okay know by the way, we had some choices here. We didn't have to eliminate W. Cuz I could have rearranged this to write that you is equal to W + V and I could have done the same thing removing the you from consideration and found that this is also equal to the span of via W and similar be a sequel to the fan of you and W I could actually chosen any through any two of these factors, but we have some choices to make that highlights the fact that basis is not going to be unique by any means, but the point is that I can throw away one of those vectors.  And still have a spanic set. That's what's always going to happen. What we just seen here is the key idea that if you have a linearly dependent set you can always throw one of them away without changing the Span. In fact, if you remember the definition of linear dependence, one of the two equivalent definitions was that at least one of the vectors is in the span of the other two. So indeed here W was in the span of the other two what that means and what this calculation shows is that if we throw that one away the other two still stand the same set that was there before the same Subspace know, what about those two vectors?  the vectors  you  envy  Okay, do those to form a basis?  I heard someone say no or maybe they said yes, it was too quiet to hear. So how about by a show of hands how many people think those to form a basis?  Must be funny people think they don't form a basis.  Nobody, okay. So that's correct. These two vectors form a basis. Why do they form a basis?  Because they are linearly independent and they are a spanning set. So by definition he rages the sound of those vectors and those two those two are linearly independent.  Normally, we would have to check that by doing row reduction. But this is another case where we have two vectors. It's easy to check is one a scale multiple. The other know we have a zero in one spot in the one in the other that's you're never going to have linear dependence in that case. Unless one of us is actually the zero vector.  So this is a linearly independent spanning set.  And therefore it is a basis.  And that's exactly how you find a basis in general gave you given a Subspace. First thing you need to do is find some spanning set for it. Typically that's done for you. By the way. The Subspace is presented or will be easy to infer from some techniques. We've already learned as we'll see in the coming slides. Once you have a spanning set doesn't thing you need to do is to prune it.  Okay, you need to say okay now go to spanning set. Is it a basis check?  As in you have to check if it's linearly independent. If not by definition. That means at least one of those vectors is in the span of the others find which one it is or which one of the ones it is and remove it. And now you have a smaller set of vectors. Is it a basis check check if that's plenty of the independent if it's not find one of the vectors is linearly dependent on the others remove it and it'll write this procedure and this procedure will eventually stopped right because those vectors there spam the Subspace which is nonempty So eventually you're going to get down to a set where there are no more vectors that are already in the span of the others. You will get down to a linearly independent set. I should add the caveat that this only works if you're talkin about finally many factors, but in our in this room were talking mostly about Vector spaces that have a basis of finitely many vectors. Okay. We'll see in a few minutes. That's not always the case.  Okay, so I called that a corollary, but I should really call it a theorem cuz we didn't stayed up there on yet that says this so this is a theorem.  If I have a Subspace H Subspace of some bigger Vector space. Okay, so we got some bigger is defined to be the span of some collection of vectors. He wants space.  then  some collection Among Us vectors is a basis a subset of that set of vectors is a basis. Just what I just said, you just proved it until you get down to a basis. So let's look at a concrete example now relevant to your homework into our understanding of the class. So the most important kind of subspaces we deal with here are so far are the column space in the null-space of a matrix structure the same one we had last lecture and use it several times this lecture. This is a 3 by 5 Matrix its column consists of those 5 vectors in R3  the column space  of H H is equal to the column space of the Matrix a definition of the column space of a matrix is defined to be the span of its columns.  So by definition there is a spanning set. No, I'd like to find a basis for the column space.  So the first question is maybe that's the basis right there. Maybe that set of 5 vectors is a basis. What do you think?  Is it a basis?  Y'all know why not someone in the back.  I heard some voices from the back so  You're not in the back for that shirt you're in the back or is that your arm cramping?  Perfect. The second Vector is manifestly twice - 2 * the first Factor, so it's definitely the set of vectors is definitely linearly dependent.  That's not linearly independent. So following our nose following are pruning procedure that we talked about in the last life. What we should do is throw away that doctor if we throw away that Vector it's not going to change the Subspace H. The Subspace H's going to be spanned by the other four vectors just as well. Okay, great. So how about let's give these guys names. Let's call him V1 V2 V3 V4 and V5 answer the proposal is to now use V1 V3 V4 and V5 to those form a basis for H.  Know why not?  Okay, so you're at not talking about pivot. So you're jumping ahead that which is good. That's what we're going to say next. But the key Point here is what we need to check.  We need to check if those remaining four columns are linearly independent and it's no longer true that you could obviously see that one of them is a multiple of another one. That's not true anymore. But that just means that no pair of parallel but doesn't mean that you couldn't have three of them that are linearly in linearly dependent even though no two of them are right, but we have a technique or checking. This means we do.  Great, and we don't have to do the reduction that love that it for us. So there's the reduced row Echelon form of the Matrix. Let me highlight this now that we've already discussed this idea and used it before but let me make this a little more precise in this example to cement it. So here are the five columns of the reduced row Echelon form and you are the five columns of the original Matrix.  Now in the reduced row Echelon form we can see that there are only two pivotal calls A1 and A3 are the pivotal columns and the others correspondent free variable. The others are non pivot. What does it really mean that the course 1 2 3 variables will we know what that means if we're riding down the solutions that which will do on the next flight when we talked about the null space, but they tell us something critical in the column space setting as well.  So we already noted from the original Matrix that V2 is -2 times we want.  But notice that we can see that immediately in the reduced row Echelon form, right? So if you look at it  this -2 in tree right there.  That -2 entry tells you that a 2.  Is equal to -2 times I want because a one is just the first standard basis vectors just 100. So if you want to see if something is a multiple of that you just have to check the first entry and indeed. That's true. Now. Let's look at a for a Forza non-committal column and it has two non-zero entries there.  Actually, I should use different color so that the second one is to first one. I'm going to color orange as well because what that first entry there is telling us is that I can find a 4 as a linear combination of the previous columns. That's what the reduced row Echelon form tells us. It tells us look for phone to get a for I can build it out of the previous columns A1 A2 and A3 by taking - 1 * a 1 + 2 x A3 because they wanted it a three they they're standard basis vectors and that's just how vectors work if you want to decompose the vector ex why you think positive x x 1 0 + y x 0 1 so that second that the equation there there's not equation. There's no equation for the fourth column there manifestly shows us that a 4 is equal to - A1.  + 2 a 3  Okay, and similarly?  We see that the fifth column.  a 5 is equal to 3 A 1 - 2 a 3  Okay, so that's what we see from the registration form of the Matrix. But here's the thing. Look over at the original Matrix now a we already noted.  That V2 is equal to -2 V 1.  I know I claim that we can also see and now check if you like that the V4 is equal to - the OnePlus 2 V3 and V5.  Is equal to 3 V 1 - 2 B 3.  Let's check that second one before equation V4 is supposed to be minus V1 plus to V3 case if I take - V1 I get four.  + 2 V 3 so4 + -  So I'm supposed to take.  Minus V1 which gives me a 3 in the first thought + 2 x V3 which gives me a 4 so minus.  Positive 3 - 4 is equal to one. Is that true?  Yeah, I apparently need to actually write it down to see I'm caught in the Blackboard feel even though I'm not using the the Blackboard. This is a field that emanates from the black boy that makes you stupid. Okay, so I may just come over here with you guys at.  write -5 - 3  + 2 * -1  Is -2 + 3 is 1 - 1 + 2 * 2 is 4 minus 1 is 3 + - 2 + 10 is 8. Okay. So there we go. Yes was just verify that that same relationship holds but we don't need to do that ever again thankfully because that's that's exactly how Ro operations work. That's the point row operations. They mess up the columns. Can you change the columns and you change the column space to be clear and we'll highlight this in a moment. The column space of a is a totally different Subspace in the column space of the reduced row Echelon form of a the span of those columns on the right is totally different from the stand of the columns on the left. But what chant what doesn't change is the relationship between the Cults?  The three row operations that we can use in sequence to get from a 2. It's reduced row Echelon form the row operations. They preserve linear relationships between the columns and that's what we see here. So you can pick off from the reduced row Echelon form what their linear dependencies between the three columns on the pivotal columns are and those are exactly the same relationships we have over on the left. So that's the reason why we can just go ahead and say well hey in the reduced row Echelon form. I see that the first and third columns are pivotal and therefore that's also true of the original Matrix and the other columns are linear combinations of those.  Now from the context of what we're interested in right now, which is finding a basis.  okay, what that tells us is that  V1  V1 and V3  form a basis  for the column space of a  Okay, that's this procedure. We want to find a basis from a spanning set. What we do is we take a Spanish that we start with and we start pruning it by removing factors that are linear combinations of the others. Well, the reduced row Echelon form tells me that the non pivotal columns are linear combinations of the pivotal ones. So we'll just throw out all the non non committal columns and we won't change the column space in that context. That is the column space to expand just by the pivotal columns.  And the pivotal columns those are linearly independent because as we know if we're going to check their linearly independent, what would we do we would do row reduction and see if they were pivotal by definition their pivotal or Not by definition by that theorem their pivotal. So that's how we do this. You want to find a basis for the column space you throw away. The non total columns the pivotal Columns of the Matrix a  form a basis for the column space of a okay. So that's a restatement of what we just said, so let's do another example here just to cement that so here is a 3 by 3 Matrix 3 columns in R3. So maybe maybe these call him spend all of our three. Maybe the column space is all of our three, maybe not we just have to check if I doing row reduction. So let's do some production here. Okay, when we need to get this to reduced row Echelon form, it's not so bad. It's pretty close already actually are we don't actually need to go all the way to reduce trash one for right?  We only need to check which columns are pivotal to do that. We only have to go to any row Echelon form. That's not going to take too much work here because I've already got a zero in the second row below the pivotal one. So I'm going to subtract twice the first row from the Third.  Can you give me a zero there a -5 there?  and -5 there looks like  0 - 1 - 100 and I can see that I'm not going to get a basis for all of our three here because the second row and the third row are parallel to each other right when I now subtract when I subtract 5 times the second row from the third I'm going to get zeros there.  I know I'm in row Echelon form not quite reduced row Echelon form, but I'm in row Echelon form and I can see.  That these two columns are pivotal, but the third one is not which means the same holds true in the original Matrix, which means if we want to find a basis of the column space.  those first two columns 1 0 2 and 3 - 1 1  form a basis  for the column space today.  zz's.  Now what am I to important caveats or highlight two important caveats? The first is that and I already said this a moment ago. It's really important that we use.  The Columns of the original Matrix a row operations, they they preserve the linear relationships between the columns, but they mess up the columns.  I think about it this way.  If I had a matrix whose columns were all pivotal 3 by 3. Matrix is called.  Then we know that that forms the basis of our three we saw that last time okay, but the reduced row Echelon form of any such Matrix is the identity Matrix.  Okay, so that means that those vectors are the standard basis vectors on the right. Those are very different from the original vectors. Now, this is actually a poor example because when you have a basis for the whole Space, it really doesn't matter which factors you use you could use the standard basis vectors, but I could just as well have chosen to of those. Okay, I could take a 2 by 3 Matrix.  maybe it's something like  this one here.  So look at the the first two vectors are 102 and 3 - 1 1.  now if I do row operations on just those two vectors  Okay, are those two vectors linearly independent? Yeah, we know they are because they are pivotal in this Matrix here. Also, they're not parallel. We can just see that if I were to carry the Matrix consisting of just those first two columns to reduced row Echelon form. I would get one zero zero zero one zero that's what you always get. If you have linearly independent vectors you get in the reduced row Echelon form. You get the standard basis Vector now those two vectors 100 and 010. Those are the standard. Those are the standard basis vectors in the X and Y directions  In the three dimensional space so the span of those vectors is the XY plane.  But the original vectors 102 and 3-1 one, right? They spend a totally different plant.  Those two planes kind of look alike. They're both planes with the similarities and they're so the span of the original column is not the same as the span of The Columns of the wrist restaurant.  Okay, that's critical here. You want to find a basis for the for the column space? You do row operations just in order to tell which Columns of the Matrix are pivotal and you use those columns in the original Matrix. You don't use the columns in the registration form. Those are not related to the Subspace General.  So that's the first caveat how much is written down here? Typically the column space of a on the column space if it's reduced row Echelon form are different things different substances. The other thing is there are zillions of bases like we saw on the first slide here.  In this example hear. Hey, I ended up deciding that we could use the first two vectors there as a basis which was true, but we could have also used the second two vectors or the first in the third any in this example. Any pair of those vectors is a perfectly fine basis.  And that would also be true in this example here that we are just working through.  Because no two of those vectors are parallel. So think about it this way. We don't do column operations when we're doing real operations. We don't swap columns, but would swap column is me swapping columns means that we're renaming the variables like if we're thinking of the Matrix if the coefficient Matrix of a system of linear equations, then swapping two columns means instead of calling a few variables X1 and X2. I'm calling them next to a next one.  Know that will change how we describe the solution set. But in terms of questions, like are these vectors linearly independent are they expanding set doesn't address those questions. It doesn't change those questions at all. So we could have reordered these vectors just as well and gotten the same answer insurance with the column space is and you can check that if I had swapped and made the third column the First Column of this Matrix, then I would find in that case that the first two columns were still pivotal which columns are pivotal depends on which order you put them in or which but the point is that there are some of the nonpareil columns in The Matrix the way we do our algorithm. We could have chosen those to be pivotal if we pre-ordered things around so it could well happen that we could have chosen other vectors from Among The Columns to find a basis. I'm not recommending you do that. I just want you to understand that it's not the case that we can say. This is the basis.  This is just one basis. But this is a good basis. You can always use this one. So if you're asked to find a basis for the column space, this is what you should do. Yes.  What a great question. I swear I didn't plant them in the audience because we're going to go into that on the next slide question was is there a minimum number of vectors that you need to form a basis? And that is a phenomenal question actually one more slide after that. So hold on the answer is yes, but not only is there a minimal there's also a maximal and they're the same every two bases. Any two faces have exactly the same number of vectors and we'll get to that in a minute. But before we get there, let's talk about the other.  What about the other kind of Subspace that we've mostly been focusing on in the last couple of lectures, space and null space? So if I give you a matrix it's no space is also a Subspace. Its a Subspace not of the codomain but if the domain so here are my same Matrix a the 3 by 5 Matrix, that means that it's no space is a Subspace of R5. It's a set of vectors which you can multiply 8 by which means that they have to have number of components equal to the number of columns of a okay. So and we know how to find this. This is what we've been doing since literally lecture one. So the null-space of a  Can be described as a set of vectors X1 X2 X3 X4 and X5.  Such that. Okay. We look at the registration form. We have the same thing. Now we can now we may as well only look at the reduced row Echelon form. We have are pivotal columns there.  That means that x 3 X1 and X3 are going to be expressed in terms of the other columns, right? We're going to have three free variables x 2 x 4 and x 5 are free variables.  And then the two non-trivial equations that we get when we interpret.  They reduced row Echelon form as the coefficient Matrix of the system. Tell us that x 1 is equal to 2 x 2.  + x 4 - 3 x 5  I'm at x 3 is equal to -2 x 4 + 2 x 5.  It's a homogeneous system. So we imagine we have zeros on the other side and we subtract in order to get that.  And then the standard thing we do know it's just a factor out the free variables and express this as x 2 x a vector and that doctor will be to 1000.  plus x 4 times a vector which is in this case 1 0 - 2 1 0  plus X 5 X Factor.  which is -3 / 0  201  okay, so there's a perfectly good description of the null space. It's the set of linear combinations of those three vectors in other words. It is the span.  Of those three vectors. Let me call them A B and C. So I have to rewrite them over here.  So the the algorithm that we use the road action algorithm that we used to solve systems of equations it automatically produces for us a spanning set for the null space.  So I've now got a spanning set. And so I can ask the question. Is it a basis if it's not I know what to do. I just start pruning it if it's not a basis, there's some Vector in there that the linear combination of the others. I find one of them I remove it and I keep going until I get down to a basis.  Is this a bassist the ones that I get from this procedure is that set linearly independent or not going to give you Thirty Seconds to Talk Amongst yourselves and decide go ahead.  All right. So sounds like you're having an active discussion. I see some people have a really pained expression on their face. So let me end your misery and let's just take a vote how many people think that these are linearly independent.  Even the paint guy thinks they are. Okay, that's good. How many people think they are linearly dependent?  One or two hands, okay with somebody who thinks they're linearly independent like to explain why they hit their linearly independent.  Yes.  Fantastic, perfect explanation each one of the vectors has a one in a place where the other two have a zero and that means you're just not going to be able to make a non-trivial linear combination of them equal to 0 Let's look at that more closely.  so let's look at  these entries  these entries  NBA centuries  Okay, if I look at the entries that correspond to the three variables to four and five in this case and I look at only those entries than the first Vector is 100. It's the first time the second letter is 010. It's the second Saturday suspect Iran. The third is 001 the third Center basis Vector. This is what always going to happen. In fact, I don't recommend you do this, but you could memorize here is how I find the null space what once I take get the reduced row Echelon form of the Matrix. All I do is I take the coefficients read across in the Rose. I put negative signs in front of them. And then I interspersed them with the standard basis vectors in the three variables slots. That's what this algorithm does.  And here's the point if I take a linearly independent set like a standard basis vectors and then I have pain some more entries in other places a pending more entries will not make them linearly dependent because if you have a non-trivial linear combination of the bigger vectors Vanishing that it will also be an auntie really a combination of the the components that correspond to the standard basis vectors here and we know that can't happen.  So we get that these guys are linearly independent.  So that's always going to happen following this procedure.  For finding the null-space you're going to get a you're going to get us panning set and that's benefit is always going to be a basis so you don't need to do any pruning at all. Okay, so there you go. That's how you find a basis for the Subspace on the basis for the null space again. There are zillions of they see these are not the only ones but these will be basis. So it's it's one way you can find  All right, great. So now let's get back to your question.  Is there a one basis that's better than another can I always find what what's what's the minimum number of elements of a basis? Maybe I'll have a basis with 76 vectors, but another one with only three vectors so clearly I want to use the one with only three vectors. Well, sorry to disappoint you but all the bases will be equally efficient or inefficient. You have any to basis for a Subspace? They must always have the same number of vectors in them. In other words. I said there are zillions of basis for any space. But there's one thing that's common among them.  All of them have the same number of vectors in it. That's up there. I'm here and I want to actually prove that there are more. I want to prove a special case of that them that will contain the whole idea of the proof and why it's true so suppose.  Let's do it with where I finally have a basis with two elements. Okay, I should really do this for a basis with n elements, but I'm going to stick tattoo to make the notation easier suppose that you won you too.  Is a basis.  for a Subspace  pitch  and let's also suppose that V1 V2 V3.  Is another Subspace for it? So here's the idea. I'm going to give a proof by contradiction already discussed this a lot. How do you how are you going to prove something like linear Independence for some abstract vectors linear Independence is a negative property. So typically he's going to prove by contradiction saying I want to prove their linearly independent. Let's assume that they're not and show that that leads to a contradiction same idea here. I'm going to assume I have two bases that have different numbers of vectors in them. And just to make things concrete. I'm going to assume that I have one that has two elements and one that has three elements, but you'll see that this idea of this proof when we do it in a second is going to generalize immediately to if I have 76 + 1 + 74 in the other. Okay doesn't matter if those two numbers are unequal the same idea here is going to show that that  So here's the here's the idea here suppose. I have two bases for the space you want you to is 1 basis and you V1 V2 V3 is another now, what does it mean that you wanted you to as a basis it in particular means that they are expanding set.  So that means that V1.  Can be written as some A1 you one plus B1 YouTube.  Okay, everyone is in the span of you want and you too if you want is a vector in the Subspace and therefore it's in the span of the basis vectors.  also  V2 is equal to sum a to you one plus be to you too.  and V3 is equal to  A3 you 1 + B3  you two, okay each one of the vectors V1 V2 and V3 because they're in the Subspace. They must be expressible as linear combinations of the basis vectors you want in YouTube.  No.  consider the Matrix  A1 A2 A3 B1 B2 B3  Hey, this is the moment not much to do with the original vectors. Remember those vectors, they might be abstract vectors and some abstract Vector spaces might be polynomials or something like that. They're not columns typically has no Matrix associated with them, but I can take those coefficients that I just found and I can make a matrix out of this rectangular Matrix on the key thing about this rectangular is a 2 by 3 Matrix the two in the three came from the two elements and and the three elements in the two bases.  What do we know about rectangular matrices with a lots of things but in particular we know that if I have more columns than Rose than those columns can't be linearly independent.  Right, we know that if I take if I compute the registration form of this Matrix, one of those columns, at least one of those columns is going to be non pivotal.  But we know then is that.  So there is some.  non-trivial  Vector X1 X2 X3  such that if I take this Matrix here a  and this is the vector X such that a x x equals 0.  But that's what we know we know that there is a non-trivial solution to the homogeneous system with that Matrix as it's coefficient Matrix because there are free variables there because there aren't enough columns to go around Rose to go around to make all the columns pivotal.  So let's write out explicitly what this says it. What's 8 * x + 8 * X  Is equal to okay. Well, I know how to multiply matrices. Let's remind ourselves. So I'm going to buy that First Column by x + 3 by entry and get a 1 x 1 plus a 2 x 2 + 83 x 3.  I'm going to move by the second column by X as well.  And it gives me b1x 1 + a b 2 x 2 + B 3 x 3.  And what I'm told is that that is the zero Vector. So what I have is there's some non-trivial coefficients X1 X2 and X3. So that a 1 x 1 + 8 2 x 2 + 83 x 3 is 0 and simile be 1 x 1 + b 2 x 2 + B 3 x 3 is 0  Now, how does that help me? What does that have to do with these original vectors? Well, let me just sort of.  Pull out of thin air the following.  Let's look at those coefficient for Sig those X's.  And look at x 1 V 1 + x 2 V 2 + x 3 V 3.  What's this compute with that thing is well. I have some equations up here.  But can that I can use to express the V's in terms of the use. So let's do that here. So this is x 1 x  A1 you one plus B1  U2  plus x 2 x A2 YouTube + B2 U2 you want their Plus?  x 3 x a3u 1 + B 3 YouTube  I know this is a linear combination. There's six terms there. But if I multiply it out Andre Shuffle things I can express this as a linear combination of u and v are of you wanted you to write each. One of these terms is a scalar multiple of you one plus a scale in math. Love you, too. So, let's recombine them in terms of you wanting you to  that says X1 A14 the you want in the first term + X2 a24 the you want in the second term + x 383  Those are all the things that multiply to 1.  And for the second terms that multiply you to I get X1 B1.  X2 V2 and x-33  okay times you too.  But hey, wait a minute.  Look at the columns or look at the Rose.  Over here and look at these coefficients.  Those coefficients that came up there. Those are the rose. Those are the entries of this Matrix ax but those are zero.  And so this is the zero Factor. So what if I got I've got a non-trivial linear combination of the bees that equal 0 and that is a contradiction.  To the fact that V1 V2 and V3 as a basis.  That's the core idea here if I have two sets of basis vectors.  Then I can write down a matrix that expresses one set as linear combinations of the other.  and vice versa  If I have two bases with different numbers of vectors, that means that Matrix will be rectangular and a rectangular Matrix must have a non pivotal row or column.  But that's going to mean that that the larger basis will be linearly dependent because of this calculation. That's the core idea here. You can't if you have one basis with two vectors then any set of three or more vectors will automatically be linearly dependent. That's what this shows.  So you can never have more than two more than two vectors in a basis, but you couldn't have less than two either for the same reasoning because if you had less than two than that first when you started with with two vectors, that would be linearly dependent by the same argument in Reverse. Okay. So this is the key factor here at the key fact of linear algebra for that medicine probably the most important there in one ear out for any two bases have the same number of elements and that gives us a key fact hear a key new word and to add to your list of vocabulary to let you know this word before you've been you you heard it a lot.  We've used it already. But now we're making it precise. The word is dimension.  So if I have a vector space it could be an abstract Vector space. It could be a Subspace of a bigger Vector space but one way or another by a vector space the dimension of vector space is the number of vectors in any given basis for it. We just saw that if you have two different bases.  They will have the same number of actors in them. So this is a well-defined quantity the dimension of a Subspace or a vector space is the number of vectors in any basis for that space.  So for example here is a vector space. It's the span of these three vectors in R3.  This is the exact an example. We saw a few size ago, and we saw that these two columns in the reduced row Echelon form are the pivotal columns and the third one is non pivotal so has to Total columns. Now, we saw that if you want to find a basis a basis for the column space of that Matrix, you can choose the pivotal columns as a basis. There are two pivotal Collins there that tells you that this Subspace the column space of that Matrix has Dimension 2 is a two-dimensional Subspace. It's a plane in R3  Any other basis you find you could in this case you could use any two of The Columns that Matrix and that will be a basis you could also do all sorts of other things. You could take three times the First Column and 76 times the second column that's not going to change the fact that there are basis.  You could also mix them up and add two of them together and subtract the other two that will also work and give you a new bassist. There's zillions of base's but no matter what you choose for a basis. It's always going to have exactly two factors in it. That's the dimension of the column space.  Great, let's quickly. Look at a couple more examples here. So here was a Subspace. We saw when we introduced abstract Vector spaces P3 the space of polynomials of degree less than or equal to 3 which were polynomials of the form a x cubed.  + b x squared + CX + d  Hey, we're a b c and d.  We're arbitrary real number that are free variables here.  So what that means?  Is that this is the span?  of the vectors x cubed x squared x + 1  There is a spanning set for that Subspace consisting of those for polynomials and those for polynomials are linearly independent because I cannot find a numerical linear combination of 1 and X that is equal to 0 I can't say Oak 2 x + 3 that's equal to 0 I mean it is equal to 041 choice of X, but these are abstract Vector. These are the basic object. There's no numbers A&B for which a x + B is always equal to zero those are linearly independent vectors. So that means that the dimension of P3  Is 4 because there is a basis with four vectors in it. So last quick example here. How about that Vector space of all polynomials?  What's the dimension of the vector space of all polynomials?  Well, let's see if we can write down a basis everything in there. Looks like you know, there could be a constant a next term and x squared term in X cubed term eventually it terminates.  But if you want to have all polynomials you want to have all possible polynomials, then you have to have terms of arbitrary degree. Any polynomial will be a linear combination of only finitely many of those but if you want to have an arbitrary polynomial, then you have to have all of those Powers there. They're all linearly independent. So what we find is the dimension  Of the space of all polynomials is actually Infinity.  Does Vector space of all polynomials is an infinite dimensional Vector space which means that a lot of the stuff we talked about today doesn't work for it. We're going to be concerned almost exclusively with finite dimensional Vector spaces in this room, so don't worry.  See you again.  UC San Diego podcast ",
  "Name": "math18_b00_wi18-02122018-1000",
  "File Name": "lecture_15.flac"
}