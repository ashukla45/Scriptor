{
  "Blurbs": {
    "* all the rest of the component components in the standard basis vectors. So it's just saving us a whole lot of work when we have an orthodontist basis. Okay. In the last five minutes here. I want to now relate back to matrix multiplication. We saw or talked about orthogonal compliments that orthogonal of two vectors orthogonal which of the factors which is defined in terms of matrix multiplication ": [
      2727.3,
      2757.9,
      101
    ],
    "1/2 + 7/2 is 8/2 to this is -4 + 8/2 + 1/2 is 4 so we can get Sarah and if you want to go ahead and compute the other two. Products you too, and if you want a new Street, you'll see that all three of those vectors are orthogonal to each other. Okay, great. So, how does that help us what helps us in a lot of ": [
      1758.1,
      1781.4,
      64
    ],
    "BMW and then we defined from that the length the length of a vector is the square root of the top part of the vector with itself, which is just the square root of the sum of the squares of the entries and we saw last time I finished my proving the cauchy Schwarz inequality, which tells you that what I've written here. At least makes sense that the dock ": [
      306.8,
      331.5,
      13
    ],
    "I took a two-dimensional Subspace like a plane. It's orthogonal complement was one-dimensional if I took a one-dimensional Subspace the line here. It's orthogonal complement was two dimensional. What's going on here what's going on is that if you take a Subspace and it's orthogonal complement their Dimensions add up to 3 in the play Space Case data to in the plant in case they add up to the dimension ": [
      868.4,
      898.8,
      32
    ],
    "Independence most of the time Linear independent vectors are not orthogonal most of the time they'll be some kind of slanted in or not all in the same plane or whatever, but it's some sort of slanted linear independent I'm saying no, I want them to be orthogonal to each other the best kind of linear Independence. We can go one small step further to say and let me also ": [
      2128.9,
      2149.8,
      79
    ],
    "Is equal to the square root of 45, which is better written is 3 times the square root of 5, so that leaves three is three times the length of U2. So if I wanted to form a north and oral basis, I'd have to take the vectors you one, which is you want hats, which is you 1/3. You too hot, which is you two over the square root ": [
      2437.4,
      2460.0,
      91
    ],
    "Million entries in each call. Okay. So this you get a square Matrix out a transpose a is square they but they didn't need to be square for this to be true. If I have any collection of columns at all and a transpose a that's going to give me the identity Matrix of the appropriate size. That's what it means to say. That vectors are orthogonal. I will continue ": [
      3049.3,
      3071.2,
      112
    ],
    "R3. What is the orthogonal complement of that Subspace? Someone like to make a guess or make a suggestion for what it looks like. What time is no way down here? Yes. Okay, a plane perpendicular going through the origin. That's a great. Guess I'll let's take a take a gander at that. So here's a piece of paper. Let's use that to represent are playing so maybe like in ": [
      695.4,
      726.8,
      26
    ],
    "So this number is just what? Okay, so you can always normalize a nonzero Vector. He can always divided by its length and make it a unit length Vector a normal vector. And so basis is called orthonormal. If all of the vectors in the bases are mutually orthogonal to each other and each one of those vectors has length one the canonical example of that the standard basis the ": [
      2232.6,
      2259.4,
      83
    ],
    "So two vectors are orthogonal. If and only if their. Product is zero, we saw this arose from consideration of Pythagoras Theorem from plane. Geometry says that if you have a right angle triangle, then the sum of the squares of the two sides on the two sides of the right angle. Some of those squares is equal to the square of the hypotenuse the square of the length of ": [
      419.8,
      445.5,
      17
    ],
    "Vector in the room? Just the zero Vector orthogonal complement of a vector space in itself is the zero Vector orthogonal complement of just the zero Subspace. Just turn around what we just said. I saw someone actually turn around not quite what I meant. But so if the orthogonal complement of everything is 0 what they're selling to complement of zero everything if I'm just a zero Vector what ": [
      925.5,
      958.5,
      34
    ],
    "Well, let's see here. So just to prove this there as usual linear Independence is a negative proper. It says that the vectors are not linearly dependent and if you're going to prove something about negative Universal Property, usually you have to argue by contradiction. We're going to say, okay. Well, let's suppose that they are linearly dependent and see what goes wrong with that assumption. So if I have ": [
      1803.7,
      1829.1,
      66
    ],
    "a 2 x 2 + 83 x 3. And then doing the same thing, but with the second row here for the second column, that's v1x 1 + b 2 x 2 + B 3 x 3. Forex to be in the null space says that that Vector there is zero, but let's recognize what's written here. What's written here is exactly this first row here. Let's call that Vector ": [
      1382.6,
      1413.4,
      51
    ],
    "a and let's go. Vector B. Okay, I guess those are actually the transposes of the vectors cuz return them on their side. This is a DOT product X and this is B. Projects. So here's what we see for a vector X to be in the null-space of a exactly means that that Vector EX. Has zero. Product with the rose of x? X is in the null space ": [
      1413.4,
      1443.4,
      52
    ],
    "a note to write too much. Okay, so let's write it as A1 A2 A3 B1 B2 B3 There are my two rows of The Matrix. And so what is the row space Aerospace? Okay with y'all use the text book conventions the column space of a transpose. That's the span of the Rose but transposed so it's the span of the first row and the second row but treated ": [
      1263.4,
      1300.4,
      47
    ],
    "a plane like this or something. Okay. Well, you know that plane that plane is perpendicular to this plane down here in the sense that you know, there's a right angle down here. But let's be careful here goes down by itself. Let's be careful here the definition of the perpendicular Subspace that are perpendicular to everything in the Subspace to look at this playing again here. This Plane intersects ": [
      726.8,
      760.4,
      27
    ],
    "about now primarily is the special case Wednesday to equal zero. 28 equals 0 Oz are parallel really the important Point here is when Theta is equal to pi over 2, but they does 90\u00b0 when the vectors are perpendicular to each other. Okay. I'm from that formula that the product is equal to the product Alliance temp the cosine of theta the cosine of 90\u00b0 is 0 and so ": [
      357.6,
      394.5,
      15
    ],
    "about some important subspaces like the null space in the column space of a matrix. Remember that the null-space of a matrix that has some Dimension the nullity the column space that Matrix has some dementia the rank and we have the rank theorem, which says that those two Dimensions they add up to the size of the space size of the of the codomain or start of the domain, ": [
      985.5,
      1007.6,
      36
    ],
    "are now open and will be open until 11:59 p.m. Next Friday. So we have to do it before classes end if you're going to do it at all. I'll keep reminding you of that as we go. You have a MyMathLab homework set that's due tomorrow night by 11:59 p.m. Your final Matlab homework is due on Friday by 11:59 p.m. And your Matlab quiz accounts for 5% of ": [
      125.6,
      148.5,
      5
    ],
    "are orthogonal is to say that if I take those column vectors put them as The Columns of a matrix a A transpose a is diagonal that's what it means to say that a collection of vectors are all mutually orthogonal. Yes. Okay, so there is a relationship with eigenvalues here and we're going to get to that next week, but you shouldn't shouldn't talk about calculating this Matrix just ": [
      2970.3,
      3001.7,
      109
    ],
    "are perpendicular to every vector. in the Subspace So on this particular picture, let's think about that for a second. So here I've got a Subspace this line. What does the perpendicular Subspace look like looking for all vectors that are orthogonal to that line? If I see someone making this kind of emotion over there that I think that's as close as you'll be able to do to explain ": [
      603.5,
      640.3,
      23
    ],
    "as columns by turning them on their side. So that's what the row space is is the span of those two vectors there. And what is the null space of a well just by definition. It's the set of vectors X in R34 which a x x is equal to 0. Okay. Well, let's explore that a little bit. Let's write it out in this example. So what does it ": [
      1300.4,
      1328.4,
      48
    ],
    "as possible or easier to work with than other vectors and one reason we can now see why that's true. Is these vectors are all orthogonal to each other? What's take the 1st 2? so if I take the first one. It with the third one say left going to be 1 * 0 + 0 * 0 + 0 * 1 what's just gives me 0 and you can ": [
      1641.8,
      1683.2,
      60
    ],
    "because the vectors are orthogonal to each other and so all we're left with is this turn right here, but that one is equal to the length of you one square. And so this whole thing is C1 times the length of you 1 squared. I know it could have been that you wanted was the zero Vector in which case would just get there all together here. That's why ": [
      1940.3,
      1964.9,
      71
    ],
    "because what I get is that from this equation I get from this calculation I get that 0 is equal to 61 times the length of you 1 squared the length of you once where it is and not zero because you want is not the zero vector and so therefore we see that C10. Okay, great. See one is it what about C2? We're just going to repeat exactly ": [
      1987.1,
      2012.8,
      73
    ],
    "but I think you're telling me that what we need to do is look at the line over here. That makes a right angle with that line. That's correct. Okay. So this line over here is V perp when this one here was be. Okay, so that's a pretty relatively easy thing to understand if I give you a line in the plane. It's perpendicular Subspace. It's orthogonal complement is ": [
      640.3,
      670.3,
      24
    ],
    "can give you insight into row space force has no space and I want to repeat the same calculations. We did there to give you a concrete way to compute when a set of vectors are orthogonal to each other. So if I give you a matrix a And this is finally going to tell us what does transpose is really useful for I'll give you a matrix a then ": [
      2757.9,
      2782.0,
      102
    ],
    "check that they form a basis. We just need to check that they are linearly independent, but we have a Theron from the last flight. These are three nonzero vectors and we just checked that they're all mutually orthogonal. That means that they're linearly independent and therefore they form a basis. That was a lot easier to check then doing row reduction on a 3 by 3 Matrix. Okay. So ": [
      2356.4,
      2375.7,
      88
    ],
    "dimension of the no space is the dimension of the whole space and that's exactly what we were saying on the last slide with the orthogonal complement business that the dimension of the row space plus the dimension of its orthogonal complement, which is the null space is and is the dimension of the whole space. And so this is just another demonstration of the rank theorem that we were ": [
      1113.4,
      1134.8,
      41
    ],
    "do the same thing with the first two or the last two any two of those vectors are perpendicular to each other be the standard basis vectors. This is the usual sort of graphic that you'll see in a first-year physics class. You take your your thumb your first and second fingers on a hand, I guess in physics. They would always use their right hand. I'm left-handed. So damn ": [
      1683.2,
      1703.3,
      61
    ],
    "elements in a given row is the number of columns is Aunt. So again, these are two vectors an RN. So this at least has a chance of making sense on the statement were supposed to see here is that exactly this if I take a row of the Matrix if I take something in the row space which is a linear combination of rose and I take something in ": [
      1219.6,
      1239.0,
      45
    ],
    "expansion terms that basis that's what the coordinate vectors all about but to find the coordinates of a basis of a vector in terms of a basis mean solving a vector equation, but if you have with other malady around it is Trivial or is very easy to solve vector equation if I have an orthogonal basis for a vector space for a Subspace to be then the coefficient Vector ": [
      2518.9,
      2543.8,
      94
    ],
    "factor? So the correct answer which I think you really probably meant is it's the line through the bottle. It's the Subspace spanned by the bottle. The orthogonal complement to this plane is this line perpendicular to it? Similarly, what's the orthogonal complement to the line through this bottle? It's the plane down here. Okay, the things that are perpendicular to a line in R3 or a plane the things ": [
      812.3,
      841.5,
      30
    ],
    "for the theorem here. It's important that we have the statement that the vectors are not zero zero Vector is orthogonal to everything but the zero Vector can't be in a list of linearly independent vectors. So the precise there I'm if I wasn't if I have a bunch of nonzero vectors that are all orthogonal then they are linearly independent and what we see now is why that's true ": [
      1964.9,
      1987.1,
      72
    ],
    "from the midterm. Do you wants to check that an vectors an RN or better yet? If you have an n-dimensional Vector space you want to check that a set of n vectors Forma basis. All you have to check is at their linearly independent any set of n vectors in an n-dimensional space form a basis. They will spend it automatically. So these are three vectors in R3 to ": [
      2333.1,
      2356.4,
      87
    ],
    "get in touch with me. I think I saw a question about their You know what? I I do not know the answer to that question. So I think what you should do. So the question is are you allowed to bring notes to the exam and I don't want to give you a false answer the mat like a part of the course is run kind of independently from ": [
      188.7,
      208.8,
      8
    ],
    "hat with itself. Which is the dot product of you over its length? With you over its length. Okay, but now we use the properties of the. Product. We have that factor one over the length of you. It's in there twice so we can Factor it out and what we get is you. You / the length of you squared? But you. You is the length of you squared. ": [
      2204.4,
      2232.6,
      82
    ],
    "here's how we do that. We say therefore. If I take that Vector c1u, 1 + C to you, too. down the line cpup I'm going to take its. Product with you want. Now on the one hand that must equal zero. Write it to you one dotted with zero. So this is that linear combination is the zero vector and therefore $0 with you on his hero, but on ": [
      1880.9,
      1910.7,
      69
    ],
    "if I multiply that matrix by its transpose on the left a transpose a that encodes the dot product of The Columns of a let's do that again with with a 2 by 3 Matrix. Actually. It doesn't matter what size The Columns are so it's take a and right it is but suppose it has three columns. Those columns they might be a million dimensional. I don't care for ": [
      2782.0,
      2809.1,
      103
    ],
    "in fact a little bit about why this is true. We're going to prove this but more important proving it is to understand what's going on here. So what does it mean? Let's look at that first statement there. oops What does it mean? To say that the orthogonal comment of the row space is the null space. So this means that if V is a vector in the row ": [
      1134.8,
      1168.7,
      42
    ],
    "insist that all the vectors have a length one and that's called normalized. So a vector is called normalized if it's length is one but that's actually easy to achieve whenever I have a nonzero Vector. Let me just do that right over here. If you is not equal to zero. Then we can normalize it. What that means is we replace it with you. / it's length. So I ": [
      2149.8,
      2184.8,
      80
    ],
    "into what the no space is. Yeah. It's the solution set to the homogeneous equation x equals 0, which seems like a very different sort of object in the column space in the row space Witcher just the vector spaces stand by The Rose or the columns, but we see they are linked together. They really are comprable things the column spacer becomes base of a transpose. The row space ": [
      1563.5,
      1586.3,
      57
    ],
    "is a Subspace of RN. What is it? It's the row space of the Matrix. This is just as the same statement about the case of a transpose is the orthogonal complement of the column space of a now remember that the row space on the column space to have the same dimension. And the rank theorem tells us that the rank the dimension of space plus the Normandy the ": [
      1084.6,
      1113.4,
      40
    ],
    "is some Subspace of this of the space Where The Roses live and then no space of the Matrix is the orthogonal complement of that thing. It's a set of vectors that are perpendicular to those and that geometrically is why the rank theorem is true. Okay, because orthogonal complement they fill out the rest of the space. Okay, so that's orthogonal complement something we're going to use and need ": [
      1586.3,
      1613.6,
      58
    ],
    "it smiley face. Then smiley face hat is defined to be smiley face / the length of smiley face. quit Okay, now what's so great about having an orthodontist basis or an orthogonal basis it's an extension of what we talked about when we were talking about orthogonally implying implying linear Independence so if I give you a basis right then every Vector in your vector space has a unique ": [
      2490.1,
      2518.9,
      93
    ],
    "it. I'm doing with my left hand. Okay, those three vectors are perpendicular to each other all three of them. They formed this orthogonal Frame. That's why the standard basis is useful, but it's not the only set of vectors that has that property. Here's a different set of vectors three vectors in R3 on all three of these vectors are perpendicular to each other. We can just calculate that ": [
      1703.3,
      1728.1,
      62
    ],
    "lets let's pick two of them at random to do. Okay. I'm going to choose say about you choose. Which two should I choose? Two and three great. So let's take the dumb product of you two and you three. Okay, so that's just going to be - 1 * - 1/2 + 2 * -2 + 1 * 7/2 Okay, so that gives me 1/2 - 4 + 7/2. ": [
      1728.1,
      1757.2,
      63
    ],
    "listening to a podcast Wednesday Let's get going. First let me briefly mentioned that you should all have seen by now that midterm to was graded. You've seen your exams. Do you know that we are using the gradescope regrade request system this time around it's open right now closes on Friday before midnight. So take a careful. Look if you feel like you want to request a read rate ": [
      2.0,
      34.0,
      0
    ],
    "mean that a x x is equal to 0 so that means well, first of all, what size is the zero in this case a is 2 by 3. Ex has to be a three dimensional Vector there for a x x is going to be to buy one going to be the two dimensional zero vector. So let's write a X x so that's X1 X2 X3. So that's ": [
      1328.4,
      1357.5,
      49
    ],
    "mutually orthogonal to each other. Let's just go ahead and check that so you won. It with you two. that's equal to 2 + 0 - 2 which is 0 You one daughter with you three? Is 2 - 10 + 8 which is 0 and you too. It with you three. Is 4 + 0 - 4 which is Sarah Victors are all orthogonal to each other. And by ": [
      2284.0,
      2316.0,
      85
    ],
    "of 5 and you Three Hats, which is you 3/3 * \u221a 5 and these ones are an orthonormal basis of our three and we'll use that shorthand fairly often. Yes. The Hat indicates that they are lengths one. So if you see a vector and it has a hat on it, that means it has length one more generally if I took give you a vector and I call ": [
      2460.0,
      2490.1,
      92
    ],
    "of a means that X. Each row of a equals 0 in other words remember that. Park with a vector being being 0 means orthogonal being in the null space means that you are perpendicular to the rows of the Matrix adjusters what matrix multiplication says so that's almost a complete proof of that statement there. We just saw that the null-space of the set of things that are perpendicular ": [
      1443.4,
      1475.8,
      53
    ],
    "of a particular problem. There's a procedure to follow within gradescope at a detailed comment has to be done by this Friday at 11:59 p.m. The one thing I will say about the midterm is that people did reasonably well on most problems, but one that caught me off-guard that you didn't do as well on as I had thought you would was problem 3 Problem 3 was all about ": [
      34.0,
      58.2,
      1
    ],
    "of all of The Columns of the Matrix a that's what a transpose a computes. It computes the dot product of The Columns of a no one quick observation here. If the columns are all orthogonal to each other. Then what does this give us? Well, let's look at this entry over here. If a one and a two orthogonal than a 1.82 is What does it mean to be ": [
      2906.4,
      2934.8,
      107
    ],
    "of the space for an RN then a Subspace and it's orthogonal complement their Dimensions add up to the dimension end of the whole Space. An actual its first out a little further. What if I tell you okay, here's three dimensional space that we're in right now. What's the orthogonal complement of three-dimensional space inside three dimensional space? What's a set of all vectors that are perpendicular to every ": [
      898.8,
      925.5,
      33
    ],
    "one vector in terms of that basis can be computed in terms of. Potts like this That is that the coefficient of the vector v in the u-1 place is just the dotted with you one divided by the length of you 1 squared and so on Down the Line, we actually basically already proved this on the last flight. Let's just do the same calculation again. So what is ": [
      2543.8,
      2570.6,
      95
    ],
    "orthogonal basis. And those are going to be the best kind of face. He's so given a Subspace V of RN a basis for it is called an orthogonal basis if all the vectors in that bases are mutually orthogonal to each other we knew that they had to be linearly independent because it's a basis what we're saying is I want them to be the best kind of linear ": [
      2104.7,
      2128.9,
      78
    ],
    "orthogonal normal. That's the same thing as saying that the Matrix a who has those columns as its columns a transpose a is the identity Matrix be careful here. Hey, hey was an attempt by 3 Matrix in this case. There were three columns a transpose was three by m. And so we see that a transpose a is 3 by 3, but that Matrix It Could Have Had a ": [
      3024.1,
      3049.3,
      111
    ],
    "orthogonal the dot product is zero? Okay, so I would get a zero there. What about a 1. A30? And by the way those same numbers are written down here because a 2.81 is the same as a 1.82 and what we get is all zeros on the off diagonal parts and then on the diagonal. We get the lengths squared. So another way of saying a collection of vectors ": [
      2934.8,
      2970.3,
      108
    ],
    "out the coefficient of you won all I need to know. How is the length of you want? Okay. So orthogonal Ade is the best kind of linear Independence. Bright, so we get linear Independence whenever we have orthogonal of the around and what that means is, you know, remember orthogonal Ade linear Independence is one of the two key elements of a basis so we can now talk about ": [
      2078.4,
      2104.7,
      77
    ],
    "products divided by the product of the lengths of the two vectors. That's a number that's between -1 and 1 which means it's a number that is the cosine of something and that something that say that is actually geometrically meaningful. It is the angle between the two vectors SoDo product and code both length and angles than code everything geometric in space. So from there we're going to talk ": [
      331.5,
      357.6,
      14
    ],
    "quite right this thing. It has some vectors that are perpendicular to everything in the plane, but it also has some of the actors that are in the plane. So this set is too big to the orthogonal complement. So what's a second guess? Yes. It's the vector made by the bottle. That's almost exactly right except that's just one vector. What about twice that factor and 76 times that ": [
      788.0,
      812.3,
      29
    ],
    "recognizing what is a Subspace? So this is a concept that the midterm demonstrated that most of you are still having a lot of trouble with just not unexpected. In fact, it's the most abstract Concept in the whole course. It's just going to take another couple passes through for you guys to get it. So the one takeaway that you should take her one of the takeaways you should ": [
      58.2,
      79.6,
      2
    ],
    "reminders first capes are now available your course evaluations. We really appreciate the feedback you and you rate your instructor you rate your ta is give detailed feedback about what you thought worked. Well what you could be improved, I'd love to see all of you submit your capes. You can do that right now. I mean not right now, please please pay attention to the lecture, but the kids ": [
      102.0,
      125.6,
      4
    ],
    "rose, then your perpendicular to every linear combination of the Rose. In other words, you're perpendicular to the row space. There is the fundamental theorem of linear algebra and its just using the words. We've learned Now. Product orthogonal those words interpreting them in terms of what matrix multiplication says. So the orthogonal complement of the row space is the null space and that finally gives us some geometric insight ": [
      1536.9,
      1563.5,
      56
    ],
    "sees that give you zero are all zeros. That's what it would mean to prove. These are linearly independent and that really complex is about enqueued if I have these were n vectors an RN so it can take about any Cube steps. Well here when we have our flag and around we can do it in end steps instead or a 10 squared steps and times and stuff. So ": [
      1853.9,
      1880.9,
      68
    ],
    "sides. and * the columns Well, the way matrix multiplication works as I take that first row and X that First Column there to get the first entry and then I take that first row X at the second column to get the second entry and so on down the line. So what I'm going to get is the first entry is a 1 transpose * 8 1 which is ": [
      2835.0,
      2865.0,
      105
    ],
    "some linear combination c1u, 1 + C2 you to + etcetera cpup equals 0. I have someone there combination of them equaling zero. I want to show that all of those coefficients are zero. Know if these were a bunch of column vectors what I would then do as I have a vector equation here and I need to do row reduction to solve it and see that the only ": [
      1829.1,
      1853.9,
      67
    ],
    "space of a and W is a vector in the null space of a Then that implies that V is perpendicular to w. In other words there. Product is zero. That's the statement that's being made here. If I take a vector from the row space and a vector from the no space those two vectors are perpendicular to each other the first let's check that these things make sense ": [
      1168.7,
      1197.1,
      43
    ],
    "take a vector I compute its length of a square with. Bond with itself, which is a nonzero number because it's not the zero vector and I divide the vector. I stay lit by that number. And the reason we do that is if I calculate the length of you hat now. Hey, let's do the square of that. The length of you had is the dot product of you ": [
      2184.8,
      2204.4,
      81
    ],
    "take for the midterm is before the final exam. You should become more comfortable with the abstract notion of Subspace and Vector space and how those fit together and how to check if something is a Subspace what that means? Okay, because given that it was not very well done on the midterm. You can be sure that it's likely to appear again on the final exam. Okay, few administrative ": [
      79.6,
      102.0,
      3
    ],
    "take the dog product of V with say you won okay, I do exactly the same calculation I did last time. And I distribute through that some. I'm taking the dog park with you one in each case. Send almost all of those terms are zero. You too. You want a zero you three. You want Israel you pee. You want is 0 and all I'm left with. Is C1 ": [
      2600.9,
      2642.3,
      97
    ],
    "talking to Ed Tech about why it keeps failing. I was told that it's working this morning. So hopefully you will have a screencast from this lecture. The one thing that I can say is you can look at my lecture screencast from last winter winter 2017, which are available on the podcast side. There's a bit of a difference in when what topic was done than relative to now ": [
      241.6,
      266.2,
      10
    ],
    "that Subspace along the line right there's a line right here. It's in both of those plans. Now the vectors that are in both of those planes. They can't be perpendicular to this one. You can't have a vector in a plane that is perpendicular to that plane with one exception. There's one vector which is perpendicular to itself or vector. Is that the zero Vector so this is not ": [
      760.4,
      788.0,
      28
    ],
    "that are perpendicular to a plane form a line in R3. Okay, so you should think about lots of examples like that to get a good feeling for what's going on here and this notice about both of these kinds of examples doing the R2 case. 3000 complement or line was a line? Okay. So you have a one-dimensional Subspace and the compliment was one-dimensional. In three dimensional space if ": [
      841.5,
      868.4,
      31
    ],
    "that the length of you one is three. The length squared of U2 is 2 squared + -1 squared + 0 squared. Which is 5 and so therefore the length of U2 is the square root of 5. And you three dotted with itself. gives 2 squared + 5 squared + 4 squared which is 4 + 25 + 16, which is 45 and therefore the length of you three. ": [
      2399.1,
      2432.6,
      90
    ],
    "that we can even take that. Part of those have to live in the same space. So then no space if I have an Emma by 10 matrix and is the number of columns the no spaces a Subspace of RN. The row space is the Subspace spanned by The Rose, although as usual. We've turn them on their side and not think of them as columns. The number of ": [
      1197.1,
      1219.6,
      44
    ],
    "the base inspectors. And the remark here is that if you won through you pee are normalized if they are length one. Then that means exactly that their length squared for example r0 R1. So what we get, is that V dotted with you Jai is equal to CJ because that you 1 squared in the denominator. That's just one. So if you have an orthodontist basis, do you want ": [
      2668.0,
      2705.5,
      99
    ],
    "the definition of a one. A one that second entry is a 1.82. And so on so we take the a ones. It with a 1 through 8 3 in the first row. A2 dotted with a 1.83 in the second row And a $3 with a one through pastry. In the third row. And so we see that in this Matrix. A transpose a the entries are the. Products ": [
      2865.0,
      2906.4,
      106
    ],
    "the inner product. Is that for any two vectors u and v The square of the length of their difference which is the inner product of their difference with itself. is equal to you squared plus b squared but there's a correction term that is minus twice the inner product of you with v. So that's where is the is equal to twice the length of the vectors u and ": [
      467.7,
      500.9,
      19
    ],
    "the instructions. So let me check into that and tell you can also inquire with the Matlab ta directly about that. Okay, so that's everything. I wanted to say administratively like to proceed now with course material. So we are now talking about. Products and geometry. so last day One more thing last day. Unfortunately, the video screencast failed to record against was the fourth time not pleased about that ": [
      208.8,
      241.6,
      9
    ],
    "the line through the origin cuz its a Subspace that is perpendicular to it. That uniquely pens it down. No, I want to come to draw in three dimensions. But okay. What if I asked you here is a Subspace of R3 is table okay the table extended in and I'll directions here. Let's suppose that this bottle here is right at the origin. So here is a Subspace of ": [
      670.3,
      695.4,
      25
    ],
    "the no space which is the solution set of a homogeneous equation ax equals 0 those two vectors must be orthogonal to each other. That's what this theorem says know, why should that be true? It will let's look it up, you know a specific size example. Understand what's going on here. Let's look at a matrix a that is I don't know. Let's make a 2 by 3. Set ": [
      1239.0,
      1263.4,
      46
    ],
    "the opposite side to the right angle. Okay. Now that's not going to be true in general but it's true if you have a right angle. So that's what I've written right here that use the length of the vector you minus B squared Partners in that triangle is the sum of the squares of the other two lengths what we saw last time in general. Using the properties of ": [
      445.5,
      467.7,
      18
    ],
    "the other hand. Let's distribute the dot product through and that gives a C1 X you one dotted with you one plus C to you too. It with you one. Plus down the line cpup. With you one. It is a thing you too. It with you one is zero you three dotted with U10 you pee. It with you one is zero. All of those terms are already zero ": [
      1910.7,
      1940.3,
      70
    ],
    "the purpose of this but it let's just say I have three of them right now to do this calculation. So that means that a transpose. Means I take those columns and I write them as the rose. To take the transpose of the columns. of a transpose if I calculate this thing a transpose a so I'm taking that Matrix whose rows are The Columns of a on their ": [
      2809.1,
      2835.0,
      104
    ],
    "the row space is the column space of the transpose so we could also talk about the null-space of the transpose. So there are four important subspaces Associated to Matrix the column space of a and a transpose and then no space of a and and the column space of a transpose another name for that is just the row space today. and here is a big throw this theorem ": [
      1033.0,
      1055.8,
      38
    ],
    "the same calculation take that supposed to zero Vector linear combination of the use and take its. Product with you to the exact same calculation was Zero all of the coefficients except for the you two going to get you two. You two time to see two is equal to 0 but you two. You to the length of you choose. Where is not 0 and therefore C2 must be ": [
      2012.8,
      2035.0,
      74
    ],
    "the way, I told you they're up basis and the way you would normally check that if I hadn't told you to take those recall on the string them together in a matrix row reduce it to check to make sure that it's invertible. That's how you know, when something is a basis of our three. We don't need to check that here. Remember this was another point of contention ": [
      2316.0,
      2333.1,
      86
    ],
    "there's a shift of about a full lecture. We're about a full lecture head of what we were back then that's what I planned it this time. So you may need to look on a different date different lecture number last time but everything we're doing here you can see slightly differently, but the same ideas for the last time around. So if you're looking for a screencast from this ": [
      266.2,
      284.0,
      11
    ],
    "these are mutually orthogonal they are not normal vectors though. So if we wanted to normalize then we also need to check the lengths. So the length of you one I'll take the dog park commit with itself and I get 1 squared + -2 squared + -2 squared + 3 + 2 positive 2 squared which is 1 + 4 + 4 which is 9 and so that says ": [
      2375.7,
      2399.1,
      89
    ],
    "this is the dream this is where you're trying to isolate those coefficients and usually when you're doing row reduction the coefficients, you know of C3. It depends on the vectors you want you to use three and you for all the all around you have to combine all of them. Not what you have. It's what you would have hoped happen before we started taking this course to figure ": [
      2056.3,
      2078.4,
      76
    ],
    "this mean here? Just to remind you about coordinate vectors K. This means that V is equal to c1u. 1 + C2 you two plus... Cpup. Cpup. Sometimes it's hard to tell the difference between Pisces and you've sorry about that. Okay, so we want to find those seas and to do so usually involves solving a vector equation, but it doesn't have to in this case because if I ": [
      2570.6,
      2600.9,
      96
    ],
    "time, it's not their look from last year's and that will be a helpful resource to you. All right, we start talking about inner product. Product last time. Okay, so the dot product I haven't even written what it is defined to be down here, but maybe I ought to so by definition that. Product is just V transpose W. Okay, I guess you a number for to call injectors ": [
      284.0,
      306.8,
      12
    ],
    "times the length of you 1 squared? I know I divide through by the length of you one square then I get this. okay, so if I want to find the coefficients a vector in terms of an orthonormal basis. I don't need to do any row reduction. All I have to do is take the dot product of that Vector with the basis vectors and calculate the length of ": [
      2642.3,
      2668.0,
      98
    ],
    "to both of the Rose to all of the Rose. Now, the row space is linear combinations of those things, but that's okay because If I take any vector v which is some linear combination Lambda a plus Mew be some linear combination of the Rose. then if I take X dotted with v that's equal to rush actually called W. Cuz I called him W up here x daughter ": [
      1475.8,
      1506.8,
      54
    ],
    "to find the coefficients of a vector in terms that orthonormal basis there just exactly equal to the dot product of a vector with the basis vector. So that's how I find the coefficients of a vector in terms of the standard basis, right? I take a vector and I think it's. Product with e one that means taking the vector and taking 1 times its first component + 0 ": [
      2705.5,
      2727.3,
      100
    ],
    "to understand now I want to talk about orthogonally of sets of vectors sets of vectors. so here's an important example that was used over and over and with using lots of computations. So an R3 say core and in general and R3, we have our standard basis vectors 1 0 0 0 1 0 0 0 1 and we use those whenever we can they make computations as easy ": [
      1613.6,
      1641.8,
      59
    ],
    "together with the rank theorem usually goes under the name the fundamental theorem of linear algebra the most important theorem of the geometry of linear algebra, and it says that if I take the null-space of a matrix, what is the orthogonal complement of that? Okay, so the null-space of a matrix is a Subspace of RN or and is the number of columns. And it's worth I don't compliment ": [
      1055.8,
      1084.6,
      39
    ],
    "v times the cosine of the angle in-between and when the vectors are orthogonal that correction turn their the inner product of u and v is 0 Okay, let's quick recap of what we did last day. So now today I want to talk about this concept of orthogonal ality. I'm going to start actually instead of talking about orthogonally just two vectors. I want to talk about orthogonally of ": [
      500.9,
      525.2,
      20
    ],
    "vector 1 0 0 0 1 0 0 0 1 they all have lengths one. Right if I have just one wanted a bunch of zeros the sum of the squares of those things is what and those vectors are orthogonal to each other. So the standard basis is an orthonormal basis. Here's another one. I rather hear is an orthogonal basis of R3 and all three of them are ": [
      2259.4,
      2284.0,
      84
    ],
    "vector spaces fucking Audi of subspaces. So if I have a Subspace of RN? and as we as we go about this, let's think concretely so for example If I have a Subspace in are at an R2 and let's say it is a one-dimensional Subspace. So here's a vector and R2 and here is the Subspace that it generates almost Okay, so that line there is the Subspace who's ": [
      525.2,
      568.2,
      21
    ],
    "vectors are perpendicular to it. Every Vector is perpendicular to the zero Vector cuz the dot product of 0 with anything zero transpose V is equal to 0 mm more demonstrations of the fact that you get this complementarity. That the the two subspaces V and it's orthogonal complement they add up and dimension to the whole Space. Now that actually relates to something that we've already seen. We talked ": [
      958.5,
      985.5,
      35
    ],
    "ways but one thing I want to know right away is that when you have that situation when you have a bunch of vectors that are all mutually orthogonal every pair of them are orthogonal to each other one thing you get immediately for free is that those vectors are linearly independent from each other or Thug analogy of a bunch of vectors implies linear Independence now, why is that? ": [
      1781.4,
      1803.7,
      65
    ],
    "we see that Two vectors are perpendicular or the word. We will more commonly uses orthogonal those mean the same thing as are orthogonal if if and only if their dog product is zero, okay, by the way will often use this symbol for a thug in a little right make it crystal clear here like an upside down t Okay to say that they are orthogonal to each other. ": [
      394.5,
      419.8,
      16
    ],
    "what it means for the vector X to be in the null-space of a matrix product. There is the zero Vector. So let's write out. What this says? Let's use matrix multiplication say this is going to be a two-dimensional vector and I get it by multiplying along the First Column the first row of the first Matrix X the column X that gives me a 1 x 1 plus ": [
      1357.5,
      1382.6,
      50
    ],
    "where your section normal limits on Thursday. So if you need it on Thursday, if your sexual needs on Thursday at 5 p.m. Your Matlock place is going to be on Tuesday at 5 p.m. If you have a conflict with that there is there are conflict. I'm set up already for you to do it on Monday. Hopefully one of those will work for you. And if not then ": [
      170.1,
      188.7,
      7
    ],
    "which is generated by that Vector there that Vector is a basis for that one dimensional Subspace. the orthogonal complement of that Subspace Is the set of all vectors that are orthogonal to every Vector in that Subspace? V perp perp perpendicular V perp is the set of all vectors. In our end, so he was a Subspace of RN and it's a set of all vectors in RN. That ": [
      568.2,
      603.5,
      22
    ],
    "which is the number of columns in that case. So we get a complementarity thing to hear and it actually is the same theorem and it's because of this fact here. If I have a matrix M by n Matrix. There are three some spaces. I guess we could take for subspaces that we've talked about. There's the null-space the row space in the column space. But really remember that ": [
      1007.6,
      1033.0,
      37
    ],
    "with W is equal to will at Saks dotted with Lambda A+ movie. I know I use those linear properties of the. Product that we saw last day that slammed a x x dotted with a plus Mew x x. It would be But we just saw up here that both of those are zero. Until I get zero and there we have it. If your perpendicular to both the ": [
      1506.8,
      1536.9,
      55
    ],
    "with that discussion on Friday. UC San Diego podcast for more visit podcast. ": [
      3071.2,
      3087.9,
      113
    ],
    "yet. Let me make one more remark in the last one minute before we go here, which is that. Well I said it what if they are orthogonal but what if they're actually orthonormal if they're worth the normal? Then all of those links are one. And so really we get this so here is a very useful statement if I want to check if a collection of vectors are ": [
      3001.7,
      3024.1,
      110
    ],
    "your grade in this class is next week. Don't sweat it. It's no big deal. It's only going to take you a few minutes. It's not like the metal of homework or you have to read very carefully in and do things related to the course material. It's more of a basic test of your familiarity with basic functions in Matlab. Okay, and it's scheduled for Tuesday at the time ": [
      148.5,
      170.1,
      6
    ],
    "zero and you just keep going down the line and you'll see that each and every one of those coefficients is 0 so, this is great for two reasons. Why It only takes M squared steps. Now you have to do and computations and. Products right for each one of these and there's any of them so that's inscribed steps instead of n cubed. The second grade thing here is ": [
      2035.0,
      2056.3,
      75
    ]
  },
  "Full Transcript": "listening to a podcast Wednesday  Let's get going.  First let me briefly mentioned that you should all have seen by now that midterm to was graded. You've seen your exams. Do you know that we are using the gradescope regrade request system this time around it's open right now closes on Friday before midnight. So take a careful. Look if you feel like you want to request a read rate of a particular problem. There's a procedure to follow within gradescope at a detailed comment has to be done by this Friday at 11:59 p.m. The one thing I will say about the midterm is that people did reasonably well on most problems, but one that caught me off-guard that you didn't do as well on as I had thought you would was problem 3  Problem 3 was all about recognizing what is a Subspace? So this is a concept that the midterm demonstrated that most of you are still having a lot of trouble with just not unexpected. In fact, it's the most abstract Concept in the whole course. It's just going to take another couple passes through for you guys to get it. So the one takeaway that you should take her one of the takeaways you should take for the midterm is before the final exam. You should become more comfortable with the abstract notion of Subspace and Vector space and how those fit together and how to check if something is a Subspace what that means? Okay, because given that it was not very well done on the midterm. You can be sure that it's likely to appear again on the final exam.  Okay, few administrative reminders first capes are now available your course evaluations. We really appreciate the feedback you and you rate your instructor you rate your ta is give detailed feedback about what you thought worked. Well what you could be improved, I'd love to see all of you submit your capes. You can do that right now. I mean not right now, please please pay attention to the lecture, but the kids are now open and will be open until 11:59 p.m. Next Friday. So we have to do it before classes end if you're going to do it at all. I'll keep reminding you of that as we go.  You have a MyMathLab homework set that's due tomorrow night by 11:59 p.m. Your final Matlab homework is due on Friday by 11:59 p.m. And your Matlab quiz accounts for 5% of your grade in this class is next week. Don't sweat it. It's no big deal. It's only going to take you a few minutes. It's not like the metal of homework or you have to read very carefully in and do things related to the course material. It's more of a basic test of your familiarity with basic functions in Matlab. Okay, and it's scheduled for Tuesday at the time where your section normal limits on Thursday. So if you need it on Thursday, if your sexual needs on Thursday at 5 p.m. Your Matlock place is going to be on Tuesday at 5 p.m. If you have a conflict with that there is there are conflict. I'm set up already for you to do it on Monday. Hopefully one of those will work for you. And if not then get in touch with me. I think I saw a question about their  You know what? I I do not know the answer to that question. So I think what you should do. So the question is are you allowed to bring notes to the exam and I don't want to give you a false answer the mat like a part of the course is run kind of independently from the instructions. So let me check into that and tell you can also inquire with the Matlab ta directly about that.  Okay, so that's everything. I wanted to say administratively like to proceed now with course material. So we are now talking about. Products and geometry.  so last day  One more thing last day. Unfortunately, the video screencast failed to record against was the fourth time not pleased about that talking to Ed Tech about why it keeps failing. I was told that it's working this morning. So hopefully you will have a screencast from this lecture. The one thing that I can say is you can look at my lecture screencast from last winter winter 2017, which are available on the podcast side. There's a bit of a difference in when what topic was done than relative to now there's a shift of about a full lecture. We're about a full lecture head of what we were back then that's what I planned it this time. So you may need to look on a different date different lecture number last time but everything we're doing here you can see slightly differently, but the same ideas for the last time around. So if you're looking for a screencast from this time, it's not their look from last year's and that will be a helpful resource to you.  All right, we start talking about inner product. Product last time. Okay, so the dot product I haven't even written what it is defined to be down here, but maybe I ought to so by definition that. Product is just V transpose W. Okay, I guess you a number for to call injectors BMW and then we defined from that the length the length of a vector is the square root of the top part of the vector with itself, which is just the square root of the sum of the squares of the entries and we saw last time I finished my proving the cauchy Schwarz inequality, which tells you that what I've written here.  At least makes sense that the dock products divided by the product of the lengths of the two vectors. That's a number that's between -1 and 1 which means it's a number that is the cosine of something and that something that say that is actually geometrically meaningful. It is the angle between the two vectors SoDo product and code both length and angles than code everything geometric in space. So from there we're going to talk about now primarily is the special case Wednesday to equal zero.  28 equals 0 Oz are parallel really the important Point here is when Theta is equal to pi over 2, but they does 90\u00b0 when the vectors are perpendicular to each other. Okay. I'm from that formula that the product is equal to the product Alliance temp the cosine of theta the cosine of 90\u00b0 is 0 and so we see that  Two vectors are perpendicular or the word. We will more commonly uses orthogonal those mean the same thing as are orthogonal if if and only if their dog product is zero, okay, by the way will often use this symbol for a thug in a little right make it crystal clear here like an upside down t  Okay to say that they are orthogonal to each other. So two vectors are orthogonal. If and only if their. Product is zero, we saw this arose from consideration of Pythagoras Theorem from plane. Geometry says that if you have a right angle triangle, then the sum of the squares of the two sides on the two sides of the right angle. Some of those squares is equal to the square of the hypotenuse the square of the length of the opposite side to the right angle. Okay. Now that's not going to be true in general but it's true if you have a right angle. So that's what I've written right here that use the length of the vector you minus B squared Partners in that triangle is the sum of the squares of the other two lengths what we saw last time in general.  Using the properties of the inner product. Is that for any two vectors u and v  The square of the length of their difference which is the inner product of their difference with itself.  is equal to  you squared plus b squared but there's a correction term that is minus twice the inner product of you with v. So that's where is the is equal to twice the length of the vectors u and v times the cosine of the angle in-between and when the vectors are orthogonal that correction turn their the inner product of u and v is 0  Okay, let's quick recap of what we did last day. So now today I want to talk about this concept of orthogonal ality.  I'm going to start actually instead of talking about orthogonally just two vectors. I want to talk about orthogonally of vector spaces fucking Audi of subspaces.  So if I have a Subspace of RN?  and as we as we go about this, let's think concretely so for example  If I have a Subspace in are at an R2 and let's say it is a one-dimensional Subspace. So here's a vector and R2 and  here is  the Subspace that it generates almost  Okay, so that line there is the Subspace who's which is generated by that Vector there that Vector is a basis for that one dimensional Subspace.  the orthogonal complement of that Subspace  Is the set of all vectors that are orthogonal to every Vector in that Subspace?  V perp perp perpendicular V perp is the set of all vectors.  In our end, so he was a Subspace of RN and it's a set of all vectors in RN.  That are perpendicular to every vector.  in the Subspace  So on this particular picture, let's think about that for a second.  So here I've got a Subspace this line.  What does the perpendicular Subspace look like looking for all vectors that are orthogonal to that line?  If I see someone making this kind of emotion over there that I think that's as close as you'll be able to do to explain but I think you're telling me that what we need to do is look at the line over here.  That makes a right angle with that line. That's correct. Okay. So this line over here is V perp when this one here was be.  Okay, so that's a pretty relatively easy thing to understand if I give you a line in the plane. It's perpendicular Subspace. It's orthogonal complement is the line through the origin cuz its a Subspace that is perpendicular to it. That uniquely pens it down. No, I want to come to draw in three dimensions. But okay. What if I asked you here is a Subspace of R3 is table okay the table extended in and I'll directions here. Let's suppose that this bottle here is right at the origin. So here is a Subspace of R3. What is the orthogonal complement of that Subspace?  Someone like to make a guess or make a suggestion for what it looks like.  What time is no way down here? Yes.  Okay, a plane perpendicular going through the origin. That's a great. Guess I'll let's take a take a gander at that. So here's a piece of paper. Let's use that to represent are playing so maybe like in a plane like this or something.  Okay. Well, you know that plane that plane is perpendicular to this plane down here in the sense that you know, there's a right angle down here. But let's be careful here goes down by itself. Let's be careful here the definition of the perpendicular Subspace that are perpendicular to everything in the Subspace to look at this playing again here.  This Plane intersects that Subspace along the line right there's a line right here.  It's in both of those plans. Now the vectors that are in both of those planes. They can't be perpendicular to this one.  You can't have a vector in a plane that is perpendicular to that plane with one exception. There's one vector which is perpendicular to itself or vector. Is that  the zero Vector so this is not quite right this thing. It has some vectors that are perpendicular to everything in the plane, but it also has some of the actors that are in the plane. So this set is too big to the orthogonal complement. So what's a second guess? Yes.  It's the vector made by the bottle. That's almost exactly right except that's just one vector. What about twice that factor and 76 times that factor? So the correct answer which I think you really probably meant is it's the line through the bottle. It's the Subspace spanned by the bottle.  The orthogonal complement to this plane is this line perpendicular to it?  Similarly, what's the orthogonal complement to the line through this bottle?  It's the plane down here. Okay, the things that are perpendicular to a line in R3 or a plane the things that are perpendicular to a plane form a line in R3.  Okay, so you should think about lots of examples like that to get a good feeling for what's going on here and this notice about both of these kinds of examples doing the R2 case.  3000 complement or line was a line? Okay. So you have a one-dimensional Subspace and the compliment was one-dimensional.  In three dimensional space if I took a two-dimensional Subspace like a plane.  It's orthogonal complement was one-dimensional if I took a one-dimensional Subspace the line here. It's orthogonal complement was two dimensional.  What's going on here what's going on is that if you take a Subspace and it's orthogonal complement their Dimensions add up to 3 in the play Space Case data to in the plant in case they add up to the dimension of the space for an RN then a Subspace and it's orthogonal complement their Dimensions add up to the dimension end of the whole Space.  An actual its first out a little further. What if I tell you okay, here's three dimensional space that we're in right now. What's the orthogonal complement of three-dimensional space inside three dimensional space?  What's a set of all vectors that are perpendicular to every Vector in the room?  Just the zero Vector orthogonal complement of a vector space in itself is the zero Vector orthogonal complement of just the zero Subspace.  Just turn around what we just said.  I saw someone actually turn around not quite what I meant. But so if the orthogonal complement of everything is 0 what they're selling to complement of zero everything if I'm just a zero Vector what vectors are perpendicular to it. Every Vector is perpendicular to the zero Vector cuz the dot product of 0 with anything zero transpose V is equal to 0 mm more demonstrations of the fact that you get this complementarity.  That the the two subspaces V and it's orthogonal complement they add up and dimension to the whole Space. Now that actually relates to something that we've already seen.  We talked about some important subspaces like the null space in the column space of a matrix. Remember that the null-space of a matrix that has some Dimension the nullity the column space that Matrix has some dementia the rank and we have the rank theorem, which says that those two Dimensions they add up to the size of the space size of the of the codomain or start of the domain, which is the number of columns in that case.  So we get a complementarity thing to hear and it actually is the same theorem and it's because of this fact here.  If I have a matrix M by n Matrix.  There are three some spaces. I guess we could take for subspaces that we've talked about. There's the null-space the row space in the column space. But really remember that the row space is the column space of the transpose so we could also talk about the null-space of the transpose. So there are four important subspaces Associated to Matrix the column space of a and a transpose and then no space of a and and the column space of a transpose another name for that is just the row space today.  and here is a big throw this theorem together with the rank theorem usually goes under the name the fundamental theorem of linear algebra the most important theorem of the geometry of linear algebra, and it says that if I take the null-space of a matrix,  what is the orthogonal complement of that? Okay, so the null-space of a matrix is a Subspace of RN or and is the number of columns.  And it's worth I don't compliment is a Subspace of RN. What is it? It's the row space of the Matrix. This is just as the same statement about the case of a transpose is the orthogonal complement of the column space of a now remember that the row space on the column space to have the same dimension.  And the rank theorem tells us that the rank the dimension of space plus the Normandy the dimension of the no space is the dimension of the whole space and that's exactly what we were saying on the last slide with the orthogonal complement business that the dimension of the row space plus the dimension of its orthogonal complement, which is the null space is and is the dimension of the whole space. And so this is just another demonstration of the rank theorem that we were in fact a little bit about why this is true. We're going to prove this but more important proving it is to understand what's going on here.  So what does it mean? Let's look at that first statement there.  oops  What does it mean?  To say that the orthogonal comment of the row space is the null space.  So this means  that  if V is a vector in the row space of a  and W is a vector in the null space of a  Then that implies that V is perpendicular to w.  In other words there. Product is zero.  That's the statement that's being made here. If I take a vector from the row space and a vector from the no space those two vectors are perpendicular to each other the first let's check that these things make sense that we can even take that. Part of those have to live in the same space. So then no space if I have an Emma by 10 matrix and is the number of columns the no spaces a Subspace of RN.  The row space is the Subspace spanned by The Rose, although as usual. We've turn them on their side and not think of them as columns. The number of elements in a given row is the number of columns is Aunt. So again, these are two vectors an RN. So this at least has a chance of making sense on the statement were supposed to see here is that exactly this if I take a row of the Matrix if I take something in the row space which is a linear combination of rose and I take something in the no space which is the solution set of a homogeneous equation ax equals 0 those two vectors must be orthogonal to each other. That's what this theorem says know, why should that be true? It will let's look it up, you know a specific size example.  Understand what's going on here.  Let's look at a matrix a that is I don't know. Let's make a 2 by 3.  Set a note to write too much. Okay, so let's write it as  A1 A2 A3 B1 B2 B3  There are my two rows of The Matrix.  And so what is the row space Aerospace?  Okay with y'all use the text book conventions the column space of a transpose.  That's the span of the Rose but transposed so it's the span of the first row and the second row but treated as columns by turning them on their side.  So that's what the row space is is the span of those two vectors there. And what is the null space of a well just by definition. It's the set of vectors X in R34 which a x x is equal to 0.  Okay. Well, let's explore that a little bit. Let's write it out in this example. So what does it mean that a x x is equal to 0 so that means well, first of all, what size is the zero in this case a is 2 by 3.  Ex has to be a three dimensional Vector there for a x x is going to be to buy one going to be the two dimensional zero vector.  So let's write a  X x so that's X1 X2 X3.  So that's what it means for the vector X to be in the null-space of a matrix product. There is the zero Vector. So let's write out.  What this says? Let's use matrix multiplication say this is going to be a two-dimensional vector and I get it by multiplying along the First Column the first row of the first Matrix X the column X that gives me a 1 x 1 plus a 2 x 2 + 83 x 3.  And then doing the same thing, but with the second row here for the second column, that's v1x 1 + b 2 x 2 + B 3 x 3.  Forex to be in the null space says that that Vector there is zero, but let's recognize what's written here. What's written here is exactly  this first row here. Let's call that Vector a and let's go. Vector B.  Okay, I guess those are actually the transposes of the vectors cuz return them on their side.  This is a DOT product X and this is B. Projects.  So here's what we see for a vector X to be in the null-space of a exactly means that that Vector EX.  Has zero. Product with the rose of x?  X is in the null space of a means that  X. Each row  of a equals 0  in other words remember that. Park with a vector being being 0 means orthogonal being in the null space means that you are perpendicular to the rows of the Matrix adjusters what matrix multiplication says  so that's almost a complete proof of that statement there. We just saw that the null-space of the set of things that are perpendicular to both of the Rose to all of the Rose. Now, the row space is linear combinations of those things, but that's okay because  If I take any vector v which is some linear combination Lambda a plus Mew be some linear combination of the Rose.  then if I take X dotted with v  that's equal to rush actually called W. Cuz I called him W up here x daughter with W is equal to will at Saks dotted with Lambda A+ movie.  I know I use those linear properties of the. Product that we saw last day that slammed a x x dotted with a plus Mew x x. It would be  But we just saw up here that both of those are zero.  Until I get zero and there we have it. If your perpendicular to both the rose, then your perpendicular to every linear combination of the Rose. In other words, you're perpendicular to the row space. There is the fundamental theorem of linear algebra and its just using the words. We've learned Now. Product orthogonal those words interpreting them in terms of what matrix multiplication says. So the orthogonal complement of the row space is the null space and that finally gives us some geometric insight into what the no space is. Yeah. It's the solution set to the homogeneous equation x equals 0, which seems like a very different sort of object in the column space in the row space Witcher just the vector spaces stand by The Rose or the columns, but we see they are linked together.  They really are comprable things the column spacer becomes base of a transpose. The row space is some Subspace of this of the space Where The Roses live and then no space of the Matrix is the orthogonal complement of that thing. It's a set of vectors that are perpendicular to those and that geometrically is why the rank theorem is true.  Okay, because orthogonal complement they fill out the rest of the space.  Okay, so that's orthogonal complement something we're going to use and need to understand now I want to talk about orthogonally of sets of vectors sets of vectors.  so  here's an important example that was used over and over and with using lots of computations. So an R3 say core and in general and R3, we have our standard basis vectors 1 0 0 0 1 0 0 0 1 and we use those whenever we can they make computations as easy as possible or easier to work with than other vectors and one reason we can now see why that's true.  Is these vectors are all orthogonal to each other?  What's take the 1st 2?  so if I take  the first one. It with the third one say  left going to be 1 * 0 + 0 * 0 + 0 * 1 what's just gives me 0 and you can do the same thing with the first two or the last two any two of those vectors are perpendicular to each other be the standard basis vectors. This is the usual sort of graphic that you'll see in a first-year physics class. You take your your thumb your first and second fingers on a hand, I guess in physics. They would always use their right hand. I'm left-handed. So damn it. I'm doing with my left hand. Okay, those three vectors are perpendicular to each other all three of them. They formed this orthogonal Frame.  That's why the standard basis is useful, but it's not the only set of vectors that has that property.  Here's a different set of vectors three vectors in R3 on all three of these vectors are perpendicular to each other. We can just calculate that lets let's pick two of them at random to do. Okay. I'm going to choose say about you choose. Which two should I choose?  Two and three great. So let's take the dumb product of you two and you three.  Okay, so that's just going to be - 1 * - 1/2 + 2 * -2 + 1 * 7/2  Okay, so that gives me 1/2 - 4 + 7/2.  1/2 + 7/2 is 8/2 to this is -4 + 8/2 + 1/2 is 4 so we can get Sarah and if you want to go ahead and compute the other two. Products you too, and if you want a new Street, you'll see that all three of those vectors are orthogonal to each other.  Okay, great. So, how does that help us what helps us in a lot of ways but one thing I want to know right away is that when you have that situation when you have a bunch of vectors that are all mutually orthogonal every pair of them are orthogonal to each other one thing you get immediately for free is that those vectors are linearly independent from each other or Thug analogy of a bunch of vectors implies linear Independence now, why is that? Well, let's see here. So just to prove this there as usual linear Independence is a negative proper. It says that the vectors are not linearly dependent and if you're going to prove something about negative Universal Property, usually you have to argue by contradiction. We're going to say, okay. Well, let's suppose that they are linearly dependent and see what goes wrong with that assumption.  So if I have some linear combination c1u, 1 + C2 you to + etcetera cpup equals 0.  I have someone there combination of them equaling zero. I want to show that all of those coefficients are zero.  Know if these were a bunch of column vectors what I would then do as I have a vector equation here and I need to do row reduction to solve it and see that the only sees that give you zero are all zeros. That's what it would mean to prove. These are linearly independent and that really complex is about enqueued if I have these were n vectors an RN so it can take about any Cube steps. Well here when we have our flag and around we can do it in end steps instead or a 10 squared steps and times and stuff. So here's how we do that. We say therefore.  If I take that Vector c1u, 1 + C to you, too.  down the line cpup  I'm going to take its. Product with you want.  Now on the one hand that must equal zero.  Write it to you one dotted with zero. So this is that linear combination is the zero vector and therefore $0 with you on his hero, but on the other hand.  Let's distribute the dot product through and that gives a C1 X you one dotted with you one plus C to you too. It with you one.  Plus down the line cpup. With you one.  It is a thing you too. It with you one is zero you three dotted with U10 you pee. It with you one is zero. All of those terms are already zero because the vectors are orthogonal to each other and so all we're left with is this turn right here, but that one is equal to the length of you one square.  And so this whole thing is C1 times the length of you 1 squared.  I know it could have been that you wanted was the zero Vector in which case would just get there all together here. That's why for the theorem here. It's important that we have the statement that the vectors are not zero zero Vector is orthogonal to everything but the zero Vector can't be in a list of linearly independent vectors. So the precise there I'm if I wasn't if I have a bunch of nonzero vectors that are all orthogonal then they are linearly independent and what we see now is why that's true because what I get is that from this equation I get from this calculation I get that 0 is equal to 61 times the length of you 1 squared the length of you once where it is and not zero because you want is not the zero vector and so therefore we see that C10.  Okay, great. See one is it what about C2? We're just going to repeat exactly the same calculation take that supposed to zero Vector linear combination of the use and take its. Product with you to the exact same calculation was Zero all of the coefficients except for the you two going to get you two. You two time to see two is equal to 0 but you two. You to the length of you choose. Where is not 0 and therefore C2 must be zero and you just keep going down the line and you'll see that each and every one of those coefficients is 0 so, this is great for two reasons. Why  It only takes M squared steps. Now you have to do and computations and. Products right for each one of these and there's any of them so that's inscribed steps instead of n cubed. The second grade thing here is this is the dream this is where you're trying to isolate those coefficients and usually when you're doing row reduction the coefficients, you know of C3. It depends on the vectors you want you to use three and you for all the all around you have to combine all of them. Not what you have. It's what you would have hoped happen before we started taking this course to figure out the coefficient of you won all I need to know. How is the length of you want?  Okay. So orthogonal Ade is the best kind of linear Independence.  Bright, so we get linear Independence whenever we have orthogonal of the around and what that means is, you know, remember orthogonal Ade linear Independence is one of the two key elements of a basis so we can now talk about orthogonal basis. And those are going to be the best kind of face. He's so given a Subspace V of RN a basis for it is called an orthogonal basis if all the vectors in that bases are mutually orthogonal to each other we knew that they had to be linearly independent because it's a basis what we're saying is I want them to be the best kind of linear Independence most of the time  Linear independent vectors are not orthogonal most of the time they'll be some kind of slanted in or not all in the same plane or whatever, but it's some sort of slanted linear independent I'm saying no, I want them to be orthogonal to each other the best kind of linear Independence.  We can go one small step further to say and let me also insist that all the vectors have a length one and that's called normalized. So a vector is called normalized if it's length is one but that's actually easy to achieve whenever I have a nonzero Vector. Let me just do that right over here.  If you is not equal to zero.  Then we can normalize it.  What that means is we replace it with you. / it's length.  So I take a vector I compute its length of a square with. Bond with itself, which is a nonzero number because it's not the zero vector and I divide the vector. I stay lit by that number.  And the reason we do that is if I calculate the length of you hat now.  Hey, let's do the square of that. The length of you had is the dot product of you hat with itself.  Which is the dot product of you over its length?  With you over its length.  Okay, but now we use the properties of the. Product. We have that factor one over the length of you. It's in there twice so we can Factor it out and what we get is you. You / the length of you squared?  But you. You is the length of you squared. So this number is just what?  Okay, so you can always normalize a nonzero Vector. He can always divided by its length and make it a unit length Vector a normal vector. And so basis is called orthonormal. If all of the vectors in the bases are mutually orthogonal to each other and each one of those vectors has length one the canonical example of that the standard basis the vector 1 0 0 0 1 0 0 0 1 they all have lengths one. Right if I have just one wanted a bunch of zeros the sum of the squares of those things is what and those vectors are orthogonal to each other. So the standard basis is an orthonormal basis.  Here's another one. I rather hear is an orthogonal basis of R3 and all three of them are mutually orthogonal to each other. Let's just go ahead and check that so you won. It with you two.  that's equal to 2 + 0 - 2 which is 0  You one daughter with you three?  Is 2 - 10 + 8 which is 0 and you too. It with you three.  Is 4 + 0 - 4 which is Sarah Victors are all orthogonal to each other. And by the way, I told you they're up basis and the way you would normally check that if I hadn't told you to take those recall on the string them together in a matrix row reduce it to check to make sure that it's invertible. That's how you know, when something is a basis of our three. We don't need to check that here. Remember this was another point of contention from the midterm. Do you wants to check that an vectors an RN or better yet? If you have an n-dimensional Vector space you want to check that a set of n vectors Forma basis. All you have to check is at their linearly independent any set of n vectors in an n-dimensional space form a basis. They will spend it automatically.  So these are three vectors in R3 to check that they form a basis. We just need to check that they are linearly independent, but we have a Theron from the last flight. These are three nonzero vectors and we just checked that they're all mutually orthogonal. That means that they're linearly independent and therefore they form a basis. That was a lot easier to check then doing row reduction on a 3 by 3 Matrix. Okay. So these are mutually orthogonal they are not normal vectors though. So if we wanted to normalize then we also need to check the lengths.  So the length of you one I'll take the dog park commit with itself and I get 1 squared + -2 squared + -2 squared + 3 + 2 positive 2 squared which is 1 + 4 + 4 which is 9 and so that says that the length of you one is three.  The length squared of U2 is 2 squared + -1 squared + 0 squared.  Which is 5 and so therefore the length of U2 is the square root of 5.  And you three dotted with itself.  gives 2 squared + 5 squared + 4 squared  which is 4 + 25 + 16, which is 45 and therefore the length of you three.  Is equal to the square root of 45, which is better written is 3 times the square root of 5, so that leaves three is three times the length of U2.  So if I wanted to form a north and oral basis, I'd have to take the vectors you one, which is you want hats, which is you 1/3.  You too hot, which is you two over the square root of 5 and you Three Hats, which is you 3/3 * \u221a 5 and these ones are an orthonormal basis of our three and we'll use that shorthand fairly often. Yes.  The Hat indicates that they are lengths one. So if you see a vector and it has a hat on it, that means it has length one more generally if I took give you a vector and I call it smiley face. Then smiley face hat is defined to be smiley face / the length of smiley face.  quit  Okay, now what's so great about having an orthodontist basis or an orthogonal basis it's an extension of what we talked about when we were talking about orthogonally implying implying linear Independence so if I give you a basis right then every Vector in your vector space has a unique expansion terms that basis that's what the coordinate vectors all about but to find the coordinates of a basis of a vector in terms of a basis mean solving a vector equation, but if you have with other malady around it is Trivial or is very easy to solve vector equation if I have an orthogonal basis for a vector space for a Subspace to be then the coefficient Vector one vector in terms of that basis can be computed in terms of. Potts like this  That is that the coefficient of the vector v in the u-1 place is just the dotted with you one divided by the length of you 1 squared and so on Down the Line, we actually basically already proved this on the last flight. Let's just do the same calculation again. So what is this mean here?  Just to remind you about coordinate vectors K. This means that V is equal to c1u. 1 + C2 you two plus... Cpup.  Cpup. Sometimes it's hard to tell the difference between Pisces and you've sorry about that. Okay, so we want to find those seas and to do so usually involves solving a vector equation, but it doesn't have to in this case because if I take the dog product of V with say you won  okay, I do exactly the same calculation I did last time.  And I distribute through that some.  I'm taking the dog park with you one in each case.  Send almost all of those terms are zero. You too. You want a zero you three. You want Israel you pee. You want is 0 and all I'm left with.  Is C1 times the length of you 1 squared?  I know I divide through by the length of you one square then I get this.  okay, so if I want to find  the coefficients  a vector in terms of an orthonormal basis. I don't need to do any row reduction. All I have to do is take the dot product of that Vector with the basis vectors and calculate the length of the base inspectors. And the remark here is that if you won through you pee  are normalized if they are length one.  Then that means exactly that their length squared for example r0 R1.  So what we get, is that V dotted with you Jai is equal to CJ because that you 1 squared in the denominator. That's just one. So if you have an orthodontist basis, do you want to find the coefficients of a vector in terms that orthonormal basis there just exactly equal to the dot product of a vector with the basis vector.  So that's how I find the coefficients of a vector in terms of the standard basis, right? I take a vector and I think it's. Product with e one that means taking the vector and taking 1 times its first component + 0 * all the rest of the component components in the standard basis vectors.  So it's just saving us a whole lot of work when we have an orthodontist basis.  Okay.  In the last five minutes here. I want to now relate back to matrix multiplication. We saw or talked about orthogonal compliments that orthogonal of two vectors orthogonal which of the factors which is defined in terms of matrix multiplication can give you insight into row space force has no space and I want to repeat the same calculations. We did there to give you a concrete way to compute when a set of vectors are orthogonal to each other. So if I give you a matrix a  And this is finally going to tell us what does transpose is really useful for I'll give you a matrix a then if I multiply that matrix by its transpose on the left a transpose a that encodes the dot product of The Columns of a let's do that again with with a 2 by 3 Matrix. Actually. It doesn't matter what size The Columns are so it's take a and right it is but suppose it has three columns.  Those columns they might be a million dimensional. I don't care for the purpose of this but it let's just say I have three of them right now to do this calculation. So that means that a transpose.  Means I take those columns and I write them as the rose.  To take the transpose of the columns.  of a transpose if I calculate this thing a transpose a  so I'm taking that Matrix whose rows are The Columns of a on their sides.  and * the columns  Well, the way matrix multiplication works as I take that first row and X that First Column there to get the first entry and then I take that first row X at the second column to get the second entry and so on down the line. So what I'm going to get is the first entry is a 1 transpose * 8 1 which is the definition of a one. A one that second entry is a 1.82.  And so on so we take the a ones. It with a 1 through 8 3 in the first row.  A2 dotted with a 1.83 in the second row  And a $3 with a one through pastry.  In the third row.  And so we see that in this Matrix.  A transpose a the entries are the. Products of all of The Columns of the Matrix a that's what a transpose a computes. It computes the dot product of The Columns of a  no one quick observation here.  If the columns are all orthogonal to each other.  Then what does this give us? Well, let's look at this entry over here. If a one and a two orthogonal than a 1.82 is  What does it mean to be orthogonal the dot product is zero? Okay, so I would get a zero there. What about a 1. A30? And by the way those same numbers are written down here because a 2.81 is the same as a 1.82 and what we get is all zeros on the off diagonal parts and then on the diagonal.  We get the lengths squared.  So another way of saying a collection of vectors are orthogonal is to say that if I take those column vectors put them as The Columns of a matrix a  A transpose a is diagonal that's what it means to say that a collection of vectors are all mutually orthogonal. Yes.  Okay, so there is a relationship with eigenvalues here and we're going to get to that next week, but you shouldn't shouldn't talk about calculating this Matrix just yet. Let me make one more remark in the last one minute before we go here, which is that. Well I said it what if they are orthogonal but what if they're actually orthonormal if they're worth the normal?  Then all of those links are one.  And so really we get this so here is a very useful statement if I want to check if a collection of vectors are orthogonal normal.  That's the same thing as saying that the Matrix a who has those columns as its columns a transpose a is the identity Matrix be careful here. Hey, hey was an attempt by 3 Matrix in this case. There were three columns a transpose was three by m. And so we see that a transpose a is 3 by 3, but that Matrix It Could Have Had a Million entries in each call. Okay. So this you get a square Matrix out a transpose a is square they but they didn't need to be square for this to be true. If I have any collection of columns at all and a transpose a that's going to give me the identity Matrix of the appropriate size. That's what it means to say. That vectors are orthogonal. I will continue with that discussion on Friday.  UC San Diego podcast for more visit podcast. ",
  "Name": "math18_b00_wi18-03072018-1000",
  "File Name": "lecture_24.flac"
}