{
  "Blurbs": {
    "And If I multiply these two matrices A1 by n Matrix times in in by 1 Matrix while the ends in the middle tell me that the loud and the ones on the outside tell me that this is a 1 by 1 Matrix, which is just a real number. Okay. Now what real number is it when you can just write it down explicitly here. Let's let's go ahead ": [
      1008.9,
      1027.3,
      39
    ],
    "But remember Pythagoras Theorem doesn't apply to all triangles. It only applies to right triangles triangles where the angle in between them is 90\u00b0 if I have a very skinny triangle, it's just not going to work by the very wide triangle obtuse triangle. It's just not going to work but something like it always does there is a Pythagorean theorem with a correction term called the law of cosines. ": [
      2308.6,
      2330.4,
      90
    ],
    "But to get there we need to develop a little more mathematical technology constantly been building more mathematical language and technology and we need to do it one more time. So we're going to move into the last set of topics for this class, which is in chapter 6 Plus out of new topics, which are really just reinterpretations of old topics with a few new ideas. This is the ": [
      881.4,
      906.9,
      34
    ],
    "D and conjugated by P, which is a fancy term for that thing, there were u x p on one side and peeing verse on the other side that that gives you a back. Okay, I'll be explored the beginning of last week similarity. This notion of similarity between mixtures is not the same thing as Roku blind at all, but it went in particular it does tell you that ": [
      241.8,
      262.5,
      9
    ],
    "I can factor out that minus sign from the first factor to get the in a productivity with you and then over here. I have to factor out the -1 twice from both the first and the second Factor just like with the determinant if it appears in both places, it comes out twice. I know I've got a few terms that I should recognize here. So that inner product ": [
      2160.4,
      2182.0,
      84
    ],
    "Is inner product thing a third important property is that it is also distributive over scalar multiplication. If I take a vector and scale it and then tickets. Park with another vector. That's the same thing as taking the dot product of the two and multiplying it by that scalar and a similar calculation using the fact that it's just given by matrix multiplication will show you that that's true. ": [
      1326.8,
      1349.7,
      52
    ],
    "Lambda is the characteristic polynomial of that guy there. Okay, and from there we can read off the algebraic multiplicity. She's a mouthful of the two eigenvalues. There's two of us are the roots of that polynomial are five and four of the zeros of it and the algebraic multiplicity of the value 5 is too cuz there's a 2 in the exponent there. Okay that factor 5 minutes time. ": [
      415.3,
      442.7,
      16
    ],
    "Let me just indulge one more thing here and notes something important that I think that's important. At least it's important to me that I want you to connect the things we're doing in this class with real human beings. So these guys pushing shorts were super important mathematicians and their date arguably coaching more than shorts but shorts and a lot of great things. This was back in the ": [
      3034.5,
      3055.2,
      117
    ],
    "Let me write it backwards. Let me move that two times in a product of u and v to the other side. So we've got is that the length of you squared plus the length of B squared is equal to the length of you minus V squared + 2 * the inner product of u and v okay, so there's a nice formula and this relates the length of ": [
      2232.9,
      2258.2,
      87
    ],
    "Matrix. And actually you suck some hard exercises about this that's always a rank 1 Matrix one of your homework exercises had you go through a derivation of that fact and every rank 1 Matrix looks like that for two vectors u and v All right. Anyway, the inner product of two vectors is symmetric doesn't matter what order I write them. Another important property is it is distributive over ": [
      1243.8,
      1267.5,
      49
    ],
    "No, by the way, if you put these two things together what that says? Is that if you take the function t? t a v is equal to you. V for some fixed Vector you But this function T, which is a function of from RN to R. Is a linear transformation. Cuz that's what those two things together said. Okay. Well, actually that's not quite right what those two ": [
      1350.8,
      1393.8,
      53
    ],
    "Okay, and that's the way you should think of this thing. So this guy over here. Is the correction term? two photographer system okay, and we'll explore that quantitatively in on the next slide, but for now noticed that if it's zero. If the inner product there is zero. we get Pythagoras Theorem Which applies to right triangles? Okay, that is to say so in other words. if the inner ": [
      2330.4,
      2378.8,
      91
    ],
    "Okay. Now it depends on it in a predictable way. We'll get to that in a moment. But I just wanted you to keep that in mind. A beautiful thing here is so the distance. We just saw Define in terms of the top part of the inner product. It turns out that angles are also encoded by the dot product. I'm going to see that in the next calculation ": [
      1969.4,
      1992.5,
      76
    ],
    "So before we get into understanding what it is how it in codes geometry. I want to First write down some of its important properties that will use so the first property I want to note. Is that it is symmetric? If I take the dot product of you with v that's the same thing as the dot product of V with you. Okay, and that you can actually just ": [
      1130.8,
      1153.5,
      44
    ],
    "Twice in the characteristic polynomial. Where's the algebraic multiplicity of for Valium value for is one cuz that factor only appears once they're okay. So that is this algebraic multiplicity, but there's also a different kind of Multiplicity which we discussed on Monday. Would you given them last lecture which is geometric multiplicity and that is when I've got an eigenvalue Lambda what that means is it a minus lamb ": [
      442.7,
      467.1,
      17
    ],
    "Vector right there. Oops. Okay, so there's the vector u - B. Now to be clear usually we draw vectors anchored at the origin. So what we should really be doing is taking that green one and translating it down to be anchored at the orchard but that's not going to change its length, right? So what do I mean by length now? Well now we've take the length of ": [
      1756.8,
      1783.3,
      67
    ],
    "You can't look at a matrix and just tell by looking at it whether it's going to be diagonalizable. I'm this example that you saw last lecture beautifully demonstrates that where these two matrices are almost exactly the same you just changed one entry, but I didn't want to it and they even have the same eigenvalues and the same characteristic polynomial. But the first Matrix a is diagonalizable and ": [
      816.4,
      837.3,
      31
    ],
    "a reason it out here. This thing here is equal to you one plus you to transpose x v Now the transpose is a linear operation on vectors. So that's you one transpose plus you to transpose x b and matrix multiplication is distributive. So that's you one transpose B+ you to transpose be which is exactly what's written here from the definition of this thing is distributive over addition. ": [
      1293.9,
      1325.8,
      51
    ],
    "addition that what that means is that if I take two vectors and some of them up and take the dog part of that song with a vector that I get the same results if I take the dog products. And add up their values are like that that's distributive. And actually we can see that it's that just follows from matrix multiplication. Why is that? Well, it was just ": [
      1267.5,
      1293.9,
      50
    ],
    "an inner product of the inner product is symmetric. It doesn't matter what order I write the two. So that means that these two terms inside here are actually the same. They're actually equal to each other. and so I'm just going to write this is -2 inner product of u x v u and v so there's our calculation. So let me rewrite it like this. This says invite. ": [
      2206.2,
      2232.9,
      86
    ],
    "and do that. Okay. So here's U1 U2 u3 or the components of you and take the dot product with V1 V2 V3. I'm just working an R34 a convenience here, but it doesn't matter. What dimension where do the same idea? So what that means is that we're taking you transpose. Let me write that is a row. And multiplying it by fee. And now I just exercised my ": [
      1027.3,
      1053.4,
      40
    ],
    "and the other one. So this one it is multilinear. There's only two columns by linear. But it's not linear in the two together. It's linear in each one separately with the other health fixed. But this is unlike the determinant property one there says that no no no. No. This is not like the determinant in the determinant if I swap two columns, I introduced a minus sign. It ": [
      1440.8,
      1464.5,
      56
    ],
    "any of those are the eigenvalues of The Matrix Now the characteristic polynomial is a polynomial. So it's here's the example you saw and lecture last day these two matrices A and B, which are almost the same except one changed entry. They have the same characteristic polynomial with your computer and Lexie last day. There it is there and I've already factored it 5-9 2 squared * 4 - ": [
      391.8,
      415.3,
      15
    ],
    "are getting simpler. Looks like they're getting more complicated. But actually you'll see in a moment that we're already mostly there to relating the left hand thing the distance from YouTube TV to the lengths of you envy somehow. So first thing we'll do is we got them pesky minus signs around. But remember this thing the dot product of something with something else is linear in each of those ": [
      2116.5,
      2139.2,
      82
    ],
    "are the same for those two matrices that's always the case. If you have two matrices that have the same characteristic polynomial that must have the same eigenvalues. So we see that the algebraic Multiplicity. for both of them of 5 is 2 1/4 is 1 and that's true for both matrices a and because they have the same characteristic polynomial. So then in class last lecture went through the ": [
      520.8,
      551.9,
      20
    ],
    "are those two vectors which are in some kind of configuration like this. One of them is shorter than the other so that the distance between my hands right now is you know this distance here. But that isn't determined Alone by the lengths of my two arms as they're put because I could just move this one like that and I just increase the distance between my hands just ": [
      1923.4,
      1945.1,
      74
    ],
    "be transposed times. They might not even make sense. If you and me both have the same size it will make sense, but they typically won't be equal dollar different sizes even so caution. This is only for Rove actors column vectors either way that you transpose you make sense. Another thing that's important to note here is that this is not equal to Vu transpose Okay, the transpose has ": [
      1197.2,
      1222.6,
      47
    ],
    "bit. I'm sorry about that. It's a little bit rushed here part of the reason is that your final exam in case you weren't aware. It's at the very beginning of exam week. Okay. So this is an important point that I think some people might be confused about based on some exchanges. I've seen on the outside of your final exam is not the Saturday at the end of ": [
      37.1,
      56.7,
      1
    ],
    "by rotating the arm around right? I haven't changed the lengths of my arms, but I've increased the distance between them or I could do this and make the distance between them as small as possible. Okay, without changing the length of my arms. So the distance between two vectors doesn't just depend on the length of the two vectors. It also depends on the angle between the two actors. ": [
      1945.1,
      1969.4,
      75
    ],
    "by some estimations Irving seagull that's the only died in 2001. And he had a student named Leonard gross who's still alive? Who had a student named me? So I just wanted to point that out because all the things were doing in this class. They were discovered in invented by real people. Okay, and those real people had to learn these things and invent things the same way you ": [
      3080.6,
      3108.0,
      119
    ],
    "calculation and computed basis for the null space has a -5 I + 8 + b - 5i and you saw that a - 5i it's no spaces two-dimensional. You do the road action. You see there are two free variables that has a two dimensional space b - 5 I on the other hand. It had only one free variable its nose specialist one-dimensional. So we Steve hear that ": [
      551.9,
      575.8,
      21
    ],
    "can I tell without doing all of that work and maybe wasting my time whether the Matrix is diagonalizable, so there's a partial answer that question. A big syrup the biggest theorem in this course and in my opinion the biggest theremin all of mathematics and we're going to get to that in the very end of the course that's are our goal here. Okay, so that's that's our plan. ": [
      859.6,
      881.4,
      33
    ],
    "chapter on what I would call geometry or quantitative geometry. This is not just for the purpose of diagonalize ability of matrices bees are important ideas. We're going to need and using you're going to need and use in future forces many of you have already seen a lot of what we're going to do today and tomorrow, so we're not going to talk about. Product. Is it two words ": [
      906.9,
      931.0,
      35
    ],
    "consisting of eigenvectors. You just string together the eigenvectors drink together a basis of each eigenspace in those will form a basis for RN. If you ever have the algebraic multiplicity exceeding the geometric multiplicity, you won't have enough eigenvectors to form a basis and the Matrix will not be diagnosed. That's more or less the end of the story and that there's no there's no better condition in general. ": [
      792.2,
      816.4,
      30
    ],
    "difference between them all drawing and green. That's the vector which let's see you minus V is a thing which when you add V2 it gives you you so that means it starts at the tip of the tail of Ian at the tip of Ian goes to the tale of you. So I have to start at the tip of the and go down to you. So it's this ": [
      1731.9,
      1756.8,
      66
    ],
    "do as well. Have a nice day. UC San Diego podcast ": [
      3108.0,
      3114.3,
      120
    ],
    "do it on your calculator. You want to figure out the angle between two vectors you take their dog product you divide that number by the product of the lengths and then you press the arc cosine button. Okay, so that's easy enough, but I want to spend the last six minutes here exploring this a little bit more carefully. So first of all, this was a picture proof, you ": [
      2644.4,
      2667.4,
      102
    ],
    "early 19th century and one great thing. He did was to Mentor a student whose name was Reese who was another very important mathematician if you take some higher math classes like math 140, you're going to see his name a lot. Reese had a student named Ian are hella who was a american-born Swedish mathematician who produced a super important analyst the most important analyst of the 20th century ": [
      3055.2,
      3080.6,
      118
    ],
    "eigenvectors for that Matrix. That's not enough to form a basis for R3. Yes. I'm afraid I can't hear you. Is the geometric multiplicity always greater than or equal to one great question. Let's answer it together. What does geometric multiplicity mean is the dimension of that? No space or no space of a minus line. So, let's suppose for a moment you the answer your question was know what ": [
      654.4,
      686.1,
      25
    ],
    "enter product V - t x v inner product you okay. I know the last term is going to have a inner product of - TV with itself. I'm going to factor out that my nasty twice. So it's plus minus t squared times the inner product of the with v. Now let me simplify that you didn't product to you is the length of you squared. You enter product ": [
      2841.8,
      2869.6,
      110
    ],
    "entries are generally we can use this to define the distance between two vectors. We said that the distance between two vectors is the length of the difference between those two vectors. Let's demonstrate this with an example here. And we can draw so here. I'm going to draw for you two vectors. The usual way we represent vectors on a grid in terms of arrows. So there's two vectors ": [
      1628.4,
      1658.2,
      63
    ],
    "entries, what is that? Anybody know? Maybe forget the third component if I give you a vector in R2, I take the sum of the squares of its entries. What does that give you? guess the square of the length. Thank you. Mister Pythagoras. Exactly. Okay. Yes, this number is the square of the length of the vector and that's how the inner product in codes lengths. Okay. So we ": [
      1572.2,
      1605.6,
      61
    ],
    "exam is so keep that in mind. I'm going to keep reminding you there are rooms that have been assigned. They're far too small for the class. So we're still negotiating with the registrar. Hopefully get some better room. So stay tuned. And there's one remaining Matlab homework assignment is due this Friday at 11:59 p.m. Okay. Alright, so I'm going to spend the first 5 minutes of this lecture ": [
      75.2,
      100.2,
      3
    ],
    "exam week. It's the Saturday at the beginning of exam week. It's in 12 days from now. Okay, and I really am sorry about that. It's a pain in the butt. I can't do anything about it. That's just when the registrar schedule your final exam. So if you want to go and pick it and protest the register, I'll be right there with you but that is when your ": [
      56.7,
      75.2,
      2
    ],
    "for the same thing? This is from chapter 6 section 6.1. So going back a little bit and not as much to do with matrices right now, but we'll get there next time. So if I have vectors an RN just are and don't worry about any crazy polynomials basis just RN regular column vectors. I can take their duct products. Okay. Another word for that is inner product, which ": [
      931.0,
      957.0,
      36
    ],
    "get this generalized Pythagorean relation a squared plus b squared equals c squared plus a correction term. And that correction term is to AV cosine of that Angle. Now if the angle with 90\u00b0 angle is pi over 2, then cosine of 90\u00b0. That's that correction term isn't there so that exactly accounts for Non, right triangles and how they behave and I thought you were in type way. Now ": [
      2460.0,
      2489.3,
      95
    ],
    "give me the length of you minus t x v squared where T is the input here for HT. What I do is out of the linear combination of u and v which is you - t x to be Okay, snap. The thing I want you to notice about this function is that this function is a non- function. It's the length of something. It's value is always positive ": [
      2790.9,
      2814.0,
      108
    ],
    "going to say well the angle between two vectors that's just the arc cosine of this ratio. It better bloody. Well be true that that ratio is never bigger than one or never smaller than -1 know. Can you look at that thing. Product of two vectors divided by the product of their lights and tell me yeah, of course, that number is between -1 and 1. Is that obvious ": [
      2717.0,
      2739.6,
      105
    ],
    "have a positive number and a negative number and usually positive to negative only if it's a vector itself is 0 so we can take it square root of square root is another positive number. So that's what we'd find me the length of a vector the length of a vector v and RN is \u221a v. V just the square root of the sum of the squares of the ": [
      1605.6,
      1628.4,
      62
    ],
    "having a basis of eigenvectors. That is that there is there is a basis of n vectors since our end so that have to be and vectors in a basis all of which are eigenvectors of the Matrix a so that happens when you have all the things I can values it can also happen. When you have some repeated eigenvalues, they have to have enough. I got to have ": [
      315.1,
      338.8,
      12
    ],
    "having inner product equal to zero. So interpark not only recordes lengths but it also record perpendicularity recordes orthogon. Ality. What's another word for the same thing when two vectors are at right angles to each other that is recorded algebraically just by there. Product being 0 and we can even be more precise than that. We can talk about angles are 90\u00b0 angles as well. So let's let's go ": [
      2410.8,
      2435.9,
      93
    ],
    "here if I repeat a vector twice when I get is a strictly positive number unless I started with the zero vector and then I get zero. Now that's important because it means that I'm assigning a positive number to each Vector teach. Nonzero Vector. Does anybody know or remember what that positive number means? If I give you a vector in R3 the sum of the squares of its ": [
      1547.8,
      1572.2,
      60
    ],
    "here. You might have to be as though the vector connecting those two if I take those three line segments my two arms and the line connecting my hands. What kind of geometric object do they form a triangle and that form it says the length squared of this guy post the link Square to this guy is equal to the length squared of the hypotenuse. This is Pythagoras Theorem. ": [
      2283.2,
      2305.7,
      89
    ],
    "impact of u and v be actually, let me let me call this last one to see that first one V Square. Let me call it. Those are just three numbers a b and see what kind of function is this. It's a quadratic polynomial but we're being told here is that this quadratic polynomial is always positive. How can I possibly be let's draw a graph. Of a generic-looking ": [
      2896.5,
      2926.3,
      112
    ],
    "is a non invertible Matrix. Animals that a man is Lambda has something and it's no space. So it's no space is not Zero Dimensional. It's at least one dimensional but it might be two dimensional or three-dimensional. I mean, it might have more than one eigenvector in its basis in a basis for it. So that's another number the dimension of the null-space of a minor slammed II and ": [
      467.1,
      489.9,
      18
    ],
    "is a square root \u221a the dot product with itself. So let's take the square and we'll take a square root in the end. Okay. So let's just calculate see what this thing is. So this by definition and now I'm going to use that other notations going to be easier to keep track that triangular bracket notation for. Product. So this is the length of this is the dot ": [
      2019.5,
      2042.7,
      78
    ],
    "is zero in the first component means it literally lives in the X2 X3 plane and its three units in the X2 Direction and two in the X3 Direction. Okay, let's try this one in red. okay, so there's my two vectors u and v now the difference between them the difference between them and I'm probably going to mess up which direction it goes. I usually do but the ": [
      1700.0,
      1731.9,
      65
    ],
    "is zero. in which case at you. You which is zero. Zero is just zero. Okay. So here we have this thing this by linear function this. Product and it satisfies these properties. It's linear and each of the entries. It's symmetric. But also it has this Nifty property down here, which the determinant certainly didn't have if I had a repeated, determinant. I always got zero where I was ": [
      1519.6,
      1547.8,
      59
    ],
    "it's not and how do we tell? What's what's the obstruction to diagonalize Ability when you talked about these Notions last lecture multiplicity of eigenvalues. So we how do we calculate the eigenvalues we write down the characteristic polynomial which is the determinant of a minus land of times. I that's a polynomial in Lambda. And as we saw its roots paint the zeros of that polynomial if there are ": [
      365.1,
      391.8,
      14
    ],
    "key is the same as being a product you so that's minus 2 times the inner product of you with v x t and then we get plus the length of V squared x t squared. What kind of function of T is this? So those things let me read just give them new names. Let me call you squared equal to a Let me call - 2 * the ": [
      2869.6,
      2896.5,
      111
    ],
    "know when we do that we have to be very careful so noticed that the cosine of an angle. Is always a number between -1 and 1/8 the angle is right if I draw the graph of the cosine function. That's not a very good picture of the drought graph of the cosine function, but roughly like that it never exceeds one or minus one on either side. So I'm ": [
      2690.6,
      2717.0,
      104
    ],
    "landham of in X-Men is Lambda in the characteristic polynomial and the Matrix is diagonalizable similar to a diagonal matrix, if and only if First of all, you need to have all real eigenvalues you have to have real only real roots for your characteristic polynomial and those multiplicities match the geometric multiplicity and the algebraic multiplicity have to match. Okay that case you can find a basis for RN ": [
      764.3,
      792.2,
      29
    ],
    "linear in each of those variables. So what that means is in particular if I if I think about freezing the second entry that you might have to be on the second entry that this is the same thing as you times the inner product as you enter product with you minus V. + - V * the inner product of you minus fee I know I'm going to use ": [
      2065.6,
      2090.4,
      80
    ],
    "listening to a podcast administrivia reminders in case you missed this in your email last week on Friday. We posted homework 7 on MyMathLab. It covers everything we did last week and what we're doing in today's lecture and it is due on Thursday night this Thursday night by 11:59 p.m. And there will be one more MyMathLab assignment after that do the following Thursday at 11:59 p.m. A little ": [
      2.0,
      37.1,
      0
    ],
    "matrix multiplication muscles and I multiply those two things you won't be one and I add. the product of those two things and I add to that the product of those two things. Okay, and that's it. So that's the dot product you just take the products of the entries that of the like entries and add them up. Okay. Alright, so great. What is that mean what we're going ": [
      1053.4,
      1087.6,
      41
    ],
    "not very nice numbers. That's okay. I've got these two vectors with non integer lengths. And then I got the difference you minus V and it's length is 3 so, how does that relate to those too? Well, the first important observation is that it's not determined by those two vectors lengths. And you can just see that with my favorite demonstration with my arms again. So imagine my arms ": [
      1899.7,
      1923.4,
      73
    ],
    "of cosines tells us what the angle means because from the last sign the last slide we saw in the last slide that in fact the length of you squared plus the length of e squared. is equal to length of you minus V squared plus twice the inner product of u and v So if we match these two up. What we see is that the inner product of ": [
      2568.5,
      2595.0,
      99
    ],
    "of the sum of the squares of the entry. So that's 1 squared + -2 squared + -2 squared. That's of course, you see it didn't matter whether I got it right for which direction the vector go is because if I'd gotten it wrong, I would have just had a minus sign in front of all those entries and that would give the same length cuz I'm taking the ": [
      1809.4,
      1829.1,
      69
    ],
    "of the things that you learn in Friday's lecture by his excellent lecture from Professor Edgar's so remember that a matrix a square Matrix all of this discussion of eigenvectors and eigenvalues only makes sense for square matrices a square and buy on Matrix is called diagonalizable. If it is similar to a diagonal matrix remember similar means that there is some invertible Matrix pee so that if you take ": [
      213.0,
      241.8,
      8
    ],
    "of you with you is by definition the inner product of you squared. That last term there. 8 - 1 squared as one and the inner part of V with v is just the length of the squared and in the middle, I'm left with these two terms. Not a lot I can do about that except I can notice that once again using properties from that first slide of ": [
      2182.0,
      2206.2,
      85
    ],
    "one eigenvector 101 dimensional space of the geometric multiplicity of any eigenspace is always at least one. So in fact in this case because the geometric the algebraic multiplicity of the eigenvalue for is one, you know, that the geometric multiplicity matches. It's also one but it's Windows powers are higher than mine when you have repeated eigenvalues that things can go wrong like in this example. Okay, so that's ": [
      712.1,
      739.8,
      27
    ],
    "or zero and it's only zero if you minus TV is equal to zero. So it's a non- function but on the other hand. This is the dot product of you minus TV with itself. And if I do the same thing I did in the last two slides ago where I multiply that out. Okay, what I'm going to get is you and product you. minus t x u ": [
      2814.0,
      2841.8,
      109
    ],
    "picture. We need to believe it from the calculation. It so that's what distance means and that's how we calculate if it's quite simple. I have a question that I want to address in. This is going to motivate the rest of the lecture. So actually read the question here so that distance the distance between u and v. How does it relate to the lengths of you and me ": [
      1854.3,
      1877.4,
      71
    ],
    "product of u and v is 0 then we really do get that you squared plus b squared length. Is equal to the length of you minus B squared and by Pythagoras Theorem that supposed to say the same thing as you and me being right angles to each other. So what we see here, is that perpendicularity of two vectors? Geometrically motivated by perpendicular is the same thing as ": [
      2378.8,
      2410.8,
      92
    ],
    "product of you minus V with itself. That's what it is. And I'd like to manipulate that expression until I get something that has the length of you in the length of e separated in it. Well, the way I can do that is to use property to from the first slide we talked about here when we talked about properties of this thing, which is that this thing is ": [
      2042.7,
      2065.6,
      79
    ],
    "product of you with v and 4 * AC is 4 times the length of you squared times the length of V squared. so that the fact that this country inside their that discriminant is negative exactly tells us after we divide 3 by 4 and take the square root that the cauchy Schwarz inequality holds true. So it's really just your high school algebra. Okay. Well you're packing up. ": [
      3003.7,
      3034.5,
      116
    ],
    "pronounced in her product. But they mean exactly the same thing and all they mean is that you transpose me. Okay. Now let's just figure this out for a second. What kind of object is you transpose V well u and v are column vectors. So you is has En Rose and one column and sohvi does as well. So you transpose is one by and it's a row vector. ": [
      979.4,
      1007.8,
      38
    ],
    "quadratic polynomial typically if you have a quadratic polynomial, it'll have two roots and this one opens upward because that a which is length of each word is positive. Typically it's going to have to roots and in between those two Roots. It's going to be negative. Right, but that can fail to happen. It might look like this or it might actually come down and touch exactly once so ": [
      2926.3,
      2953.9,
      113
    ],
    "reviewing And discussing a little bit again. Will you learn last week on Monday on Friday and lecture about eigenvalues and eigenvectors and diagonalization and then we're going to move on to chapter 6 going to talk about some things from sections 6.1 and 6.7 which really repeat each other to a large extent. So we're going to be looking at some things from both of those sections. Okay. Next ": [
      100.2,
      125.3,
      4
    ],
    "see from the formula that I wrote down over here that some you want to be one plus YouTube V2 plus use 3 V3 if I exchange the roles of you and envy that would just be v1u 1 + 2 + vs Ryu Street. And of course Matrix Matrix multiplication real number multiplication is commutative. So it doesn't matter what order I write those. Let me know that what ": [
      1153.5,
      1173.6,
      45
    ],
    "should really trust picture proofs cuz it's easy to be deceived by them. I would like to hear everything here was true. But one thing to keep in mind is that everything I drink here was in the plane. Okay, where is this mattress made of accident or 76 and I'm using this intuition to Define really what angles mean for vectors that are in r76 maybe? Okay, but you ": [
      2667.4,
      2690.6,
      103
    ],
    "so let's let's look back at that picture that u and v there. And they have different lengths of the length of you. We could calculate its the square root of 1 squared + 1 squared. So this is \u221a 2 + V. It's length is the square root of 3 squared + 2 squared + 3 squared is 92 squared is for so that's the square root of 13 ": [
      1877.4,
      1899.7,
      72
    ],
    "some eigenvalues examples like the rotation matrix by 90 degrees which has no real eigenvalues at all. So that one is not diagonalizable in the sense. Okay, but as long as you have some eigenvalues, even if you know the characteristic polynomial it might be Lambda -3 to the power n meaning. There's only one eigenvalue 3, it's still possible that that Matrix is diagonalizable, but it's also possible that ": [
      338.8,
      365.1,
      13
    ],
    "square root by taking the square root of the dot product of vector with itself. But the angle is also encoded there. Okay, how is it in coded there? Well like this. Stata is equal to dividing through its the inner product of you with v / the length of u x length of v and I have to take the arc cosine of that. So that's how you would ": [
      2617.9,
      2644.4,
      101
    ],
    "squared minus 4AC if that number is positive then you're going to get two distinct real Roots. So because of this fact here that thing under the square root sign there must be negative or else we would have a negative value for this function at some points. So that must be a negative number but let's just substituting. What is b squared. So be is -2 times the inner ": [
      2977.3,
      3003.7,
      115
    ],
    "squares of the centuries and adding them up. So that's the square root of 1 + 4 + 4. Which is the \u221a 9 or 3. So that's the distance between those two vectors the distance between if you like the distance between their tips is 3 zactly 3 units on that picture, which is believable from the picture, but we don't need to you don't believe it from the ": [
      1829.1,
      1854.3,
      70
    ],
    "than or equal to but it can be strictly less than like in this example here when that happens. You cannot diagnose The Matrix you just won't have enough eigenvectors around. Okay, cuz the way you build a basis for RN out of eigenvectors of the way you try to figure out what the eigenvalues are and then compute a basis for each eigenspace and you put those basis vectors ": [
      603.8,
      629.4,
      23
    ],
    "that Vector so the distance between you and V. Is the length of you minus V now, we just we can see in the picture what you might have to be is but we can also just calculate you might have to be here. You must be a 1-0, which is 1 1-3, which is -2 and 0 - 2 which is -2 From the definition that's the square root ": [
      1783.3,
      1809.4,
      68
    ],
    "that means is that you transpose V is the same as V transpose you Okay, and that's true. But a word of caution here, this is only true because those those are row vectors. I mean if I have any two matrices A and B that have the right sizes to multiply a transpose times be that product is not generally going to equal be transposed times at in fact ": [
      1173.6,
      1197.2,
      46
    ],
    "that number is the geometric multiplicity and then the fact of the matter is that those two numbers don't have to match so you can have the geometric multiplicity the dimensions that no space. It can be smaller than the algebraic multiplicity these examples down here demonstrate that so both A and B those two matrices there cuz they have the same characteristic polynomial. Right here. And so the eigenvalues ": [
      489.9,
      520.8,
      19
    ],
    "that the length of you squared plus the length of V squared. is equal to the length of you minus V squared plus a correction term, which is twice the length of you times the length of the times the cosine of the angle in between the vectors u and v Play whatever that means while we know what it means wood rot in the plane, but actually the law ": [
      2544.6,
      2568.5,
      98
    ],
    "that we do at roughly how and then we'll see precisely how right after that. This question still stands. How do those two relate to each other what we saw, you know philosophically it's going to involve the distances and the angle but let's go back to the definition. So this distance here. Okay, the distance between the two vectors. Is the length of you minus B and this thing ": [
      1992.5,
      2019.5,
      77
    ],
    "the eigenvalues of those too much. These are the same and the eigenspaces have the same dimensions. Thanks to their isomorphic eigenspaces. So that's what it means to be diagonalizable. And in fact, we introduced that notion as a corollary basically have a proof that we gave that if your Matrix has if you're invited Matrix has n distinct eigenvalues. Get all and eigenvalues that is so that I can ": [
      262.5,
      289.8,
      10
    ],
    "the formula of here. Okay, so that just means you one squared plus you two squared plus you three squared. This is in the 3 by 3 setting is the sum of the squares of the entries. Now that's important. One reason. It's important that we can see immediately that is a positive number 8. In fact, it's strictly positive strictly positive unless You equal 0 unless the vector itself ": [
      1490.6,
      1519.6,
      58
    ],
    "the geometric multiplicity of the eigenvalue 54 B is only one Well, it's Jim Patrick. Multiple. City is to Okay, so changing just one entry of the Matrix can make a startling difference to that profile of the Matrix. They have the same eigenvalues with the same algebraic multiplicity is but they can have different geometric multiplicity is always the geometric multiplicity never exceeds the algebraic multiplicity. It's always less ": [
      575.8,
      603.8,
      22
    ],
    "the linearity in the second variable say well, actually that's the inner product of you with you. Plus the inner product of you with - V. in the first part Okay, that's not and this guy is in a product of minus V with you plus the inner product of -2 be with minus V. That's the second part. Now I've got four terms and it doesn't look like things ": [
      2090.4,
      2116.5,
      81
    ],
    "the second Matrix is not so it's really hard to tell when this obstruction occurs or not. It's a deep algebraic problem. But there is a partial answer so I can ask how am I supposed to deal with this? I want to diagonalize Matrix. We know that that's a really useful technique for doing things like taking powers of the Matrix, which is important lots of applications. So, how ": [
      837.3,
      859.6,
      32
    ],
    "the second variable cuz it was the same way there. So this should remind you of something that should remind you of when we talked about properties of the determinant, which of the determinant was multilinear if I freeze all but one column, it's linear that one column in the same is true here. If I freeze either one of the two columns, then the this inner product is linear ": [
      1418.1,
      1440.8,
      55
    ],
    "the simple matrix product. It includes everything about geometry EOC. It encodes distances and angles all together in one simple package. It sounds to me like I'm a Salesman selling you the inner product and I really I am trying to and I hope you will buy it. Okay, cuz it's going to be really important for us and for almost everything that you do mathematically scientifically from now on. ": [
      1107.9,
      1130.8,
      43
    ],
    "them is you minus fee which goes here. So there's you minus B, and this is exactly the same picture. okay, this is exactly same picture as above and if we match we see that a Is equal to the length of you? and B is equal to the length of e and C is equal to the length of you minus p So the law of cosines says exactly ": [
      2513.3,
      2544.6,
      97
    ],
    "things literally say is that if I take T of you Is equal to you. It be for a fixed V by cuz it's the first variables that you variable that we're modifying with linear operations there but because of property one because you don't use the same as me. You the fact that this thing is linear and the first variable means it's also going to be linear in ": [
      1393.8,
      1418.1,
      54
    ],
    "those in the sense that instead of spending 20% of the time on those like we have in terms of time in the class will spend maybe 35% of the time on those so maybe instead of 2 out of 8 questions of a 3 out of 8 questions on that material. That's the idea. Okay. Alright, so let us get moving. Talking about diagonalization. I want to review some ": [
      188.3,
      213.0,
      7
    ],
    "through this one more time. We've just gone through this calculation. Okay, let's match terms even more precisely. So if you remember from your high school or probably high school geometry class or trigonometry class. The law of cosines says that's if you have any triangle has sides or a b and c and the angle in between the A and B sides is Theta measured in radians. Then you ": [
      2435.9,
      2460.0,
      94
    ],
    "time on Wednesday will move on to section 6.2 orthogonal Ade. Are there any questions before we begin because I do hear a lot of talking. Any questions to share with the class? Yes. Okay. Okay, good question. So the question is the midterm midterm too. Well, it was built on stuff that you learn in the first third of the course. It was not explicitly cumulative. The final exam ": [
      125.3,
      166.8,
      5
    ],
    "to get into that in the rest of this lecture. But first, I want to point out a few properties of it before we get into what it means that those of you who've taken for example math 20c already which some of you may have you've already seen this for sure and you know what it means. Okay, and really the amazing thing is that this simple algebraic thinking ": [
      1087.6,
      1107.9,
      42
    ],
    "to go in the beginning. Okay, and why is that will V whenever V was a column vector and buy one you transpose. Is that vert for one by N. I can do that product because the ones in the middle match, but this thing here is going to be in by an that's not even the right kind of objects. Not a number to Matrix and buy on Square ": [
      1222.6,
      1243.8,
      48
    ],
    "to you? If it is then you know, we need to talk it is I don't think it's obvious at all. But it's Estero. Okay, and it's called the cauchy Schwarz inequality. Cauchy Schwarz inequality probably the most important basic inequality in all of mathematics. It says exactly that but if I take you. V and / the length of you times the length of E, I get a number ": [
      2739.6,
      2766.8,
      106
    ],
    "together and you hope to form a basis of our end. And in this example hear the Matrix be okay. It's geometric multiplicity of the eigenvalue 5 is just one you can only get one eigenvector out of that one and the Daikin value for its algebraic multiplicity as one which means it's geometric multiplicity. Also can't be more than one so you could only ever get two linearly independent ": [
      629.4,
      654.4,
      24
    ],
    "two vectors u and v can be interpreted as dividing through by 2 equal to the length of you times the length of the times the cosine of the angle in between them. And that is precisely quantitatively how the inner product in codes both lengths and angles. You can figure out the length of a vector just by taking the sum of the squares the entries and taking the ": [
      2595.0,
      2617.9,
      100
    ],
    "u and v what's actually plot them. So the vector u110e question. Is there a question for you is 110 it's there to component of zero. That means it's it's actually living in the X1 X2 plane. And where is it there? It's at the point 1-1. So that's Right here. pistolettes draw 10 Okay, so there's that Vector you. and the vector v Well, that's a 03 to so ": [
      1658.2,
      1700.0,
      64
    ],
    "values as we now know where the roots of the characteristic polynomial degree in polynomial that has at most n real if it has a distinct real Roots all different. Then this will happen The Matrix will be diagonalizable. It will have at least similar to a diagonal matrix and that actually came up from the second equivalence statement here, which is the diagonalize ability is the same thing as ": [
      289.8,
      315.1,
      11
    ],
    "variables. That means that it distributes over Sam has but it also means that distributes over scale are multiples so I can factor out those minus signs, but I got is that this is the inner product of you with you. Minus the inner product of you with a V and this this Prim over here, which is minus in a product of inner product of -2 be with you. ": [
      2139.2,
      2160.4,
      83
    ],
    "was antisymmetric or alternating if I switch to Collins it changes the sign here. It's symmetric if I switch to columns that you count it doesn't change the value and there's one more very important property probably the most important property of the inner product, which is it if I take the dot product or inner product of a vector with itself. What do I get? Well, let's look at ": [
      1464.5,
      1490.6,
      57
    ],
    "what can go wrong. So let me summarize that all on the next flight here. So this is just repeating what we just said out loud and reasoned out together. If a is a matrix Square Matrix and has a repeated eigenvalue, then the geometric multiplicity the dimension of the null space today. Might as well and I is less than or equal to the algebraic multiplicity its power of ": [
      739.8,
      764.3,
      28
    ],
    "what we just saw is. Okay. Let's let's do this for a vectors u and v know those were three dimensional vectors, but let's work in the plain where they live so we can always think of two vectors is living in two Dimensions will make that even more precise next lecture. Okay, and what we saw was we've got two vectors u and v and the vector in between ": [
      2489.3,
      2513.3,
      96
    ],
    "which is no bigger than one in absolute value. And actually we have exactly enough time. I'd like to show you why this is true like to show you a proof of this fact. Okay, so here's how we're going to prove it. I'm going to define a function a regular old function like the kind that you saw in your algebra class when you were in high school. It's ": [
      2766.8,
      2790.9,
      107
    ],
    "will be explicitly cumulative. Hey will be about twice as long as your midterm exams and it will have questions like the ones you saw on midterm 1 as well as like those the ones you saw him enter into and some new questions now, it will not be evenly distributed because you haven't been tested yet on chapters 5 6 and 7. So it's going to concentrate more on ": [
      166.8,
      188.3,
      6
    ],
    "would it mean that the dimension of that? No space is zero. It would mean there are no eigenvectors. That's what you just said. Right and there's no free variables, but it means the null-space is 0 means there are no nonzero vectors in there which means there's no eigenvectors. But how can that be Lyme disease supposed to be an eigenvalue. So every eigenvalue has to have at least ": [
      686.1,
      712.1,
      26
    ],
    "you can have three possibilities with a quadratic polynomial. It can have two Roots one double root or no roots at all. How can we tell what we have a formula for the roots? The roots are negative B plus or minus the square root of B squared minus 4AC over 2A. There's a quadratic formula and you see from here that if that number inside the square root B ": [
      2953.9,
      2977.3,
      114
    ],
    "you might not have heard before if you haven't been reading the textbook what it means exactly the same thing and there's two common notation sport. You probably seen the first one more than the last one. So the first one is you put a. Between the two vectors and that's what product does triangular brackets. Around the two vectors with a, in between and usually that one would be ": [
      957.0,
      979.4,
      37
    ],
    "you the length of the and the inner product of u and v no. Let's ignore that inner product of you with v term for just a moment here and we have something here that says length of you squared plus length of B squared is equal to length of you minus B squared ignoring that other term Let's think about this geometrically you was over here via is over ": [
      2258.2,
      2283.2,
      88
    ]
  },
  "Full Transcript": "listening to a podcast administrivia reminders  in case you missed this in your email last week on Friday. We posted homework 7 on MyMathLab. It covers everything we did last week and what we're doing in today's lecture and it is due on Thursday night this Thursday night by 11:59 p.m. And there will be one more MyMathLab assignment after that do the following Thursday at 11:59 p.m.  A little bit. I'm sorry about that. It's a little bit rushed here part of the reason is that your final exam in case you weren't aware. It's at the very beginning of exam week. Okay. So this is an important point that I think some people might be confused about based on some exchanges. I've seen on the outside of your final exam is not the Saturday at the end of exam week. It's the Saturday at the beginning of exam week. It's in 12 days from now. Okay, and I really am sorry about that. It's a pain in the butt. I can't do anything about it. That's just when the registrar schedule your final exam. So if you want to go and pick it and protest the register, I'll be right there with you but that is when your exam is so keep that in mind. I'm going to keep reminding you there are rooms that have been assigned. They're far too small for the class. So we're still negotiating with the registrar. Hopefully get some better room. So stay tuned.  And there's one remaining Matlab homework assignment is due this Friday at 11:59 p.m. Okay. Alright, so I'm going to spend the first 5 minutes of this lecture reviewing And discussing a little bit again. Will you learn last week on Monday on Friday and lecture about eigenvalues and eigenvectors and diagonalization and then we're going to move on to chapter 6 going to talk about some things from sections 6.1 and 6.7 which really repeat each other to a large extent. So we're going to be looking at some things from both of those sections. Okay. Next time on Wednesday will move on to section 6.2 orthogonal Ade.  Are there any questions before we begin because I do hear a lot of talking.  Any questions to share with the class?  Yes.  Okay.  Okay, good question. So the question is the midterm midterm too. Well, it was built on stuff that you learn in the first third of the course. It was not explicitly cumulative. The final exam will be explicitly cumulative. Hey will be about twice as long as your midterm exams and it will have questions like the ones you saw on midterm 1 as well as like those the ones you saw him enter into and some new questions now, it will not be evenly distributed because you haven't been tested yet on chapters 5 6 and 7. So it's going to concentrate more on those in the sense that instead of spending 20% of the time on those like we have in terms of time in the class will spend maybe 35% of the time on those so maybe instead of 2 out of 8 questions of a 3 out of 8 questions on that material. That's the idea. Okay.  Alright, so let us get moving.  Talking about diagonalization. I want to review some of the things that you learn in Friday's lecture by his excellent lecture from Professor Edgar's  so remember that a matrix a square Matrix all of this discussion of eigenvectors and eigenvalues only makes sense for square matrices a square and buy on Matrix is called diagonalizable. If it is similar to a diagonal matrix remember similar means that there is some invertible Matrix pee so that if you take D and conjugated by P, which is a fancy term for that thing, there were u x p on one side and peeing verse on the other side that that gives you a back. Okay, I'll be explored the beginning of last week similarity. This notion of similarity between mixtures is not the same thing as Roku blind at all, but it went in particular it does tell you that the eigenvalues of those too much. These are the same and the eigenspaces have the same dimensions.  Thanks to their isomorphic eigenspaces. So that's what it means to be diagonalizable. And in fact, we introduced that notion as a corollary basically have a proof that we gave that if  your Matrix has if you're invited Matrix has n distinct eigenvalues.  Get all and eigenvalues that is so that I can values as we now know where the roots of the characteristic polynomial degree in polynomial that has at most n real if it has a distinct real Roots all different. Then this will happen The Matrix will be diagonalizable. It will have at least similar to a diagonal matrix and that actually came up from the second equivalence statement here, which is the diagonalize ability is the same thing as having a basis of eigenvectors. That is that there is there is a basis of n vectors since our end so that have to be and vectors in a basis all of which are eigenvectors of the Matrix a so that happens when you have all the things I can values it can also happen.  When you have some repeated eigenvalues, they have to have enough. I got to have some eigenvalues examples like the rotation matrix by 90 degrees which has no real eigenvalues at all. So that one is not diagonalizable in the sense. Okay, but as long as you have some eigenvalues, even if you know the characteristic polynomial it might be Lambda -3 to the power n meaning. There's only one eigenvalue 3, it's still possible that that Matrix is diagonalizable, but it's also possible that it's not and how do we tell? What's what's the obstruction to diagonalize Ability when you talked about these Notions last lecture multiplicity of eigenvalues. So we how do we calculate the eigenvalues we write down the characteristic polynomial which is the determinant of a minus land of times. I that's a polynomial in Lambda. And as we saw its roots paint the zeros of that polynomial if there are any of those are the eigenvalues of The Matrix  Now the characteristic polynomial is a polynomial. So it's here's the example you saw and lecture last day these two matrices A and B, which are almost the same except one changed entry. They have the same characteristic polynomial with your computer and Lexie last day. There it is there and I've already factored it 5-9 2 squared * 4 - Lambda is the characteristic polynomial of that guy there.  Okay, and from there we can read off the algebraic multiplicity. She's a mouthful of the two eigenvalues. There's two of us are the roots of that polynomial are five and four of the zeros of it and the algebraic multiplicity of the value 5 is too cuz there's a 2 in the exponent there. Okay that factor 5 minutes time. Twice in the characteristic polynomial. Where's the algebraic multiplicity of for Valium value for is one cuz that factor only appears once they're okay. So that is this algebraic multiplicity, but there's also a different kind of Multiplicity which we discussed on Monday. Would you given them last lecture which is geometric multiplicity and that is when I've got an eigenvalue Lambda what that means is it a minus lamb is a non invertible Matrix.  Animals that a man is Lambda has something and it's no space. So it's no space is not Zero Dimensional. It's at least one dimensional but it might be two dimensional or three-dimensional. I mean, it might have more than one eigenvector in its basis in a basis for it. So that's another number the dimension of the null-space of a minor slammed II and that number is the geometric multiplicity and then the fact of the matter is that those two numbers don't have to match so you can have the geometric multiplicity the dimensions that no space. It can be smaller than the algebraic multiplicity these examples down here demonstrate that so both A and B those two matrices there cuz they have the same characteristic polynomial.  Right here. And so the eigenvalues are the same for those two matrices that's always the case. If you have two matrices that have the same characteristic polynomial that must have the same eigenvalues. So we see that the algebraic Multiplicity.  for both of them  of 5 is 2 1/4 is 1 and that's true for both matrices a and because they have the same characteristic polynomial. So then in class last lecture went through the calculation and computed basis for the null space has a -5 I + 8 + b - 5i and you saw that a - 5i it's no spaces two-dimensional. You do the road action. You see there are two free variables that has a two dimensional space b - 5 I on the other hand. It had only one free variable its nose specialist one-dimensional. So we Steve hear that the geometric multiplicity of the eigenvalue 54 B is only one  Well, it's Jim Patrick. Multiple. City is to  Okay, so changing just one entry of the Matrix can make a startling difference to that profile of the Matrix. They have the same eigenvalues with the same algebraic multiplicity is but they can have different geometric multiplicity is always the geometric multiplicity never exceeds the algebraic multiplicity. It's always less than or equal to but it can be strictly less than like in this example here when that happens. You cannot diagnose The Matrix you just won't have enough eigenvectors around. Okay, cuz the way you build a basis for RN out of eigenvectors of the way you try to figure out what the eigenvalues are and then compute a basis for each eigenspace and you put those basis vectors together and you hope to form a basis of our end. And in this example hear the Matrix be okay. It's geometric multiplicity of the eigenvalue 5 is just one you can only get one eigenvector out of that one and the  Daikin value for its algebraic multiplicity as one which means it's geometric multiplicity. Also can't be more than one so you could only ever get two linearly independent eigenvectors for that Matrix. That's not enough to form a basis for R3. Yes.  I'm afraid I can't hear you.  Is the geometric multiplicity always greater than or equal to one great question. Let's answer it together. What does geometric multiplicity mean is the dimension of that? No space or no space of a minus line. So, let's suppose for a moment you the answer your question was know what would it mean that the dimension of that? No space is zero.  It would mean there are no eigenvectors. That's what you just said. Right and there's no free variables, but it means the null-space is 0 means there are no nonzero vectors in there which means there's no eigenvectors. But how can that be Lyme disease supposed to be an eigenvalue. So every eigenvalue has to have at least one eigenvector 101 dimensional space of the geometric multiplicity of any eigenspace is always at least one. So in fact in this case because the geometric the algebraic multiplicity of the eigenvalue for is one, you know, that the geometric multiplicity matches. It's also one but it's Windows powers are higher than mine when you have repeated eigenvalues that things can go wrong like in this example.  Okay, so that's what can go wrong. So let me summarize that all on the next flight here. So this is just repeating what we just said out loud and reasoned out together. If a is a matrix Square Matrix and has a repeated eigenvalue, then the geometric multiplicity the dimension of the null space today. Might as well and I is less than or equal to the algebraic multiplicity its power of landham of in X-Men is Lambda in the characteristic polynomial and the Matrix is diagonalizable similar to a diagonal matrix, if and only if  First of all, you need to have all real eigenvalues you have to have real only real roots for your characteristic polynomial and those multiplicities match the geometric multiplicity and the algebraic multiplicity have to match. Okay that case you can find a basis for RN consisting of eigenvectors. You just string together the eigenvectors drink together a basis of each eigenspace in those will form a basis for RN. If you ever have the algebraic multiplicity exceeding the geometric multiplicity, you won't have enough eigenvectors to form a basis and the Matrix will not be diagnosed. That's more or less the end of the story and that there's no there's no better condition in general. You can't look at a matrix and just tell by looking at it whether it's going to be diagonalizable. I'm this example that you saw last lecture beautifully demonstrates that where these two matrices are almost exactly the same you just changed one entry, but I didn't want to it and they even have the same eigenvalues and the same characteristic polynomial.  But the first Matrix a is diagonalizable and the second Matrix is not so it's really hard to tell when this obstruction occurs or not. It's a deep algebraic problem.  But there is a partial answer so I can ask how am I supposed to deal with this? I want to diagonalize Matrix. We know that that's a really useful technique for doing things like taking powers of the Matrix, which is important lots of applications. So, how can I tell without doing all of that work and maybe wasting my time whether the Matrix is diagonalizable, so there's a partial answer that question.  A big syrup the biggest theorem in this course and in my opinion the biggest theremin all of mathematics and we're going to get to that in the very end of the course that's are our goal here. Okay, so that's that's our plan. But to get there we need to develop a little more mathematical technology constantly been building more mathematical language and technology and we need to do it one more time. So we're going to move into the last set of topics for this class, which is in chapter 6 Plus out of new topics, which are really just reinterpretations of old topics with a few new ideas. This is the chapter on what I would call geometry or quantitative geometry. This is not just for the purpose of diagonalize ability of matrices bees are important ideas.  We're going to need and using you're going to need and use in future forces many of you have already seen a lot of what we're going to do today and tomorrow, so we're not going to talk about. Product. Is it two words for the same thing? This is from chapter 6 section 6.1.  So going back a little bit and not as much to do with matrices right now, but we'll get there next time. So if I have vectors an RN just are and don't worry about any crazy polynomials basis just RN regular column vectors. I can take their duct products. Okay. Another word for that is inner product, which you might not have heard before if you haven't been reading the textbook what it means exactly the same thing and there's two common notation sport. You probably seen the first one more than the last one. So the first one is you put a. Between the two vectors and that's what product does triangular brackets.  Around the two vectors with a, in between and usually that one would be pronounced in her product. But they mean exactly the same thing and all they mean is  that you transpose me.  Okay. Now let's just figure this out for a second. What kind of object is you transpose V well u and v are column vectors. So you is has En Rose and one column and sohvi does as well. So you transpose is one by and it's a row vector.  And If I multiply these two matrices A1 by n Matrix times in in by 1 Matrix while the ends in the middle tell me that the loud and the ones on the outside tell me that this is a 1 by 1 Matrix, which is just a real number.  Okay. Now what real number is it when you can just write it down explicitly here. Let's let's go ahead and do that.  Okay. So here's U1 U2 u3 or the components of you and take the dot product with V1 V2 V3. I'm just working an R34 a convenience here, but it doesn't matter. What dimension where do the same idea?  So what that means is that we're taking you transpose. Let me write that is a row.  And multiplying it by fee.  And now I just exercised my matrix multiplication muscles and I multiply those two things you won't be one and I add.  the product of those two things  and I add to that the product of those two things.  Okay, and that's it. So that's the dot product you just take the products of the entries that of the like entries and add them up.  Okay. Alright, so great. What is that mean what we're going to get into that in the rest of this lecture. But first, I want to point out a few properties of it before we get into what it means that those of you who've taken for example math 20c already which some of you may have you've already seen this for sure and you know what it means. Okay, and really the amazing thing is that this simple  algebraic thinking the simple matrix product. It includes everything about geometry EOC. It encodes distances and angles all together in one simple package. It sounds to me like I'm a Salesman selling you the inner product and I really I am trying to and I hope you will buy it. Okay, cuz it's going to be really important for us and for almost everything that you do mathematically scientifically from now on. So before we get into understanding what it is how it in codes geometry. I want to First write down some of its important properties that will use so the first property I want to note.  Is that it is symmetric?  If I take the dot product of you with v that's the same thing as the dot product of V with you.  Okay, and that you can actually just see from the formula that I wrote down over here that some you want to be one plus YouTube V2 plus use 3 V3 if I exchange the roles of you and envy that would just be v1u 1 + 2 + vs Ryu Street. And of course Matrix Matrix multiplication real number multiplication is commutative. So it doesn't matter what order I write those. Let me know that what that means is that you transpose V is the same as V transpose you  Okay, and that's true.  But a word of caution here, this is only true because those those are row vectors. I mean if I have any two matrices A and B that have the right sizes to multiply a transpose times be that product is not generally going to equal be transposed times at in fact be transposed times. They might not even make sense. If you and me both have the same size it will make sense, but they typically won't be equal dollar different sizes even so caution. This is only for Rove actors column vectors either way that you transpose you make sense. Another thing that's important to note here is that this is not equal to  Vu transpose  Okay, the transpose has to go in the beginning.  Okay, and why is that will V whenever V was a column vector and buy one you transpose. Is that vert for one by N. I can do that product because the ones in the middle match, but this thing here is going to be in by an that's not even the right kind of objects. Not a number to Matrix and buy on Square Matrix. And actually you suck some hard exercises about this that's always a rank 1 Matrix one of your homework exercises had you go through a derivation of that fact and every rank 1 Matrix looks like that for two vectors u and v  All right. Anyway, the inner product of two vectors is symmetric doesn't matter what order I write them. Another important property is it is distributive over addition that what that means is that if I take two vectors and some of them up and take the dog part of that song with a vector that I get the same results if I take the dog products.  And add up their values are like that that's distributive. And actually we can see that it's that just follows from matrix multiplication. Why is that? Well, it was just a reason it out here. This thing here is equal to you one plus you to transpose x v  Now the transpose is a linear operation on vectors. So that's you one transpose plus you to transpose x b and matrix multiplication is distributive. So that's you one transpose B+ you to transpose be which is exactly what's written here from the definition of this thing is distributive over addition.  Is inner product thing a third important property is that it is also distributive over scalar multiplication.  If I take a vector and scale it and then tickets. Park with another vector.  That's the same thing as taking the dot product of the two and multiplying it by that scalar and a similar calculation using the fact that it's just given by matrix multiplication will show you that that's true.  No, by the way, if you put these two things together what that says?  Is that if you take the function t?  t a v is equal to you. V for some fixed Vector you  But this function T, which is a function of from RN to R.  Is a linear transformation.  Cuz that's what those two things together said. Okay. Well, actually that's not quite right what those two things literally say is that if I take T of you  Is equal to you. It be for a fixed V by cuz it's the first variables that you variable that we're modifying with linear operations there but because of property one because you don't use the same as me. You the fact that this thing is linear and the first variable means it's also going to be linear in the second variable cuz it was the same way there.  So this should remind you of something that should remind you of when we talked about properties of the determinant, which of the determinant was multilinear if I freeze all but one column, it's linear that one column in the same is true here. If I freeze either one of the two columns, then the this inner product is linear and the other one. So this one it is multilinear. There's only two columns by linear.  But it's not linear in the two together. It's linear in each one separately with the other health fixed.  But this is unlike the determinant property one there says that no no no. No. This is not like the determinant in the determinant if I swap two columns, I introduced a minus sign. It was antisymmetric or alternating if I switch to Collins it changes the sign here. It's symmetric if I switch to columns that you count it doesn't change the value and there's one more very important property probably the most important property of the inner product, which is it if I take the dot product or inner product of a vector with itself.  What do I get? Well, let's look at the formula of here. Okay, so that just means you one squared plus you two squared plus you three squared. This is in the 3 by 3 setting is the sum of the squares of the entries. Now that's important. One reason. It's important that we can see immediately that is a positive number 8. In fact, it's strictly positive strictly positive unless  You equal 0 unless the vector itself is zero.  in which case  at you. You which is zero. Zero is just zero.  Okay. So here we have this thing this by linear function this. Product and it satisfies these properties. It's linear and each of the entries. It's symmetric. But also it has this Nifty property down here, which the determinant certainly didn't have if I had a repeated, determinant. I always got zero where I was here if I repeat a vector twice when I get is a strictly positive number unless I started with the zero vector and then I get zero.  Now that's important because it means that I'm assigning a positive number to each Vector teach. Nonzero Vector. Does anybody know or remember what that positive number means?  If I give you a vector in R3 the sum of the squares of its entries, what is that?  Anybody know?  Maybe forget the third component if I give you a vector in R2, I take the sum of the squares of its entries. What does that give you?  guess  the square of the length. Thank you. Mister Pythagoras. Exactly. Okay.  Yes, this number is the square of the length of the vector and that's how the inner product in codes lengths. Okay. So we have a positive number and a negative number and usually positive to negative only if it's a vector itself is 0 so we can take it square root of square root is another positive number. So that's what we'd find me the length of a vector the length of a vector v and RN is \u221a v. V just the square root of the sum of the squares of the entries are generally we can use this to define the distance between two vectors. We said that the distance between two vectors is the length of the difference between those two vectors.  Let's demonstrate this with an example here.  And we can draw so here. I'm going to draw for you two vectors.  The usual way we represent vectors on a grid in terms of arrows. So there's two vectors u and v what's actually plot them. So the vector u110e question.  Is there a question for you is 110 it's there to component of zero. That means it's it's actually living in the X1 X2 plane. And where is it there? It's at the point 1-1. So that's  Right here.  pistolettes draw 10  Okay, so there's that Vector you.  and the vector v  Well, that's a 03 to so is zero in the first component means it literally lives in the X2 X3 plane and its three units in the X2 Direction and two in the X3 Direction.  Okay, let's try this one in red.  okay, so there's my two vectors u and v  now the difference between them the difference between them and I'm probably going to mess up which direction it goes. I usually do but the difference between them all drawing and green. That's the vector which let's see you minus V is a thing which when you add V2 it gives you you so that means it starts at the tip of the tail of Ian at the tip of Ian goes to the tale of you. So I have to start at the tip of the and go down to you. So it's this Vector right there. Oops.  Okay, so there's the vector u - B.  Now to be clear usually we draw vectors anchored at the origin. So what we should really be doing is taking that green one and translating it down to be anchored at the orchard but that's not going to change its length, right? So what do I mean by length now? Well now we've take the length of that Vector so the distance  between you and V.  Is the length of you minus V now, we just we can see in the picture what you might have to be is but we can also just calculate you might have to be here. You must be a 1-0, which is 1 1-3, which is -2 and 0 - 2 which is -2  From the definition that's the square root of the sum of the squares of the entry. So that's 1 squared + -2 squared + -2 squared.  That's of course, you see it didn't matter whether I got it right for which direction the vector go is because if I'd gotten it wrong, I would have just had a minus sign in front of all those entries and that would give the same length cuz I'm taking the squares of the centuries and adding them up. So that's the square root of 1 + 4 + 4.  Which is the \u221a 9 or 3. So that's the distance between those two vectors the distance between if you like the distance between their tips is 3 zactly 3 units on that picture, which is believable from the picture, but we don't need to you don't believe it from the picture. We need to believe it from the calculation.  It so that's what distance means and that's how we calculate if it's quite simple.  I have a question that I want to address in. This is going to motivate the rest of the lecture.  So actually read the question here so that distance the distance between u and v. How does it relate to the lengths of you and me so let's let's look back at that picture that u and v there.  And they have different lengths of the length of you. We could calculate its the square root of 1 squared + 1 squared. So this is \u221a 2 + V. It's length is the square root of 3 squared + 2 squared + 3 squared is 92 squared is for so that's the square root of 13 not very nice numbers. That's okay. I've got these two vectors with non integer lengths.  And then I got the difference you minus V and it's length is 3 so, how does that relate to those too? Well, the first important observation is that it's not determined by those two vectors lengths.  And you can just see that with my favorite demonstration with my arms again. So imagine my arms are those two vectors which are in some kind of configuration like this. One of them is shorter than the other so that the distance between my hands right now is you know this distance here.  But that isn't determined Alone by the lengths of my two arms as they're put because I could just move this one like that and I just increase the distance between my hands just by rotating the arm around right? I haven't changed the lengths of my arms, but I've increased the distance between them or I could do this and make the distance between them as small as possible.  Okay, without changing the length of my arms. So the distance between two vectors doesn't just depend on the length of the two vectors. It also depends on the angle between the two actors. Okay. Now it depends on it in a predictable way. We'll get to that in a moment. But I just wanted you to keep that in mind. A beautiful thing here is so the distance. We just saw Define in terms of the top part of the inner product. It turns out that angles are also encoded by the dot product. I'm going to see that in the next calculation that we do at roughly how and then we'll see precisely how right after that.  This question still stands. How do those two relate to each other what we saw, you know philosophically it's going to involve the distances and the angle but let's go back to the definition. So this distance here.  Okay, the distance between the two vectors.  Is the length of you minus B and this thing is a square root \u221a the dot product with itself. So let's take the square and we'll take a square root in the end. Okay. So let's just calculate see what this thing is. So this by definition and now I'm going to use that other notations going to be easier to keep track that triangular bracket notation for. Product. So this is the length of this is the dot product of you minus V with itself.  That's what it is. And I'd like to manipulate that expression until I get something that has the length of you in the length of e separated in it.  Well, the way I can do that is to use property to from the first slide we talked about here when we talked about properties of this thing, which is that this thing is linear in each of those variables.  So what that means is in particular if I if I think about freezing the second entry that you might have to be on the second entry that this is the same thing as you times the inner product as you enter product with you minus V.  + - V * the inner product of you minus fee  I know I'm going to use the linearity in the second variable say well, actually that's the inner product of you with you.  Plus the inner product of you with - V.  in the first part  Okay, that's not and this guy is in a product of minus V with you plus the inner product of -2 be with minus V.  That's the second part.  Now I've got four terms and it doesn't look like things are getting simpler. Looks like they're getting more complicated. But actually you'll see in a moment that we're already mostly there to relating the left hand thing the distance from YouTube TV to the lengths of you envy somehow. So first thing we'll do is we got them pesky minus signs around. But remember this thing the dot product of something with something else is linear in each of those variables. That means that it distributes over Sam has but it also means that distributes over scale are multiples so I can factor out those minus signs, but I got is that this is the inner product of you with you.  Minus the inner product of you with a V and this this Prim over here, which is minus in a product of inner product of -2 be with you. I can factor out that minus sign from the first factor to get the in a productivity with you and then over here. I have to factor out the -1 twice from both the first and the second Factor just like with the determinant if it appears in both places, it comes out twice.  I know I've got a few terms that I should recognize here. So that inner product of you with you is by definition the inner product of you squared.  That last term there.  8 - 1 squared as one and the inner part of V with v is just the length of the squared and in the middle, I'm left with these two terms. Not a lot I can do about that except I can notice that once again using properties from that first slide of an inner product of the inner product is symmetric. It doesn't matter what order I write the two.  So that means that these two terms inside here are actually the same.  They're actually equal to each other.  and so I'm just going to write this is -2 inner product of u x v u and v  so there's our calculation. So let me rewrite it like this. This says invite. Let me write it backwards. Let me move that two times in a product of u and v to the other side. So we've got is that the length of you squared plus the length of B squared is equal to the length of you minus V squared + 2 * the inner product of u and v  okay, so there's a nice formula and this relates the length of you the length of the and the inner product of u and v  no.  Let's ignore that inner product of you with v term for just a moment here and we have something here that says length of you squared plus length of B squared is equal to length of you minus B squared ignoring that other term  Let's think about this geometrically you was over here via is over here.  You might have to be as though the vector connecting those two if I take those three line segments my two arms and the line connecting my hands. What kind of geometric object do they form a triangle and that form it says the length squared of this guy post the link Square to this guy is equal to the length squared of the hypotenuse. This is Pythagoras Theorem.  But remember Pythagoras Theorem doesn't apply to all triangles. It only applies to right triangles triangles where the angle in between them is 90\u00b0 if I have a very skinny triangle, it's just not going to work by the very wide triangle obtuse triangle. It's just not going to work but something like it always does there is a Pythagorean theorem with a correction term called the law of cosines. Okay, and that's the way you should think of this thing. So this guy over here.  Is the correction term?  two photographer system  okay, and  we'll explore that quantitatively in on the next slide, but for now noticed that if it's zero.  If the inner product there is zero.  we get  Pythagoras Theorem  Which applies to right triangles?  Okay, that is to say so in other words.  if the inner product of u and v is 0  then we really do get that you squared plus b squared length.  Is equal to the length of you minus B squared and by Pythagoras Theorem that supposed to say the same thing as you and me being right angles to each other.  So what we see here, is that perpendicularity of two vectors?  Geometrically motivated by perpendicular is the same thing as having inner product equal to zero.  So interpark not only recordes lengths but it also record perpendicularity recordes orthogon. Ality. What's another word for the same thing when two vectors are at right angles to each other that is recorded algebraically just by there. Product being 0  and we can even be more precise than that. We can talk about angles are 90\u00b0 angles as well. So let's let's go through this one more time. We've just gone through this calculation.  Okay, let's match terms even more precisely. So if you remember from your high school or probably high school geometry class or trigonometry class. The law of cosines says that's if you have any triangle has sides or a b and c and the angle in between the A and B sides is Theta measured in radians. Then you get this generalized Pythagorean relation a squared plus b squared equals c squared plus a correction term.  And that correction term is to AV cosine of that Angle. Now if the angle with 90\u00b0 angle is pi over 2, then cosine of 90\u00b0. That's that correction term isn't there so that exactly accounts for  Non, right triangles and how they behave and I thought you were in type way. Now what we just saw is. Okay. Let's let's do this for a vectors u and v know those were three dimensional vectors, but let's work in the plain where they live so we can always think of two vectors is living in two Dimensions will make that even more precise next lecture.  Okay, and what we saw was we've got two vectors u and v  and the vector in between them is you minus fee which goes here.  So there's you minus B, and this is exactly the same picture.  okay, this is exactly same picture as above and if we match we see that a  Is equal to the length of you?  and B is equal to the length of e and C is equal to the length of you minus p  So the law of cosines says exactly that the length of you squared plus the length of V squared.  is equal to the length of you minus V squared plus a correction term, which is twice the length of you times the length of the times the cosine of the angle in between the vectors u and v  Play whatever that means while we know what it means wood rot in the plane, but actually the law of cosines tells us what the angle means because from the last sign the last slide we saw in the last slide that in fact the length of you squared plus the length of e squared.  is equal to length of you minus V squared plus twice the inner product of u and v  So if we match these two up.  What we see is that the inner product of two vectors u and v can be interpreted as dividing through by 2 equal to the length of you times the length of the times the cosine of the angle in between them.  And that is precisely quantitatively how the inner product in codes both lengths and angles. You can figure out the length of a vector just by taking the sum of the squares the entries and taking the square root by taking the square root of the dot product of vector with itself.  But the angle is also encoded there. Okay, how is it in coded there? Well like this.  Stata is equal to dividing through its the inner product of you with v / the length of u x length of v and I have to take the arc cosine of that.  So that's how you would do it on your calculator. You want to figure out the angle between two vectors you take their dog product you divide that number by the product of the lengths and then you press the arc cosine button.  Okay, so that's easy enough, but I want to spend the last six minutes here exploring this a little bit more carefully. So first of all, this was a picture proof, you should really trust picture proofs cuz it's easy to be deceived by them. I would like to hear everything here was true. But one thing to keep in mind is that everything I drink here was in the plane.  Okay, where is this mattress made of accident or 76 and I'm using this intuition to Define really what angles mean for vectors that are in r76 maybe?  Okay, but you know when we do that we have to be very careful so noticed that the cosine of an angle.  Is always a number between -1 and 1/8 the angle is right if I draw the graph of the cosine function.  That's not a very good picture of the drought graph of the cosine function, but roughly like that it never exceeds one or minus one on either side. So I'm going to say well the angle between two vectors that's just the arc cosine of this ratio.  It better bloody. Well be true that that ratio is never bigger than one or never smaller than -1 know. Can you look at that thing. Product of two vectors divided by the product of their lights and tell me yeah, of course, that number is between -1 and 1. Is that obvious to you?  If it is then you know, we need to talk it is I don't think it's obvious at all.  But it's Estero.  Okay, and it's called the cauchy Schwarz inequality.  Cauchy Schwarz inequality probably the most important basic inequality in all of mathematics. It says exactly that but if I take you. V and / the length of you times the length of E, I get a number which is no bigger than one in absolute value.  And actually we have exactly enough time. I'd like to show you why this is true like to show you a proof of this fact.  Okay, so here's how we're going to prove it.  I'm going to define a function a regular old function like the kind that you saw in your algebra class when you were in high school.  It's give me the length of you minus t x v squared where T is the input here for HT. What I do is out of the linear combination of u and v which is you - t x to be  Okay, snap. The thing I want you to notice about this function is that this function is a non- function. It's the length of something. It's value is always positive or zero and it's only zero if you minus TV is equal to zero.  So it's a non- function but on the other hand.  This is the dot product of you minus TV with itself.  And if I do the same thing I did in the last two slides ago where I multiply that out. Okay, what I'm going to get is you and product you.  minus  t x u enter product V - t x v inner product you  okay. I know the last term is going to have a inner product of - TV with itself. I'm going to factor out that my nasty twice. So it's plus minus t squared times the inner product of the with v.  Now let me simplify that you didn't product to you is the length of you squared.  You enter product key is the same as being a product you so that's minus 2 times the inner product of you with v x t and then we get plus the length of V squared x t squared.  What kind of function of T is this?  So those things let me read just give them new names. Let me call you squared equal to a  Let me call - 2 * the impact of u and v be actually, let me let me call this last one to see that first one V Square. Let me call it.  Those are just three numbers a b and see what kind of function is this. It's a quadratic polynomial but we're being told here is that this quadratic polynomial is always positive.  How can I possibly be let's draw a graph.  Of a generic-looking quadratic polynomial typically if you have a quadratic polynomial, it'll have two roots and this one opens upward because that a which is length of each word is positive. Typically it's going to have to roots and in between those two Roots. It's going to be negative.  Right, but that can fail to happen. It might look like this or it might actually come down and touch exactly once  so you can have three possibilities with a quadratic polynomial. It can have two Roots one double root or no roots at all. How can we tell what we have a formula for the roots?  The roots are negative B plus or minus the square root of B squared minus 4AC over 2A. There's a quadratic formula and you see from here that if that number inside the square root B squared minus 4AC if that number is positive then you're going to get two distinct real Roots. So because of this fact here that thing under the square root sign there must be negative or else we would have a negative value for this function at some points.  So that must be a negative number but let's just substituting. What is b squared. So be is -2 times the inner product of you with v  and 4 * AC is 4 times the length of you squared times the length of V squared.  so that  the fact that this country inside their that discriminant is negative exactly tells us after we divide 3 by 4 and take the square root that the cauchy Schwarz inequality holds true. So it's really just your high school algebra.  Okay. Well you're packing up. Let me just indulge one more thing here and notes something important that I think that's important. At least it's important to me that I want you to connect the things we're doing in this class with real human beings. So these guys pushing shorts were super important mathematicians and their date arguably coaching more than shorts but shorts and a lot of great things. This was back in the early 19th century and one great thing. He did was to Mentor a student whose name was Reese who was another very important mathematician if you take some higher math classes like math 140, you're going to see his name a lot.  Reese had a student named Ian are hella who was a american-born Swedish mathematician who produced a super important analyst the most important analyst of the 20th century by some estimations Irving seagull that's the only died in 2001.  And he had a student named Leonard gross who's still alive?  Who had a student named me?  So I just wanted to point that out because all the things were doing in this class. They were discovered in invented by real people. Okay, and those real people had to learn these things and invent things the same way you do as well. Have a nice day.  UC San Diego podcast ",
  "Name": "math18_b00_wi18-03052018-1000",
  "File Name": "lecture_23.flac"
}