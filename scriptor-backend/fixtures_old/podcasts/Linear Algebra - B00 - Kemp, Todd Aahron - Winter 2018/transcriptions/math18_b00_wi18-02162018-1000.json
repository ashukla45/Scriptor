{
  "Blurbs": {
    "* 4 * 3 * 2 which is 120 calculations to do a 5 by 5 to terminal. How about a 10 by 10 matrix? 10 * 9 * 8 * 7 * 6 * 5 * 4 * 3 * 210 factorial which is over 10 million that will not be on the midterm. Well, actually it might because there's a better way to compute determinants which will get ": [
      2792.6,
      2819.7,
      106
    ],
    "- 1 * - 2 - 0 and what I get is 3 * 2 x - 2 which is -12 So that was a lot less than 120 calculations. No. If you review that systematically and noticed one thing so in this example hear a I had almost everything below the main diagonal zero there was only a couple entries that too and that -2 there below the mountain ": [
      3017.0,
      3048.5,
      115
    ],
    "3 * number of calculations in each to i-22 terminate. How many calculations in a two by two determinant just two of them. So we have to do 4 * 3 * 2 calculations that number's called 4 factorial 24. So doing a 4 by 4 to terminal is going to require 24 calculations in a whole mess of writing. How about 5 by 5? It's going to take 5 ": [
      2770.3,
      2792.6,
      105
    ],
    "And for the last one down here, I got a 0 in one place. So I get 1 * -1 - 0 * 2 so that one's nonzero. It gives me a minus one so I can - 2 * -1 x -1 At all together I get 0 + 0 + -2 x -1 x -1 is -2. That's the determinant of The Matrix. No. That's correct. But it ": [
      2530.6,
      2560.9,
      96
    ],
    "Do I listen to a podcast on Wednesday evening today? We are going to finish the discussion of section 4.4 that we got through most of last day and move on to chapter 3 chapter 3 is about determinants in general. We're going to spend the rest of today's lecture and all of Wednesday's lecture next week on determinants before we move on to chapter 5 I get values. So ": [
      2.0,
      28.6,
      0
    ],
    "Hazard to it is not equal to our to but it is isomorphic to are too because there are exactly two linearly independent factors that spent All right, let's do one more example before we move on. so I should say here. Why do we care that this thing is an isomorphism I'm saying it shows they have the same form great, but that really means something specific. It means ": [
      1189.0,
      1220.8,
      45
    ],
    "I just have a vector space. then if I have a basis for it some basis vectors B1 B2 up to be and I'll call that basis script EB Capital be if I have a basis for it, then what's so great about that among other things is it allows us to give an address to every point in the Subspace independent of whatever description you have of the vector ": [
      52.0,
      74.5,
      2
    ],
    "I want to make a comment about it first, which is So when we studied linear Transformations, we said linear Transformations are the same as Matrix Transformations. So you're saying Okay. So this Tia has a matrix not so fast because before when we talked about linear Transformations, we were talking about linear Transformations from our $3 7 from RN to RM. When you're talking about linear Transformations between two ": [
      493.2,
      520.5,
      21
    ],
    "If you want to use fancy words like in your textbook But it's a very simple idea. We just if I've got a Subspace of dimension for then I only need four numbers to describe all the factors in it. So let's just use those four numbers keeping in mind that which phone number for Tuesday pens on the basis that we have hiding in the background that we need ": [
      379.2,
      397.2,
      16
    ],
    "Okay, so determining or calculating the co-factors of a 7 by 7 Matrix requires that you know how to compute 6 by 6 determinants. But I I remove one row in one column. I'm left with a matrix of one size smaller. And in order to calculate the cofactor, I need to know how to compute the determinant of that Matrix. That's what it's defined to be. So you don't ": [
      2239.2,
      2265.8,
      86
    ],
    "Okay. Well the c22 class doctor means I delete the first the second row and the second column from The Matrix, so that leaves me with. the determinants of 1000 * a sign but in this case it's the two to sign which is Plus. and then finally, I have to add -2 that the entry in the 3-2 position of the Matrix x a sign which is the Maya ": [
      2441.5,
      2473.9,
      93
    ],
    "So let's rewrite those over here 101 + 310 And let's call them B1 and B2. Okay, so the column space of a is spanned by those two vectors. So now let me call that basis be it's written there. So let's write down the coordinate Vector of some other vectors that are in that space. So for example, the vector 5 1/2 that third Colin was The Matrix a ": [
      830.0,
      865.9,
      34
    ],
    "Subspace will be uniquely expressible as someone near combination of those three vectors. So I take those coefficients in a linear combination write them in a column Vector in R 3 and that tells me where the vector is. Maybe I should write that down to be clear here it At the arrow here is actually the hard Direction. If I tell you. Set the coordinates of a particular vector. ": [
      244.0,
      270.7,
      10
    ],
    "That's how you can use coordinate transformations to make sure that you never have to deal with the abstraction. You can always view any Vector space any finite dimensional Vector space as RN for some n once you fix a basis, you have a coordinate transformation and you can always think of any Vector space as RN use the tools we developed throughout this course 2 answer any question about ": [
      1566.2,
      1587.5,
      59
    ],
    "The first number tells you the row. The second number tells you to call him. So the first row and the second column there. Now we know how to calculate to buy two terminals. So let's do that here. So this is -4 * 9 -7 * 6 Okay. So 4 * 9 is 36 and 7 * 6 is 42. So when I get here is 6 so that ": [
      2181.2,
      2212.1,
      84
    ],
    "They're going to have a basis br1 7 - 3 if I tell you what its coordinates are in terms of a particular basis. What that means is that the vector is Just One X the first basis Vector + 7 x II basis Vector - 3 * the third base is Spectre. This is in the special case where and is equal to 3, right then a basis will ": [
      272.1,
      297.6,
      11
    ],
    "Vector spaces that are already presented as column Vector spaces then yes, I really need a transformation is a matrix transformation as we proved but if V there is p to the vector space of polynomials of degree less than or equal to 2. It doesn't even make sense to ask for a matrix 44 the coordinate transformation cuz the domain is not a space of columns. We wouldn't even ": [
      520.5,
      547.1,
      22
    ],
    "W is equal to T A V plus DFW the T respected Auntie respect scalar multiplication so we can verify that directly but all that is doing for us again is you know, realizing that on either side of of tea in the vector space or an RN Addition and scalar multiplication work the way they're supposed to work, right that's you get that if I want to add up ": [
      442.9,
      470.2,
      19
    ],
    "a course in numerical linear algebra are lots of out that in ways that you can try to counter it. The other thing is though as we're going to see you in a few minutes. It's also for the two by two case. It's easy to compute determinants in the 3 by 3 case. It is possible to write down a formula that you may have seen before that's involves, ": [
      1935.5,
      1954.9,
      74
    ],
    "a huge amount of freedom in it. Okay. I'm telling you pick any road you want picking call me want and go on down the line and you'll get the same answer which is quite remarkable just to demonstrate what this means now, let's go. Let's do this in. In this example here. So here's a 3 by 3 Matrix. So I'm going to pick some row or some column ": [
      2323.3,
      2345.2,
      89
    ],
    "a is invertible. If and only if that quantity the determinant of a 80 - BC is non-zero, that's really where the term determinant comes from because it determines whether the Matrix is invertible. That's what first arose and in that case. We have this formula that for the universe what you do is you / that nuns remember? And well for a two by two you swap the diagonal ": [
      1774.5,
      1804.6,
      68
    ],
    "a matrix a 3 by 3 Matrix. So it's column space and it's null-space are both subspaces of R3 in this case. I know I've already asked Matlab to do the row reduction for us to save some time. So here's the reduced row Echelon form of that Matrix. So let's write down. A basis for the column space, where am I going to find a basis for the column ": [
      774.6,
      802.7,
      32
    ],
    "a precise mathematical way that if I have an n-dimensional Subspace, it looks like RN. Just like when we started talking about these things and span maid if you had the intuition, okay, so this this Matrix here, it's got a basis for its column space of only two vectors. Okay, does that mean that they span R2? No, it doesn't even make sense. Cuz the Matrix actually had columns ": [
      721.2,
      747.4,
      30
    ],
    "all you're going to produce is the product of the diagonal entries. So there's one case where where determinants are easy to compute if your Matrix is triangular the determinant is just the product of the diagonal entries one calculation. And that's going to be useful to us will continue with that next Wednesday. Have a good long weekend. UC San Diego podcast ": [
      3105.5,
      3130.0,
      118
    ],
    "along the first call Write the determinant of a here using the cofactor expansion. So I need that sign pattern The Matrix, but now I'm going to say well, I know what I think I know it well enough so that I'll know where the signs come in. So that's going to be let's see if I expand along at first, I'll get three times the determinant and now I'll ": [
      2865.3,
      2885.6,
      109
    ],
    "along the first column. I'm going to get only this part contributes cuz of all the zeros and then in the next phase if I choose the first column. Same thing will happen and I'll only have to deal with this part of the Matrix and so on. So what you'll see from that is that if you have a triangular Matrix where everything is zero below the main diagonal, ": [
      3079.2,
      3105.5,
      117
    ],
    "and future linear algebra courses, or if you go on in a computer vision course or it in several kinds of Statistics courses, you will need to understand change of basis Matrix and you will learn it then Okay, it's a little tricky but it can be handled systematically, but regardless as long as you stay fixed in one bases the whole time you're fine. Okay, so that's I think ": [
      1608.1,
      1630.1,
      61
    ],
    "answers of the Matrix Beyond five or six digits of accuracy. So if the determinant is .007, but you actually were off by 1% in one of the entries that maybe the actual determine was 0 or maybe it was .03, right? I mean the amount of error in your entries if you have a real-world situation might be much larger than the calculation you get for the determinant. When ": [
      1881.8,
      1908.5,
      72
    ],
    "as a linear combination of the standard basis vectors. It's got a one in the first base inspector slot from -2 in the second base is Victor slot on a wall. So if I fix the standard basis of polynomials, then those three polynomials there are just these three column vectors. Those are coordinate vectors. If I give you three coordinate vectors in R3 and I want to establish that ": [
      1456.6,
      1485.4,
      55
    ],
    "basis that that's the unique way of there are the coordinates right in your face. Okay, but if we're in the case, where were talking about column vectors already a Subspace of RN or are seven or whatever? Then we know what we need to do here because that's a vector equation V is equal to unknown coefficients linearly combining the bees which are known we want to solve for ": [
      333.9,
      359.2,
      14
    ],
    "bees are a basis and therefore any linear combination of them isn't so this is a one-to-one and onto linear transformation if it had a matrix, it would have an invertible Matrix that it standard Matrix a one-to-one and onto the same as a convertible. It's an invertible function. I can go back and forth. That's kind of point of coordinates. Right? I should be able to tell you what ": [
      641.7,
      663.6,
      27
    ],
    "buy two determinants. But now you also know how to compute any 4 by 4 to terminal because if you do a 4 by 4 determinant what's going to happen is you're going to do a choose a row or column you're going to expand along it and you're going to get a combination plus and minus signs x a linear combination of four three by three determinants, but now, ": [
      2699.1,
      2721.3,
      102
    ],
    "by getting rid of the exes and just writing a matrix of coefficients we can do the same thing here. Once we have a basis in mind. We have our Subspace. We have fixed the basis. I'm going to use it to describe all the factors in the Subspace. I do so by dropping the bees I just right in the exes are those axes are called the coordinates of ": [
      152.6,
      173.3,
      6
    ],
    "by two determinants on the last slide. That determine 80 - PC it determines whether a matrix is invertible or not. And it has a simple formula for hire size matrices. There is a thing called the determinant. It is some polynomial function of the entries you get it by combining the entries with product and sums and minus signs in some particular way that has the same property that ": [
      2000.7,
      2027.2,
      77
    ],
    "change bases and use a different bases who would get a different coordinate Vector the coordinates depend on what basis we choose. Okay. So once we do that that gives us a representation of our Subspace as column vectors, even if it's some abstract space of polynomials or matrices or something else we can think of them think of the vectors as column vectors call investors living in our n ": [
      199.0,
      224.8,
      8
    ],
    "cofactor expansion. And again, in this case. It's makes most sense to expand along the First Column. You don't have to use the same column every time the cofactor expansion says you can choose any roller column that works for you. So you can change which roller calling you're using data from one step of this process to the next that's fine. In this case. It still makes most sense ": [
      2917.2,
      2937.5,
      111
    ],
    "come occasionally infeasible quickly. Okay, so we'll get around that by the end of this lecture or early next lecture. But keep that in mind as we go with it the definition. I'm about to give you for the determinant. It defines what it is, but it quickly becomes computationally invisible to actually use it. So this is the top of the slide just summarize what we said about two ": [
      1976.7,
      2000.7,
      76
    ],
    "depend on which basis you choose. Anyway, that was a slight the side here. What is great though, is that this linear transformation in the coordinate transformation the one that's written there? It's not just a linear transformation. It's also got all the nice properties that we would want. It is one-to-one and it is onto. Okay, so remember 1 to 1 means that? Looking at a particular column Vector ": [
      571.6,
      596.6,
      24
    ],
    "determinants are small, the system of equations is numerically unstable small perturbations in the coefficients of the Matrix can drastically change what the universe looks like or whether or not exist. So that's one problem with determinants right now. If you're going to divide by a number, it might be really nonzero, but really small and in numerical applications that can present real problems. If you go on and take ": [
      1908.5,
      1935.5,
      73
    ],
    "entries and negate the off diagonal entries. So that's where the terminals came from for us. But that's specific to the to buy to setting OK. I want to point out a couple of things that are computationally troubling already or won this competition the trouble even in the two by two setting on another that's more General as we go forward. So that number there a D minus BC ": [
      1804.6,
      1832.8,
      69
    ],
    "everything that I wanted to say about the coordinate transformation and coordinate vectors and hopefully this gives you some hope that if you have been confused about abstract Vector spaces as long as you can convince yourself that you can find a basis of that Vector space then the confusion should all go away because you'll just need to work in RN once you translate to the coordinate vectors. So ": [
      1630.1,
      1653.7,
      62
    ],
    "everywhere else. Okay, where the number of elements there is going to be. The number of the number of components is going to be the number of bases factors the dimension of the space. 2 in this example what we see here anyway. Is that the coordinate transformation T from the column space of a onto R2? is an isomorphism coordinate Vector tells us how to view the column space ": [
      1154.4,
      1189.0,
      44
    ],
    "ex wants your accent on the right hand side. There's exactly one thing on the left hand side that it comes from but that's just the observation that's written right down here, which is it. If I tell you the X's if I tell you the X's you can recover the the vector immediately once I tell you the X's then there is exactly one factor that came from and ": [
      596.6,
      619.0,
      25
    ],
    "express the basis, Vector B2. In terms of the Basis B1 and B2. Well one way you can do it as is 0 * b 1 + 1 * be too, but we know that the views for my bases. So once we have one way to do it, it's the only way to do it. There's a unique solution. So that's presented right to us there. So in fact ": [
      1090.4,
      1112.9,
      42
    ],
    "form and we see the leading ones. Are in all three columns and all three rows. Therefore this is a bassist. It's so those three column vectors form a basis for R3 and because the coordinate transformation is an isomorphism IT preserves all the properties of vector spaces. It follows that the vectors themselves 1 X - 1 + x - 1 squared r a basis for that space P2. ": [
      1536.1,
      1565.0,
      58
    ],
    "generic 4 by 4 Matrix You're going to choose a roller column. So there's four entries in the in the in a row and you're going to have to add up. For three by three determinants of those issues Aurora column that has three entries and the cofactor expansion is going to spend that as 3 to buy to determine its so so far we had to do 4 * ": [
      2745.7,
      2770.3,
      104
    ],
    "have three elements. So if I tell you what the basis is and I tell you the coordinates of a vector then it's immediate in routine to tell you what to do for you to report to me what that doctor is you just use those coordinates to linearly combine the basis vector and get the vector that you're interested in. The typically harder thing to do is to go ": [
      297.6,
      318.4,
      12
    ],
    "in R7, but the span of those vectors looks like R2. It is isomorphic to our to enter the isomorphism or an isomorphism that you can choose is the coordinate transformation. You just need two numbers to describe any element of the vector space. Therefore, it looks like R2. So let's fill around with a specific example here to cement all of the stuff we've been doing so here is ": [
      747.4,
      774.6,
      31
    ],
    "in a moment, I'm going to show you how to compute the determinant of a 3 by 3 Matrix. Doing so requires that you already know how to compute the determinant of a 2 by 2 Matrix which we do question. The first row you're right. What I just did here was C12. So let's change that. This is the C12 the cofactor C12 C12 means like with Matrix notation. ": [
      2150.4,
      2181.2,
      83
    ],
    "in the space can be represented as a linear combination of the basis vectors. That's because of the basis of spanning set. But that's for wedding spending said but an additional basis is linearly independent and we saw last time what that means is that the representation that you get expanding that Vector in terms of basis vectors its unique. There's only one way to do it is any veteran ": [
      100.3,
      125.0,
      4
    ],
    "independent, which means that you can avoid all the confusion you might have about thinking of polynomials as vectors and work with column vectors once you fix a basis, So let's use that the same applies to spanning sets. So let's use that to establish the following fact, here are three polynomials of degree less than or equal to 2x - 1 + x - 1 squared I claimed that ": [
      1274.7,
      1302.8,
      48
    ],
    "invest in that position down here is -1 times the determinant of Now I have to delete the third row and the second column that leaves me with the Matrix 1-2 0-1. And finally, I have the third entry is 0 * + 1 * the determinant of 1524, but I don't have to do most of those calculations because I get a zero immediately for each of those from ": [
      2645.8,
      2676.9,
      100
    ],
    "is going to be very far more concrete than anything. We've been doing since the first or second lecture. We're going to be doing something that's just quite computational at the moment. What is the determinant we've already encountered determinants in the two by two setting so if we have a two-by-two matrix a general one there a b c d. We discovered that if you wanted to. Do row ": [
      1684.1,
      1710.4,
      64
    ],
    "is the one to cofactor of this Matrix. Are you now know how to do all co-factors I could do the three three cofactor here, which just means that I ignore the third row and the third column that would leave me just the upper. We're here 1245 and I calculate the determinant of that but I have to * a + sign cuz that's in the three three factors. ": [
      2212.1,
      2236.4,
      85
    ],
    "it determines whether the Matrix is invertible or not for a 2 for an end by a square Matrix, but in general there isn't a formula for it that is nice and easy to compute instead. I'm going to give you a recursive way to compute it. This is the way your text defines the determinant. It's a standard way to find determinant and it's a recursive method that involves ": [
      2027.2,
      2046.1,
      78
    ],
    "it's also on to Meaning that every Vector in r n is hit by the transformation. And again, that's the same observation that's down here, which is I want to know if the vector 1 7-3 is in the range of the linear transformation. Yeah it is because I can use those coefficients to add up the bees and I get a vector back in my Vector space because the ": [
      619.0,
      641.7,
      26
    ],
    "just so you have a horizon there for what you should be reading it had about. So let's begin. And let's continue. With a reminder of what we did in the last 20 minutes of last days lecture. We talked about coordinates. So what's so great about a basis if I have a Subspace of an abstract Vector space and it could be a Subspace of some other Vector space. ": [
      28.6,
      52.0,
      1
    ],
    "know figure figure something out and tell me what you think. It's going to be. It might be punished. But if you stay quiet, we've been jumping jacks in here before remember. Okay, so somebody raise their hand and tell me what size it yet size 2y. There's two basis vectors. And so we need to Coefficients to combine them, that's right. Okay, so this is going to be some ": [
      916.7,
      948.3,
      37
    ],
    "know how to compute 6 by 6 to Terminal C at well, actually now you do because of the next slide. So here is a definition of the determinants. So I give you a matrix a there's a general looking Square Matrix a the determinants of that Matrix is defined by the following thing you pick any road you want. Pick the first row js1 pick the seventh row js7 ": [
      2265.8,
      2291.7,
      87
    ],
    "know how to multiply those things by matrix. Okay, so it was slightly misleading to say that every linear transformation has a matrix every linear transformation from RN to RM has a matrix linear transformations in general doesn't even make sense to say they have a matrix unless you present your vector space as a space of columns, which you can do using the coordinate transformation, but that's going to ": [
      547.1,
      571.6,
      23
    ],
    "let's go ahead and work this all out to this is -5 times the determinant of this first Square Matrix over here pay which I am I get by multiplying with diagonal and Auntie diagonal entries, but there's zero is in both of those so that one gives me a zero. and similarly this determinant there zeros in both places. So for that one I get + 4 * 0 ": [
      2502.4,
      2529.6,
      95
    ],
    "man dagannoth at work. Not zero if I had had zeros everywhere below the main diagonal then I could have just done the same thing. I just did but choosing the First Column every time so if I have zeros below we're just about done here if I have all zeros below the main diagonal Everything is 0 down here. Then when I do the cofactor expansion. If I expand ": [
      3048.5,
      3079.2,
      116
    ],
    "matrix a is in the column Space by definition. All the columns are in the column space their stand by themselves. So they're in the column space, but we claimed that the column space to spend by just the first two factors. That means that Vector 512 is going to be in the span of B1 and B2 in a unique way because there's two columns are linearly independent. So ": [
      865.9,
      888.9,
      35
    ],
    "means that we get a minus one in the first slot and I won in the second slot. Now what about X - 1 squared? Well now I'm going to need to use some polynomial algebra. She'll do over here when we did this actually last day. So I'll do it again. So X - 1 squared is x squared minus 2x minus I Plus. So that's already presented now ": [
      1429.8,
      1456.6,
      54
    ],
    "minus sign of the three true position of design pattern X the determinants of what's left when I delete the Third row and second column and that gives me the Matrix 1-0 2-1. I know I have written the determinants of this 3 by 3 Matrix as a combination of determinants of two by two matrices and I already know how to compute determinant of 2 by 2 matrices. So ": [
      2473.9,
      2502.4,
      94
    ],
    "morph means form and ISO means same. So this means the two things on the two sides of the function have the same form what it means is that they look the same. Okay, if you squint you can't tell them apart. The coordinate transformation is an isomorphism between the vector space you start with and r n where n is the dimension of the space. It tells you in ": [
      691.5,
      721.2,
      29
    ],
    "n is the dimension of the space? And the great thing about this function that it's the kind that we know how to analyze in this course. It's a linear transformation. I'm not going to prove that because it's sort of routine and mind-numbingly boring and to do what we need to do. If we want to prove it is a linear transformation is prove the key of b + ": [
      421.0,
      442.9,
      18
    ],
    "one wrinkle, which is that I also have to look in this sign pattern Matrix here. So what I'm Computing these things so there's the beginning you should always write this down to I have a 3 by 3 Matrix. I write down the sign pattern Matrix next to it, which is just I put chess board pattern of pluses and minuses on it always starting with a plus in ": [
      2105.0,
      2124.6,
      81
    ],
    "or third column which have some zeros in them immediately for us to to use so let's rewrite the Matrix down here again. So this is 15024 - 1 + 0 - 206 Panda long the last row. Okay, if I do that then I'll see the determinant of the Matrix is equal to 0 times some sub determinant some some cofactor 0 * right explicitly this time first design ": [
      2587.8,
      2620.8,
      98
    ],
    "pattern is plus minus plus so we got a Plus One X the determinants of the stuff after I delete the first row and the third the first column in the first row. So there's a 5-0 4-1 there, but I didn't mean to write that down cuz there's a zero right here. So whatever it is, it's not going to contribute then I get + -2 x design pattern ": [
      2620.8,
      2645.8,
      99
    ],
    "reduction on that and find a reduced row Echelon form of that a generically right with generic a b c and d there you can do it if you're super careful, but we found that at some point in the calculation. We did this in this room at some point of the calculation this quantity 80 - 2 BCE comes up it comes up in one of the slots that ": [
      1710.4,
      1729.4,
      65
    ],
    "something called the cofactor expansion which sounds fancy but I'm going to show you where it is right now and it's not fair. It's just routine. You just need to know how to do it. So let's let's do a specific example here. So here is a 3 by 3 Matrix and I'm going to tell you how to compute some of the co-factors of this Matrix. So we're going ": [
      2046.1,
      2066.7,
      79
    ],
    "space like it could be some abstract space of polynomials or it could be a Subspace in R7 but of Dimension 3 and so it's kind of inefficient to describe elements of that Subspace using 7 numbers because since it's a three-dimensional Subspace, you should really only need three numbers to describe elements of that Subspace and that's what coordinates do for you so because it's a basis Every Vector ": [
      74.5,
      100.3,
      3
    ],
    "space? Buy a hand summary. Yeah, yes. Heaven Cox, okay, which of the pivot columns First and second columns in which of the two major is right in here. Very good the first one OK the first and second Columns of the reduced row Echelon form contain pivotal ones that tells you that the first and second Columns of the original Matrix, they are pivotal and they form a basis. ": [
      802.7,
      830.0,
      33
    ],
    "span all of pee too. So I need to establish that are linearly independent. That's actually not going to be that hard to do directly. But I also need to establish that every polynomial of degree 2 or less like for example, x squared minus 7x + 21. I need to be able to write that as a linear combination of these three. And now if you start fussing around ": [
      1324.7,
      1346.0,
      50
    ],
    "that gives me 0 they're the only one that contributes is the -1. So I get - 1 * this - 1 Time is the sub determinants X the cofactor, which is formed by. Everything that is not colored there. So that's 150 - 200 that's a really easy to terminate to compute. I'm already down to two by two. So I got 3 * 2 * - 1 * ": [
      2989.7,
      3017.0,
      114
    ],
    "that if I want to establish properties of a collection of vectors, I can look at the images of those vectors under the isomorphism and establish the properties there and it'll work on either side. So if I have a set of vectors, maybe they're abstract vectors like polynomials. And I want to answer the question. Are they linearly independent? Okay, if I fix a basis and use the coordinate ": [
      1220.8,
      1247.6,
      46
    ],
    "that is the end of chapter 4 as we're covering it and now I'd like to go on to chapter 3. All right chapter 3 is a very different flavor from the rest of the book. In fact chapter 3 doesn't isn't really linear algebra. Determinants are not really linear algebra determinants are multilinear algebra, and we'll discuss what that means next day a little bit. But for now, this ": [
      1653.7,
      1684.1,
      63
    ],
    "that is to say the entries are five. 4 + -2 x those co-factors, so I need to figure out what those co-factors are. It's a what are the co-factors here? Well, so for that first one C12, that's what we did on the last slide. So now I'm going to delete the first row as well. And that's going to give me here five times the determinant. Of that ": [
      2384.2,
      2413.6,
      91
    ],
    "that might be zero in which case The Matrix is not invertible. The theorem is that the Matrix is invertible. If and only if that number is non-zero, so that's great for theoretical purposes. But in a real world application, if you're if you've got some Matrix that comes up it from some statistical application that you're working on. And you would like to invert that Matrix you calculate determinant ": [
      1832.8,
      1854.5,
      70
    ],
    "that says is that the X1 and X2 hear. Those are two and one. Okay. Those are the coordinates of the point 5 1/2. That's the coordinate Vector of the of the vector v 1/2 in the basis that we exhibited of the column space that lets us another question. How about 310 what are its coordinates in the basis Peak? any thoughts Yes. 01 exactly if you want to ": [
      1052.3,
      1090.4,
      41
    ],
    "the X's there. That's a vector equation. That's the sort of thing. We've been doing since day one of this course and we need to use row reduction and find what the ex's are. And the point is that because the bees are linearly independent. There will be a unique Solution. That's why I would have called coordinates. Okay. So those are coordinates and that's the so-called unique representation theorem. ": [
      359.2,
      379.2,
      15
    ],
    "the coordinates of any point is and if I tell you the coordinates you should be able to tell me where the point is you can go back and forth. So there's a fancy word for such linear Transformations a linear transformation. That is both one-to-one and onto is called an isomorphism. Your textbook does use this word. Don't be scared by it. It just means it's from Greek. So ": [
      663.6,
      691.5,
      28
    ],
    "the entries of the Matrix. And now the thing that I have left is actually the same conversation. I already did its 2 x - 2 * -1 x -1. Give me -2 same answer as before. That's the way it's going to work out. So now, you know how to compute any 3 by 3 determinant using the cofactor expansion that requires you to be able to compute to ": [
      2676.9,
      2699.1,
      101
    ],
    "the only caveat is you have to keep in mind what basis you're using and be consistent about it because if you change the basis the coordinate representation will change and if you mix and match you can run into trouble there is a systematic way to handle changing bases de that section 4.7 in the textbook. We're not going to do that in this course, if you go on ": [
      1587.5,
      1608.1,
      60
    ],
    "the space can be written to uniquely as a linear combination of the basis vectors does a unique set of coordinates x 1 x 2 of The X whatever it is is expressed as x1v OnePlus X to be two-plus up to xnvn. So just like we did in the first lecture where we had a bunch of x's hanging around in some equations and we decided to simplify notation ": [
      125.0,
      152.6,
      5
    ],
    "the upper left-hand corner. and then for the 2-1 cofactor If I look at where those two intersected, which was there, then I look at that sign and that sign in the sign pattern is a negative. So I put a negative there and that's the definition of the 2-1 cofactor now noticed that it requires us to calculate the determinant but it's a determinant of one size lower. So ": [
      2124.6,
      2150.4,
      82
    ],
    "the vector in question three to vector v has a coordinate Vector, which is a column back to the kind that we've been setting for most of this class will denote it like like this over here. We put square brackets around the vector v with a sub be indicating that these are the coordinates of that Vector in the basis be keeping in mind that if we were to ": [
      173.3,
      199.0,
      7
    ],
    "the way that's written there. It says hey look I've got some vector and I want to figure out what its coordinates are. Hey, so you can do that with ad hoc. Methods will see that in a few examples here. You just have to figure out some way to express it in terms of the basis vectors. And once you figured out some way, you know, since it's a ": [
      318.4,
      333.9,
      13
    ],
    "they form a basis for R3. How do I check that? I need to establish that there a spanning set for our three and we know that means that I should look at the Matrix with those columns. But I need to check if it's a Spanish that I need to check that all the roads are pivotal. I also need it to be a linearly independent set which means ": [
      1485.4,
      1515.4,
      56
    ],
    "those three polynomials form a basis for P2. Pitu remember is the vector space of polynomials of degree at most 2. So I want to show that there a basis now in order to show that they're of Base. This one thing I can do is work right from the definition and establish that those three polynomials 1 X - 1 + x - 1 squared are linearly independent and ": [
      1302.8,
      1324.7,
      49
    ],
    "to buy two thing that's left to 0-1 0 but remember there's one more piece of information we need for the cofactor, which is this sign pattern Matrix. And so this is the two one cofactor. So I look in the 2-1 position which is a minus sign. So I have 2 * -1 there. So that's that first term then I add to it four times the c22 cofactor. ": [
      2413.6,
      2439.7,
      92
    ],
    "to call him back. There's that just means that I add them component wise and if I want to have two vectors on the left hand side if I had them both in terms of B, then I just add up the coefficients of B12 if you wanted so far. So this T is a linear transformation called the coordinate transformation. but it's not just any old linear transformation, but ": [
      470.2,
      493.2,
      20
    ],
    "to compute c214 this Matrix the 2-1 cofactor. What is that? Mean? What it means is that I take the second row. That was the first take the second row. and the first column and I ignore them. Take a second row in the First Column and ignore them. I'm left. With these entries down here. And the 2-1 cofactor is the determinant of that thing that's left. Almost there's ": [
      2066.7,
      2105.0,
      80
    ],
    "to do is I'm going to calculate the coordinate vectors of the polynomials were actually interested in in terms of that basis. So the polynomial one. Cancel that basis. Well, it's the first basis Vector in be so by what we said on the last slide. That's just going to give us 100 which is just to say that that the polynomial one is 1 * 1 + 0 * ": [
      1377.8,
      1402.9,
      52
    ],
    "to expand a long. I'm going to pick the second column to expand the long. I so luscious Fresno right down with this formula says in that context. It says that the determinant of this Matrix a is so I'm using the second column. so I have to take C1 to C2 and C3 to And add them up multiplying by the entries a12. A 2-2 and a 3-2. And ": [
      2345.2,
      2384.2,
      90
    ],
    "to it. 101 310 and 512 but hey, look, that's the Matrix a I did indeed. We remember that the reduced row Echelon form of the Matrix a it tells us how to express the non-committal columns as linear combinations of the pivotal ones and so this this coefficient to here and this coefficients one here, they match up with. the leading ones the leading ones in the two basis ": [
      979.5,
      1020.2,
      39
    ],
    "to keep consistent now once we do that if I fix a basis than this actually gives me a function, right? So I've got a vector space V and I take any Vector in there. I can compute its coordinates. So Associated each vector v is its coordinate Vector, so that's a function. That takes the vector space V and as the domain and the codomain is r n where ": [
      397.2,
      421.0,
      17
    ],
    "to next time. But before we get there, let me highlight at least one nice feature. Which is everything I just said is is generic if the Matrix has generic entries, like if I give you a random Matrix if I produce the entries randomly, then they're never going to be zero. Okay, we're very very unlikely that any answer is going to be zero. But in the actual example, ": [
      2819.7,
      2844.3,
      107
    ],
    "to use the First Column cuz it's mostly zeros there. So this one's going to be three times. Okay. Now I have a two and the sign pattern says the one 1 entry is always gives in a plus sign. So that's fine X the following thing. It's the determinant of this down here, which is 15024 - 1 + 0 - 2 0 I know I don't quite have ": [
      2937.5,
      2964.2,
      112
    ],
    "transformation for that basis to view those polynomials as column vectors. Then I have tools that we've developed in this course to answer whether those column vectors are linearly independent. And the point is that an isomorphism preserves linear Independence. So if you figure out that the column vectors which are the coordinates vectors of those polynomials are linearly independent that tells you that the original polynomial vectors are linearly ": [
      1247.6,
      1274.7,
      47
    ],
    "trying to figure out how to do that, you'll see that you can do it by solving some linear equations. That's what's going to go down to but we don't need to fuss around we can do it systematically instead by saying hey first Fix a basis. Let's fix the so-called standard basis. 1 x x squared 4 P2 Hey, so it's called us bases be. Know what I'm going ": [
      1346.0,
      1377.8,
      51
    ],
    "use that absolute value notation. You see where it starts to be useful time. Is this part down here. answer the determinant of that 4 by 4 Matrix 2 - 573 0150024 - 100 - 2 0 and the other five terms are all 0 so I won't even write them down. Right? So now I need to come get this four by four determinants. And now I'll use the ": [
      2885.6,
      2917.2,
      110
    ],
    "value bars like the absolute value of the Matrix a you shouldn't pronounce it. That way you shouldn't think of it that way. It's not an absolute value. In fact, it is often negative. Okay, but that notation is useful, especially when your Computing lots of determinants. It's easier to write 2 bars then Det. And what we saw in the two by two case was that a two-by-two matrix ": [
      1753.5,
      1774.5,
      67
    ],
    "vector. in our two Okay, I know what Vector is it? Well the way we figure that out, right? What does this say? This this says literally we have to solve the equation 512 is equal to x 1 x the first bassist Rector 101 + X2 X II basis Vector 3 1 0 We know how to solve Matrix equations like that. We write down the augmented Matrix Associated ": [
      948.3,
      979.5,
      38
    ],
    "vectors. And so what those things tell us right. There is that the third column is equal to 2 times the first + 1 * this So that is to say let's just verify that. If I take two times the first ones or 1 + 1 x II e-310. I get 2 + 3 which is 550 + 1 + 1 + 2 + 0 which is to what ": [
      1020.2,
      1052.3,
      40
    ],
    "was also dumb because in this procedure, I'm going to add up along any row or column. I'm telling you that I got the same answer no matter which row or column I choose to expand a long. And there's a good choice here several good choices for once to choose to stand alongside. That wasn't one of them why. Yes. Perfect, cuz I could have used the third row ": [
      2560.9,
      2587.8,
      97
    ],
    "we just worked with and in this example hear a ton of the entries are zero. And so if we are clever if we are judicious about which row or column expand a long will save a lot of time. So here is a 5 by 5 Matrix. It could take up to a hundred and twenty calculations in order to compute the determinant here, but actually if I expand ": [
      2844.3,
      2865.3,
      108
    ],
    "we see that in general. the coordinates of B J In the basis B. Are the standard basis vectors? Okay, so that is D3 MP3 might be a column Vector of size 78 or it might be a polynomial of degree 321. Okay, but whatever. It is in terms of the basis B. It's going to be. Represented by the vector with a 1 in the third position of zeros ": [
      1112.9,
      1154.4,
      43
    ],
    "what I want to do is figure out what is the coordinate Vector of this column Vector 5 1/2 in terms of that basis before we figure it out. I want you to tell me what size Vector this thing is. Astoria Column Vector of what height? somebody from this side of the Room Place You know if you get it wrong, it is no nothing any punishment. So, you ": [
      888.9,
      916.7,
      36
    ],
    "where n is the dimension of the Subspace. So that means that if I did indeed have a three-dimensional Subspace of our 7 that I was driving and I fix a basis for that three dimensional Subspace. Well by definition of Dimension, that means that any basis I choose will have three vectors in it. So I fix a basis of three vectors and that means that every veteran the ": [
      224.8,
      244.0,
      9
    ],
    "x - is 1 * 1 + 0 * x + 0 times x squared. Okay. Now what about the second base of the second Vector in that list X - 1 I need to expand that in terms of the bassist One X x squared x minus one is already presented as X which is the second base inspector - 1 which is the first base inspector. So that ": [
      1402.9,
      1429.8,
      53
    ],
    "you compute the co-factors along all entries in that row. You multiply them by the entry in that row. And you add those up and that's the determinant. Okay, you pick any column that you want and you calculate the co-factors along that column and add them up X the entries in that column and that will give you the same number. This is a bizarre definition because it's got ": [
      2291.7,
      2323.3,
      88
    ],
    "you find a determinant is .007. That's a really small number. That means that it's not zero, but if you invert if you take its reciprocal, okay, it's going to be a number with 8 zeros in it. It's a huge number. So you're the inverse is going to be a big Matrix with huge entries. And here's the thing in a real world application. You might not know the ": [
      1854.5,
      1881.8,
      71
    ],
    "you hope is going to be pivotal. So you would have to divide Surabaya to continue the row reduction and if it's zero, then something different happened. So that quantity we call the determinant of that quality a D minus determinant of a two-by-two matrix and the general notation for this is debt a in some cases and you textbook uses this as well. It's denoted as like with absolute ": [
      1729.4,
      1753.5,
      66
    ],
    "you know how to compute three by three determinants. So each one of those you're going to compute by expanding along some roller column. And they were just two by two determinants and so on Down the Line. You now know how to compute determinants. You should never ever do this to compute determinants. Why is that? Well, let's let's analyze what I just said. So if I have a ": [
      2721.3,
      2745.7,
      103
    ],
    "you know, just the Matrix and trees have to add up six terms. You can reduce it to kind of too complicated looking terms. I'm not going to write the formula here because if you try to do it for 4 by 4, there's no reasonable formula and as the size of the Matrix grows actually Computing the determinant from the definition. We're going to see in a second becomes ": [
      1954.9,
      1976.7,
      75
    ],
    "you need to check that all the columns are pivotal. In other words, I need to check that the Matrix is invertible one way or another that's what I need to check. So what I need to do is row reduction to find a row Echelon form for this Matrix. How many steps the row reduction do I need to do here? Not this Matrix is already in row Echelon ": [
      1515.4,
      1536.1,
      57
    ],
    "zeros everywhere. Hey, but I have that there's several choices here. Well, actually there's a good choice now using the cofactor expansion. I'm going to spend along the last column. So I'm going to get 3 * 2 * Okay, so the sign pattern Matrix + - + - + - + - + says that for that last call on my start with a plus sign and what that ": [
      2964.2,
      2989.7,
      113
    ]
  },
  "Full Transcript": "Do I listen to a podcast on Wednesday evening today? We are going to finish the discussion of section 4.4 that we got through most of last day and move on to chapter 3 chapter 3 is about determinants in general. We're going to spend the rest of today's lecture and all of Wednesday's lecture next week on determinants before we move on to chapter 5 I get values. So just so you have a horizon there for what you should be reading it had about. So let's begin.  And let's continue.  With a reminder of what we did in the last 20 minutes of last days lecture. We talked about coordinates. So what's so great about a basis if I have a Subspace of an abstract Vector space and it could be a Subspace of some other Vector space. I just have a vector space.  then if I have a basis for it some basis vectors B1 B2 up to be and I'll call that basis script EB Capital be if I have a basis for it, then what's so great about that among other things is it allows us to give an address to every point in the Subspace independent of whatever description you have of the vector space like it could be some abstract space of polynomials or it could be a Subspace in R7 but of Dimension 3 and so it's kind of inefficient to describe elements of that Subspace using 7 numbers because since it's a three-dimensional Subspace, you should really only need three numbers to describe elements of that Subspace and that's what coordinates do for you so because it's a basis  Every Vector in the space can be represented as a linear combination of the basis vectors. That's because of the basis of spanning set. But that's for wedding spending said but an additional basis is linearly independent and we saw last time what that means is that the representation that you get expanding that Vector in terms of basis vectors its unique. There's only one way to do it is any veteran the space can be written to uniquely as a linear combination of the basis vectors does a unique set of coordinates x 1 x 2 of The X whatever it is is expressed as x1v OnePlus X to be two-plus up to xnvn.  So just like we did in the first lecture where we had a bunch of x's hanging around in some equations and we decided to simplify notation by getting rid of the exes and just writing a matrix of coefficients we can do the same thing here. Once we have a basis in mind. We have our Subspace. We have fixed the basis. I'm going to use it to describe all the factors in the Subspace. I do so by dropping the bees I just right in the exes are those axes are called the coordinates of the vector in question three to vector v has a coordinate Vector, which is a column back to the kind that we've been setting for most of this class will denote it like like this over here.  We put square brackets around the vector v with a sub be indicating that these are the coordinates of that Vector in the basis be keeping in mind that if we were to change bases and use a different bases who would get a different coordinate Vector the coordinates depend on what basis we choose.  Okay. So once we do that that gives us a representation of our Subspace as column vectors, even if it's some abstract space of polynomials or matrices or something else we can think of them think of the vectors as column vectors call investors living in our n where n is the dimension of the Subspace. So that means that if I did indeed have a three-dimensional Subspace of our 7 that I was driving and I fix a basis for that three dimensional Subspace. Well by definition of Dimension, that means that any basis I choose will have three vectors in it. So I fix a basis of three vectors and that means that every veteran the Subspace will be uniquely expressible as someone near combination of those three vectors. So I take those coefficients in a linear combination write them in a column Vector in R 3 and that tells me where the vector is. Maybe I should write that down to be clear here it  At the arrow here is actually the hard Direction.  If I tell you.  Set the coordinates of a particular vector.  They're going to have a basis br1 7 - 3  if I tell you what its coordinates are in terms of a particular basis.  What that means is that the vector is Just One X the first basis Vector + 7 x II basis Vector - 3 * the third base is Spectre. This is in the special case where and is equal to 3, right then a basis will have three elements. So if I tell you what the basis is and I tell you the coordinates of a vector then it's immediate in routine to tell you what to do for you to report to me what that doctor is you just use those coordinates to linearly combine the basis vector and get the vector that you're interested in. The typically harder thing to do is to go the way that's written there. It says hey look I've got some vector and I want to figure out what its coordinates are.  Hey, so you can do that with ad hoc. Methods will see that in a few examples here. You just have to figure out some way to express it in terms of the basis vectors. And once you figured out some way, you know, since it's a basis that that's the unique way of there are the coordinates right in your face. Okay, but if we're in the case, where were talking about column vectors already a Subspace of RN or are seven or whatever?  Then we know what we need to do here because that's a vector equation V is equal to unknown coefficients linearly combining the bees which are known we want to solve for the X's there. That's a vector equation. That's the sort of thing. We've been doing since day one of this course and we need to use row reduction and find what the ex's are. And the point is that because the bees are linearly independent. There will be a unique Solution. That's why I would have called coordinates. Okay. So those are coordinates and that's the so-called unique representation theorem. If you want to use fancy words like in your textbook  But it's a very simple idea. We just if I've got a Subspace of dimension for then I only need four numbers to describe all the factors in it. So let's just use those four numbers keeping in mind that which phone number for Tuesday pens on the basis that we have hiding in the background that we need to keep consistent now once we do that if I fix a basis than this actually gives me a function, right? So I've got a vector space V and I take any Vector in there. I can compute its coordinates. So Associated each vector v is its coordinate Vector, so that's a function.  That takes the vector space V and as the domain and the codomain is r n where n is the dimension of the space?  And the great thing about this function that it's the kind that we know how to analyze in this course. It's a linear transformation. I'm not going to prove that because it's sort of routine and mind-numbingly boring and to do what we need to do. If we want to prove it is a linear transformation is prove the key of b + W is equal to T A V plus DFW the T respected Auntie respect scalar multiplication so we can verify that directly but all that is doing for us again is you know, realizing that on either side of of tea in the vector space or an RN  Addition and scalar multiplication work the way they're supposed to work, right that's you get that if I want to add up to call him back. There's that just means that I add them component wise and if I want to have two vectors on the left hand side if I had them both in terms of B, then I just add up the coefficients of B12 if you wanted so far. So this T is a linear transformation called the coordinate transformation.  but it's not just any old linear transformation, but I want to make a comment about it first, which is  So when we studied linear Transformations, we said linear Transformations are the same as Matrix Transformations. So you're saying Okay. So this Tia has a matrix not so fast because before when we talked about linear Transformations, we were talking about linear Transformations from our $3 7 from RN to RM.  When you're talking about linear Transformations between two Vector spaces that are already presented as column Vector spaces then yes, I really need a transformation is a matrix transformation as we proved but if V there is p to the vector space of polynomials of degree less than or equal to 2.  It doesn't even make sense to ask for a matrix 44 the coordinate transformation cuz the domain is not a space of columns. We wouldn't even know how to multiply those things by matrix. Okay, so it was slightly misleading to say that every linear transformation has a matrix every linear transformation from RN to RM has a matrix linear transformations in general doesn't even make sense to say they have a matrix unless you present your vector space as a space of columns, which you can do using the coordinate transformation, but that's going to depend on which basis you choose. Anyway, that was a slight the side here. What is great though, is that this linear transformation in the coordinate transformation the one that's written there?  It's not just a linear transformation. It's also got all the nice properties that we would want. It is one-to-one and it is onto.  Okay, so remember 1 to 1 means that?  Looking at a particular column Vector ex wants your accent on the right hand side. There's exactly one thing on the left hand side that it comes from but that's just the observation that's written right down here, which is it. If I tell you the X's if I tell you the X's you can recover the the vector immediately once I tell you the X's then there is exactly one factor that came from and it's also on to  Meaning that every Vector in r n is hit by the transformation. And again, that's the same observation that's down here, which is I want to know if the vector 1 7-3 is in the range of the linear transformation. Yeah it is because I can use those coefficients to add up the bees and I get a vector back in my Vector space because the bees are a basis and therefore any linear combination of them isn't so this is a one-to-one and onto linear transformation if it had a matrix, it would have an invertible Matrix that it standard Matrix a one-to-one and onto the same as a convertible. It's an invertible function. I can go back and forth. That's kind of point of coordinates. Right? I should be able to tell you what the coordinates of any point is and if I tell you the coordinates you should be able to tell me where the point is you can go back and forth.  So there's a fancy word for such linear Transformations a linear transformation. That is both one-to-one and onto is called an isomorphism.  Your textbook does use this word.  Don't be scared by it.  It just means it's from Greek. So morph means form and ISO means same. So this means the two things on the two sides of the function have the same form what it means is that they look the same.  Okay, if you squint you can't tell them apart.  The coordinate transformation is an isomorphism between the vector space you start with and r n where n is the dimension of the space. It tells you in a precise mathematical way that if I have an n-dimensional Subspace, it looks like RN.  Just like when we started talking about these things and span maid if you had the intuition, okay, so this this Matrix here, it's got a basis for its column space of only two vectors. Okay, does that mean that they span R2? No, it doesn't even make sense. Cuz the Matrix actually had columns in R7, but the span of those vectors looks like R2. It is isomorphic to our to enter the isomorphism or an isomorphism that you can choose is the coordinate transformation. You just need two numbers to describe any element of the vector space. Therefore, it looks like R2.  So let's fill around with a specific example here to cement all of the stuff we've been doing so here is a matrix a 3 by 3 Matrix. So it's column space and it's null-space are both subspaces of R3 in this case.  I know I've already asked Matlab to do the row reduction for us to save some time. So here's the reduced row Echelon form of that Matrix.  So let's write down.  A basis for the column space, where am I going to find a basis for the column space?  Buy a hand summary.  Yeah, yes.  Heaven Cox, okay, which of the pivot columns  First and second columns in which of the two major is right in here.  Very good the first one OK the first and second Columns of the reduced row Echelon form contain pivotal ones that tells you that the first and second Columns of the original Matrix, they are pivotal and they form a basis. So let's rewrite those over here 101 + 310  And let's call them B1 and B2.  Okay, so the column space of a is spanned by those two vectors. So now let me call that basis be it's written there. So let's write down the coordinate Vector of some other vectors that are in that space. So for example, the vector 5 1/2  that third Colin was The Matrix a matrix a is in the column Space by definition. All the columns are in the column space their stand by themselves. So they're in the column space, but we claimed that the column space to spend by just the first two factors. That means that Vector 512 is going to be in the span of B1 and B2 in a unique way because there's two columns are linearly independent. So what I want to do is figure out what is the coordinate Vector of this column Vector 5 1/2 in terms of that basis before we figure it out. I want you to tell me what size Vector this thing is.  Astoria Column Vector of what height?  somebody from this side of the Room Place  You know if you get it wrong, it is no nothing any punishment. So, you know figure figure something out and tell me what you think. It's going to be.  It might be punished. But if you stay quiet, we've been jumping jacks in here before remember.  Okay, so somebody raise their hand and tell me what size it yet size 2y.  There's two basis vectors. And so we need to  Coefficients to combine them, that's right. Okay, so this is going to be some vector.  in our two  Okay, I know what Vector is it? Well the way we figure that out, right? What does this say? This this says literally we have to solve the equation 512 is equal to x 1 x the first bassist Rector 101 + X2 X II basis Vector 3 1 0  We know how to solve Matrix equations like that. We write down the augmented Matrix Associated to it.  101  310  and 512 but hey, look, that's the Matrix a  I did indeed. We remember that the reduced row Echelon form of the Matrix a it tells us how to express the non-committal columns as linear combinations of the pivotal ones and so this  this coefficient to here  and this coefficients one here, they match up with.  the leading ones  the leading ones in the two basis vectors. And so what those things tell us right. There is that the third column is equal to 2 times the first + 1 * this  So that is to say let's just verify that.  If I take two times the first ones or 1 + 1 x II e-310. I get 2 + 3 which is 550 + 1 + 1 + 2 + 0 which is to what that says is that the X1 and X2 hear. Those are two and one.  Okay. Those are the coordinates of the point 5 1/2. That's the coordinate Vector of the of the vector v 1/2 in the basis that we exhibited of the column space that lets us another question. How about  310  what are its coordinates in the basis Peak?  any thoughts  Yes.  01 exactly if you want to express the basis, Vector B2.  In terms of the Basis B1 and B2. Well one way you can do it as is 0 * b 1 + 1 * be too, but we know that the views for my bases. So once we have one way to do it, it's the only way to do it. There's a unique solution. So that's presented right to us there. So in fact we see that in general.  the coordinates of B J  In the basis B.  Are the standard basis vectors?  Okay, so that is  D3  MP3 might be a column Vector of size 78 or it might be a polynomial of degree 321. Okay, but whatever. It is in terms of the basis B.  It's going to be.  Represented by the vector with a 1 in the third position of zeros everywhere else.  Okay, where the number of elements there is going to be. The number of the number of components is going to be the number of bases factors the dimension of the space.  2 in this example what we see here anyway.  Is that the coordinate transformation T from the column space of a onto R2?  is an isomorphism  coordinate Vector tells us how to view the column space Hazard to it is not equal to our to but it is isomorphic to are too because there are exactly two linearly independent factors that spent  All right, let's do one more example before we move on.  so  I should say here.  Why do we care that this thing is an isomorphism I'm saying it shows they have the same form great, but that really means something specific.  It means that if I want to establish properties of a collection of vectors, I can look at the images of those vectors under the isomorphism and establish the properties there and it'll work on either side. So if I have a set of vectors, maybe they're abstract vectors like polynomials.  And I want to answer the question. Are they linearly independent?  Okay, if I fix a basis and use the coordinate transformation for that basis to view those polynomials as column vectors.  Then I have tools that we've developed in this course to answer whether those column vectors are linearly independent.  And the point is that an isomorphism preserves linear Independence. So if you figure out that the column vectors which are the coordinates vectors of those polynomials are linearly independent that tells you that the original polynomial vectors are linearly independent, which means that you can avoid all the confusion you might have about thinking of polynomials as vectors and work with column vectors once you fix a basis,  So let's use that the same applies to spanning sets. So let's use that to establish the following fact, here are three polynomials of degree less than or equal to 2x - 1 + x - 1 squared  I claimed that those three polynomials form a basis for P2.  Pitu remember is the vector space of polynomials of degree at most 2.  So I want to show that there a basis now in order to show that they're of Base. This one thing I can do is work right from the definition and establish that those three polynomials 1 X - 1 + x - 1 squared are linearly independent and span all of pee too. So I need to establish that are linearly independent. That's actually not going to be that hard to do directly. But I also need to establish that every polynomial of degree 2 or less like for example, x squared minus 7x + 21. I need to be able to write that as a linear combination of these three. And now if you start fussing around trying to figure out how to do that, you'll see that you can do it by solving some linear equations. That's what's going to go down to but we don't need to fuss around we can do it systematically instead by saying hey first  Fix a basis. Let's fix the so-called standard basis.  1 x x squared  4 P2  Hey, so it's called us bases be.  Know what I'm going to do is I'm going to calculate the coordinate vectors of the polynomials were actually interested in in terms of that basis. So the polynomial one.  Cancel that basis. Well, it's the first basis Vector in be so by what we said on the last slide. That's just going to give us 100 which is just to say that that the polynomial one is 1 * 1 + 0 * x - is 1 * 1 + 0 * x + 0 times x squared.  Okay. Now what about the second base of the second Vector in that list X - 1  I need to expand that in terms of the bassist One X x squared x minus one is already presented as X which is the second base inspector - 1 which is the first base inspector. So that means that we get a minus one in the first slot and I won in the second slot.  Now what about X - 1 squared?  Well now I'm going to need to use some polynomial algebra. She'll do over here when we did this actually last day. So I'll do it again. So X - 1 squared is x squared minus 2x minus I Plus.  So that's already presented now as a linear combination of the standard basis vectors. It's got a one in the first base inspector slot from -2 in the second base is Victor slot on a wall.  So if I fix the standard basis of polynomials, then those three polynomials there are just these three column vectors. Those are coordinate vectors.  If I give you three coordinate vectors in R3 and I want to establish that they form a basis for R3. How do I check that?  I need to establish that there a spanning set for our three and we know that means  that I should look at the Matrix with those columns.  But I need to check if it's a Spanish that I need to check that all the roads are pivotal. I also need it to be a linearly independent set which means you need to check that all the columns are pivotal.  In other words, I need to check that the Matrix is invertible one way or another that's what I need to check. So what I need to do is row reduction to find a row Echelon form for this Matrix. How many steps the row reduction do I need to do here?  Not this Matrix is already in row Echelon form and we see the leading ones.  Are in all three columns and all three rows.  Therefore this is a bassist.  It's so those three column vectors form a basis for R3 and because the coordinate transformation is an isomorphism IT preserves all the properties of vector spaces. It follows that the vectors themselves 1 X - 1 + x - 1 squared r a basis for that space P2.  That's how you can use coordinate transformations to make sure that you never have to deal with the abstraction. You can always view any Vector space any finite dimensional Vector space as RN for some n once you fix a basis, you have a coordinate transformation and you can always think of any Vector space as RN use the tools we developed throughout this course 2 answer any question about the only caveat is you have to keep in mind what basis you're using and be consistent about it because if you change the basis the coordinate representation will change and if you mix and match you can run into trouble there is a systematic way to handle changing bases de that section 4.7 in the textbook. We're not going to do that in this course, if you go on and future linear algebra courses, or if you go on in a computer vision course or it in several kinds of Statistics courses, you will need to understand change of basis Matrix and you will learn it then  Okay, it's a little tricky but it can be handled systematically, but regardless as long as you stay fixed in one bases the whole time you're fine.  Okay, so that's I think everything that I wanted to say about the coordinate transformation and coordinate vectors and hopefully this gives you some hope that if you have been confused about abstract Vector spaces as long as you can convince yourself that you can find a basis of that Vector space then the confusion should all go away because you'll just need to work in RN once you translate to the coordinate vectors.  So that is the end of chapter 4 as we're covering it and now I'd like to go on to chapter 3.  All right chapter 3 is a very different flavor from the rest of the book.  In fact chapter 3 doesn't isn't really linear algebra.  Determinants are not really linear algebra determinants are multilinear algebra, and we'll discuss what that means next day a little bit.  But for now, this is going to be very far more concrete than anything. We've been doing since the first or second lecture. We're going to be doing something that's just quite computational at the moment.  What is the determinant we've already encountered determinants in the two by two setting so if we have a two-by-two matrix a general one there a b c d.  We discovered that if you wanted to.  Do row reduction on that and find a reduced row Echelon form of that a generically right with generic a b c and d there you can do it if you're super careful, but we found that at some point in the calculation. We did this in this room at some point of the calculation this quantity 80 - 2 BCE comes up it comes up in one of the slots that you hope is going to be pivotal.  So you would have to divide Surabaya to continue the row reduction and if it's zero, then something different happened. So that quantity we call the determinant of that quality a D minus determinant of a two-by-two matrix and the general notation for this is debt a in some cases and you textbook uses this as well. It's denoted as like with absolute value bars like the absolute value of the Matrix a you shouldn't pronounce it. That way you shouldn't think of it that way. It's not an absolute value. In fact, it is often negative. Okay, but that notation is useful, especially when your Computing lots of determinants. It's easier to write 2 bars then Det.  And what we saw in the two by two case was that a two-by-two matrix a is invertible. If and only if that quantity the determinant of a 80 - BC is non-zero, that's really where the term determinant comes from because it determines whether the Matrix is invertible. That's what first arose and in that case. We have this formula that for the universe what you do is you / that nuns remember?  And well for a two by two you swap the diagonal entries and negate the off diagonal entries.  So that's where the terminals came from for us.  But that's specific to the to buy to setting OK. I want to point out a couple of things that are computationally troubling already or won this competition the trouble even in the two by two setting on another that's more General as we go forward. So that number there a D minus BC that might be zero in which case The Matrix is not invertible.  The theorem is that the Matrix is invertible. If and only if that number is non-zero, so that's great for theoretical purposes. But in a real world application, if you're if you've got some Matrix that comes up it from some statistical application that you're working on.  And you would like to invert that Matrix you calculate determinant you find a determinant is .007.  That's a really small number. That means that it's not zero, but if you invert if you take its reciprocal, okay, it's going to be a number with 8 zeros in it. It's a huge number. So you're the inverse is going to be a big Matrix with huge entries. And here's the thing in a real world application. You might not know the answers of the Matrix Beyond five or six digits of accuracy. So if the determinant is .007, but you actually were off by 1% in one of the entries that maybe the actual determine was 0 or maybe it was .03, right? I mean the amount of error in your entries if you have a real-world situation might be much larger than the calculation you get for the determinant.  When determinants are small, the system of equations is numerically unstable small perturbations in the coefficients of the Matrix can drastically change what the universe looks like or whether or not exist. So that's one problem with determinants right now. If you're going to divide by a number, it might be really nonzero, but really small and in numerical applications that can present real problems. If you go on and take a course in numerical linear algebra are lots of out that in ways that you can try to counter it.  The other thing is though as we're going to see you in a few minutes. It's also for the two by two case. It's easy to compute determinants in the 3 by 3 case. It is possible to write down a formula that you may have seen before that's involves, you know, just the Matrix and trees have to add up six terms. You can reduce it to kind of too complicated looking terms. I'm not going to write the formula here because if you try to do it for 4 by 4, there's no reasonable formula and as the size of the Matrix grows actually Computing the determinant from the definition. We're going to see in a second becomes come occasionally infeasible quickly.  Okay, so we'll get around that by the end of this lecture or early next lecture. But keep that in mind as we go with it the definition. I'm about to give you for the determinant. It defines what it is, but it quickly becomes computationally invisible to actually use it.  So this is the top of the slide just summarize what we said about two by two determinants on the last slide.  That determine 80 - PC it determines whether a matrix is invertible or not. And it has a simple formula for hire size matrices. There is a thing called the determinant. It is some polynomial function of the entries you get it by combining the entries with product and sums and minus signs in some particular way that has the same property that it determines whether the Matrix is invertible or not for a 2 for an end by a square Matrix, but in general there isn't a formula for it that is nice and easy to compute instead. I'm going to give you a recursive way to compute it. This is the way your text defines the determinant. It's a standard way to find determinant and it's a recursive method that involves something called the cofactor expansion which sounds fancy but I'm going to show you where it is right now and it's not fair. It's just routine. You just need to know how to do it. So let's let's do a specific example here.  So here is a 3 by 3 Matrix and I'm going to tell you how to compute some of the co-factors of this Matrix. So we're going to compute c214 this Matrix the 2-1 cofactor. What is that? Mean? What it means is that I take the second row. That was the first take the second row.  and the first column  and I ignore them.  Take a second row in the First Column and ignore them. I'm left.  With these entries down here.  And the 2-1 cofactor is the determinant of that thing that's left.  Almost there's one wrinkle, which is that I also have to look in this sign pattern Matrix here. So what I'm Computing these things so there's the beginning you should always write this down to I have a 3 by 3 Matrix. I write down the sign pattern Matrix next to it, which is just I put chess board pattern of pluses and minuses on it always starting with a plus in the upper left-hand corner.  and then for the 2-1 cofactor  If I look at where those two intersected, which was there, then I look at that sign and that sign in the sign pattern is a negative. So I put a negative there and that's the definition of the 2-1 cofactor now noticed that it requires us to calculate the determinant but it's a determinant of one size lower. So in a moment, I'm going to show you how to compute the determinant of a 3 by 3 Matrix.  Doing so requires that you already know how to compute the determinant of a 2 by 2 Matrix which we do question.  The first row you're right. What I just did here was C12. So let's change that.  This is the C12 the cofactor C12 C12 means like with Matrix notation. The first number tells you the row. The second number tells you to call him. So the first row and the second column there.  Now we know how to calculate to buy two terminals. So let's do that here. So this is -4 * 9  -7 * 6  Okay. So 4 * 9 is 36 and 7 * 6 is 42.  So when I get here is 6 so that is the one to cofactor of this Matrix.  Are you now know how to do all co-factors I could do the three three cofactor here, which just means that I ignore the third row and the third column that would leave me just the upper. We're here 1245 and I calculate the determinant of that but I have to * a + sign cuz that's in the three three factors.  Okay, so determining or calculating the co-factors of a 7 by 7 Matrix requires that you know how to compute 6 by 6 determinants.  But I I remove one row in one column. I'm left with a matrix of one size smaller.  And in order to calculate the cofactor, I need to know how to compute the determinant of that Matrix. That's what it's defined to be.  So you don't know how to compute 6 by 6 to Terminal C at well, actually now you do because of the next slide.  So here is a definition of the determinants. So I give you a matrix a there's a general looking Square Matrix a  the determinants of that Matrix is defined by the following thing you pick any road you want.  Pick the first row js1 pick the seventh row js7 you compute the co-factors along all entries in that row.  You multiply them by the entry in that row.  And you add those up and that's the determinant. Okay, you pick any column that you want and you calculate the co-factors along that column and add them up X the entries in that column and that will give you the same number. This is a bizarre definition because it's got a huge amount of freedom in it. Okay. I'm telling you pick any road you want picking call me want and go on down the line and you'll get the same answer which is quite remarkable just to demonstrate what this means now, let's go. Let's do this in. In this example here. So here's a 3 by 3 Matrix.  So I'm going to pick some row or some column to expand a long. I'm going to pick the second column to expand the long.  I so luscious Fresno right down with this formula says in that context. It says that the determinant of this Matrix a  is so I'm using the second column.  so I have to take C1 to C2 and C3 to  And add them up multiplying by the entries a12.  A 2-2 and a 3-2.  And that is to say the entries are five.  4 + -2 x those co-factors, so I need to figure out what those co-factors are.  It's a what are the co-factors here? Well, so for that first one C12, that's what we did on the last slide. So now I'm going to delete the first row as well.  And that's going to give me here five times the determinant.  Of that to buy two thing that's left to 0-1 0 but remember there's one more piece of information we need for the cofactor, which is this sign pattern Matrix.  And so this is the two one cofactor. So I look in the 2-1 position which is a minus sign. So I have 2 * -1 there. So that's that first term then I add to it four times the c22 cofactor.  Okay. Well the c22 class doctor means I delete the first the second row and the second column from The Matrix, so that leaves me with.  the determinants  of 1000 * a sign but in this case it's the two to sign which is Plus.  and then finally, I have to add -2 that the entry in the 3-2 position of the Matrix x a sign which is the Maya minus sign of the three true position of design pattern X the determinants of what's left when I delete the  Third row and second column and that gives me the Matrix 1-0 2-1.  I know I have written the determinants of this 3 by 3 Matrix as a combination of determinants of two by two matrices and I already know how to compute determinant of 2 by 2 matrices. So let's go ahead and work this all out to this is -5 times the determinant of this first Square Matrix over here pay which I am I get by multiplying with diagonal and Auntie diagonal entries, but there's zero is in both of those so that one gives me a zero.  and similarly  this determinant there zeros in both places. So for that one I get + 4 * 0  And for the last one down here, I got a 0 in one place. So I get 1 * -1 - 0 * 2 so that one's nonzero. It gives me a minus one so I can - 2 * -1 x -1  At all together I get 0 + 0 + -2 x -1 x -1 is -2. That's the determinant of The Matrix.  No.  That's correct. But it was also dumb because in this procedure, I'm going to add up along any row or column. I'm telling you that I got the same answer no matter which row or column I choose to expand a long.  And there's a good choice here several good choices for once to choose to stand alongside. That wasn't one of them why.  Yes.  Perfect, cuz I could have used the third row or third column which have some zeros in them immediately for us to to use so let's rewrite the Matrix down here again. So this is 15024 - 1 + 0 - 206 Panda long the last row.  Okay, if I do that then I'll see the determinant of the Matrix is equal to 0 times some sub determinant some some cofactor 0 * right explicitly this time first design pattern is plus minus plus so we got a Plus One X the determinants of the stuff after I delete the first row and the third the first column in the first row. So there's a 5-0 4-1 there, but I didn't mean to write that down cuz there's a zero right here. So whatever it is, it's not going to contribute then I get + -2 x design pattern invest in that position down here is -1 times the determinant of  Now I have to delete the third row and the second column that leaves me with the Matrix 1-2 0-1.  And finally, I have the third entry is 0 * + 1 * the determinant of 1524, but I don't have to do most of those calculations because I get a zero immediately for each of those from the entries of the Matrix. And now the thing that I have left is actually the same conversation. I already did its 2 x - 2 * -1 x -1. Give me -2 same answer as before. That's the way it's going to work out.  So now, you know how to compute any 3 by 3 determinant using the cofactor expansion that requires you to be able to compute to buy two determinants. But now you also know how to compute any 4 by 4 to terminal because if you do a 4 by 4 determinant what's going to happen is you're going to do a choose a row or column you're going to expand along it and you're going to get a combination plus and minus signs x a linear combination of four three by three determinants, but now, you know how to compute three by three determinants. So each one of those you're going to compute by expanding along some roller column.  And they were just two by two determinants and so on Down the Line.  You now know how to compute determinants.  You should never ever do this to compute determinants.  Why is that? Well, let's let's analyze what I just said. So if I have a generic 4 by 4 Matrix  You're going to choose a roller column. So there's four entries in the in the in a row and you're going to have to add up.  For three by three determinants of those issues Aurora column that has three entries and the cofactor expansion is going to spend that as 3 to buy to determine its so so far we had to do 4 * 3 * number of calculations in each to i-22 terminate. How many calculations in a two by two determinant just two of them. So we have to do 4 * 3 * 2 calculations that number's called 4 factorial 24. So doing a 4 by 4 to terminal is going to require 24 calculations in a whole mess of writing.  How about 5 by 5? It's going to take 5 * 4 * 3 * 2 which is 120 calculations to do a 5 by 5 to terminal. How about a 10 by 10 matrix?  10 * 9 * 8 * 7 * 6 * 5 * 4 * 3 * 210 factorial which is over 10 million that will not be on the midterm.  Well, actually it might because there's a better way to compute determinants which will get to next time. But before we get there, let me highlight at least one nice feature.  Which is everything I just said is is generic if the Matrix has generic entries, like if I give you a random Matrix if I produce the entries randomly, then they're never going to be zero. Okay, we're very very unlikely that any answer is going to be zero.  But in the actual example, we just worked with and in this example hear a ton of the entries are zero. And so if we are clever if we are judicious about which row or column expand a long will save a lot of time. So here is a 5 by 5 Matrix. It could take up to a hundred and twenty calculations in order to compute the determinant here, but actually if I expand along the first call  Write the determinant of a here using the cofactor expansion. So I need that sign pattern The Matrix, but now I'm going to say well, I know what I think I know it well enough so that I'll know where the signs come in.  So that's going to be let's see if I expand along at first, I'll get three times the determinant and now I'll use that absolute value notation. You see where it starts to be useful time. Is this part down here.  answer the determinant of that 4 by 4 Matrix 2 - 573  0150024 - 100 - 2 0  and the other five terms are all 0 so I won't even write them down. Right? So now I need to come get this four by four determinants. And now I'll use the cofactor expansion. And again, in this case. It's makes most sense to expand along the First Column. You don't have to use the same column every time the cofactor expansion says you can choose any roller column that works for you. So you can change which roller calling you're using data from one step of this process to the next that's fine. In this case. It still makes most sense to use the First Column cuz it's mostly zeros there. So this one's going to be three times. Okay. Now I have a two and the sign pattern says the one 1 entry is always gives in a plus sign. So that's fine X the following thing. It's the determinant of this down here, which is 15024 - 1 + 0 - 2 0  I know I don't quite have zeros everywhere. Hey, but I have that there's several choices here. Well, actually there's a good choice now using the cofactor expansion. I'm going to spend along the last column.  So I'm going to get 3 * 2 *  Okay, so the sign pattern Matrix + - + - + - + - + says that for that last call on my start with a plus sign and what that that gives me 0 they're the only one that contributes is the -1. So I get - 1 * this - 1  Time is the sub determinants X the cofactor, which is formed by.  Everything that is not colored there. So that's 150 - 200 that's a really easy to terminate to compute. I'm already down to two by two. So I got 3 * 2 * - 1 * - 1 * - 2 - 0 and what I get is 3 * 2 x - 2 which is -12  So that was a lot less than 120 calculations. No.  If you review that systematically and noticed one thing so in this example hear a I had almost everything below the main diagonal zero there was only a couple entries that too and that -2 there below the mountain man dagannoth at work. Not zero if I had had zeros everywhere below the main diagonal then I could have just done the same thing. I just did but choosing the First Column every time so if I have zeros below we're just about done here if I have all zeros below the main diagonal  Everything is 0 down here. Then when I do the cofactor expansion.  If I expand along the first column.  I'm going to get only this part contributes cuz of all the zeros and then in the next phase if I choose the first column.  Same thing will happen and I'll only have to deal with this part of the Matrix and so on. So what you'll see from that is that if you have a triangular Matrix where everything is zero below the main diagonal, all you're going to produce is the product of the diagonal entries. So there's one case where where determinants are easy to compute if your Matrix is triangular the determinant is just the product of the diagonal entries one calculation.  And that's going to be useful to us will continue with that next Wednesday. Have a good long weekend.  UC San Diego podcast ",
  "Name": "math18_b00_wi18-02162018-1000",
  "File Name": "lecture_17.flac"
}