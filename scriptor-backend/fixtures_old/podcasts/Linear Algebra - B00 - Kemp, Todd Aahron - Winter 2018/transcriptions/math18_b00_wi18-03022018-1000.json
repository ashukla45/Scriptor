{
  "Blurbs": {
    "0 * the two by two determinant of four- Lambda 025 - land and then you see that the professor carefully engineered this this is lower triangular, isn't it? And so I don't have to do any work at all at this point. I'm going to have 5 - Lambda squared * 4 - Lambda. Okay. Now I have to see I wasn't here on Monday when Professor Kemp did ": [
      376.3,
      414.3,
      8
    ],
    "10 okay, and then -4 0 1/4 + 0 And that's 5 at 5 p 3. It isn't it once I can do the arithmetic right now. I'm going to do I'm going to call. I'm going to go over here. I'm going to write d. Anaheim Airport how to make D this way. So how did I make day? it does happen to be the diagonal of a what ": [
      2385.3,
      2436.9,
      55
    ],
    "Camp will clear up any questions I left. Have a great weekend and he'll catch up with you then. If you get around me children's a story I will tell. Pretty Boy Floyd in Oklahoma It was them. It was Saturday afternoon. His wife is not him in his wagon is in the town they roll. Thera definition of approached him In a manner rather lose using vulgar words language. ": [
      3023.5,
      3297.1,
      67
    ],
    "Do I listen to a podcast and how they relate to length and orthogon ality? Okay, that's our plan. Are there questions before we charge into this? Yes. When will the exam be graded we're expecting the scores to come out by Monday morning. That's our hope riderta is a promise that they can do that for us. other question alright, so Let's see. I'm going to put up a ": [
      2.0,
      54.7,
      0
    ],
    "Echelon form 0 right? Thank you, see how I did that then you'll see how to do that on your own and it'll be faster than writing out the equation won't it? Okay. Don't tell Todd I let you in on that secret. You don't have to use this right? Cuz you know, he'll if you write out the equation of course gives it to you right guy, but this ": [
      1831.8,
      1858.7,
      42
    ],
    "F p inverse a p Is equal to d a diagonal matrix? for some convertible p if I can do this and said to be diagonal in the theorem Hey. remember my button November Matrix a is diagonalizable if and all my app a has and linearly independent eigenvectors eigenvectors and we saw this in the computation. We put the eigenvectors into a matrix. They're linearly independent. It's invertible and ": [
      2729.5,
      2817.8,
      61
    ],
    "God's got to help me out if we're going to get through our agenda here. Okay now. the point here is that now I'm going to compute. the characteristic polynomial a v All right, which is the determinant? a B minus x i You know, what I'm going to do is I'm going to observe that. B is p inverse AP And I'm going to subtract X. And I'm going ": [
      795.7,
      843.0,
      16
    ],
    "I had let me just had say C1 C2 add a name to be zero wealthy one has to be zero cuz the entry corresponding this has to be all right. Mmc2 has to be zero because the entry corresponding this has to be there up. So these will be linearly independent by the way, you chose them. Okay, just of course, they'd be linearly independent the road out the ": [
      2095.0,
      2123.6,
      49
    ],
    "I had to do this. Scene where this is going. Okay. So so what would I buy if I if I had SE X1 X2 X3. We're all free. Maybe I should be okay. That can't be right. All right. One of them has to be a pivotal girl or the whole things are okay for by 4 Matrix. Can you picture that? Okay, and then I had a three-dimensional ": [
      1992.3,
      2025.3,
      46
    ],
    "I have distinct if I have eigenvectors corresponding to distinct eigenvalues what I know about him. Seen that right. What can you say about eigenvectors corresponding to distinct eigenvalues? Their what? Linearly independent, that should be on a flash card. right Okay, I can vectors corresponding to distinct eigenvalues are linearly independent. Okay, but I don't have distinct eigenvalues for a matrix that has the where's my Wii we computed ": [
      1176.6,
      1224.1,
      24
    ],
    "I want the null-space of a -4 I slept probably means I want to compute a minus for isos do that. Can I have the let me just This means I'm looking for a solution to a -4 i x is equal to 0 right. Okay, if axes in the null-space of a -4 eyed, and that's true, but that's the same as a x x is equal to 4X, ": [
      1456.5,
      1495.3,
      31
    ],
    "I was promising. Geometric multiplicity, which is the dimension of the eigenspace namely the null-space of B- the eigenvalue times. I can be less than the algebraic Multiplicity. This is an example. the algebraic multiplicity is to The geometric multiplicity which is the dimension of the null space of B- five eyes only. And B is not diagonalizable. Why are theorem? All right. That's all I have time for Professor. ": [
      2978.2,
      3023.5,
      66
    ],
    "I'll write invertible p okay, and now the claim is that the characteristic polynomial of a is the same as the characteristic polynomial be since I have extra going to use x Guy and then of course Professor Kemp gets the clean up the mess I make on Monday, right? Are you ready? Okay. So what's the characteristic polynomial of a x? What is it? determine today minus x i ": [
      739.4,
      794.5,
      15
    ],
    "I'm going to call P. I still have that. Okay. minus 120 010 + - 201 how did I make pee? Pulled out of a hat, right? Yeah. But I wrote the vectors that span the null space of a minor course. This is an eigenvector corresponding to for isn't it? Write the species are both eigenvectors corresponding to 5 on Thursday. That's how I did I hear these are ": [
      2165.8,
      2217.4,
      51
    ],
    "Matrix a in our example over there this is going to be the determinant a four- Lambda 2-0 0 5 - Lambda 0-2 4 + 5 minus Lambda. Okay, and this is the notation the book uses the vertical bars mean where Computing the determinant right? Unfortunately, some people will write determinant in front of this because they don't want you to confuse the vertical bars with what? Absolute value ": [
      292.0,
      336.8,
      6
    ],
    "Now what do I want to do with it? I want to find the null space of a -4 I how do I do that? Okay, I want to row reduce it to Epsilon form. I think this is going to now and I packed the Matrix to which this would be simple. I can clearly clear the third entries, right? So I'm going to this this guy is going ": [
      1602.1,
      1627.4,
      35
    ],
    "Okay, so determinant. a p inverse X the determinant of a minus x i Tennessee determinant Of course, these are numbers. So now I can multiply in any order I want right. That's the beauty of the determinant this guy, of course. This is the determinant. The P inverse X the determinant of p and what's this guy? What am I trying to prove? I'm skidding knee-deep in a computation. ": [
      1013.6,
      1062.8,
      21
    ],
    "So the second one's - 2 + 10, which is 8. And then I'm going to get zero. And I'm going to Waverly this is P1. This is P2. MP3 hey, what's this? Come on. How does it compare to P1? Or P1 respecting it. Of course you were what's a x p 1? 4p one because P1 is an eigenvector corresponding to for isn't it? Okay Ru with me ": [
      2271.5,
      2323.7,
      53
    ],
    "Wallah. We can write p and verse AP is diagonal this equations always through this a people's Pee Dee does not require The Columns of P to be linearly independent right could be zero and a p equals PD, right? Okay, but in order to have pee and Bruce a p equal deep he has to be invertible. And so this is the content of this theorem. If you have ": [
      2817.8,
      2844.6,
      62
    ],
    "What are the span is the span of this the same as span of that? That cuz the span of one vector is just all multiples, right? Is everybody with me? How did I do that? What's the first thing I did? Set the free variable component to what? One and then I just looked at what the others had to be to make the matrix product with the reduced ": [
      1802.5,
      1831.8,
      41
    ],
    "What was I trying to do when I started? Yeah. That's the characteristic polynomial. Okay, Anna. And of course now, this is the this is the determinant. P inverse x p write again, because the What's the P inverse x p? Okay, what's the internment of the identity one? So this is one? Okay, I started where do I start started here? And I went to hear here here. Here ": [
      1062.8,
      1107.3,
      22
    ],
    "a free variable. I mean the free variable for the homogeneous equation, right? Okay. So if I write a -4 i x x equals 0 x has three components X1 X2 X3, which one's the free one X2, okay. Now what so what I'm going to do is I'm going to put a one. In for my free variable, right? Cuz once I set a value for that the others ": [
      1708.3,
      1742.0,
      38
    ],
    "a full set of eigenvectors, then you have an invertible Matrix P for which you can do this. All right. All right. I want to I have 2 minutes. I don't have time to do the complete computation. I'm going to look at be. Look at I'm going to look at B is going to have the same eigenvalues work that out. I just changed this entry. It's off the ": [
      2844.6,
      2884.0,
      63
    ],
    "a is similar. to be of course A & B are November by November matrices. then the characteristic polynomial of a It's the same as the characteristic polynomial of be. And of course again if I change this to ax with the statement be any different. Would you know what polynomial we were talking about if the variable is X? Okay, then if I asked you what that polynomial was ": [
      564.3,
      619.9,
      11
    ],
    "a minus Orlando Eye Can this is going to be a polynomial? All right, and it just to emphasize this it only depends on a all right. So imprecise that I could have p a a backs what would p.m. XB? The determinant of what? Perfect. Okay, and okay, I won't write it. I was going to write down what piece of a of joe-bob. Exactly. The determiner of a ": [
      163.9,
      215.2,
      3
    ],
    "and three zero in the fourth one one. Now the beauty of this is Will these be linearly independent. Yes, that's why the shortcut is so cool. By choosing your free variables with these values. They have to be linearly independent. Don't they? Because in order for a linear combination of the B 0 you have to have the coefficient b04 each one of those, don't you? see that if ": [
      2060.3,
      2095.0,
      48
    ],
    "are determined, aren't they? Okay, and if I N N I want this to be in the null space. So if I multiply this guy times this it has to be zero since this guy this entries. I just need to set this one and if I set it to be minus 1/2 Then this guy comes that guy will be zero no matter what that is, right. Hey, this ": [
      1742.0,
      1768.3,
      39
    ],
    "be minus 120000 - 250. Out and so when we can so I have a 0 row hair. 2 times the first row added to the seconds going to give me 0 0 1 and then I can use this one to clear that and I get this. What's the dimension of the null space? a 3-5 I its 1 This is called geometric Multiplicity. And this is the answer ": [
      2926.8,
      2978.2,
      65
    ],
    "can Vector corresponding to four have to be linearly independent, right that has to be but if I have an eigenvector of algebraic multiplicity to maybe I have two different eigenvectors that are linearly independent. Or maybe I don't. That's going to be the issue. So let this evolve alone. That's a great question. Keep it in mind. Okay. And because there's another multiplicity called geometric multiplicity that we have ": [
      1266.2,
      1296.5,
      26
    ],
    "diagonal and you can check that the characteristic polynomial be is the same as a however. There's a difference to look at Lambda equals 5. and a right at PB of x is X is a is 5 - x squared from 4 - x the same characteristic polynomial and if I look at a B kind of keep my ears and be straightener B- v i It's going to ": [
      2884.0,
      2926.8,
      64
    ],
    "else is it? Yes, the diagonal matrix with the eigenvalues on it. And of course five is listed twice because it has algebraic multiplicity what to okay. Now what I'm going to do is I'm going to compute have two pieces. All right. I'm going to compute p x d. Damn, right it out. Where's my P - 120? 010 minus 201 Deer were here, which is for. 5 and ": [
      2436.9,
      2479.4,
      56
    ],
    "equation and did that cuz it's equivalent to doing this really, okay. What should I pick for this one? How about this one? -2 act you guys are good. Pretty easy, right? That's so you can pick them out pretty quickly. Once you have the reduced Echelon form right now what I'm going to do. Is I'm going to go over here. Doctor now that right now they have these ": [
      2123.6,
      2165.8,
      50
    ],
    "evaluated at Lander you could do that, right Okay, just checking. Okay, the polynomial that the variable is going along for the ride the polynomial describes the relationship, doesn't it? Are you with me now? It's Friday. Sunny outside supposed to rain though, right? Okay. Maybe we'll be inside and be able to get that beer out of the refrigerator by the time it's raining. Okay, are you with me ": [
      619.9,
      649.1,
      12
    ],
    "fact. Because if you say a peek was PVP could be anything peekaboo 0 right? Are we going to allow P equals 0? No, sew what similarity. Okay invertible Matrix P for which? Okay, so I'm going to write it this way. For some reason I write this you could probably let me insist that that that means of course that pee has to be invertible right for some but ": [
      698.8,
      739.4,
      14
    ],
    "fight on the diagonal okay, and if I multiply Noticed that when I multiply each row by the First Column I get exactly this column don't I? And If I multiply each row by the second column I get exactly this one don't I? And finally the third call him it's the same thing. I'll get that these are equal. and that means now one other thing p Which is ": [
      2479.4,
      2520.9,
      57
    ],
    "for the vertical bars. Don't mean absolute value here. This could be negative, right? and how would you expand this? Yeah. Yes, it would wouldn't it let's do this along the third row where I have two zeros and then of course the checkered backboard pattern says out of + - + - + + so that means that the coefficient 5 - Lambda comes out the other two or ": [
      336.8,
      376.3,
      7
    ],
    "good as it was. I was hoping you'd help me out. I know I'm supposed to do this cuz Todd left me some notes supposed to talk about eigenspaces. But he assured me that you guys have seen him. So what do I do? What's the eigenspace of a corresponding to 4 wishes this guy here, right? So I want to know space. Of what? A - 4i. Okay. So ": [
      1423.2,
      1456.5,
      30
    ],
    "here and got here. They're the same. Does similar matrices have the same characteristic polynomial? All right. That's agenda item number three. All right. Now what I'm going to do is I'm going to find the eigenspaces of a all right? Okay any questions on this? Before we tackle the eigenspaces. That's a great question. What's the relevant of algebraic Multiplicity? Okay, so we have a theorem that says if ": [
      1107.3,
      1176.6,
      23
    ],
    "identity p is of course the identity the point of this is that this is now going to be the determine. I put brackets here peeing verse. X ASAP - x x i x p hi because I can factor out the P inverse. Are you with me? Can I have two with matrix multiplication? I have to be sure that I keep it on the right side, but matrix ": [
      876.4,
      919.6,
      18
    ],
    "is a shortcut your watching closely, aren't you? And then I'm going to check this one. Well in order for this guy * this to be zero, what does this guy have to be? Is there a well there it is? Right. That's the one vector that spans my no space not course if you don't like - 1/2 and actually it's going to be stylistically I can do this. ": [
      1768.3,
      1801.1,
      40
    ],
    "is equal to D. right because AP is equal to PD, right? And If I multiply by this equation. P inverse x a x p has two equal p inverse x p x d in this is course the identity. So just take this equation multiplying the left by PN verse which I know exists because p is invertible and I get this relationship that says that a is similar ": [
      2598.8,
      2643.0,
      59
    ],
    "is inside information. Okay? Keep it quiet. All right. So now I know I have the no space is the span of this. All right. And now I'm going to look at land equals 5. How's the other one? and I need to look at that a -5 I and Elsia - 120 And I have zero zero zero. and a minus 240 Okay, and look at this. The second ": [
      1858.7,
      1918.0,
      43
    ],
    "isn't it? Say yes for me. Yeah, it's okay. Thank you. Right so that's why the null-space of a -4 eyes important. It has eigenvectors in it. Okay. Of course zeros in the null space is their own eigenvector. No. That's an important point. So the null-space write a Subspace always has zero, doesn't it? Okay, but zero is not an eigenvector. So when I say eigenspace I'm tossing in ": [
      1495.3,
      1535.6,
      32
    ],
    "matrix a and then I'm going to that way I can pull it out whenever I need it. Okay, so there is my Matrix a with columns for 20050 in - 245 also going to put up a matrix be that. I'm hoping to get to a little bit later. And Matrix B is almost the same. as Matrix a in fact, I'm going to change just one entry. Daniel ": [
      54.7,
      104.3,
      1
    ],
    "mine is Joe Bob * the identity exactly the polynomial and the variable is just there to indicate. The polynomial right and then we can plug in values for the variable. All right, which we're going to do in a moment, okay? So that's all well. Let's see in fact. We can go over here. And for example. with a equal to this by the determinant I just there's that ": [
      215.2,
      266.0,
      4
    ],
    "multiplication passes through the scalar multiplication, right? GameStop in a p x x p in Versailles Peas the same as P inverse X whatever I get my x p on the left by X. Okay, matrix multiplication requires a keep track of the order. All right, and And that's the same as the determinant. Appian verse * a - x * the identity x p So I can pack your ": [
      919.6,
      961.9,
      19
    ],
    "now? The theorem says if a is similar to B, then the characteristic polynomials are the same and you're going to help me prove it. What can I say about the relationship between a and b? Similar, right? So what does that mean? I know you told me that you'd heard about similar matrices, right? So what does that mean? Well similar implies that but similar says something stronger in ": [
      649.1,
      698.8,
      13
    ],
    "null-space how many free variables 3 and I'd have three slots for free variables. How would I pick them for my three vectors? Except one of them to one in the other two two zero, right? Cancel the first one. I'd set suppose they were X2 X3 X4. It's the next to two 1/3 and 420. The next one is at 203-2140. And then the fourth one. I'd have two ": [
      2025.3,
      2060.3,
      47
    ],
    "okay. What's the rank of that Matrix? 2 2 + 1 is the number of columns which is that's the rank theorem, right? So we know we're looking for one vector to span the null space. This are the dimension of the null space is one because the Matrix has Rank 2. Okay. Now, how do I find it now? My what's my free variable? Which one? Now remember this ": [
      1675.2,
      1708.3,
      37
    ],
    "pee out on the right. right A good all right. Remember the old Star Trek with Spock you say affirmative. Okay. I know I'm getting old nobody remembers that show anymore, right? Okay. Now, let's see. I'm going to go over here. The determinant is multiplicative, right? That means I can write this is the determinant of a product of matrices which is equal to the product of the determinant. ": [
      961.9,
      1012.0,
      20
    ],
    "right at squared. That's so so we say Landa equals 5 is an eigenvalue. Okay. with algebraic multiplicity 2 Okay. and and now it's sociated with each of these. Okay. I'm working on my agenda here. So we told you what a characteristic polynomial is. We told you what algebraic multiplicity is. All right. Let's see. Now I'm told you know, it's similar matrices are. Okay, good. Alright, so. If ": [
      469.0,
      564.3,
      10
    ],
    "right? When are we expecting for the next column? 5p to fast-track it I'm going to get 00. first row I'm getting 0 Okay, alright and then five hits. I get five and then this guy, is that okay? and then the last one I'm going to see I'm getting So this one is -8. 0 -2 is a 4 * -2 is -8 right? Lindsay route + -200 - ": [
      2323.7,
      2385.3,
      54
    ],
    "row third row is already zero II Rose a multiple of the first so I can write 1020000 0-0 as my reduced Echelon form. Can't talk. And so what's the dimension of the null space? eBay -5 it v i I'm looking for two vectors. What can be the span? two guys and you're waiting to see how the shortcut works here. Where do my free variables? Okay, so it's ": [
      1918.0,
      1960.1,
      44
    ],
    "see this is the entry on changing. All right, so I'll pull out either one of those matrices when I need them. Okay. So have you heard the term characteristic polynomial? Okay. definition given a November by November Matrix a Of course. What else can we if it's November by November? What's its shape Square? Okay. the polynomial Pizza Dave Landa Is defined to be equal to the determinant? of ": [
      104.3,
      163.9,
      2
    ],
    "stuff that gets written on his iPad cuz I'm writing on the chalkboard does not always match what it was supposed to be. Yeah, cuz you can't get good Hardware right Minneapolis going downhill, right? So you write something on it. And if you won't write the write stuff for you. Okay, so I want a checking that we subtract 4 from the diagonal elements and that's a -4. Okay. ": [
      1580.1,
      1602.1,
      34
    ],
    "that's what we've done here. Okay? Lambda equals 4 I'd like the eigenspace. Okay. corresponding to four That's what I want. Now did Professor Kemp give a special symbol to this? Let me know. Okay, I'll take that as a no. All right, so I'll just call it this what is it then how do I get it? You done this, right? I'm getting older. My memory is not as ": [
      1373.5,
      1423.2,
      29
    ],
    "the Matrix of eigenvectors Arts columns linearly independent. Yeah, okay. These guys are linearly independent. The other eigenvector corresponds to a different eigenvector, so it has to be linearly independent from these doesn't it? That's so P The Matrix of eigenvectors has linearly independent calling there for its what? Invertible p is invertible. Get me right this. Pig has linearly independent columns pee is invertible ants P inverse a p ": [
      2520.9,
      2598.8,
      58
    ],
    "the characteristic polynomial for a and it only had two eigenvalues didn't it? K5 and 4/5 at multiplicity 2. So we're expecting for 3 by 3 Matrix that we could have as many as three eigenvalues, but I only have two because one of them has Multiplicity. Okay, so that's that's the issue that multiplicity brings into play. Not well. Now course an eigenvector corresponding to five and then I ": [
      1224.1,
      1266.2,
      25
    ],
    "there's a sort of a tradition here. We use Lambda cuz we like to use Lambda for eigenvalues, but I'm indicating here. You know, there's no reason why we couldn't say that the x is the variable of our polynomial. Okay soul and it is just a flag. Okay, we're going to talk about eigenvalues, but you could use any letter for the variable you want. Okay. So for the ": [
      266.0,
      292.0,
      5
    ],
    "these are eigenvectors. And so now what I'm going to do. Is I'm going to compute a p. 420 M. Glad I picked an Easy A. that's the kind of courses you like right easy is i n c - 120 010 + - 201 can I want to compute this? play p is so - 4 I'm going to I'm going to multiply each row by the First Column. ": [
      2217.4,
      2271.5,
      52
    ],
    "this but yet. Have you found eigenvalues yet? Okay. So what am I I can values for this Matrix. 5 and 4 so the eigenvalues are the values that make the characteristic polynomial 0. PA of Landa is 0 is exactly the same as saying Lambda is an eigenvalue of a okay. Art and Soul and now what you'll notice that the factor corresponding to Lambda equals 5 is quadratic ": [
      414.3,
      469.0,
      9
    ],
    "this notation for characteristic polynomial. Okay, but you've seen it. So you're ready if it shows up somewhere else. That what your what your book does is it just says that the characteristic polynomial? Is the determinant of a minus X by the characteristic polynomial for a guy of course it is but you know, you could give it a symbol that indicates that the polynomial piece of that and ": [
      1343.2,
      1373.5,
      28
    ],
    "to be a zero. I'll just put this guy here. Right so I can do that and then I can clear the 4 with this as well, right? And I have a reduced Echelon form that good. You guys are expert row reduction out, right? Okay, so so now the null-space of a -4 I Can be characterized as the span what what's the dimension of the null space? One, ": [
      1627.4,
      1675.2,
      36
    ],
    "to do something slick. X's X's a variable but it's a variable corresponding to scalars right? It's a scalar variable so I can pull it out. It's it's the multiplying whole Matrix. I'm going to P inverse X the identity x p All right is p inverse X the identity x p the same as the identity? Yeah, okay. Maybe I'll write that. I'm just simply replacing p and verse ": [
      843.0,
      876.4,
      17
    ],
    "to elucidate. So what I care about multiplicity, and the reason I care is how many linearly independent eigenvectors do I have because that's going to be a fundamental part of our diagonalize ability tharum. That's coming up. Okay. That's a great question question. All right. So let's find some eigenspaces Shelly. Now your book doesn't back. Your book doesn't actually. If I'm not mistaken, your book doesn't actually introduced ": [
      1296.5,
      1343.2,
      27
    ],
    "to what kind of Matrix what kind of Matrix is d? diagonal so when I have three linearly independent eigenvectors for 3 by 3 Matrix, then this computation shows that a is similar to diagonal matrix Ventry's are the eigenvalues listed by multiplicity on the diagonal. Okay. and the definition a November by November Matrix I want to use a different day here. November by November 8th 6 is diagonalizable ": [
      2643.0,
      2718.5,
      60
    ],
    "what was X2 + 10x remember the free variables for the homogeneous equation, right? Damn, so what I'm going to do is I'm going to set. My free Rivals like that. Play for the first one. I'm going to set x221 x320 for the second one. I'm going to set x220 next 3 to 1. What would I do if I had three free variables? 4 by 4 Matrix and ": [
      1960.1,
      1992.3,
      45
    ],
    "where to buy UC San Diego podcast ": [
      3303.9,
      3308.1,
      68
    ],
    "zero, aren't I? Just so you know, okay zero is not an eigenvector, but it is an element of the null space. So a - 4i is 0 2-0 010 - 224 + 1 Y. Is that a - 4 I? Okay, by the way, I think Todd posted a screencast of his lecture from last year. Okay, and I watched it and he has the same trouble. I do ": [
      1535.6,
      1580.1,
      33
    ]
  },
  "Full Transcript": "Do I listen to a podcast and how they relate to length and orthogon ality? Okay, that's our plan. Are there questions before we charge into this?  Yes.  When will the exam be graded we're expecting the scores to come out by Monday morning. That's our hope riderta is a promise that they can do that for us.  other question  alright, so  Let's see.  I'm going to put up a matrix a  and then I'm going to  that way I can pull it out whenever I need it.  Okay, so there is my Matrix a with columns for 20050 in - 245 also going to put up a matrix be that. I'm hoping to get to a little bit later.  And Matrix B is almost the same.  as Matrix a  in fact, I'm going to change just one entry.  Daniel see this is the entry on changing. All right, so I'll pull out either one of those matrices when I need them.  Okay. So have you heard the term characteristic polynomial?  Okay.  definition  given a November by November Matrix a  Of course. What else can we if it's November by November? What's its shape Square? Okay.  the polynomial  Pizza Dave Landa  Is defined to be equal to the determinant?  of a minus Orlando Eye  Can this is going to be a polynomial? All right, and it just to emphasize this it only depends on a all right. So imprecise that I could have p a a backs what would p.m. XB?  The determinant of what?  Perfect.  Okay, and okay, I won't write it. I was going to write down what piece of a of joe-bob.  Exactly. The determiner of a mine is Joe Bob * the identity exactly the polynomial and the variable is just there to indicate.  The polynomial right and then we can plug in values for the variable. All right, which we're going to do in a moment, okay?  So that's all well.  Let's see in fact.  We can go over here. And for example.  with a equal to this  by the determinant  I just there's that there's a sort of a tradition here. We use Lambda cuz we like to use Lambda for eigenvalues, but I'm indicating here. You know, there's no reason why we couldn't say that the x is the variable of our polynomial. Okay soul and it is just a flag. Okay, we're going to talk about eigenvalues, but you could use any letter for the variable you want. Okay. So for the Matrix a  in our example over there this is going to be  the determinant  a four- Lambda  2-0  0  5 - Lambda 0-2 4  + 5  minus Lambda. Okay, and this is the notation the book uses the vertical bars mean where Computing the determinant right?  Unfortunately, some people will write determinant in front of this because they don't want you to confuse the vertical bars with what?  Absolute value for the vertical bars. Don't mean absolute value here. This could be negative, right?  and  how would you expand this?  Yeah.  Yes, it would wouldn't it let's do this along the third row where I have two zeros and then of course the checkered backboard pattern says out of + - + - + + so that means that the coefficient 5 - Lambda comes out the other two or 0 * the two by two determinant of four- Lambda 025 - land and then you see that the professor carefully engineered this this is lower triangular, isn't it?  And so I don't have to do any work at all at this point. I'm going to have 5 - Lambda squared * 4 - Lambda.  Okay.  Now I have to see I wasn't here on Monday when Professor Kemp did this but yet. Have you found eigenvalues yet?  Okay. So what am I I can values for this Matrix.  5 and 4 so the eigenvalues are the values that make the characteristic polynomial 0.  PA of Landa is 0  is exactly the same as saying Lambda is an eigenvalue of a  okay.  Art and Soul and now what you'll notice that the factor  corresponding to Lambda equals 5 is quadratic right at squared.  That's so  so we say  Landa  equals 5 is an eigenvalue.  Okay.  with algebraic multiplicity 2  Okay.  and  and now it's sociated with each of these. Okay. I'm working on my agenda here. So we told you what a characteristic polynomial is.  We told you what algebraic multiplicity is.  All right.  Let's see.  Now I'm told you know, it's similar matrices are.  Okay, good. Alright, so.  If a is similar.  to be  of course A & B are November by November matrices.  then  the characteristic polynomial of a  It's the same as the characteristic polynomial of be.  And of course again if I change this to ax with the statement be any different.  Would you know what polynomial we were talking about if the variable is X?  Okay, then if I asked you what that polynomial was evaluated at Lander you could do that, right  Okay, just checking. Okay, the polynomial that the variable is going along for the ride the polynomial describes the relationship, doesn't it?  Are you with me now? It's Friday. Sunny outside supposed to rain though, right? Okay.  Maybe we'll be inside and be able to get that beer out of the refrigerator by the time it's raining.  Okay, are you with me now? The theorem says if a is similar to B, then the characteristic polynomials are the same and you're going to help me prove it.  What can I say about the relationship between a and b?  Similar, right? So what does that mean?  I know you told me that you'd heard about similar matrices, right?  So what does that mean?  Well similar implies that but similar says something stronger in fact.  Because if you say a peek was PVP could be anything peekaboo 0 right?  Are we going to allow P equals 0?  No, sew what similarity.  Okay invertible Matrix P for which?  Okay, so I'm going to write it this way.  For some reason I write this you could probably let me insist that that that means of course that pee has to be invertible right for some but I'll write invertible p  okay, and now the claim is that the characteristic polynomial of a is the same as the characteristic polynomial be since I have extra going to use x  Guy and then of course Professor Kemp gets the clean up the mess I make on Monday, right?  Are you ready?  Okay. So what's the characteristic polynomial of a x?  What is it?  determine today minus x i  God's got to help me out if we're going to get through our agenda here.  Okay now.  the point here is that  now I'm going to compute.  the characteristic polynomial a v  All right, which is the determinant?  a B minus x i  You know, what I'm going to do is I'm going to observe that.  B is p inverse  AP  And I'm going to subtract X.  And I'm going to do something slick.  X's X's a variable but it's a variable corresponding to scalars right? It's a scalar variable so I can pull it out. It's it's the multiplying whole Matrix. I'm going to P inverse X the identity x p  All right is p inverse X the identity x p the same as the identity?  Yeah, okay. Maybe I'll write that.  I'm just simply replacing p and verse identity p is of course the identity the point of this is that this is now going to be the determine.  I put brackets here peeing verse.  X ASAP  - x x i x p  hi because I can factor out the P inverse.  Are you with me?  Can I have two with matrix multiplication? I have to be sure that I keep it on the right side, but matrix multiplication passes through the scalar multiplication, right?  GameStop in a p x x p in Versailles Peas the same as P inverse X whatever I get my x p on the left by X. Okay, matrix multiplication requires a keep track of the order. All right, and  And that's the same as the determinant.  Appian verse  * a - x * the identity x p  So I can pack your pee out on the right.  right  A good all right.  Remember the old Star Trek with Spock you say affirmative.  Okay. I know I'm getting old nobody remembers that show anymore, right? Okay.  Now, let's see. I'm going to go over here.  The determinant is multiplicative, right?  That means I can write this is the determinant of a product of matrices which is equal to the product of the determinant.  Okay, so determinant.  a p inverse X the determinant  of a minus x i  Tennessee determinant  Of course, these are numbers. So now I can multiply in any order I want right.  That's the beauty of the determinant this guy, of course.  This is the determinant.  The P inverse X the determinant of p and what's this guy?  What am I trying to prove?  I'm skidding knee-deep in a computation. What was I trying to do when I started? Yeah.  That's the characteristic polynomial.  Okay, Anna. And of course now, this is the  this is the determinant.  P inverse x p  write again, because the  What's the P inverse x p?  Okay, what's the internment of the identity one? So this is one?  Okay, I started where do I start started here?  And I went to hear here here.  Here here and got here.  They're the same.  Does similar matrices have the same characteristic polynomial?  All right.  That's agenda item number three.  All right. Now what I'm going to do is I'm going to find the eigenspaces of a all right?  Okay any questions on this?  Before we tackle the eigenspaces.  That's a great question. What's the relevant of algebraic Multiplicity?  Okay, so  we have a theorem that says if I have distinct if I have eigenvectors corresponding to distinct eigenvalues what I know about him.  Seen that right.  What can you say about eigenvectors corresponding to distinct eigenvalues?  Their what?  Linearly independent, that should be on a flash card.  right  Okay, I can vectors corresponding to distinct eigenvalues are linearly independent. Okay, but I don't have distinct eigenvalues for a matrix that has the where's my Wii we computed the characteristic polynomial for a and it only had two eigenvalues didn't it?  K5 and 4/5 at multiplicity 2. So we're expecting for 3 by 3 Matrix that we could have as many as three eigenvalues, but I only have two because one of them has Multiplicity.  Okay, so that's that's the issue that multiplicity brings into play.  Not well.  Now course an eigenvector corresponding to five and then I can Vector corresponding to four have to be linearly independent, right that has to be but if I have an eigenvector of algebraic multiplicity to maybe I have two different eigenvectors that are linearly independent.  Or maybe I don't.  That's going to be the issue. So let this evolve alone. That's a great question. Keep it in mind. Okay.  And because there's another multiplicity called geometric multiplicity that we have to elucidate.  So what I care about multiplicity, and the reason I care is how many linearly independent eigenvectors do I have because that's going to be a fundamental part of our diagonalize ability tharum. That's coming up. Okay. That's a great question question.  All right.  So let's find some eigenspaces Shelly.  Now your book doesn't back. Your book doesn't actually.  If I'm not mistaken, your book doesn't actually introduced this notation for characteristic polynomial.  Okay, but you've seen it. So you're ready if it shows up somewhere else.  That what your what your book does is it just says that the characteristic polynomial?  Is the determinant of a minus X by the characteristic polynomial for a guy of course it is but you know, you could give it a symbol that indicates that the polynomial piece of that and that's what we've done here. Okay?  Lambda equals 4  I'd like the eigenspace.  Okay.  corresponding to four  That's what I want. Now did Professor Kemp give a special symbol to this?  Let me know.  Okay, I'll take that as a no. All right, so I'll just call it this what is it then how do I get it?  You done this, right?  I'm getting older. My memory is not as good as it was. I was hoping you'd help me out.  I know I'm supposed to do this cuz Todd left me some notes supposed to talk about eigenspaces.  But he assured me that you guys have seen him.  So what do I do? What's the eigenspace of a corresponding to 4 wishes this guy here, right?  So I want to know space.  Of what?  A - 4i. Okay. So I want the null-space of a -4 I slept probably means I want to compute a minus for isos do that.  Can I have the let me just  This means I'm looking for a solution to a -4 i x is equal to 0 right.  Okay, if axes in the null-space of a -4 eyed, and that's true, but that's the same as a x x is equal to 4X, isn't it?  Say yes for me. Yeah, it's okay. Thank you. Right so that's why the null-space of a -4 eyes important. It has eigenvectors in it.  Okay.  Of course zeros in the null space is their own eigenvector.  No.  That's an important point. So the null-space write a Subspace always has zero, doesn't it?  Okay, but zero is not an eigenvector. So when I say eigenspace I'm tossing in zero, aren't I?  Just so you know, okay zero is not an eigenvector, but it is an element of the null space. So a - 4i is 0  2-0  010 - 224 + 1 Y. Is that a - 4 I?  Okay, by the way, I think Todd posted a screencast of his lecture from last year.  Okay, and I watched it and he has the same trouble. I do stuff that gets written on his iPad cuz I'm writing on the chalkboard does not always match what it was supposed to be.  Yeah, cuz you can't get good Hardware right Minneapolis going downhill, right? So you write something on it. And if you won't write the write stuff for you.  Okay, so I want a checking that we subtract 4 from the diagonal elements and that's a -4. Okay. Now what do I want to do with it?  I want to find the null space of a -4 I how do I do that?  Okay, I want to row reduce it to Epsilon form. I think this is going to now and I packed the Matrix to which this would be simple. I can clearly clear the third entries, right?  So I'm going to this this guy is going to be a zero.  I'll just put this guy here.  Right so I can do that and then I can clear the 4 with this as well, right?  And I have a reduced Echelon form that good.  You guys are expert row reduction out, right? Okay, so  so now  the null-space of a -4 I  Can be characterized as the span what what's the dimension of the null space?  One, okay. What's the rank of that Matrix?  2 2 + 1 is the number of columns which is that's the rank theorem, right? So we know we're looking for one vector to span the null space.  This are the dimension of the null space is one because the Matrix has Rank 2.  Okay.  Now, how do I find it now? My what's my free variable?  Which one?  Now remember this a free variable. I mean the free variable for the homogeneous equation, right?  Okay. So if I write a -4 i x x equals 0 x has three components X1 X2 X3, which one's the free one X2, okay.  Now what so what I'm going to do is I'm going to put a one.  In for my free variable, right? Cuz once I set a value for that the others are determined, aren't they?  Okay, and if I N N I want this to be in the null space. So if I multiply this guy times this it has to be zero since this guy this entries. I just need to set this one and if I set it to be minus 1/2  Then this guy comes that guy will be zero no matter what that is, right.  Hey, this is a shortcut your watching closely, aren't you?  And then I'm going to check this one. Well in order for this guy * this to be zero, what does this guy have to be?  Is there a well there it is?  Right. That's the one vector that spans my no space not course if you don't like - 1/2  and actually  it's going to be stylistically I can do this.  What are the span is the span of this the same as span of that?  That cuz the span of one vector is just all multiples, right?  Is everybody with me? How did I do that?  What's the first thing I did?  Set the free variable component to what?  One and then I just looked at what the others had to be to make the matrix product with the reduced Echelon form 0 right?  Thank you, see how I did that then you'll see how to do that on your own and it'll be faster than writing out the equation won't it?  Okay.  Don't tell Todd I let you in on that secret.  You don't have to use this right? Cuz you know, he'll if you write out the equation of course gives it to you right guy, but this is inside information. Okay? Keep it quiet.  All right. So now I know I have the no space is the span of this.  All right.  And now I'm going to look at land equals 5.  How's the other one?  and I need to look at that a -5 I  and Elsia - 120  And I have zero zero zero.  and a minus 240  Okay, and look at this. The second row third row is already zero II Rose a multiple of the first so I can write 1020000 0-0 as my reduced Echelon form. Can't talk.  And so what's the dimension of the null space?  eBay -5 it v i  I'm looking for two vectors. What can be the span?  two guys  and you're waiting to see how the shortcut works here. Where do my free variables?  Okay, so it's what was X2 + 10x remember the free variables for the homogeneous equation, right?  Damn, so what I'm going to do is I'm going to set.  My free Rivals like that.  Play for the first one. I'm going to set x221 x320 for the second one. I'm going to set x220 next 3 to 1.  What would I do if I had three free variables?  4 by 4 Matrix and I had to do this.  Scene where this is going. Okay. So so what would I buy if I if I had SE X1 X2 X3. We're all free.  Maybe I should be okay. That can't be right. All right. One of them has to be a pivotal girl or the whole things are okay for by 4 Matrix.  Can you picture that?  Okay, and then I had a three-dimensional null-space how many free variables 3 and I'd have three slots for free variables. How would I pick them for my three vectors?  Except one of them to one in the other two two zero, right?  Cancel the first one. I'd set suppose they were X2 X3 X4. It's the next to two 1/3 and 420. The next one is at 203-2140. And then the fourth one. I'd have two and three zero in the fourth one one.  Now the beauty of this is Will these be linearly independent.  Yes, that's why the shortcut is so cool.  By choosing your free variables with these values. They have to be linearly independent. Don't they?  Because in order for a linear combination of the B 0 you have to have the coefficient b04 each one of those, don't you?  see that  if I had let me just had say C1 C2 add a name to be zero wealthy one has to be zero cuz the entry corresponding this has to be all right.  Mmc2 has to be zero because the entry corresponding this has to be there up. So these will be linearly independent by the way, you chose them.  Okay, just of course, they'd be linearly independent the road out the equation and did that cuz it's equivalent to doing this really, okay.  What should I pick for this one?  How about this one?  -2 act you guys are good. Pretty easy, right?  That's so you can pick them out pretty quickly. Once you have the reduced Echelon form right now what I'm going to do.  Is I'm going to go over here.  Doctor now that right now they have these I'm going to call P.  I still have that. Okay.  minus 120  010 + - 201 how did I make pee?  Pulled out of a hat, right?  Yeah.  But I wrote the vectors that span the null space of a minor course. This is an eigenvector corresponding to for isn't it?  Write the species are both eigenvectors corresponding to 5 on Thursday.  That's how I did I hear these are these are eigenvectors. And so now what I'm going to do.  Is I'm going to compute a p.  420 M. Glad I picked an Easy A.  that's the kind of courses you like right easy is  i n c - 120  010  + - 201  can I want to compute this?  play p  is so - 4  I'm going to I'm going to multiply each row by the First Column. So the second one's - 2 + 10, which is 8.  And then I'm going to get zero.  And I'm going to Waverly this is P1.  This is P2.  MP3  hey, what's this?  Come on.  How does it compare to P1?  Or P1 respecting it. Of course you were what's a x p 1?  4p one because P1 is an eigenvector corresponding to for isn't it?  Okay Ru with me right? When are we expecting for the next column?  5p to fast-track it  I'm going to get 00.  first row I'm getting 0  Okay, alright and then five hits. I get five and then this guy, is that okay?  and then the last one I'm going to see I'm getting  So this one is -8.  0  -2 is a  4 * -2 is -8 right?  Lindsay route  + -200 - 10  okay, and then -4 0  1/4 + 0  And that's 5 at 5 p 3.  It isn't it once I can do the arithmetic right now. I'm going to do I'm going to call.  I'm going to go over here. I'm going to write d.  Anaheim Airport  how to make D this way. So how did I make day?  it does happen to be the diagonal of a  what else is it?  Yes, the diagonal matrix with the eigenvalues on it. And of course five is listed twice because it has algebraic multiplicity what to okay. Now what I'm going to do is I'm going to compute have two pieces.  All right.  I'm going to compute p x d.  Damn, right it out. Where's my P - 120?  010  minus 201  Deer were here, which is for.  5  and fight on the diagonal  okay, and if I multiply  Noticed that when I multiply each row by the First Column I get exactly this column don't I?  And If I multiply each row by the second column I get exactly this one don't I?  And finally the third call him it's the same thing. I'll get that these are equal.  and that means now  one other thing  p  Which is the Matrix of eigenvectors Arts columns linearly independent.  Yeah, okay. These guys are linearly independent.  The other eigenvector corresponds to a different eigenvector, so it has to be linearly independent from these doesn't it? That's so P The Matrix of eigenvectors has linearly independent calling there for its what?  Invertible p is invertible.  Get me right this.  Pig  has linearly independent columns  pee is invertible  ants  P inverse a p is equal to D.  right  because  AP is equal to PD, right?  And If I multiply by this equation.  P inverse  x a x p  has two equal p inverse x p x d in this is course the identity.  So just take this equation multiplying the left by PN verse which I know exists because p is invertible and I get this relationship that says that a is similar to what kind of Matrix  what kind of Matrix is d?  diagonal so  when I have three linearly independent eigenvectors for 3 by 3 Matrix, then this computation shows that a is similar to diagonal matrix Ventry's are the eigenvalues listed by multiplicity on the diagonal.  Okay.  and  the definition  a November by November Matrix  I want to use a different day here.  November by November 8th 6  is diagonalizable  F  p inverse a p  Is equal to d a diagonal matrix?  for some convertible p  if I can do this  and said to be diagonal  in the theorem  Hey.  remember my button November Matrix a  is diagonalizable  if  and all my app  a has  and linearly independent eigenvectors  eigenvectors  and we saw this in the computation. We put the eigenvectors into a matrix. They're linearly independent. It's invertible and Wallah.  We can write p and verse AP is diagonal this equations always through this a people's Pee Dee does not require The Columns of P to be linearly independent right could be zero and a p equals PD, right?  Okay, but in order to have pee and Bruce a p equal deep he has to be invertible. And so this is the content of this theorem. If you have a full set of eigenvectors, then you have an invertible Matrix P for which you can do this.  All right.  All right.  I want to I have 2 minutes.  I don't have time to do the complete computation. I'm going to look at be.  Look at I'm going to look at B is going to have the same eigenvalues work that out. I just changed this entry. It's off the diagonal and you can check that the characteristic polynomial be is the same as a however. There's a difference to look at Lambda equals 5.  and a right at PB of x  is X is a is 5 - x squared from 4 - x the same characteristic polynomial and if I look at a  B  kind of keep my ears and be straightener B- v i  It's going to be minus 120000 - 250.  Out and so when we can so I have a 0 row hair.  2 times the first row added to the seconds going to give me 0 0 1 and then I can use this one to clear that and I get this.  What's the dimension of the null space?  a 3-5 I  its 1  This is called geometric Multiplicity.  And this is the answer I was promising.  Geometric multiplicity, which is the dimension of the eigenspace namely the null-space of B- the eigenvalue times. I can be less than the algebraic Multiplicity.  This is an example.  the algebraic multiplicity is to  The geometric multiplicity which is the dimension of the null space of B- five eyes only.  And B is not diagonalizable.  Why are theorem?  All right. That's all I have time for Professor. Camp will clear up any questions I left.  Have a great weekend and he'll catch up with you then.  If you get around me children's a story I will tell.  Pretty Boy Floyd in Oklahoma  It was them.  It was Saturday afternoon.  His wife is not him in his wagon is in the town they roll.  Thera definition of approached him  In a manner rather lose using vulgar words language.  where to buy  UC San Diego podcast ",
  "Name": "math18_b00_wi18-03022018-1000",
  "File Name": "lecture_22.flac"
}