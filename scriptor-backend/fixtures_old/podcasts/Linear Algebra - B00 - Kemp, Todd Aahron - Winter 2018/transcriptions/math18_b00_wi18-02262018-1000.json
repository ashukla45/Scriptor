{
  "Blurbs": {
    "-6 + 6 which is 0 + 2 * -3 + 0 + 8 is -6 + 8 is 2 And that is indeed twice that original Vector - 301 as we knew it had to be so that is an eigenvector with eigenvalue. 2 so is the first one 1/2 is it 1-0, but actually the eigenspace is a Subspace. So we're free to scale the vectors in and ": [
      600.8,
      633.9,
      21
    ],
    "2 by 2 Matrix. This is a super nice Matrix is the identity Matrix. What are the eigenvalues of this Matrix again in the two by two cases going to be pretty easy to do just like we did as above Let's figure out what are the values of land. For which B- land of times the identity. Has a non-trivial null space and we can do that. by calculating ": [
      947.4,
      971.4,
      31
    ],
    "I can get to one from the other from one to the other by conjugating by P by x p on one side of the Universe on the other that thing changes the Matrix, but it doesn't change the eigenvalues eigenvalue. Stay the same and more over the eigenspaces for those two for it for the eigenvalues on one side the other day I can spaces can change but what ": [
      2705.0,
      2726.1,
      90
    ],
    "I have two vectors, it's easy to recognize when their linearly dependent that happens. If and only if they are. They are very quiet. parallel So let's assume for contradiction that there parallel. Let's suppose that W is some scalar multiple c x v and then there's one in key observations. I want to make here could that's eb-0. Know why not? Cuz that would make W 0 and we ": [
      1373.8,
      1409.1,
      45
    ],
    "Is nothing other than the null-space of a - 2 * the identity Matrix, so that's all this is just telling you to do that find the null space or find a basis for the null space of a - 2 X 2 Matrix. So first let's figure out what is a -2 times the identity Matrix. So that is 4 - 1 6-2 1-6 2-1 8 I have to ": [
      302.5,
      326.2,
      11
    ],
    "Lambda? That thing the null-space of a - Lambda is called the eigenspace. With eigenvalue land. We only really use that terminology when it when Lambda is an eigenvalue. That is the eigenvectors of the nonzero vectors in an eigenspace. So we only call it an eigenspace if it has some nonzero vectors in it. There were looking for special numbers land up for which that thing the null-space of ": [
      195.0,
      221.2,
      7
    ],
    "Listening to a podcast find eigenvalues and then next time of course, you have a midterm on Wednesday evening. So the next lecture after this one on Wednesday will be a review lecture. So that midterm one more reminder is from 8 to 10 p.m. On Wednesday evening in three different rooms to Ann Peterson one and Galbraith your room until the Simon are posted on Triton and hopefully you ": [
      2.0,
      28.0,
      0
    ],
    "Mew and Lambda what I get is this funny equation tracing through the whole thing. What I got is that c x movie is equal to c x Lambda B. And let me subtract that. that says c x Miu - Lambda x v is 0 so here's what I've got. I've got some vector v. I know that it's not zero. I've got a constancy. I know that it's ": [
      1522.9,
      1559.2,
      49
    ],
    "So a x V1 is equal to Lambda one-v-one Etc. So I can write this as Lambda 1 V1. Lambda 2 V 2 app to Lambda and V on this is just a different way of rewriting what we saw on the last slide. I know if I've got a matrix and what I do to it is I scale each of its columns by some number that can be ": [
      2163.0,
      2192.7,
      69
    ],
    "So no two of them are linearly dependent on each other and actually we could row reduce it and it's not going to be hard to check that it is actually an invertible Matrix, but when we subtract twice the identity from it, it becomes very not invertible it becomes well. Let's see here. What's the rank of this Matrix? It's a good practice for the mid-term. What does rank ": [
      368.3,
      390.9,
      13
    ],
    "These are eigenvectors for a so that means that everyone is equal to Lambda One X V one and so on. So this last line here this just as the same thing as X1 Lambda One V one plus X to Lambda to be to sxm, Landon What does that say that says? Hey, I got this arbitrary Vector W. I know I can expand it somehow in terms of ": [
      1841.0,
      1871.8,
      59
    ],
    "a - Lambda I has nonzero vectors in it. And what we saw last day is that well, if you're given a loud and told it's an eigenvalue it is then routine for us now that we know reduction so well to figure out what the eigenvectors are. What is the eigenspace that is I just give you a Lambda on you just report to me find a basis for ": [
      221.2,
      243.9,
      8
    ],
    "a minus line two eyes. No space that's something you know how to do really? Well hopefully for the midterm for any given Matrix, but it's typically hard to find those Landis for which the null space is not trivial. And so we're going to start by looking at some more examples of finding the eigenspace will produce basis for the eigenspace once we know and I can value But ": [
      243.9,
      266.7,
      9
    ],
    "a one-to-one and onto linear transformation. It preserves all linear properties of vectors and so in particular if I had a basis of three eigenvectors for this eigenspace before and all I'm doing is I'm transforming them by linear transformation given by P. Then those three factors transformed to a basis of the new eigenspace. So that means that well the vectors in the eigenspace change. The number of the ": [
      2870.7,
      2896.1,
      95
    ],
    "a very important their distinct eigenvalues. Give us a linearly independent know why should I care about that? Why should I care about having linearly independent eigenvectors? Well, here's why So if I have soda says is a corollary to that they're on is if I have all of distinct eigenvalues supposed to have a square Matrix. Seven by seven and all seven and I and I have seven distinct ": [
      1613.8,
      1641.4,
      52
    ],
    "about what kind of properties eigenvectors can have so we saw in the last two examples. Does 2 2 by 2 matrices? We saw both of them had only one single eigenvalue. Typically, that's not going to happen. Typically if you have a 2 by 2 Matrix, you'll find two different eigenvalues when I say typically I mean most of the time sometimes sometimes there might be a repeated eigenvalue ": [
      1143.1,
      1174.4,
      38
    ],
    "already know them. I posted to practice midterms last week. Hopefully you've been working through them and doing all their other exercises several people have already posted their solutions to the midterms on Piazza with other people responding about what they thought was right in what they thought was wrong. You should join that discussion that's going to be a great way to study and one more administrivia reminder that ": [
      28.0,
      52.7,
      1
    ],
    "and b are similar if there is some invertible Matrix p For which a sequel to PBP inverse? So there is an invertible Matrix here to which will call Q and that's P inverse. So this is that V is equal to q a and then P inverse. So what is p e p is Q in verse in the inverse of the inverse is the original Matrix. And so ": [
      2547.3,
      2576.9,
      84
    ],
    "are not linearly are linearly independent first things first. We can know that they are not zero. How is it we know that they're not zero. Could one of them be zero that I have an eigenvector that zero. No by definition eigenvectors are always nonzero Vector. So if I'm telling you that's an eigenvector, then it must be nonzero. And I know there is an eigenvector. I'm telling you ": [
      1310.6,
      1339.9,
      43
    ],
    "as an eigenvalue. That's their only I can value but we can very different behavior for the eigenvectors the eigenspace of eigenvalue one for the Matrix a is one-dimensional. Just the first entered basis Vector expensive for the second Matrix here. The eigenspace is two-dimensional span. For example by that Vector plus another one. Okay. No, I've already by the way. I've snuck in here what we're going to do ": [
      1072.7,
      1100.1,
      35
    ],
    "at the end of the lecture, which is how do you find eigenvalues in this case? We we found this tricky way to find eigenvalues which was to say. Hey, I'm looking for Lambda for which a mine is Lambda. I has a non-trivial no space. That's what the eigenvectors are. That's the same thing as a minus Lambda. I having determinant 0 and for two by two cases real ": [
      1100.1,
      1118.7,
      36
    ],
    "away from this course in anything you're going to do in the future that uses linear algebra. So listen up, So here is a summary of everything that we said last day about eigenvalues and eigenvectors are things that are associated to a square matrices only makes sense for square matrices. So if you have a square n by n Matrix an eigenvector Is a nonzero vector v? With the ": [
      78.7,
      109.9,
      3
    ],
    "basis B. Are the unique numbers x 1/2 x n such that W is expanded in terms of those numbers in terms of the basis. So that is to say this is the same thing as writing W expand x 1 x v 1 + X2 X V2 + x + x via. That's true. Whenever we have a basis what's great here is that those vectors form of those ": [
      1769.9,
      1799.0,
      57
    ],
    "be Queso with a is pbpn verse that tells you that. B is equal to P inverse AP which looks like almost the same thing, which is what I wanted if a is similar to be then be should be similar to a except now. The inverse is on the left instead of on the right. So that's not quite the same statement. Right? Well, but the statement is a ": [
      2523.1,
      2547.3,
      83
    ],
    "be should be similar to a that might not be apparently true from the way it's written here, but it is true because if a is similar to be then we can rewrite this like we did on the last slide as a x p is equal to p x b. And now I can X p.m. Verse on the left and get that P inverse. AP is equal to ": [
      2497.5,
      2523.1,
      82
    ],
    "by the inverse. Let's x p inverse on the right when I get the a is equal to p d e p inverse already in your Matlab homework. This is super important. We're going to talk about this more next time. So on Friday and next week as well, but if you have a basis of eigenvectors what that says is that your Matrix can be written in this form ": [
      2249.5,
      2277.7,
      72
    ],
    "calculations to show that there is no such pee but and I actually encourage you to do that. That would be a fun example for you guys to work out but actually will see on the next slide a good reason a good understanding of why they are not similar. So here's why. If I have two similar matrices, and I'm not saying what time is diagonal or not? Like ": [
      2652.5,
      2677.3,
      88
    ],
    "call them. You and me. So you as an eigenvector with eigenvalue Lambda for a and v is an eigenvector with eigenvalue miu4, but actually let me let me change my rotation call this one instead of you. Let me call it w cuz I think that's what I the notation I use on the next slide. Okay. So what we'd like to see is that these two eigenvectors wnv ": [
      1268.8,
      1310.6,
      42
    ],
    "compute that for this example here A- Lambda I Is the Matrix one- lamb. 101 - Lambda? and the determinant of that is 1 - Lambda squared - 0 so the question is for which Lambda? is 1 - Lambda squared equal to zero Well, that's easy to solve in this case. Only Lambda equals 0 equals 1. So that's the only eigenvalue of this Matrix a so now let's ": [
      755.6,
      802.2,
      26
    ],
    "coordinates is scale each of the coordinates by those eigenvalues and I can write that is matrix multiplication. If I just scale each of the coordinates by a number that's the same thing as multiplying that coordinate vector by a diagonal matrix cab schematically ground Zeroes their meeting all the non Dying Light Reserve 0 and you put those eigenvalues on the diagonal. That's what's great about having a basis ": [
      1984.9,
      2010.5,
      64
    ],
    "d Now, let me put another Matrix in here. Let's take those vectors the eigenvectors. String them together in a matrix. Let's call that Matrix. deadliest form of basis This is a square Matrix here. And this Matrix p is an invertible Matrix because it's columns form a basis for RN. So there's something that happens here that we can just drive without reference to the coordinate vectors what happens ": [
      2073.5,
      2128.2,
      67
    ],
    "diagonal matrix on the left? So what this says here is hey, what is this this V1 V2? That's the thing that we called p and this diagonal matrix that was with called D. So I get this equation. I get that a x p is equal to p x d. And then one more thing we see from that is this Matrix P was invertible. So I can multiply ": [
      2221.2,
      2249.5,
      71
    ],
    "distinct eigenvalues. I have two distinct eigenvalues and eigenvectors for those two distinct eigenvalues. Those eigenvectors are automatically linearly independent. I said something algebraic about the eigenvalues whether they're equal or not tells you something about the relationship between the eigenvectors. They must be linearly independent. This is actually a superpower. So I want to show you why this is true. Your textbook has a full proof of this. I'm ": [
      1199.1,
      1227.8,
      40
    ],
    "doesn't change is there dimensions? So how does this all work? Why is this true? well, so let Lambda be an eigenvalue. for the first Matrix a and so it's eigenspace. Is the null space of a minor slammed attempts identity Matrix? No, I want to see what happens when I look at V instead. So let's take some nonzero Vector in there. So let's take some eigenvector V in ": [
      2726.1,
      2768.3,
      91
    ],
    "doing the same thing. We've been doing for a while. Now. Let's look at a couple more examples. So here's a matrix a pick 2 by 2 Matrix. No, I haven't told you what its eigenvalues are. Okay, but actually it's not hard to figure out what it's like and values are. Because this is a triangular Matrix. So it's easy to compute the determinant of a triangular Matrix as ": [
      664.0,
      695.9,
      23
    ],
    "eigenvalues for it. Then that means there are 7 linearly independent eigenvectors. But if I have 7 linearly independent vectors in RN, they form a a basis and there's an important thing for you to remember for the mid-term. If you want to check if a set of vectors in RN forms the basis for RN it's enough to know that there are enough of them and they're linearly independent ": [
      1641.4,
      1668.7,
      53
    ],
    "eigenvalues. See you Wednesday for mid-term review. UC San Diego podcast ": [
      2973.4,
      2979.8,
      99
    ],
    "equal to c x a v so what does that mean? Will I've got an equation for A V A V is equal to Mew X be so this is equal to c x Miu. that's a Lambda x w, but ww.w is equal to C x v so this is equal to Lambda times CB. So just using the fact that BMW are eigenvectors with these two different eigenvalues, ": [
      1480.2,
      1522.9,
      48
    ],
    "equivalent matrices may or may not be similar to each other similar matrices may or may not be Roku belong to each other totally different notion here is a good example the matrices A and B that we started with that was our second example today. These matrices are clearly row equivalent to each other to get from The Matrix to The Matrix be just do one step of row ": [
      2607.0,
      2630.2,
      86
    ],
    "explore that a little more. This is what we just saw. that what the Matrix what the Matrix a what may by a does to Any Given vector? If you expand it in terms of the basis of eigenvectors is to multiply that coordinate vector by this diagonal matrix with lamb, doesn't it? Let's give that Matrix there a name. so it multiplies a coordinate vector by this diagonal matrix ": [
      2039.1,
      2073.5,
      66
    ],
    "figure out its eigenspace. a - 1 * I That's the Matrix. 0100 which is conveniently already in reduced row Echelon form for us. I so I want to find a basis for that eigenspace. It says that the null space of a -1 * I is equal to the set of vectors X1 X2 such that. Are we see that X2 is a X1 is a free variable hear ": [
      802.2,
      846.3,
      27
    ],
    "found only eigenvalue that big was one and we saw that had a one-dimensional eigenspace. Now let's look down here in this example. If I take B minus 1 times the identity Matrix. Well, that's just the identity Matrix minus the identity Matrix. What's the null-space of the zero Matrix? What vectors does the zero Matrix kill? Everyone know space is a set of vectors that gets sent to zero ": [
      1001.1,
      1039.7,
      33
    ],
    "from the standard basis to the eigenbasis. This is saying exactly the same thing as is written above that. The action of a on coordinate vectors in the v bases is just a x the diagonal matrix d but as you saw when you're doing your Matlab homework this relationship here allows you to do things like take powers of the Matrix a really easily cuz if I want to ": [
      2301.0,
      2324.9,
      74
    ],
    "go ahead and compute that. I'm sorry. I know space is not zero. So i e we want a minus Lambda. I not invertible. Do you want a minus Lambda? I not invertible. The one way to check that and this will be an easy check for 2 by 2 Matrix. is to find land. For which the determinant of a minus Lambda I is 0 okay, so let's just ": [
      724.8,
      755.6,
      25
    ],
    "going to do the special case where we have to I can vectors which demonstrates the entire point of the proof without getting into notation problems and making it hard to understand. So we'll just look at the case. of two distinct eigenvalues Okay, so let's say that Lambda and Mew. R2 eigenvalues of a matrix a and they're not equal to distinct eigenvalues. And supposed to have eigenvectors. Let's ": [
      1227.8,
      1268.8,
      41
    ],
    "have this abstract basis of vectors and vectors be around then I say hey look and I got this Vector. I'm just thinking of w is a column Vector its coefficients in the coordinate basis X-14 xn. And I want to know multiplication by a on the victim to Victor W. What is that really do to W. What is it due to its coordinates? All it does to its ": [
      1963.8,
      1984.9,
      63
    ],
    "here if I have two messages that are similar by a matrix p If I have an eigenvector V with some eigenvalue Lambda for the 1st. Then that I can value Lambda is also an eigenvalue for the Matrix B and its eigenvector isn't the original eigenvector V. It's p x fee. Poppy is an invertible Matrix. It is in the language. We use last time an isomorphism. Hey, it's ": [
      2844.0,
      2870.7,
      94
    ],
    "here. Can we look back at the calculation? We did near the beginning of the lecture. We saw that both of these are both these matrices had the same eigenvalues. The only had one is an eigenvalue. But the first one a the eigenspace for that I can value had to mention one in the second example the eigenspace for that when I can buy head Dimension to those are ": [
      2921.2,
      2946.0,
      97
    ],
    "if I take the Matrix a and X The Matrix p A is the original Matrix whose eigenvectors are the v's and eigenvalues are the lambdas? 8 * P. Let's remember the definition of matrix multiplication. I'm multiplying a x The Matrix whose columns are the v's. That's the same thing as just multiplying the matrix by each of those columns. But the eigenvectors the vectors V1 Savion are eigenvectors. ": [
      2128.2,
      2163.0,
      68
    ],
    "inverse b p So putting that in here this says that P inverse VP V is equal to Lambda V. now I'm going to multiply both sides by P on the left that says that BP V is equal to Lambda p b but hey, what is that say that says that this Vector here. PV is an eigenvector Abby with eigenvalue weather and that basically is the whole story ": [
      2796.0,
      2844.0,
      93
    ],
    "is Pivot. And there is the reduced row Echelon form of this Matrix. So that tells me that the null space. of that Matrix, which is a -2 Tennessee identity can be written as the set of vectors X1 X2 X3 such that well the free variables are these two they don't have pivotal ones in those columns. And so the only thing we're going to get is a constraint ": [
      493.7,
      525.8,
      18
    ],
    "is in the null-space of that Matrix a my slime to I so the proper statement here would be the set of eigenvectors of a forgiving eigenvalue. Lambda is equal to the no space of a minus Lambda. Except zero to take out the zero actor. But the no space that thing the null space is a Subspace. Okay, and we call it the eigenspace so for each real number, ": [
      166.7,
      195.0,
      6
    ],
    "know that the vectors are not 0 so we know that this scalar multiple C is not 0 so I got this wnv. I'm assuming for seven times the other or a half times the other some nonzero scalar multiple. How does that help us hear? Well start using the fact that there are eigenvectors. So if I take a x v V is an eigenvector with eigenvalue Mew so ": [
      1409.1,
      1439.6,
      46
    ],
    "like in those two examples where you have only one eigenvalue, okay for a larger Matrix, but most of the time you're going to find if you have a 2 by 2 Matrix, they'll be two distinct eigenvalues of a three-by-three most of the time they'll be three distinct eigenvalues. And what's great about that is the following theorem. If you have a bunch of eigenvectors of a matrix. Four ": [
      1174.4,
      1199.1,
      39
    ],
    "mean here is slightly false is equal to the null-space of a minus Lambda. I that was the key observations made last day, but there's one small thing wrong with that statement. What is it? There is one vector that is in the no space of a minus Lambda eye and is not an eigenvector. What Vector is that? The zero Vector zero Vector is in any Subspace in particular ": [
      139.8,
      166.7,
      5
    ],
    "mean? The number of pivotal columns or the number of pivotal Rose or the dimension of the column space or the dimension of the row space does all mean the same thing. So what's the number of pivotal Rose here just one cuz all three are scaled multiples of each other all equal. So this is to this one. It's Melody is 2 personality plus the rank is equal to ": [
      390.9,
      418.2,
      14
    ],
    "means that you can fully understand the action of the Matrix and it's as simple as it gets so let me explain why so why is that so nice? This is super nice. Why is that? Well, let's think about it this way. So suppose I take that basis. I'm going to basis. Let's call it B1 B2 B3. VN a basis of eigenvectors That's a basis for RN in ": [
      1695.3,
      1729.5,
      55
    ],
    "most of the time. You will have a basis of eigenvectors. And therefore your Matrix will be similar to a diagonal matrix, which means the computations of things like powers that Matrix will be really easy to do. Okay, if you follow that that path. I will explore this notion of similarity a lot next time. Okay, but I wanted to introduce it to you right now. Let me know ": [
      2451.6,
      2473.6,
      80
    ],
    "my basis. Pay those two vectors form a basis for the eigenspace of the eigenvector to those are both eigenvalue eigenvector s48. Let's just double-check that sanity check over here. If I take the Matrix a i x let's II Vector. Let's say That's what I get. I get - 3 * 4 is 12 + 0 + 6 - 6 2 * - 3 + 0 + 6 is ": [
      557.1,
      600.8,
      20
    ],
    "not isomorphic to have different dimensions and therefore it's impossible to find a matrix P for which a is equal to PVP in reverse. So that's what similarity tells you tells you that the eigenvalues are the same and that the eigenspaces are isomorphic. They have the same dimensions. Okay. So we'll stop there for today and on Friday will continue with characteristic polynomials in general and how to find ": [
      2946.0,
      2973.4,
      98
    ],
    "not zero and I've got this number Mew - lamb to hear from you and Linda are the eigenvalues which I assumed were distinct, which means that you my slime. Is not zero product. Nonzero number times. Nonzero number times nonzero vector and somehow that's equaling the zero Vector that's impossible. And that is the conclusion of the proof. A similar proof to this will show you that the same ": [
      1559.2,
      1591.4,
      50
    ],
    "of a -2 x identity. It's a two-dimensional space. So I'm going to be looking for a basis with two vectors in it, but we know how to do that now. Okay, if I want to find a basis for the null space of a matrix of this Matrix, what do I do? Row reduction we should be used to that by now. Although I've been telling you lately that ": [
      447.0,
      471.6,
      16
    ],
    "of eigenvectors. It says that if I want to figure out what matrix multiplication by a does to a vector if I expand that Vector in the eigenbasis than all the Matrix does is 2 * this diagonal matrix with the eigenvalues on the diagonal which is the easiest kind of matrix multiplication there is So it's easy to understand what the Matrix a does to any vector? So let's ": [
      2010.5,
      2039.1,
      65
    ],
    "of them that are linearly independent doesn't change. So that's what similarity does for you two matrices are similar what that really means is that they have the same eigenvalues and they have the same profile of eigenspaces. The eigenspaces aren't the same as the original Matrix, but they look the same. They're isomorphic. They have the same dimensions as before. So if we look back at this example down ": [
      2896.1,
      2921.2,
      96
    ],
    "on X1 in terms of X2 and X3 that first equation that says x 1 - 1/2 x 2 + 3 x 3 equals 0. So I got x 1 is equal to 1/2 not - 1/2 + 1/2 * x 2 - 3 x 3 Which as usual I can separate out as x 2 x 1/2 1 0. + x 3 x - 3 0 1 and there's ": [
      525.8,
      557.1,
      19
    ],
    "one quick thing here. So if I said that you two in the front row are very similar, I'm not saying that I'm just if I said that it implies a two-way relationship. It means that you're similar to him and you are similar to him. So if I use the word similar it should have this should have this symmetry to it. So they is similar to be then ": [
      2473.6,
      2497.5,
      81
    ],
    "or that there are enough of them and they span are at you don't have to check both linear independence and spanning set if you already know the dimension to satisfy. Okay, so if I've got an distinct eigenvalues, that means that I'll have a basis of eigenvectors. Okay, that's really the important thing here. When you have a basis of eigenvectors for your Matrix things are super nice. That ": [
      1668.7,
      1695.3,
      54
    ],
    "property that when you multiply it by a all that does is to scale it by some scaling Factor, Lambda. Okay that a x b for that vector v is just some number 3 times a week. And the number that it's gets killed by is called the eigenvalue associated to that eigenvector. now the set of all eigenvectors for a with a given eigenvalue is well, the sooner I ": [
      109.9,
      139.8,
      4
    ],
    "quick to calculate determinants and that gives me a polynomial equation. I need to solve for Lambda that polynomial is called the characteristic polynomial of the Matrix and we'll talk about that more at the end of the lecture. This is a way to find eigenvalues in general. It's the way that you guys will learn to find that conducts. Okay. So here now, I want to start to talk ": [
      1118.7,
      1143.1,
      37
    ],
    "realized as matrix multiplication. But the trick is it can be realized as matrix multiplication on the rights. This is V1 V2 VN times the diagonal matrix with the lambdas on the right. You can verify that for yourself if you want. But that's how you achieve scaling The Columns of a matrix u x diagonal matrix on the right. Do you want to see all the rows u x ": [
      2192.7,
      2221.2,
      70
    ],
    "reduction you subtract the second row from the first and replace the first roll with it. But they are not similar matrices. Now we could actually prove that directly right now. We could we could do a contradiction argument and say hey, let's see if we can find a matrix P for which AP which AZ will do PVP in verse and then go ahead and do a linear algebra ": [
      2630.2,
      2652.5,
      87
    ],
    "right that First Column is non pivotal. So X1 of a - 9 dies a free variable x o x 2 is going to be in terms of x 1 and that first equation there. It says 0 x 1 + x 2 equals 0 so that says X2 is 0 + x 1 as a free variable. Okay, and actually that tells us immediately that this is just the ": [
      846.3,
      871.2,
      28
    ],
    "some invertible Matrix P. You can use it to multiply on the left and the right by P&P inverse by a diagonal matrix and the super nice thing about that. Is that what does. Does it really implements that coordinate Vector business? Okay. If you want to use language that you might have seen from reading in the textbook sections we didn't do p is the change of basis Matrix ": [
      2277.7,
      2301.0,
      73
    ],
    "span of the first base inspector for standard basis Vector 1 0 that's the eigenspace here. So to conclude we see that. the only eigenvalue of a is 1 and it has the one-dimensional eigenspace spanned by the first standard basis vector So that's the full information about eigenvalues and eigenvectors of that Matrix. Yes. I call it one-dimensional because it's Dimension is what the dimension of a Subspace is ": [
      871.2,
      923.4,
      29
    ],
    "squared down the line. It's easy to take powers of a diagonal matrix. And therefore if I have this decomposition of a matrix as PDP inverse. Then I can do things like take powers of it really easily as you explored with an example taking the hundredth power of Matrix. That would have taken a hundred years if you just done it all by hand, but if you first do ": [
      2356.3,
      2377.6,
      76
    ],
    "subtract from it twice the identity Matrix, which is 2 0 0 0 2 0 0 0 2 Okay. and so that is 2 - 1/6 2 - 1/6 and 2 - 1/6 so that first Matrix, they're actually it's not hard to check that that first Matrix there. It's invertible. Paint that the 1A up there like no one of those Rose is a scalar multiple of the other. ": [
      326.2,
      368.3,
      12
    ],
    "take a squared that's pdpn verse where it so let's write that that just means multiply that by itself. But the p x a p inverse they canceled inside giving us the identity Matrix. So this is PD squared PN verse. That squaring a diagonal matrix is quite easy. The product of two diagonal matrices is just the diagonal matrix whose entries are the products of 1 squared Lambda 2 ": [
      2324.9,
      2356.3,
      75
    ],
    "that Lambda is an eigenvalue which means it is the scalar multiple that appears for some eigenvector. So there is no my convector. I'm choosing one calling a w it's some nonzero vector. So I want to show that these two nonzero vectors are in fact a linearly independent. I'm going to argue by contradiction when I say well suppose that they are linearly dependent. What does that mean when ": [
      1339.9,
      1373.8,
      44
    ],
    "that Mel space. What does it mean to say that as an eigenvalue for Landon eigenvalue for a with eigenvector V A V is equal to Lambda V? now A and B are similar, so that means that b is equal to P A P inverse for some invertible Matrix p but as we saw in last night, that's the same thing as saying that a is equal to P ": [
      2768.3,
      2796.0,
      92
    ],
    "that says that a x b is equal to muv. Okay. Now what if I also take a x w? 8 * W is equal to Lambda times. W. That's the definition of them being eigenvectors with us eigenvalues. Know this first one here. I mean, I've got an equation W is equal to c x v So 8 * W is equal to a x c v. which is ": [
      1439.6,
      1480.2,
      47
    ],
    "the answer isn't always a reduction. But in this case it is Okay. So what I have to do is row reduce that Matrix, so I'll go ahead and subtract the first row from the second which gives me all zeros there. And I'll also subtract the first row from the third which gives me all zeros there. Now. I'm practically done. The only last thing that I could do ": [
      471.6,
      493.7,
      17
    ],
    "the basis and the way I would do that is first x w and then repeat the same procedure. Okay do the row reduction to figure out where in the column space it is to find the coefficients of it, but I don't need to do any more work because here they are. 8 * W is VSCO fashion X me OnePlus this Corporation X V2 down the line and ": [
      1895.2,
      1916.2,
      61
    ],
    "the basis vectors. In fact the way I would figure out what those coefficients in the coordinate Vector are is to Robert option so I can figure out what those exes are now. I also want to multiply The vector W by a and see what that new Vector is. So since the D is form a basis. I could also find the coefficients of that W in terms of ": [
      1871.8,
      1895.2,
      60
    ],
    "the equation 8 * 8 is equal to p x b x p inverse. Those are called similar matrices. What we just saw is that if you have a basis of eigenvectors for a then a is similar to a diagonal matrix. So for example as we saw from the first theorem if all if you're in my a matrix has and the stink diagonal values, which will be true ": [
      2426.2,
      2451.6,
      79
    ],
    "the number of columns. So we have one plus two is three. So nobody is to that means so we can tell right away. Let's write it. Everything. We just said the rank of this thing. Is one there for the nullity of this thing? Is to another Melody is the dimension of the space. To the dimension of the Space Ranger stood at the eigenspace of to the null-space ": [
      418.2,
      447.0,
      15
    ],
    "the size of any basis. Can we produce the basis for this eigenspace for the null-space of a - I and it has just one veteran it so it is one-dimensional. Eigenvectors and eigenvalues and determine your not on your midterm, but the notion of diminishing surely is so make sure that we're all clear on that what it means just count the number of basis vectors. Here's another example ": [
      923.4,
      947.4,
      30
    ],
    "then we'll stay in the eigenspace. So in fact, I did I think I'd prefer to replace this one by twice it. 120 Rochester has integer entries so you could choose the two yellow vectors there or you could choose this Vector 120 together with the second one of those are both bases in space. Okay, so there is how we find eigenvectors knowing and I can value it's just ": [
      633.9,
      664.0,
      22
    ],
    "then we're going to talk about how you would actually go about finding the eigenvalues, which is harder problem. So let's start with an example. So here is a matrix a square Matrix 3 by 3, and I'm telling you that to is an eigenvalue for that Matrix. Okay, so let's find a basis for the corresponding eigenspace. What does that mean? So the eigenspace? for a with eigenvalue 2 ": [
      266.7,
      301.5,
      10
    ],
    "thing is true for any number of vectors if I have a very large Matrix and I have 76 distinct eigenvalues in it. Then the eigenvectors any true choice of eigenvectors for those 76 eigenvalues will all be linearly independent. And if you want to see the details of how you modify this proof for that general setting look in the textbook is written out there carefully for you. That's ": [
      1591.4,
      1613.8,
      51
    ],
    "this determinant Well, that's just the determinant of the Matrix one- Lambda one- Lambda. Which is actually the same as the thing we got up above. And so we have the exact same thing as before. Only Lambda equals 1 is an eigenvalue. So this Matrix only has the eigenvalue Lambda equals 1. Now we want to find the eigenspace according to that. Okay, in the first example again, we ": [
      971.4,
      1001.1,
      32
    ],
    "this procedure find eigenvectors write the essay as PDP inverse, then it becomes very quick and routine to take Pipe hours, which is something you might want to do a lot of examples Okay. Now this kind of expression here were a is equal to p x some other Matrix x p inverse that's more meaningful than just being able to compute easily with it. So there's here is a ": [
      2377.6,
      2401.1,
      77
    ],
    "those are super simple expressions in terms of the coordinates of w itself. So what this says? Is that the coordinates of a w in the basis B? Are just Lambda 1 * x 1 Lambda 2 * x 2 Lambda end times XM. That's very simple. In fact, we can write that in a really nice way. We can write this as follows. So if I forget now, I ": [
      1916.2,
      1963.8,
      62
    ],
    "those basis vectors are eigenvectors for the Matrix a so if I want to figure out what the Matrix a. What does a do? 2 in generic Vector w Well, aw is equal to a x x 1 V 1 + x 2 V 2 + xnvn and using linearity of matrix multiplication. That's X1 AV 1 + X2 AV 2 + x navn But here's the the great part. ": [
      1799.0,
      1840.1,
      58
    ],
    "vectors each one of those Associated to some eigenvalue. Let's call them Lambda one Lambda to Lambda straight down the line. Okay. So what's nice about that is if it's a basis. That means that any vector. wnrn can be expanded In the basis Peak, right? That's what a basis is. In fact, we have a notation for that if I take a vector wnrn then it's coordinates in the ": [
      1729.5,
      1769.9,
      56
    ],
    "very important notion. We're going to explore now similarity of matrices. So now let's let's forget for a moment where this all came from forget talking about eigenvectors and eigenvalues. But it gave us this equation that I want to explore May hold another context. So I give you two square matrices they Envy They are called similar if you can find some invertible Matrix P for which they satisfy ": [
      2401.1,
      2426.2,
      78
    ],
    "we saw and determinants determinants. Tell us when a matrix is invertible or not. Remember what we're looking for is the one to find Lambda. Such that a minus Lambda II has a non-trivial null space. Will one way to check that would be just to find Lambda for which the determinant of a minus Lambda I is not zero. So, you know, this is to buy to let's just ": [
      695.9,
      724.8,
      24
    ],
    "we saw in the in the case of of a matrix that has the bases of eigenvectors, but if I just have two similar matrices that is the a sequel to PBP and rest for some invertible Matrix P. Then they have the same eigenvalues. And the eigenspaces for those eigenvalues have the same dimensions. Hey, this is what's really important about similarity transformation. It's two matrices are similar if ": [
      2677.3,
      2705.0,
      89
    ],
    "when I X The Matrix when I multiply The Matrix Zero by some Vector. I want to know when does that equal zero? All vectors the null-space of the zero Matrix is every vector. Okay, so this is two dimensional. If you want a basis, you may as well just choose the standard basis. So here we have two examples to 2 by 2 matrices. They both only have one ": [
      1039.7,
      1072.7,
      34
    ],
    "yes indeed if a and b are similar then DNA are similar. So this taking a p a x p on one side and P Universe on the other side. That's a symmetric sort of transformation the fancy word we use for the mathematics is conjugation. We are conjugating be by P. All right, very important Point here as you move forward. Similarity is not the same as row equivalent ": [
      2576.9,
      2607.0,
      85
    ],
    "your MyMathLab homework set number 6 on determinants is due tomorrow night at 11:59 p.m. pencil one more reminder this stuff we're doing today is not on this midterm, but I'm glad you're all here because it's certainly going to be on the final and it's super important. We're moving into the most important part of the course eigenvalues and eigenvectors are the most important tools that you will take ": [
      52.7,
      78.7,
      2
    ]
  },
  "Full Transcript": "Listening to a podcast find eigenvalues and then next time of course, you have a midterm on Wednesday evening. So the next lecture after this one on Wednesday will be a review lecture. So that midterm one more reminder is from 8 to 10 p.m. On Wednesday evening in three different rooms to Ann Peterson one and Galbraith your room until the Simon are posted on Triton and hopefully you already know them.  I posted to practice midterms last week. Hopefully you've been working through them and doing all their other exercises several people have already posted their solutions to the midterms on Piazza with other people responding about what they thought was right in what they thought was wrong. You should join that discussion that's going to be a great way to study and one more administrivia reminder that your MyMathLab homework set number 6 on determinants is due tomorrow night at 11:59 p.m.  pencil  one more reminder this stuff we're doing today is not on this midterm, but I'm glad you're all here because it's certainly going to be on the final and it's super important. We're moving into the most important part of the course eigenvalues and eigenvectors are the most important tools that you will take away from this course in anything you're going to do in the future that uses linear algebra. So listen up,  So here is a summary of everything that we said last day about eigenvalues and eigenvectors are things that are associated to a square matrices only makes sense for square matrices. So if you have a square n by n Matrix an eigenvector  Is a nonzero vector v?  With the property that when you multiply it by a all that does is to scale it by some scaling Factor, Lambda.  Okay that a x b for that vector v is just some number 3 times a week.  And the number that it's gets killed by is called the eigenvalue associated to that eigenvector.  now the set of all eigenvectors for a  with a given eigenvalue is well, the sooner I mean here is slightly false is equal to the null-space of a minus Lambda. I that was the key observations made last day, but there's one small thing wrong with that statement. What is it?  There is one vector that is in the no space of a minus Lambda eye and is not an eigenvector. What Vector is that? The zero Vector zero Vector is in any Subspace in particular is in the null-space of that Matrix a my slime to I so the proper statement here would be the set of eigenvectors of a forgiving eigenvalue. Lambda is equal to the no space of a minus Lambda.  Except zero to take out the zero actor.  But the no space that thing the null space is a Subspace. Okay, and we call it the eigenspace so for each real number, Lambda?  That thing the null-space of a - Lambda is called the eigenspace.  With eigenvalue land. We only really use that terminology when it when Lambda is an eigenvalue. That is the eigenvectors of the nonzero vectors in an eigenspace. So we only call it an eigenspace if it has some nonzero vectors in it. There were looking for special numbers land up for which that thing the null-space of a - Lambda I has nonzero vectors in it.  And what we saw last day is that well, if you're given a loud and told it's an eigenvalue it is then routine for us now that we know reduction so well to figure out what the eigenvectors are. What is the eigenspace that is I just give you a Lambda on you just report to me find a basis for a minus line two eyes. No space that's something you know how to do really? Well hopefully for the midterm for any given Matrix, but it's typically hard to find those Landis for which the null space is not trivial. And so we're going to start by looking at some more examples of finding the eigenspace will produce basis for the eigenspace once we know and I can value  But then we're going to talk about how you would actually go about finding the eigenvalues, which is harder problem.  So let's start with an example. So here is a matrix a square Matrix 3 by 3, and I'm telling you that to is an eigenvalue for that Matrix.  Okay, so let's find a basis for the corresponding eigenspace.  What does that mean? So the eigenspace?  for a  with eigenvalue  2  Is nothing other than the null-space of a - 2 * the identity Matrix, so that's all this is just telling you to do that find the null space or find a basis for the null space of a - 2 X 2 Matrix. So first let's figure out what is a -2 times the identity Matrix. So that is 4 - 1 6-2 1-6 2-1 8 I have to subtract from it twice the identity Matrix, which is 2 0 0 0 2 0 0 0 2  Okay.  and so that is  2 - 1/6  2 - 1/6 and 2 - 1/6  so that first Matrix, they're actually it's not hard to check that that first Matrix there. It's invertible.  Paint that the 1A up there like no one of those Rose is a scalar multiple of the other. So no two of them are linearly dependent on each other and actually we could row reduce it and it's not going to be hard to check that it is actually an invertible Matrix, but when we subtract twice the identity from it, it becomes very not invertible it becomes well. Let's see here. What's the rank of this Matrix?  It's a good practice for the mid-term. What does rank mean?  The number of pivotal columns or the number of pivotal Rose or the dimension of the column space or the dimension of the row space does all mean the same thing. So what's the number of pivotal Rose here just one cuz all three are scaled multiples of each other all equal. So this is to this one. It's Melody is  2 personality plus the rank is equal to the number of columns. So we have one plus two is three. So nobody is to that means so we can tell right away. Let's write it. Everything. We just said the rank of this thing.  Is one there for the nullity of this thing?  Is to another Melody is the dimension of the space.  To the dimension of the Space Ranger stood at the eigenspace of to the null-space of a -2 x identity. It's a two-dimensional space. So I'm going to be looking for a basis with two vectors in it, but we know how to do that now. Okay, if I want to find a basis for the null space of a matrix of this Matrix, what do I do?  Row reduction we should be used to that by now. Although I've been telling you lately that the answer isn't always a reduction. But in this case it is  Okay. So what I have to do is row reduce that Matrix, so I'll go ahead and subtract the first row from the second which gives me all zeros there.  And I'll also subtract the first row from the third which gives me all zeros there. Now. I'm practically done. The only last thing that I could do is Pivot.  And there is the reduced row Echelon form of this Matrix. So that tells me that the null space.  of that Matrix, which is a -2 Tennessee identity can be written as the set of vectors X1 X2 X3 such that well the free variables are these two  they don't have pivotal ones in those columns. And so  the only thing we're going to get is a constraint on X1 in terms of X2 and X3 that first equation that says x 1 - 1/2 x 2 + 3 x 3 equals 0. So I got x 1 is equal to 1/2 not - 1/2 + 1/2 * x 2 - 3 x 3  Which as usual I can separate out as x 2 x 1/2 1 0.  + x 3 x - 3 0 1  and there's my basis.  Pay those two vectors form a basis for the eigenspace of the eigenvector to those are both eigenvalue eigenvector s48. Let's just double-check that sanity check over here. If I take the Matrix a i x let's II Vector. Let's say  That's what I get. I get - 3 * 4 is 12 + 0 + 6 - 6  2 * - 3 + 0 + 6 is -6 + 6 which is 0 + 2 * -3 + 0 + 8 is -6 + 8 is 2  And that is indeed twice that original Vector - 301 as we knew it had to be so that is an eigenvector with eigenvalue.  2  so is the first one 1/2 is it 1-0, but actually  the eigenspace is a Subspace. So we're free to scale the vectors in and then we'll stay in the eigenspace. So in fact, I did I think I'd prefer to replace this one by twice it.  120 Rochester has integer entries so you could choose the two yellow vectors there or you could choose this Vector 120 together with the second one of those are both bases in space.  Okay, so there is how we find eigenvectors knowing and I can value it's just doing the same thing. We've been doing for a while. Now. Let's look at a couple more examples.  So here's a matrix a  pick 2 by 2 Matrix.  No, I haven't told you what its eigenvalues are. Okay, but actually it's not hard to figure out what it's like and values are.  Because this is a triangular Matrix.  So it's easy to compute the determinant of a triangular Matrix as we saw and determinants determinants. Tell us when a matrix is invertible or not. Remember what we're looking for is the one to find Lambda.  Such that a minus Lambda II has a non-trivial null space.  Will one way to check that would be just to find Lambda for which the determinant of a minus Lambda I is not zero. So, you know, this is to buy to let's just go ahead and compute that. I'm sorry. I know space is not zero. So i e we want a minus Lambda. I not invertible.  Do you want a minus Lambda? I not invertible.  The one way to check that and this will be an easy check for 2 by 2 Matrix.  is to find land. For which the determinant of a minus Lambda I  is 0  okay, so let's just compute that for this example here A- Lambda I  Is the Matrix one- lamb. 101 - Lambda?  and the determinant of that  is 1 - Lambda squared - 0  so the question is  for which Lambda?  is 1 - Lambda squared equal to zero  Well, that's easy to solve in this case. Only Lambda equals 0 equals 1.  So that's the only eigenvalue of this Matrix a  so now let's figure out its eigenspace.  a - 1 * I  That's the Matrix.  0100 which is conveniently already in reduced row Echelon form for us.  I so I want to find a basis for that eigenspace.  It says that the null space of a -1 * I  is equal to the set of vectors X1 X2 such that.  Are we see that X2 is a X1 is a free variable hear right that First Column is non pivotal. So X1 of a - 9 dies a free variable x o x 2 is going to be in terms of x 1 and that first equation there. It says 0 x 1 + x 2 equals 0 so that says X2 is 0 + x 1 as a free variable.  Okay, and actually that tells us immediately that this is just the span of the first base inspector for standard basis Vector 1 0 that's the eigenspace here. So to conclude we see that.  the only  eigenvalue  of a  is 1  and it has  the one-dimensional  eigenspace  spanned by  the first standard basis vector  So that's the full information about eigenvalues and eigenvectors of that Matrix. Yes.  I call it one-dimensional because it's Dimension is what the dimension of a Subspace is the size of any basis.  Can we produce the basis for this eigenspace for the null-space of a - I and it has just one veteran it so it is one-dimensional.  Eigenvectors and eigenvalues and determine your not on your midterm, but the notion of diminishing surely is so make sure that we're all clear on that what it means just count the number of basis vectors. Here's another example 2 by 2 Matrix. This is a super nice Matrix is the identity Matrix. What are the eigenvalues of this Matrix again in the two by two cases going to be pretty easy to do just like we did as above  Let's figure out what are the values of land. For which B- land of times the identity.  Has a non-trivial null space and we can do that.  by calculating this determinant  Well, that's just the determinant of the Matrix one- Lambda one- Lambda.  Which is actually the same as the thing we got up above.  And so we have the exact same thing as before.  Only Lambda equals 1 is an eigenvalue. So this Matrix only has the eigenvalue Lambda equals 1.  Now we want to find the eigenspace according to that. Okay, in the first example again, we found only eigenvalue that big was one and we saw that had a one-dimensional eigenspace.  Now let's look down here in this example. If I take B minus 1 times the identity Matrix. Well, that's just the identity Matrix minus the identity Matrix.  What's the null-space of the zero Matrix?  What vectors does the zero Matrix kill?  Everyone know space is a set of vectors that gets sent to zero when I X The Matrix when I multiply The Matrix Zero by some Vector. I want to know when does that equal zero?  All vectors the null-space of the zero Matrix is every vector.  Okay, so this is two dimensional.  If you want a basis, you may as well just choose the standard basis.  So here we have two examples to 2 by 2 matrices. They both only have one as an eigenvalue. That's their only I can value but we can very different behavior for the eigenvectors the eigenspace of eigenvalue one for the Matrix a is one-dimensional.  Just the first entered basis Vector expensive for the second Matrix here. The eigenspace is two-dimensional span. For example by that Vector plus another one.  Okay. No, I've already by the way. I've snuck in here what we're going to do at the end of the lecture, which is how do you find eigenvalues in this case? We we found this tricky way to find eigenvalues which was to say. Hey, I'm looking for Lambda for which a mine is Lambda. I has a non-trivial no space. That's what the eigenvectors are. That's the same thing as a minus Lambda. I having determinant 0 and for two by two cases real quick to calculate determinants and that gives me a polynomial equation. I need to solve for Lambda that polynomial is called the characteristic polynomial of the Matrix and we'll talk about that more at the end of the lecture. This is a way to find eigenvalues in general. It's the way that you guys will learn to find that conducts.  Okay.  So here now, I want to start to talk about what kind of properties eigenvectors can have so we saw in the last two examples.  Does 2 2 by 2 matrices? We saw both of them had only one single eigenvalue.  Typically, that's not going to happen. Typically if you have a 2 by 2 Matrix, you'll find two different eigenvalues when I say typically I mean most of the time sometimes sometimes there might be a repeated eigenvalue like in those two examples where you have only one eigenvalue, okay for a larger Matrix, but most of the time you're going to find if you have a 2 by 2 Matrix, they'll be two distinct eigenvalues of a three-by-three most of the time they'll be three distinct eigenvalues. And what's great about that is the following theorem.  If you have a bunch of eigenvectors of a matrix.  Four distinct eigenvalues. I have two distinct eigenvalues and eigenvectors for those two distinct eigenvalues. Those eigenvectors are automatically linearly independent.  I said something algebraic about the eigenvalues whether they're equal or not tells you something about the relationship between the eigenvectors. They must be linearly independent. This is actually a superpower.  So I want to show you why this is true. Your textbook has a full proof of this. I'm going to do the special case where we have to I can vectors which demonstrates the entire point of the proof without getting into notation problems and making it hard to understand. So we'll just look at the case.  of two distinct  eigenvalues  Okay, so let's say that Lambda and Mew.  R2  eigenvalues  of a matrix a  and they're not equal to distinct eigenvalues.  And supposed to have eigenvectors. Let's call them.  You and me.  So you as an eigenvector with eigenvalue Lambda for a and v is an eigenvector with eigenvalue miu4, but actually let me let me change my rotation call this one instead of you. Let me call it w cuz I think that's what I the notation I use on the next slide.  Okay. So what we'd like to see is that these two eigenvectors wnv are not linearly are linearly independent first things first. We can know that they are not zero.  How is it we know that they're not zero.  Could one of them be zero that I have an eigenvector that zero.  No by definition eigenvectors are always nonzero Vector. So if I'm telling you that's an eigenvector, then it must be nonzero. And I know there is an eigenvector. I'm telling you that Lambda is an eigenvalue which means it is the scalar multiple that appears for some eigenvector. So there is no my convector. I'm choosing one calling a w it's some nonzero vector.  So I want to show that these two nonzero vectors are in fact a linearly independent. I'm going to argue by contradiction when I say well suppose that they are linearly dependent.  What does that mean when I have two vectors, it's easy to recognize when their linearly dependent that happens. If and only if they are.  They are very quiet.  parallel  So let's assume for contradiction that there parallel. Let's suppose that W is some scalar multiple c x v  and then there's one in key observations. I want to make here could that's eb-0.  Know why not?  Cuz that would make W 0 and we know that the vectors are not 0 so we know that this scalar multiple C is not 0 so I got this wnv. I'm assuming for seven times the other or a half times the other some nonzero scalar multiple. How does that help us hear? Well start using the fact that there are eigenvectors. So if I take a x v  V is an eigenvector with eigenvalue Mew so that says that a x b is equal to muv.  Okay.  Now what if I also take a x w?  8 * W is equal to Lambda times. W.  That's the definition of them being eigenvectors with us eigenvalues.  Know this first one here. I mean, I've got an equation W is equal to c x v  So 8 * W is equal to a x c v.  which is equal to c x a v  so what does that mean? Will I've got an equation for A V A V is equal to Mew X be so this is equal to c x Miu.  that's a Lambda x w, but ww.w is equal to  C x v so this is equal to Lambda times CB.  So just using the fact that BMW are eigenvectors with these two different eigenvalues, Mew and Lambda what I get is this funny equation tracing through the whole thing.  What I got is that c x movie is equal to c x Lambda B.  And let me subtract that.  that says c x Miu - Lambda x v  is 0  so here's what I've got.  I've got some vector v. I know that it's not zero.  I've got a constancy. I know that it's not zero and I've got this number Mew - lamb to hear from you and Linda are the eigenvalues which I assumed were distinct, which means that you my slime. Is not zero product. Nonzero number times. Nonzero number times nonzero vector and somehow that's equaling the zero Vector that's impossible.  And that is the conclusion of the proof.  A similar proof to this will show you that the same thing is true for any number of vectors if I have a very large Matrix and I have 76 distinct eigenvalues in it. Then the eigenvectors any true choice of eigenvectors for those 76 eigenvalues will all be linearly independent.  And if you want to see the details of how you modify this proof for that general setting look in the textbook is written out there carefully for you. That's a very important their distinct eigenvalues. Give us a linearly independent know why should I care about that? Why should I care about having linearly independent eigenvectors? Well, here's why  So if I have soda says is a corollary to that they're on is if I have all of distinct eigenvalues supposed to have a square Matrix.  Seven by seven and all seven and I and I have seven distinct eigenvalues for it.  Then that means there are 7 linearly independent eigenvectors. But if I have 7 linearly independent vectors in RN, they form a  a basis and there's an important thing for you to remember for the mid-term. If you want to check if a set of vectors in RN forms the basis for RN it's enough to know that there are enough of them and they're linearly independent or that there are enough of them and they span are at you don't have to check both linear independence and spanning set if you already know the dimension to satisfy.  Okay, so if I've got an distinct eigenvalues, that means that I'll have a basis of eigenvectors.  Okay, that's really the important thing here. When you have a basis of eigenvectors for your Matrix things are super nice. That means that you can fully understand the action of the Matrix and it's as simple as it gets so let me explain why so why is that so nice?  This is super nice.  Why is that? Well, let's think about it this way. So suppose I take that basis. I'm going to basis. Let's call it B1 B2 B3.  VN  a basis of eigenvectors  That's a basis for RN in vectors each one of those Associated to some eigenvalue. Let's call them Lambda one Lambda to Lambda straight down the line.  Okay.  So what's nice about that is if it's a basis.  That means that any vector.  wnrn  can be expanded  In the basis Peak, right? That's what a basis is. In fact, we have a notation for that if I take a vector wnrn  then it's coordinates in the basis B.  Are the unique numbers x 1/2 x n such that W is expanded in terms of those numbers in terms of the basis. So that is to say this is the same thing as writing W expand x 1 x v 1 + X2 X V2 + x + x via.  That's true. Whenever we have a basis what's great here is that those vectors form of those those basis vectors are eigenvectors for the Matrix a so if I want to figure out what the Matrix a.  What does a do?  2 in generic Vector w  Well, aw is equal to a x x 1 V 1 + x 2 V 2 + xnvn and using linearity of matrix multiplication. That's X1 AV 1 + X2 AV 2 + x navn  But here's the the great part.  These are eigenvectors for a  so that means that everyone is equal to Lambda One X V one and so on.  So this last line here this just as the same thing as X1 Lambda One V one plus X to Lambda to be to  sxm, Landon  What does that say that says? Hey, I got this arbitrary Vector W. I know I can expand it somehow in terms of the basis vectors. In fact the way I would figure out what those coefficients in the coordinate Vector are is to Robert option so I can figure out what those exes are now. I also want to multiply  The vector W by a and see what that new Vector is. So since the D is form a basis. I could also find the coefficients of that W in terms of the basis and the way I would do that is first x w and then repeat the same procedure. Okay do the row reduction to figure out where in the column space it is to find the coefficients of it, but I don't need to do any more work because here they are.  8 * W is VSCO fashion X me OnePlus this Corporation X V2 down the line and those are super simple expressions in terms of the coordinates of w itself. So what this says?  Is that the coordinates of a w in the basis B?  Are just Lambda 1 * x 1 Lambda 2 * x 2 Lambda end times XM.  That's very simple. In fact, we can write that in a really nice way.  We can write this as follows.  So if I forget now, I have this abstract basis of vectors and vectors be around then I say hey look and I got this Vector. I'm just thinking of w is a column Vector its coefficients in the coordinate basis X-14 xn.  And I want to know multiplication by a on the victim to Victor W. What is that really do to W. What is it due to its coordinates? All it does to its coordinates is scale each of the coordinates by those eigenvalues and I can write that is matrix multiplication. If I just scale each of the coordinates by a number that's the same thing as multiplying that coordinate vector by a diagonal matrix cab schematically ground Zeroes their meeting all the non Dying Light Reserve 0 and you put those eigenvalues on the diagonal.  That's what's great about having a basis of eigenvectors. It says that if I want to figure out what matrix multiplication by a does to a vector if I expand that Vector in the eigenbasis than all the Matrix does is 2 * this diagonal matrix with the eigenvalues on the diagonal which is the easiest kind of matrix multiplication there is  So it's easy to understand what the Matrix a does to any vector?  So let's explore that a little more. This is what we just saw.  that  what the Matrix what the Matrix a what may by a does to Any Given vector?  If you expand it in terms of the basis of eigenvectors is to multiply that coordinate vector by this diagonal matrix with lamb, doesn't it? Let's give that Matrix there a name.  so it multiplies a coordinate vector by this diagonal matrix d  Now, let me put another Matrix in here. Let's take those vectors the eigenvectors.  String them together in a matrix.  Let's call that Matrix.  deadliest form of basis  This is a square Matrix here.  And this Matrix p is an invertible Matrix because it's columns form a basis for RN.  So there's something that happens here that we can just drive without reference to the coordinate vectors what happens if I take the Matrix a and X The Matrix p  A is the original Matrix whose eigenvectors are the v's and eigenvalues are the lambdas?  8 * P. Let's remember the definition of matrix multiplication. I'm multiplying a x The Matrix whose columns are the v's. That's the same thing as just multiplying the matrix by each of those columns.  But the eigenvectors the vectors V1 Savion are eigenvectors. So a x V1 is equal to Lambda one-v-one Etc. So I can write this as Lambda 1 V1.  Lambda 2 V 2  app to Lambda and V on this is just a different way of rewriting what we saw on the last slide.  I know if I've got a matrix and what I do to it is I scale each of its columns by some number that can be realized as matrix multiplication. But the trick is it can be realized as matrix multiplication on the rights.  This is V1 V2 VN times the diagonal matrix with the lambdas on the right.  You can verify that for yourself if you want.  But that's how you achieve scaling The Columns of a matrix u x diagonal matrix on the right.  Do you want to see all the rows u x diagonal matrix on the left?  So what this says here is hey, what is this this V1 V2? That's the thing that we called p  and this diagonal matrix that was with called D. So I get this equation. I get that a x p is equal to p x d.  And then one more thing we see from that is this Matrix P was invertible.  So I can multiply by the inverse. Let's x p inverse on the right when I get the a is equal to p d e p inverse already in your Matlab homework.  This is super important. We're going to talk about this more next time. So on Friday and next week as well, but if you have a basis of eigenvectors what that says is that your Matrix can be written in this form some invertible Matrix P. You can use it to multiply on the left and the right by P&P inverse by a diagonal matrix and the super nice thing about that. Is that what does. Does it really implements that coordinate Vector business? Okay. If you want to use language that you might have seen from reading in the textbook sections we didn't do p is the change of basis Matrix from the standard basis to the eigenbasis. This is saying exactly the same thing as is written above that. The action of a on coordinate vectors in the v bases is just a x the diagonal matrix d  but as you saw when you're doing your Matlab homework this relationship here allows you to do things like take powers of the Matrix a really easily cuz if I want to take a squared  that's pdpn verse where it so let's write that that just means multiply that by itself.  But the p x a p inverse they canceled inside giving us the identity Matrix. So this is PD squared PN verse.  That squaring a diagonal matrix is quite easy. The product of two diagonal matrices is just the diagonal matrix whose entries are the products of 1 squared Lambda 2 squared down the line.  It's easy to take powers of a diagonal matrix. And therefore if I have this decomposition of a matrix as PDP inverse.  Then I can do things like take powers of it really easily as you explored with an example taking the hundredth power of Matrix. That would have taken a hundred years if you just done it all by hand, but if you first do this procedure find eigenvectors write the essay as PDP inverse, then it becomes very quick and routine to take Pipe hours, which is something you might want to do a lot of examples  Okay. Now this kind of expression here were a is equal to p x some other Matrix x p inverse that's more meaningful than just being able to compute easily with it. So there's here is a very important notion. We're going to explore now similarity of matrices. So now let's let's forget for a moment where this all came from forget talking about eigenvectors and eigenvalues.  But it gave us this equation that I want to explore May hold another context. So I give you two square matrices they Envy  They are called similar if you can find some invertible Matrix P for which they satisfy the equation 8 * 8 is equal to p x b x p inverse. Those are called similar matrices. What we just saw is that if you have a basis of eigenvectors for a then a is similar to a diagonal matrix.  So for example as we saw from the first theorem if all if you're in my a matrix has and the stink diagonal values, which will be true most of the time.  You will have a basis of eigenvectors. And therefore your Matrix will be similar to a diagonal matrix, which means the computations of things like powers that Matrix will be really easy to do. Okay, if you follow that that path.  I will explore this notion of similarity a lot next time. Okay, but I wanted to introduce it to you right now. Let me know one quick thing here.  So if I said that you two in the front row are very similar, I'm not saying that I'm just if I said that it implies a two-way relationship. It means that you're similar to him and you are similar to him.  So if I use the word similar it should have this should have this symmetry to it. So they is similar to be then be should be similar to a that might not be apparently true from the way it's written here, but it is true because if a is similar to be then we can rewrite this like we did on the last slide as a x p is equal to p x b.  And now I can X p.m. Verse on the left and get that P inverse.  AP  is equal to be  Queso with a is pbpn verse that tells you that.  B is equal to P inverse AP which looks like almost the same thing, which is what I wanted if a is similar to be then be should be similar to a except now. The inverse is on the left instead of on the right. So that's not quite the same statement. Right? Well, but the statement is a and b are similar if there is some invertible Matrix p  For which a sequel to PBP inverse?  So there is an invertible Matrix here to which will call Q and that's P inverse.  So this is that V is equal to q a and then P inverse. So what is p e p is Q in verse in the inverse of the inverse is the original Matrix. And so yes indeed if a and b are similar then DNA are similar.  So this taking a p a x p on one side and P Universe on the other side. That's a symmetric sort of transformation the fancy word we use for the mathematics is conjugation. We are conjugating be by P.  All right, very important Point here as you move forward.  Similarity is not the same as row equivalent equivalent matrices may or may not be similar to each other similar matrices may or may not be Roku belong to each other totally different notion here is a good example the matrices A and B that we started with that was our second example today. These matrices are clearly row equivalent to each other to get from The Matrix to The Matrix be just do one step of row reduction you subtract the second row from the first and replace the first roll with it.  But they are not similar matrices.  Now we could actually prove that directly right now. We could we could do a contradiction argument and say hey, let's see if we can find a matrix P for which AP which AZ will do PVP in verse and then go ahead and do a linear algebra calculations to show that there is no such pee but and I actually encourage you to do that. That would be a fun example for you guys to work out but actually will see on the next slide a good reason a good understanding of why they are not similar.  So here's why.  If I have two similar matrices, and I'm not saying what time is diagonal or not? Like we saw in the in the case of of a matrix that has the bases of eigenvectors, but if I just have two similar matrices that is the a sequel to PBP and rest for some invertible Matrix P. Then they have the same eigenvalues.  And the eigenspaces for those eigenvalues have the same dimensions.  Hey, this is what's really important about similarity transformation.  It's two matrices are similar if I can get to one from the other from one to the other by conjugating by P by x p on one side of the Universe on the other that thing changes the Matrix, but it doesn't change the eigenvalues eigenvalue. Stay the same and more over the eigenspaces for those two for it for the eigenvalues on one side the other day I can spaces can change but what doesn't change is there dimensions?  So how does this all work? Why is this true?  well, so  let Lambda be an eigenvalue.  for the first Matrix a  and so  it's eigenspace.  Is the null space of a minor slammed attempts identity Matrix? No, I want to see what happens when I look at V instead. So let's take some nonzero Vector in there. So let's take some eigenvector V in that Mel space.  What does it mean to say that as an eigenvalue for Landon eigenvalue for a with eigenvector V A V is equal to Lambda V?  now A and B are similar, so that means that b is equal to P A P inverse for some invertible Matrix p  but as we saw in last night, that's the same thing as saying that a is equal to P inverse b p  So putting that in here this says that P inverse VP V is equal to Lambda V.  now I'm going to multiply both sides by P on the left that says that BP V is equal to Lambda p b  but hey, what is that say that says that this Vector here.  PV  is an eigenvector  Abby  with eigenvalue  weather  and that basically is the whole story here if I have two messages that are similar by a matrix p  If I have an eigenvector V with some eigenvalue Lambda for the 1st.  Then that I can value Lambda is also an eigenvalue for the Matrix B and its eigenvector isn't the original eigenvector V. It's p x fee.  Poppy is an invertible Matrix. It is in the language. We use last time an isomorphism. Hey, it's a one-to-one and onto linear transformation. It preserves all linear properties of vectors and so in particular if I had a basis of three eigenvectors for this eigenspace before and all I'm doing is I'm transforming them by linear transformation given by P. Then those three factors transformed to a basis of the new eigenspace.  So that means that well the vectors in the eigenspace change. The number of the of them that are linearly independent doesn't change.  So that's what similarity does for you two matrices are similar what that really means is that they have the same eigenvalues and they have the same profile of eigenspaces. The eigenspaces aren't the same as the original Matrix, but they look the same. They're isomorphic. They have the same dimensions as before.  So if we look back at this example down here.  Can we look back at the calculation? We did near the beginning of the lecture.  We saw that both of these are both these matrices had the same eigenvalues. The only had one is an eigenvalue. But the first one a the eigenspace for that I can value had to mention one in the second example the eigenspace for that when I can buy head Dimension to those are not isomorphic to have different dimensions and therefore it's impossible to find a matrix P for which a is equal to PVP in reverse.  So that's what similarity tells you tells you that the eigenvalues are the same and that the eigenspaces are isomorphic. They have the same dimensions. Okay. So we'll stop there for today and on Friday will continue with characteristic polynomials in general and how to find eigenvalues. See you Wednesday for mid-term review.  UC San Diego podcast ",
  "Name": "math18_b00_wi18-02262018-1000",
  "File Name": "lecture_20.flac"
}