{
    "Blurbs": {
        "2 p.m. Worldwide dates are hard because people have chosen lots of different crazy ways to specify dates for automatically will talk about the details later. So a boy that they sell in spreadsheets you have ID you have date and your glucose the presumably this person's glucose for this ID value was taken on some days are like that shouldn't be missing data. So the Assumption we have to ": [
            2704.4,
            2728.9,
            110
        ],
        "50 to 75 range. But we have a spread across all of these. I would gather that if you didn't know you would probably guessed because it's in the middle of the range and that's a safe. Guess these people tweet about this all the time data scientist talk about it all the time. And that is at 80 to 90% of their time is working on getting the data ": [
            2259.7,
            2279.8,
            89
        ],
        "8 ball it is a lot of fun, but it's usually a lot of money to go. However through the school we do get passes for 90% off. Yay. So exciting right lots of energy in this room guys. Anyway, it's usually $50 a person do the school to $10 a ticket in each ticket gets to people in so it's only $5 a person this includes everything you need ": [
            1.9,
            29.9,
            0
        ],
        "API to access those Sobe is also correct. I'm going to disagree with web scraping because the other to exist and they are easier and then manual curation as this is the last one is always a possibility by Manuel curation. I mean, you can go look for the data and then look at the article and then type in the information yourself or copy and paste but that is ": [
            2065.9,
            2088.3,
            84
        ],
        "I'll give you a few more seconds. 3 2 1 alright, so we have lots of people think B and fewer people saying D in the movie suicide so you can see it going to explain their thinking and choosing their off their answer. Okay. So the argument here was for choosing D because you if you hear you clarify that your unit is pounds and maybe pounds universally would ": [
            2874.7,
            2916.2,
            116
        ],
        "Then you have to say what day do you exactly want and then you have to promise them that you are who you say you are and then demonstrate that you are who you say you are and that's the third step. Simonton hetp because these are the types of methods that you can use to access information from an API. These are some of the most popular methods so ": [
            909.4,
            931.6,
            38
        ],
        "a result of this printer pole. That's not just the quick 51% So you can understand it much more deeply when you do something like this. This is the sentiment analysis which just captures what we talked about previously for sentiment analysis. You can take all of the words and all of the tweets related to this you can be overwhelming with the words had a positive sentiment which makes ": [
            1382.6,
            1403.1,
            60
        ],
        "a small capability of API today in the context of getting data. If you guys are ways for software and applications to talk to one another so their rules for interaction. For purposes the way to access an API is to First choose a method. We're going to watch city sets then build a URL and then get authorization. They have to determine how you're going to get the data. ": [
            882.5,
            909.4,
            37
        ],
        "a very observation for each variable should be stored in a different row. So this row includes information for a single observation and that's consistent across the row. First rule variables in Collins second alterations in Row. The Third is that there should be one table for each type of data. These are the same two types that I showed. The 4 1/4 is 1/10. You have more than one ": [
            2507.6,
            2531.2,
            101
        ],
        "a website first. All right, See you in data science is don't reinvent the wheel if the information is already out there don't work harder than you need to to get it. So always start by looking for available datasets. If those don't exist, your next step would be going to an API and then only go to web scraping if it's allowed and if you can't get it from ": [
            1698.1,
            1717.8,
            75
        ],
        "about what stored in it. You will see this all the time and date in the real world. And then it's your job to go figure out what information is stored in there or if you just had the helpful variable name that would save you a lot of time. Dates are hard workers programmatically. We'll talk about this at some point. But there is a standard and is the ": [
            2664.7,
            2684.9,
            108
        ],
        "all be really good at it. And I think right now since data science isn't evolving feel that we don't always teach it but we expect everybody to know how to do it. So that's why I'm spending a number of a bunch of time talking about how to take data that looks something like this which we're going to all know with Messy in a second and then why ": [
            2336.3,
            2354.4,
            93
        ],
        "all of Twitter's I did it. So we have to do for this if you have to go to their API documentation and there are Developer documentation for lots of different popular websites and you have to go through all of this and figure out how to specifically specify the information he wants you have to know what information you want and then you have to use the documentation to ": [
            994.6,
            1015.8,
            42
        ],
        "an open Beta index. So there's no shortage of data that are found in tabular format in spreadsheets that can be retrieved from the internet. But what if the data aren't in a spreadsheet ready and waiting for you? example block information on Twitter people write lots of things people make polls. So there's this data scientist at Stitch fix named Hilary Parker and she posted this pole. And so ": [
            749.0,
            776.2,
            31
        ],
        "and a fact that you can use a provider's API to get data that you wouldn't have had. Otherwise we talked about the basic methods for getting that information so briefly, but would you use this for the return back to that example I started with this is a simple example where you have a very specific question. Just what is the larger context around this question. So if you ": [
            1252.6,
            1275.4,
            54
        ],
        "and get into a usable form at this is a spreadsheet that came from the Australian government and this is a blog post from so many miles in pain and he showed the fact that you have information kind of all over the place. So you have his header appear that doesn't really include data that you'd want to analyze and then you have what you actually want in your ": [
            2354.4,
            2372.4,
            94
        ],
        "and look for these types of information and will then use them to their advantage and to your detriment. Okay, so these are secrets secrets So that covers the getting authorization part. So now we know that we're going to use the get methods to get information. We're going to build our URL that's going to return us some Jason information and we can get that information after we authorize ": [
            1187.4,
            1209.7,
            51
        ],
        "and scanned it using an app or you took pictures of your computer screen. Make sure you refer back to the slides at the end of the third lecture to see how to submit it as a PDF or stop by during office hours and make sure you know how to do that going forward. That's it for the first assignment you will get feedback by this weekend and you ": [
            372.7,
            393.0,
            14
        ],
        "are getting data from Twitter all the time and using this answer questions that are much more in-depth than the simple example, I explain here They're also research project that go through GitHub and take people say online jupyter notebook and analyze how people are using them and what's included in each of them. So getting a sense of how people are analyzing data left an example of how you ": [
            1423.8,
            1445.3,
            62
        ],
        "around to avoid cells or stuff missing so I could data set is a rectangular data set and these are the easiest to work with programmatically. So the goal is to take those messy datasets and get them in this format. Any questions about the general principles of Tidy data and what it means to take a messy dataset into a tidy format. Okay. Their number of rules for good ": [
            2571.0,
            2598.9,
            104
        ],
        "as a gift is $50 value right on the ticket. Once again, we get a limited number of these passes in the beginning of the semester and about first come first serve until we sell out. It's for giant paintball. There's a lot of fun $10 for one ticket gets two people in and you have two years to bring anybody you like on any day of the week like ": [
            46.8,
            64.2,
            2
        ],
        "at Vanderbilt biostats apartment and we have here is a visualization of all of the priests that happened in response to this initial sweep. Sweep circle is a tweet. I'm right here is the initial sweets. You can see their number conversations that were spawned off of that first and then there are some long branches were there was a lot of conversation that happened and this is an interactive ": [
            1296.2,
            1318.6,
            56
        ],
        "back to the data and getting rid of anything that you're not supposed to have are not supposed to be using. Okay. Lots of stretchy talk we discussed basics of apis and HTTP methods we talked about getting information from an API. We walk through a quick example. The last know what if there's no APR you can't get that do you want from it? This is where the case ": [
            1550.3,
            1572.1,
            68
        ],
        "can ask an interesting question. You can likely get data that are of interest to you. Okay, so I feel like bananas course I'll sound like a broken record on this the common theme of Jessica is because you can get the data doesn't mean you should get it or use it on their terms and agreement that we use for all of the apps that we use in all ": [
            1466.9,
            1491.4,
            64
        ],
        "class that's fine dancer here is because you would want to as we said put the unit up in the column header. So I wanted to have some discussions are the best option would be you just let the values for your specify the unit in a column header. Okay, we're not going to do that one just as I'm taking data that are a mess and getting them into ": [
            2996.5,
            3014.7,
            119
        ],
        "columns over here and I assure you of groups that you would actually want that to be a column itself there lots of missing spots in his junk down here. This is something that would not be very easy to work with. Once you read it into python live process of taking it from this untidy format to a tidy data format, which we're going to Define is through the ": [
            2372.4,
            2391.8,
            95
        ],
        "consistently from somebody who already has seen your question before and can help guide you throughout it. And that's why we're going with infection. You should know your point of contact. I'll monitor Grading fairness across graders and do what that should anything arise. They're all using a rubric to grade but they're providing you feedback specific to your project. So you can incorporate that in your next assignment in ": [
            258.3,
            280.4,
            9
        ],
        "data and make some basic visualisations. That's the goal of what programming I want you to be able to do. I also have contact information. I'm in the second bullet point to hear from the person who made announcements at beginning of class last lecture. EXO as everybody is settling I want to recap the first assignment y'all did really well submitting. I think only three people didn't submit a ": [
            194.0,
            217.4,
            6
        ],
        "data then we'll be talking a little bit about wrangling date of going to start on that topic will finish it on Thursday, and then I'll cover a little bit about the exam to help you that all figure out what you should study what you should be focusing on. So quick reminders as per usual assignment due this Friday is a reading assignment. The first exam is a week ": [
            148.1,
            172.6,
            4
        ],
        "dates or names or specifics. I really want to make sure did you get the big Concepts from the readings note that while tidy data and data wrangling at the topic or covered? I won't be asked anything specific from that reading because you haven't discussed it in section yet. You should understand the code for that in class. The best way to do it is go back to the ": [
            3138.7,
            3158.6,
            126
        ],
        "determine from the documentation from the provider. We're going to get that data back and Json format. But before we can get that data back, you need to authorize yourself. So you need to some way to say that you are who you say you are. I did do that. The most common way is using a Wawa store open authorization. There isn't going through all this is so that ": [
            1089.5,
            1111.5,
            46
        ],
        "do with data. It's a good place to start. What do you think data from 5:38 in the course? They are referring to be unfamiliar with 538 a new source, they cover politics and they cover Sports and they cover science and health and they release date us at the high some of their articles and those can be accessed online. I mention will work with some of this this ": [
            661.3,
            682.9,
            27
        ],
        "do you like getting Google Calendar invites from your friends for lunch is cock etcetera and then the other options were absolutely not a mess. So there's some disagreement among the respondents and there were twenty-five hundred votes because there's also a lot of discussion that happened so there were 40 replace of this and then 20 retweets and then conversations that happened off of the retweets and it's not ": [
            776.2,
            796.2,
            32
        ],
        "either biordi is not the right answer? Yes. Okay, so be this is easier to calculate on because it's just the values and then D the option here. It's hard because it has the units. So how do we decide between B&D? So who wants to make the final call on be? if I show hands anybody for be Severity so anybody for big for anyone predict? Highway asleep in ": [
            2954.2,
            2996.5,
            118
        ],
        "far. What are we talking about getting data? We're going to be very generally discussing what an API is and how it works. And then we're going to start talking about where you can look for data and get interesting data sets. To review where unit definition that data are anything that can be stored in a computer. I've been using this term tabular data and spreadsheets and I've been ": [
            571.7,
            600.3,
            23
        ],
        "figure out already the same for the same people across the date of that says it's a different number that they've used those really were they use consistent labeling and that use consistent number and when you are storing data, What type of tiny data and spreadsheets tidy data or rectangular what I mean? Is that all of the data fit into a rectangle and I don't have to loop ": [
            2549.8,
            2571.0,
            103
        ],
        "figure out how to get it. And there are different ways. I'm at each of the API. So here we have Twitter and then Google's API and then get hugs API and we should know is that these versions change often? So you may have this really great script that gets all the information you want from a source from a provider and then they change the API and you ": [
            1015.8,
            1037.1,
            43
        ],
        "first have to tell in this case GitHub who you are and retrieve authorization so that they know I can't control who's getting what from their on their database. You then create an Olaf application from this you will get with known as a client ID and a client secret neither would have provided by GitHub or Google or Twitter or whatever you're trying to get the data from. After ": [
            1126.9,
            1148.6,
            48
        ],
        "from formatted into the form. They can actually use it in and it said hopefully improve over time. So if more people understand how data should look then they can get it right from the beginning and you as a data scientist can spend less time working on this part of her right now and for the past 10 years, this is what it's been. So here's that other caveat ": [
            2279.8,
            2297.1,
            90
        ],
        "from today. It is multiple choice. It will happen in the last 30 minutes of class. And your second assignment is due next Friday. Just so you all can plan ahead. There are programming covered in three lectures. That is this Thursday is next Thursday's and Thursdays after lecture. So my goal at the end of that is you can all read a dataset in and you can Wrangle the ": [
            172.6,
            194.0,
            5
        ],
        "getting rid of deleted tweets from your analysis before using them for research. They distinguish between research and All of this is covered in the developer agreement for Twitter and it changes. So when you're using these types of data for project, it's up to you to make sure you're in agreement with their Twitter policy or whatever API are using their policy. You're following the rules and you're going ": [
            1530.3,
            1550.3,
            67
        ],
        "graphic. So on this website, you can hover over any point and see the conversation that happened so very quickly can understand all of a conversation that happened and if I were to summarize this briefly hear there was in the beginning some wording clarification. So some people remember the question was do you want somebody else to send you a Google invite for coffee or lunch date? Carrots, he's ": [
            1318.6,
            1341.2,
            57
        ],
        "has rules on how to handle it. If you go and take a ton of data to answer your super interesting question. And then what is half of those users that you've taken that day that been deleted all those trees. Is it okay for you who know I have these data to continue to analyze them despite the fact that somebody else deleted those so they have rules about ": [
            1511.5,
            1530.3,
            66
        ],
        "have something really interesting for my project most of your familiar with casual. There's a very similar platform called date of that world data is plural is a fetal Google sheet filled with more than 600 datasets across a ton of different topics. They have policing information. They have politics information they have health information. So if you want to search through there for any project you never have to ": [
            639.4,
            661.3,
            26
        ],
        "have to be good at it over time. So this is a quote from DJ patil. He was a chief scientist under the Obama Administration and the first Chief data scientist, and he said that good data scientist understand it deep way that the heavy lifting a cleanup and preparation isn't something that gets in the way of solving the problem. It is the problem. This means that we should ": [
            2315.2,
            2336.3,
            92
        ],
        "have to redo everything so know that the documentation are invaluable and a source of frustration when you're working with apis. So so far we talked about the fact that you were in use get to retrieve the information. That's pretty the method we used. We're going to build a set of URL that specifies the data were going to retrieve and Windows data are returned to us are most ": [
            1037.1,
            1063.0,
            44
        ],
        "here. I'm anybody would explain why they chose be. Okay, so the conversation there was I mention that it's available but I did say through which Avenue and that was somewhat intentional so that I could ask this question later on and we could have this discussion so you can directly download the data sets from data. Fivethirtyeight.com. But they also host them on GitHub. So you could use an ": [
            2032.6,
            2065.9,
            83
        ],
        "if you have to slide earlier than an hour ago will not be in the size you downloaded but it is up there now. I'm at with ar Library liaison this morning and I learn about a new resource and the library has at UCSD a lot of data sets by Topic in the link is down here. So you can search for data from now Austin or linking to ": [
            682.9,
            702.4,
            28
        ],
        "if you want to get information which accomplishes reading information from a source, this is a method we're going to be focusing on today, but I know that there are other ways in which you can interact with an API, you can create one create a new resource. You can update you can get rid of it. And these are the methods that you would use on the left to ": [
            931.6,
            950.9,
            39
        ],
        "incredibly important day. I talked about already make sure that your variable names are consistent from one table to the next but also within a variable or variables should be coded the same way. So here if your you have the options of female and male they should be Mount you shouldn't write lowercase female for one uppercase email for another and an F for another individual. It's much easier ": [
            2616.5,
            2639.4,
            106
        ],
        "information about the number of cylinders for a vehicle. That number is limited even though the value it's really a categorical data location to take this example cuz I don't know that much about cars, but you can have four or six cylinders are probably another value but there are only a few options. Are there any questions why something that is a number like cylinders would be categorical and ": [
            503.2,
            525.0,
            20
        ],
        "is no easy spreadsheet to get you have to go retrieve it from the API. All right. next one You want to analyze where geographically calls to police have occurred in Albuquerque, New Mexico same options? Closet in a few seconds 3 2 1 All right, so overwhelmingly people said available data, but BNC were possible options here who wants to explain their thought process here. I'll tell you that ": [
            1867.9,
            1925.5,
            79
        ],
        "is not to trick you or to deceive you in any of these. So when you understand the topics, these are the topics covered we're through seven. So we'll finish 8 on Thursday. I do want you to think during the exam right? Like I don't want it to be so obvious. The question is this guy looks like the sky and you say blue. How to study the first ": [
            3095.2,
            3117.5,
            124
        ],
        "just wanted to clarify this a little bit. The reason I want both is because when you make visualisations, they differ based on the type of variable you had I want you to experience with both so you need some sort of number and you need some sort of categorical value. I'm going to clarify a slide I had nv800 lecture. The Cowboys are quantitative and qualitative quantitative are numbers ": [
            454.4,
            478.7,
            18
        ],
        "line of defense for your setting should be the lecture material that can be your notes. It can be you re listening to the podcast. It can be reviewing clicker questions vast majority of the questions on the exam will come from what was discussed in lecture. The readings are fair game. Only the first two readings will have questions specifically from them, but I don't want you to memorize ": [
            3117.5,
            3138.7,
            125
        ],
        "lot of information or to make you pay which makes sense. So if you post these after getting them on the Internet, somebody else can take them and then charge your credit card for all the data that they just got for free and you just paid for so we talk about get home in previous lesson should never be stored on get how people write crawlers that go through ": [
            1169.0,
            1187.4,
            50
        ],
        "make is that this date still down here and this date still down here and so forth. The Raven make those assumptions which could be incorrect what if the ordering changed of this it's important to explicitly State what information is in their similarly. This is example of what case worth not rectangular cuz if I were to go around the outfit this week to have like camel humps or ": [
            2728.9,
            2749.2,
            111
        ],
        "make these requests since they were just going to focus on getting information from an API. So the only method what are you talking about today is get So when you're sitting at your computer and decide that there's not a day to sit out there for you, but there is information after that you want to get you would use these HTTP Methods at your computer to then connect ": [
            950.9,
            972.2,
            40
        ],
        "make you apply Concepts. We asked without everything being obvious so that everybody got the 100 cuz then I have no idea whether or not we've actually learnt or learned anything and if I taught anything BB recall questions where you just have to remember stuff that have been presented to you though the application where I've caught something and then you have to apply the concept. Again, my goal ": [
            3073.8,
            3095.2,
            123
        ],
        "new nickname for UCSD. I got to solve a riddle and lots of you I had data science in your team names. I appreciated it among other creative things be grading is happening this week. Just so you all are aware. The way grading is happening. Is that the same TA or I will grade your projects throughout the quarter. This is the system so that you are getting feedback ": [
            237.1,
            258.3,
            8
        ],
        "not a website allows you to scrape their information and their ways to be that way. We won't talk about in detail. There are lots of popular platforms that will cut you off if you take too much information from you than at any one point in time so know that that is likely to happen if you don't look into whether or not it's okay to scrape information from ": [
            1679.5,
            1698.1,
            74
        ],
        "not be the way in which you would enter weight because most countries would use a kilogram. Okay, so that's one for anybody have a against d Yes. Okay, so we are even here is 4B that you just want the value in here and you can specify up here in the column maybe that this is town or somewhere else great. Anybody else have a thought as to why ": [
            2916.2,
            2954.2,
            117
        ],
        "not just Federal day that you're limited to cities more and more are rolling out open City-Data portals. And here's a list of a few where you can go in and get information about a city in particular. That summer is here. If you want to look at cities across and see what types of day do they have on that is available for the United States and globally there's ": [
            728.5,
            749.0,
            30
        ],
        "not quantitative. You need one of each in your project in the dataset. This is up here for your information. Just so y'all can plan ahead. This is what you'll be doing in section each week. We're currently in week 4, there was a Monday section that already met they were we behind so they did the reading and the programme practice. Everybody else will just get programming practice in ": [
            525.0,
            548.7,
            21
        ],
        "notebooks run the code changed a little bit anticipate what the output should be after you change it see if that happens that will really ensure that you understand it. I'll actually it's up on Triton head but it's also click link clickable on this link from the slides. There's an exam study guide. It's not comprehensive. It don't have everything on there. But it's a good place to check ": [
            3158.6,
            3178.6,
            127
        ],
        "of a spreadsheet. These are rose. These are columns. The information that are stored in columns are going to be variables in this case. So this is information you collect from each in his case person. So variables are stored in columns. An observation are stored in rows in this case. These observations are people but they don't have to be an everyday to say. So here we could see ": [
            2414.8,
            2440.3,
            97
        ],
        "of the software that we use and we just kind of like go through and click and say I will follow those when you're getting data from somebody else that you can use in either research or your own for your own purposes those agreements matter a lot more and it's up to you to make sure you understand what you're agreeing to you. Whenever you're accessing an API example ": [
            1491.4,
            1511.5,
            65
        ],
        "of web scraping comes in. We talked about this a bit last lecture and this is from last lecture. We talked about that phone flies example where they had the day and then the quote and then the medical explanation as to why it was an untruth with a URL in it. Where you at scrape is and this is just the basics of it behind everything. You see on ": [
            1572.1,
            1592.5,
            69
        ],
        "often in Json format and we talked about this last lecture. So Jason santor JavaScript object notation and just as a reminder, Jason is returned in key value pairs between phrases with colon in between. I told you we're going to get stuff. So that's the issue of him HTTP method we're going to use and then we specify what we want to get using a specific URL that we ": [
            1063.0,
            1089.5,
            45
        ],
        "often stored in spreadsheet. So I'm saying tabular data or spreadsheet data. Those are the same thing. These type of data can be found in lots and lots of different places on your first assignment you were directed at a few places. I'm going to mention a few more today. You're welcome to change your data set. If you look at one of these sources and say hey, they might ": [
            619.5,
            639.4,
            25
        ],
        "on Thursday. Thermostat says the question is what percentage of a data scientist time is spent preparing data that I can be used to answer an interesting question. And that is the definition of data wrangling getting it so you can answer the question you want. Give me a few more seconds. 3 2 1 okay. So here are the results a lot of you said we're somewhere in the ": [
            2205.3,
            2259.7,
            88
        ],
        "on Thursday. UC San Diego podcast ": [
            3199.2,
            3210.5,
            129
        ],
        "on anything I talked about this far and I presented in the beginning. I already did everybody like us 30. Second minute break talk to each other and then I can have everybody Focus back in cuz I know this is a lot of lecture so you guys have a minute to talk to each other and get it all out and back in. All right. For the second half ": [
            2109.1,
            2184.8,
            86
        ],
        "on so I called you before continuous is trying to avoid using quantitative and qualitative cuz they are such similar words and they don't actually conjure up exactly what they are for everybody. But we'll use these quantitative think quantities of some sort of number qualitative think some sort of category. The reason I'm asking is because let's say you were working with data about cars and you had some ": [
            478.7,
            503.2,
            19
        ],
        "other places, but there are some proprietary data sets that you would only have access to with your UCSD login information. This is another great resource for Topix. After that, Dana. Gov is now up and running you can get information from here. Now that the government is open and the US Census provide swapping information about individuals in the United States that are collected from the senses and it's ": [
            702.4,
            728.5,
            29
        ],
        "people are charging and rather than going to all of the websites and clicking on them individually and I figure now and copying the numbers down manually and entering them in a spreadsheet and then looking all them. This is a case where you would use web scraping to do that systematically for you. Same disclaimer web scraping sound like okay. It is up to you to determine whether or ": [
            1656.7,
            1679.5,
            73
        ],
        "process of data wrangling. So I talked about in the programming lecture what the differences are between a data scientist in a computer scientist when it comes to the actual position and data scientist have to be really really good and quick at working with these tabular data. Imma, spend a little bit of time talking about the terms that were going to use. So the first here's a snapshot ": [
            2391.8,
            2414.8,
            96
        ],
        "project or an assignment the first assignment which is good. And those three people I'm not sure are in the class of that scrape. Y'all did really well. I've been where with what I've seen so far. I've gone through look at your question in just looked at your sources for data says I also appreciate a lot of your team names. So in your team name, I learned a ": [
            217.4,
            237.1,
            7
        ],
        "prone to making errors cuz we're human and the computers are better at making fewer hours. As long as we tell them the right thing. The manual puration is not going to be the idea for your creation of any dataset, but will sometimes be necessary. I'm so it's not the answer to any of these just because there aren't any of our three 1st Avenues. Any questions so far ": [
            2088.3,
            2109.1,
            85
        ],
        "ready a few more seconds. 3 2 1 All right, so vast majority of you said available data. Second most popular with a TI and then some people said manual tration are who wants to go with what explain why they pick the option that they did. Okay, so funny, but you didn't hear the argument was for using the API because if you're making commits to get Hub you ": [
            1803.8,
            1848.9,
            77
        ],
        "real tidy data is that each variable should be stored in a single column. This may seem obvious looking at this dataset when you look at other people say they said you'll see that this is not necessarily the case and we'll come back to that and examples of adding a second last name first name one piece of information stored in each column for each variable. The second is ": [
            2485.7,
            2507.6,
            100
        ],
        "really easy. Just looking on Twitter to get a sense of what everybody said. So you don't have the data to answer that question. If you really want to understand the full scope of a question, you have the initial though, but that's it out of plants. So what do you do if you can use available methods to get that information for yourself? So I want a very high-level ": [
            796.2,
            819.2,
            33
        ],
        "request goes to a website and then the website is displayed. So if you can get websites using a sweater called can we get other data or information you would want from the internet? And obviously this is in a lecture you can and the way you do. This is using application programming interface or a p i can do a whole lot and we're only going to focus on ": [
            861.2,
            882.5,
            36
        ],
        "same as an API be ready to go courses later on and learn a lot more details me like she didn't really tell me the whole story and that's true. Okay, that's the morning transfer protocol. This is a way that allow messages to be sent on the internet. So you all do this every day you sit down at your computer you type in the URL and then magically ": [
            838.8,
            861.2,
            35
        ],
        "section this week. You always ever even doing discuss the readings the week after you submit the reading quiz, and you'll get time to work on your projects as time permits infection. This is just for y'all to plan. Whole lot of housekeeping staff today whole lot of reminders good work on the first assignment so far get ready to your second assignment any questions about anything I said so ": [
            548.7,
            571.7,
            22
        ],
        "sections that are not collapse are those we have to add new information where you have to have to read your data set in we have to Wrangle the data set on the mission. You can change edit update your question you should do so if you get feedback telling you from your TDA or is that your questions not enough you can find or use a different way to ": [
            412.3,
            431.1,
            16
        ],
        "set you find something that will work better you find another day that they will complement your first date is that you're welcome to add that in on your second assignment. You must include feedback and programming is required on this assignment and the third is I'm not the 4th assignment. Okay, Army. Simon says that your day is that has to have variables that are numerical and categorical. I ": [
            431.1,
            454.4,
            17
        ],
        "something on it. This is an example and in the morning you'll have if something that's untidy that could be entered in a tidy format that's easier to work with. Schedule, this is Imagine for this is one of the title principles just one thing in a cell is over here on the left. We have one value of a person's weight in pounds on in a Cell over here ": [
            2749.2,
            2769.6,
            112
        ],
        "spreadsheets. This is also part of your reading for this week. We're going to walk through the examples of the seven of these now and again keep these in mind again. I'll show you obviously examples but this happens all the time in the real world. So it's really important to understand these ahead of time to know what to look for. So consistency is the first rule if it's ": [
            2598.9,
            2616.5,
            105
        ],
        "standard you should try to use the iso 8601 is the way to approach it where you have four values for year to four months and two four digit in that order. So no other crazy way in which you can enter date or time should be used and reason the dates and times are difficult to work with is because time zones exist. So 2 p.m. Here is not ": [
            2684.9,
            2704.4,
            109
        ],
        "talk about HTTP and apis today and I'm going to compare this to my undergraduate for I study biology and I learned a lot of things and then I got to grad school and learned that almost everything was taught with kind of true but there wasn't there a lot under the hood that I wasn't taught in a lot exceptions all the rules. I told Todd was hot the ": [
            819.2,
            838.8,
            34
        ],
        "that format is called Data wrangling will talk about the Technics techniques used to do that next lecture, but I'm going to spend the last few minutes talking about the first exam. Carfi 30 multiple choice questions last 30 minutes of class a week from today. I wanted to give you all a thought after the process in the way that I develop exam. The Bible and development exam is ": [
            3014.7,
            3036.9,
            120
        ],
        "that having kids makes your mind. Goofy. There is over here at the point that I have anything invite you're not going to have confusion about time or place. There's some discussion about who uses to do managers hear people talking about paper journals, and then there's a soft-shoe talking about Microsoft Office Products and wedding tangent about somebody's wedding. All of this, you can quickly understand what happened as ": [
            1360.5,
            1382.6,
            59
        ],
        "that we have four observations. So seven variables seven columns for observations for Rose. And I'm going to talk about types of data here. For example, you might say that this information was information collected from somebody went to the doctor's office and they had to fill out a survey and give this information to the doctor. This may have been measurements taken at your visit that was taken by ": [
            2440.3,
            2464.5,
            98
        ],
        "the Tidy data paper from Hadley Wickham This is just so your notes are complete. I won't walk through all of these now. I'm going to have you all do one of the two here. So the first one take a look carefully at these spreadsheets and it's up to you to determine which of these spreadsheets is there fast. Feel free to talk to each other and discuss why. ": [
            2812.0,
            2828.5,
            115
        ],
        "the initial resources. So we're going to do a few clicker questions and have discussions about this. One second. Is it open? Christmas Eve one second not working yet. You guys can't click and right kind of pain. Okay, reminder frequency code is AD that's on the board. A croissant even super busy on GitHub and you want to write a blog post summarizing what you've been up to? Get ": [
            1717.8,
            1803.8,
            76
        ],
        "the internet. There is HTML behind it. And HTML is characterized by these tags that open with these carrots and an end with a carrots with a Flash and you use these to specify what information to pull from a web page. That's like the highest level of web scraping. There are python packages to help you do this. There are packages help you do this and you can use ": [
            1592.5,
            1616.2,
            70
        ],
        "the majority got it right on this one. Anybody would defend their thought process there. What is the date of the data should be available should not Diamond. So I'm going to disappear little bit but agree on the second part of what you said. So I left out a key piece of information here. Is that Albuquerque New Mexico has and open data portal the data are available through ": [
            1925.5,
            1956.5,
            80
        ],
        "the most people felt positively about this but there was a faction that had some negative feelings about this. Okay. So the first one I didn't use a simple example here of how you would use to reduce API to get a very limited set of Jada but social scientists are using Twitter to ask very interesting questions about how we work with each other and a social network. They ": [
            1403.1,
            1423.8,
            61
        ],
        "the nurse or by The Physician and these are two different types of data because this came from is so very and this came from the physician during your meeting. Okay, so adorable store in Collins operations, Jordan Rose and different types of data. Generating about this this week for your meeting. Whoever hasn't done that so far. You'll get more details on this for their four rules. The first ": [
            2464.5,
            2485.7,
            99
        ],
        "their portal and that's what you would access and that would be easier than b or c because the data exist. the Skip One Night last one I just had a super interesting article on 5:38 and I want to explore the data myself is the best place to start. Are closing in a few seconds? 3 2 1 All right. We had some disagreement here who wants to defend ": [
            1956.5,
            2009.9,
            81
        ],
        "their position here. What were your thoughts on choosing the choice to be made? Okay. So the argument here is for a available data and the statement made was at 5:38 post all of their data online. I'm going to put a slate patio on that. They don't make all of their data available that they make a lot of their data available. So I agree that a is possible ": [
            2009.9,
            2032.6,
            82
        ],
        "their self and then it's up to you to process their spots. So you can see the first part of this lecture was about getting data from a source or you just get your tabular data right away. This one takes a lot more steps and of a lot more work, but you can get data that you wouldn't have had otherwise, so just to recap the same process that ": [
            1209.7,
            1229.5,
            52
        ],
        "these other valleys. It might be a mistake rather than using font color. It's best to have another variable that says hey this one might be an outlier. Is included for you to look through I won't go through all of this, but it's a summary of everything. We talked about and better approaches to bad naming. I have this in here to summarize your points that are made in ": [
            2788.0,
            2812.0,
            114
        ],
        "these to go and get the data you want from a website. So why would you go ahead and use that? What if you were a real estate agent and you're new to a neighborhood and you want to figure out what's going on in your neighborhood there are lots of websites where you go for information like Zillow or Redfin and there are lots of others and if you ": [
            1616.2,
            1635.1,
            71
        ],
        "this. So I had to put dookie on a slide and you should make sure next time that your group members have been added. I added them this time. I will not do that going forward you make sure your group members are added on Crisco. Okay, most of you this is not apply to if you did not submit it as a PDF if you print it out something ": [
            351.2,
            372.7,
            13
        ],
        "to and retrieve the data that are stored somewhere else. So nothing ever going to be using is get that's going to retrieve it. Now you should build a set of URLs that are going to return the data. So you had to tell the the place where the data are stored. What day do you want? You can just ask for everything and get all of the information in ": [
            972.2,
            994.6,
            41
        ],
        "to play for the entire day. The only additional costs is that you have to purchase paint which is very inexpensive. These tickets are also valid for 2 years. I know everybody in here is very busy with school and work. They must be part of this school to purchase these tickets. However, anybody is more than welcome to go to bring family and friends. You can even give it ": [
            29.9,
            46.8,
            1
        ],
        "to summarize information if everything is typed consistently Plus you so choose good names for things there rules about avoiding Extra Spaces. So programmatically mail without a space is different than male with a space underneath of it underscores should be used instead of spaces and meaningful names of this comes back to our variable naming. So this is a better column header than F1 which doesn't give any information ": [
            2639.4,
            2664.7,
            107
        ],
        "to take everything we've learned. I take a few questions from each and so I wanted to be balanced across the material that we talked about. There will be a similar number of questions from each topic we talked about so like I said that out the first and then I go check it afterwards. So I'm not just texted testing 1 topic. I am not out to get anybody ": [
            3036.9,
            3054.4,
            121
        ],
        "to this is that we spend eighty 90% of our time trying to get the data in the form we want. And we don't get to do the more interesting part at the end that said since a tarantula as a big part of the job. You have to be good at it. You have to not hate it cuz you'll spend a lot of time doing it and you ": [
            2297.1,
            2315.2,
            91
        ],
        "two boxes are people saying why wouldn't that be the first point of contact but if we had like discussed it and then they sent me it to verify that's great for some clarification happened here. There was this tweet about people having very strong feelings about online calendar etiquette, which is something that lots of people apparently have very strong feelings for down here some discussion about the fact ": [
            1341.2,
            1360.5,
            58
        ],
        "type of data on more than one table. You need a column in each with the same column label that allows them to be joining merged these might seem obvious looking at the spreadsheets were showing right here, but I have seen countless examples where this would be Capital ID in one and then over here this week, maybe being member underscore ID and then it's up to me to ": [
            2531.2,
            2549.8,
            102
        ],
        "using them interchangeably the purpose of this course. Those are the same. We're really going to discuss what type of data are today and I've mentioned that these are the types of data that day of scientists work with most often. These are data that are what you would store in Microsoft Excel or what you would store in Google Sheets their information entered in rows and columns and they're ": [
            600.3,
            619.5,
            24
        ],
        "want to understand all of those Reese's retweets and responses You could use what is API to explore this so you can ask Twitter for all of the responses to the street all of the retreat's and all of the conversations that happened as a result of those retweets and you could visualize them. This is done by losing to get dicey know who is a form of 50 students ": [
            1275.4,
            1296.2,
            55
        ],
        "wanted to get a sense as to what's going on with the houses being listed in your neighborhood, you could use web scraping to get a bunch of information quickly from all the different sources rather than relying on anyone individually. Or you can use it for your background research. If you decide you want to start a car dealership, you would want to get some information about what other ": [
            1635.1,
            1656.7,
            72
        ],
        "we use for getting website is the same idea. We use for making a request to an API. You determine your method you then interpret them at the API interprets the method and gets the information you request and then it's up to you to process what you get from the API endpoint. Okay, so far we've talked about tabular data loss of sources for getting that type of information ": [
            1229.5,
            1252.6,
            53
        ],
        "we're going to do the first star. That was very East Coast 2nd half second half we're going to be talking about tidy data and data wrangling. So this is the idea of how to make spreadsheet data at work for you rather than against you and how to get data that aren't good into a good format and we're going to clarify what all of that means today and ": [
            2184.8,
            2205.3,
            87
        ],
        "what your hands who would like to go? Okay guys. If you like a chicken, please raise your hands and I will come to you. Thank you. Thank you. I ever get settled. There are a number of things were going to come cover today. So we can talk about some housekeeping stop and a brief it about the first and second assignment. Were they going to talk about getting ": [
            64.2,
            148.1,
            3
        ],
        "when I write a dance, but I'm also not out to ask questions like is the sky blue and have one option b B12 be blue when I have to be 17 and then something else that doesn't make sense. So I need to test that were actually learning something and that everything's not obvious. So it's a balance between asking questions that make you recall information and questions that ": [
            3054.4,
            3073.8,
            122
        ],
        "while this looks like one piece of information really up to you have the value and you have the unit. It's much better to put the unit up at the top. Everybody's clear. This includes weight in pounds rather than having two pieces of information in a single cell. Don't use Font color and highlighting azada hear. The person is trying to say this value is way different than all ": [
            2769.6,
            2788.0,
            113
        ],
        "will have your grades for the first assignment by this weekend. The second assignment just to make sure we're all on the same page when you open the second assignment. It might look slightly different than before because there will be some cells that are collapsed. You can click on these and see all the contents music lapse just so you can see where you should focus your attention the ": [
            393.0,
            412.3,
            15
        ],
        "would be able to pull that information from the API and then be able to analyze it there. So all the information about everything you didn't get help would be accessible through their API. So on this one I'm going to argue that that is the correct answer. The reason that available data aren't there is cuz the days that you wanted probably doesn't exist as a dataset yet. There ": [
            1848.9,
            1867.9,
            78
        ],
        "would use an API to do the science of data science that we talked about and very simple you can also use these to organize your Google Drive account. So you can interact with it without clicking and dragging all the stuff you have to do on Google Drive. I just listed a few popular API say this is by no means exhaustive. There are lots of places where you ": [
            1445.3,
            1466.9,
            63
        ],
        "you are asking questions about sports the most popular which are the NBA and the Olympics Public Health was the second one with the most uncommon topic being gun control or gun violence. A lot of people are asking questions about the tech industry and employment in the tech industry number project about economics education entertainment and then a number of topics on being covered by fewer group. So if ": [
            302.3,
            330.5,
            11
        ],
        "you have a client ID and secret fuel generator token and this access token is what ultimately will provide you access to the API to get the data that you want and verify who you are who you say you are right morning. These should never be shared or posted publicly. Also with apis you have some amount of information you can get back to free but you want a ": [
            1148.6,
            1169.0,
            49
        ],
        "you want to see somebody else is working on a similar question or similar datasite be happy to put you in contact with another group. Okay, this is on Piazza really quickly here. These are the names of teens who did not put their team members on their submission their names were on the document but they didn't add them to breathe part of me thinks that you just did ": [
            330.5,
            351.2,
            12
        ],
        "you're familiar with the terms when you go to a tempest in the future. You have at least heard them but know that there will be steps along the way they will have to figure out when you go to do this when you apply for authorization. The first thing I have to do is apply on the website for the API on to what you want accent. Do you ": [
            1111.5,
            1126.9,
            47
        ],
        "your assignment this week. Should be incorporated in your submission next week. Very briefly. This is a summary of the topics. Y'all have chosen for your projects. So I just went through and took all of the project questions. You were asking and gave it a general topic so you can see all the different states along the x-axis and the number of projects along the y-axis the lot of ": [
            280.4,
            302.3,
            10
        ],
        "your understanding. It's broken down by lecture has questions from lecture from the readings from python. If you understand all of those that have a good understanding of all the questions and Concepts on there. You should be okay. We'll start with questions on Thursday that you have that any material. So feel free to bring them feel any questions about the exam. Those are great for Thursday to be ": [
            3178.6,
            3199.2,
            128
        ]
    },
    "File Name": "Introduction to Data Science - A00 - Ellis, Shannon Elizabeth - Winter 2019-lecture_7.flac",
    "Full Transcript": "8 ball it is a lot of fun, but it's usually a lot of money to go. However through the school we do get passes for 90% off. Yay. So exciting right lots of energy in this room guys.  Anyway, it's usually $50 a person do the school to $10 a ticket in each ticket gets to people in so it's only $5 a person this includes everything you need to play for the entire day. The only additional costs is that you have to purchase paint which is very inexpensive. These tickets are also valid for 2 years. I know everybody in here is very busy with school and work. They must be part of this school to purchase these tickets. However, anybody is more than welcome to go to bring family and friends. You can even give it as a gift is $50 value right on the ticket. Once again, we get a limited number of these passes in the beginning of the semester and about first come first serve until we sell out. It's for giant paintball. There's a lot of fun $10 for one ticket gets two people in and you have two years to bring anybody you like on any day of the week like what your hands who would like to go?  Okay guys. If you like a chicken, please raise your hands and I will come to you. Thank you.  Thank you.  I ever get settled. There are a number of things were going to come cover today.  So we can talk about some housekeeping stop and a brief it about the first and second assignment. Were they going to talk about getting data then we'll be talking a little bit about wrangling date of going to start on that topic will finish it on Thursday, and then I'll cover a little bit about the exam to help you that all figure out what you should study what you should be focusing on.  So quick reminders as per usual assignment due this Friday is a reading assignment. The first exam is a week from today. It is multiple choice. It will happen in the last 30 minutes of class. And your second assignment is due next Friday. Just so you all can plan ahead. There are programming covered in three lectures. That is this Thursday is next Thursday's and Thursdays after lecture. So my goal at the end of that is you can all read a dataset in and you can Wrangle the data and make some basic visualisations. That's the goal of what programming I want you to be able to do. I also have contact information. I'm in the second bullet point to hear from the person who made announcements at beginning of class last lecture.  EXO as everybody is settling I want to recap the first assignment y'all did really well submitting. I think only three people didn't submit a project or an assignment the first assignment which is good. And those three people I'm not sure are in the class of that scrape. Y'all did really well. I've been where with what I've seen so far. I've gone through look at your question in just looked at your sources for data says I also appreciate a lot of your team names. So in your team name, I learned a new nickname for UCSD. I got to solve a riddle and lots of you I had data science in your team names. I appreciated it among other creative things be grading is happening this week. Just so you all are aware. The way grading is happening. Is that the same TA or I will grade your projects throughout the quarter. This is the system so that you are getting feedback consistently from somebody who already has seen your question before and can help guide you throughout it. And that's why we're going with infection. You should know your point of contact. I'll monitor  Grading fairness across graders and do what that should anything arise. They're all using a rubric to grade but they're providing you feedback specific to your project. So you can incorporate that in your next assignment in your assignment this week. Should be incorporated in your submission next week.  Very briefly. This is a summary of the topics. Y'all have chosen for your projects. So I just went through and took all of the project questions. You were asking and gave it a general topic so you can see all the different states along the x-axis and the number of projects along the y-axis the lot of you are asking questions about sports the most popular which are the NBA and the Olympics Public Health was the second one with the most uncommon topic being gun control or gun violence. A lot of people are asking questions about the tech industry and employment in the tech industry number project about economics education entertainment and then a number of topics on being covered by fewer group.  So if you want to see somebody else is working on a similar question or similar datasite be happy to put you in contact with another group.  Okay, this is on Piazza really quickly here. These are the names of teens who did not put their team members on their submission their names were on the document but they didn't add them to breathe part of me thinks that you just did this. So I had to put dookie on a slide and you should make sure next time that your group members have been added. I added them this time. I will not do that going forward you make sure your group members are added on Crisco. Okay, most of you this is not apply to if you did not submit it as a PDF if you print it out something and scanned it using an app or you took pictures of your computer screen. Make sure you refer back to the slides at the end of the third lecture to see how to submit it as a PDF or stop by during office hours and make sure you know how to do that going forward.  That's it for the first assignment you will get feedback by this weekend and you will have your grades for the first assignment by this weekend.  The second assignment just to make sure we're all on the same page when you open the second assignment. It might look slightly different than before because there will be some cells that are collapsed. You can click on these and see all the contents music lapse just so you can see where you should focus your attention the sections that are not collapse are those we have to add new information where you have to have to read your data set in we have to Wrangle the data set on the mission. You can change edit update your question you should do so if you get feedback telling you from your TDA or is that your questions not enough you can find or use a different way to set you find something that will work better you find another day that they will complement your first date is that you're welcome to add that in on your second assignment. You must include feedback and programming is required on this assignment and the third is I'm not the 4th assignment.  Okay, Army. Simon says that your day is that has to have variables that are numerical and categorical. I just wanted to clarify this a little bit. The reason I want both is because when you make visualisations, they differ based on the type of variable you had I want you to experience with both so you need some sort of number and you need some sort of categorical value.  I'm going to clarify a slide I had nv800 lecture.  The Cowboys are quantitative and qualitative quantitative are numbers on so I called you before continuous is trying to avoid using quantitative and qualitative cuz they are such similar words and they don't actually conjure up exactly what they are for everybody. But we'll use these quantitative think quantities of some sort of number qualitative think some sort of category. The reason I'm asking is because let's say you were working with data about cars and you had some information about the number of cylinders for a vehicle. That number is limited even though the value it's really a categorical data location to take this example cuz I don't know that much about cars, but you can have four or six cylinders are probably another value but there are only a few options. Are there any questions why something that is a number like cylinders would be categorical and not quantitative.  You need one of each in your project in the dataset.  This is up here for your information. Just so y'all can plan ahead. This is what you'll be doing in section each week. We're currently in week 4, there was a Monday section that already met they were we behind so they did the reading and the programme practice. Everybody else will just get programming practice in section this week. You always ever even doing discuss the readings the week after you submit the reading quiz, and you'll get time to work on your projects as time permits infection. This is just for y'all to plan.  Whole lot of housekeeping staff today whole lot of reminders good work on the first assignment so far get ready to your second assignment any questions about anything I said so far.  What are we talking about getting data? We're going to be very generally discussing what an API is and how it works. And then we're going to start talking about where you can look for data and get interesting data sets.  To review where unit definition that data are anything that can be stored in a computer.  I've been using this term tabular data and spreadsheets and I've been using them interchangeably the purpose of this course. Those are the same. We're really going to discuss what type of data are today and I've mentioned that these are the types of data that day of scientists work with most often. These are data that are what you would store in Microsoft Excel or what you would store in Google Sheets their information entered in rows and columns and they're often stored in spreadsheet. So I'm saying tabular data or spreadsheet data. Those are the same thing.  These type of data can be found in lots and lots of different places on your first assignment you were directed at a few places. I'm going to mention a few more today. You're welcome to change your data set. If you look at one of these sources and say hey, they might have something really interesting for my project most of your familiar with casual. There's a very similar platform called date of that world data is plural is a fetal Google sheet filled with more than 600 datasets across a ton of different topics. They have policing information. They have politics information they have health information. So if you want to search through there for any project you never have to do with data. It's a good place to start.  What do you think data from 5:38 in the course? They are referring to be unfamiliar with 538 a new source, they cover politics and they cover Sports and they cover science and health and they release date us at the high some of their articles and those can be accessed online. I mention will work with some of this this if you have to slide earlier than an hour ago will not be in the size you downloaded but it is up there now. I'm at with ar Library liaison this morning and I learn about a new resource and the library has at UCSD a lot of data sets by Topic in the link is down here. So you can search for data from now Austin or linking to other places, but there are some proprietary data sets that you would only have access to with your UCSD login information. This is another great resource for Topix.  After that, Dana. Gov is now up and running you can get information from here. Now that the government is open and the US Census provide swapping information about individuals in the United States that are collected from the senses and it's not just Federal day that you're limited to cities more and more are rolling out open City-Data portals. And here's a list of a few where you can go in and get information about a city in particular.  That summer is here. If you want to look at cities across and see what types of day do they have on that is available for the United States and globally there's an open Beta index. So there's no shortage of data that are found in tabular format in spreadsheets that can be retrieved from the internet.  But what if the data aren't in a spreadsheet ready and waiting for you?  example  block information on Twitter people write lots of things people make polls. So there's this data scientist at Stitch fix named Hilary Parker and she posted this pole. And so do you like getting Google Calendar invites from your friends for lunch is cock etcetera and then the other options were absolutely not a mess. So there's some disagreement among the respondents and there were twenty-five hundred votes because there's also a lot of discussion that happened so there were 40 replace of this and then 20 retweets and then conversations that happened off of the retweets and it's not really easy. Just looking on Twitter to get a sense of what everybody said.  So you don't have the data to answer that question. If you really want to understand the full scope of a question, you have the initial though, but that's it out of plants.  So what do you do if you can use available methods to get that information for yourself?  So I want a very high-level talk about HTTP and apis today and I'm going to compare this to my undergraduate for I study biology and I learned a lot of things and then I got to grad school and learned that almost everything was taught with kind of true but there wasn't there a lot under the hood that I wasn't taught in a lot exceptions all the rules. I told Todd was hot the same as an API be ready to go courses later on and learn a lot more details me like she didn't really tell me the whole story and that's true. Okay, that's the morning transfer protocol. This is a way that allow messages to be sent on the internet. So you all do this every day you sit down at your computer you type in the URL and then magically request goes to a website and then the website is displayed.  So if you can get websites using a sweater called can we get other data or information you would want from the internet? And obviously this is in a lecture you can and the way you do. This is using application programming interface or a p i can do a whole lot and we're only going to focus on a small capability of API today in the context of getting data.  If you guys are ways for software and applications to talk to one another so their rules for interaction.  For purposes the way to access an API is to First choose a method.  We're going to watch city sets then build a URL and then get authorization. They have to determine how you're going to get the data. Then you have to say what day do you exactly want and then you have to promise them that you are who you say you are and then demonstrate that you are who you say you are and that's the third step.  Simonton hetp because these are the types of methods that you can use to access information from an API.  These are some of the most popular methods so if you want to get information which accomplishes reading information from a source, this is a method we're going to be focusing on today, but I know that there are other ways in which you can interact with an API, you can create one create a new resource. You can update you can get rid of it. And these are the methods that you would use on the left to make these requests since they were just going to focus on getting information from an API. So the only method what are you talking about today is get  So when you're sitting at your computer and decide that there's not a day to sit out there for you, but there is information after that you want to get you would use these HTTP Methods at your computer to then connect to and retrieve the data that are stored somewhere else.  So nothing ever going to be using is get that's going to retrieve it.  Now you should build a set of URLs that are going to return the data. So you had to tell the the place where the data are stored. What day do you want? You can just ask for everything and get all of the information in all of Twitter's I did it.  So we have to do for this if you have to go to their API documentation and there are Developer documentation for lots of different popular websites and you have to go through all of this and figure out how to specifically specify the information he wants you have to know what information you want and then you have to use the documentation to figure out how to get it.  And there are different ways. I'm at each of the API. So here we have Twitter and then Google's API and then get hugs API and we should know is that these versions change often? So you may have this really great script that gets all the information you want from a source from a provider and then they change the API and you have to redo everything so know that the documentation are invaluable and a source of frustration when you're working with apis.  So so far we talked about the fact that you were in use get to retrieve the information. That's pretty the method we used. We're going to build a set of URL that specifies the data were going to retrieve and Windows data are returned to us are most often in Json format and we talked about this last lecture. So Jason santor JavaScript object notation and just as a reminder, Jason is returned in key value pairs between phrases with colon in between.  I told you we're going to get stuff. So that's the issue of him HTTP method we're going to use and then we specify what we want to get using a specific URL that we determine from the documentation from the provider. We're going to get that data back and Json format. But before we can get that data back, you need to authorize yourself. So you need to some way to say that you are who you say you are.  I did do that. The most common way is using a Wawa store open authorization.  There isn't going through all this is so that you're familiar with the terms when you go to a tempest in the future. You have at least heard them but know that there will be steps along the way they will have to figure out when you go to do this when you apply for authorization. The first thing I have to do is apply on the website for the API on to what you want accent. Do you first have to tell in this case GitHub who you are and retrieve authorization so that they know I can't control who's getting what from their on their database.  You then create an Olaf application from this you will get with known as a client ID and a client secret neither would have provided by GitHub or Google or Twitter or whatever you're trying to get the data from.  After you have a client ID and secret fuel generator token and this access token is what ultimately will provide you access to the API to get the data that you want and verify who you are who you say you are right morning. These should never be shared or posted publicly. Also with apis you have some amount of information you can get back to free but you want a lot of information or to make you pay which makes sense. So if you post these after getting them on the Internet, somebody else can take them and then charge your credit card for all the data that they just got for free and you just paid for so we talk about get home in previous lesson should never be stored on get how people write crawlers that go through and look for these types of information and will then use them to their advantage and to your detriment. Okay, so these are secrets secrets  So that covers the getting authorization part. So now we know that we're going to use the get methods to get information. We're going to build our URL that's going to return us some Jason information and we can get that information after we authorize their self and then it's up to you to process their spots. So you can see the first part of this lecture was about getting data from a source or you just get your tabular data right away. This one takes a lot more steps and of a lot more work, but you can get data that you wouldn't have had otherwise, so just to recap the same process that we use for getting website is the same idea. We use for making a request to an API. You determine your method you then interpret them at the API interprets the method and gets the information you request and then it's up to you to process what you get from the API endpoint.  Okay, so far we've talked about tabular data loss of sources for getting that type of information and a fact that you can use a provider's API to get data that you wouldn't have had. Otherwise we talked about the basic methods for getting that information so briefly, but would you use this for the return back to that example I started with this is a simple example where you have a very specific question. Just what is the larger context around this question. So if you want to understand all of those Reese's retweets and responses  You could use what is API to explore this so you can ask Twitter for all of the responses to the street all of the retreat's and all of the conversations that happened as a result of those retweets and you could visualize them. This is done by losing to get dicey know who is a form of 50 students at Vanderbilt biostats apartment and we have here is a visualization of all of the priests that happened in response to this initial sweep. Sweep circle is a tweet. I'm right here is the initial sweets. You can see their number conversations that were spawned off of that first and then there are some long branches were there was a lot of conversation that happened and this is an interactive graphic. So on this website, you can hover over any point and see the conversation that happened so very quickly can understand all of a conversation that happened and if I were to summarize this briefly hear there was in the beginning some wording clarification. So some people remember the question was do you want somebody else to send you a Google invite for coffee or lunch date?  Carrots, he's two boxes are people saying why wouldn't that be the first point of contact but if we had like discussed it and then they sent me it to verify that's great for some clarification happened here. There was this tweet about people having very strong feelings about online calendar etiquette, which is something that lots of people apparently have very strong feelings for down here some discussion about the fact that having kids makes your mind. Goofy. There is over here at the point that I have anything invite you're not going to have confusion about time or place. There's some discussion about who uses to do managers hear people talking about paper journals, and then there's a soft-shoe talking about Microsoft Office Products and wedding tangent about somebody's wedding.  All of this, you can quickly understand what happened as a result of this printer pole. That's not just the quick 51% So you can understand it much more deeply when you do something like this.  This is the sentiment analysis which just captures what we talked about previously for sentiment analysis. You can take all of the words and all of the tweets related to this you can be overwhelming with the words had a positive sentiment which makes the most people felt positively about this but there was a faction that had some negative feelings about this. Okay. So the first one I didn't use a simple example here of how you would use to reduce API to get a very limited set of Jada but social scientists are using Twitter to ask very interesting questions about how we work with each other and a social network. They are getting data from Twitter all the time and using this answer questions that are much more in-depth than the simple example, I explain here  They're also research project that go through GitHub and take people say online jupyter notebook and analyze how people are using them and what's included in each of them. So getting a sense of how people are analyzing data left an example of how you would use an API to do the science of data science that we talked about and very simple you can also use these to organize your Google Drive account. So you can interact with it without clicking and dragging all the stuff you have to do on Google Drive.  I just listed a few popular API say this is by no means exhaustive. There are lots of places where you can ask an interesting question. You can likely get data that are of interest to you.  Okay, so I feel like bananas course I'll sound like a broken record on this the common theme of Jessica is because you can get the data doesn't mean you should get it or use it on their terms and agreement that we use for all of the apps that we use in all of the software that we use and we just kind of like go through and click and say I will follow those when you're getting data from somebody else that you can use in either research or your own for your own purposes those agreements matter a lot more and it's up to you to make sure you understand what you're agreeing to you. Whenever you're accessing an API example has rules on how to handle it. If you go and take a ton of data to answer your super interesting question. And then what is half of those users that you've taken that day that been deleted all those trees. Is it okay for you who know I have these data to continue to analyze them despite the fact that somebody else deleted those so they have rules about getting rid of deleted tweets from your analysis before using them for research. They distinguish between research and  All of this is covered in the developer agreement for Twitter and it changes. So when you're using these types of data for project, it's up to you to make sure you're in agreement with their Twitter policy or whatever API are using their policy. You're following the rules and you're going back to the data and getting rid of anything that you're not supposed to have are not supposed to be using.  Okay.  Lots of stretchy talk we discussed basics of apis and HTTP methods we talked about getting information from an API. We walk through a quick example. The last know what if there's no APR you can't get that do you want from it? This is where the case of web scraping comes in. We talked about this a bit last lecture and this is from last lecture. We talked about that phone flies example where they had the day and then the quote and then the medical explanation as to why it was an untruth with a URL in it.  Where you at scrape is and this is just the basics of it behind everything. You see on the internet. There is HTML behind it. And HTML is  characterized by these tags that open with these carrots and an end with a carrots with a Flash and you use these to specify what information to pull from a web page.  That's like the highest level of web scraping. There are python packages to help you do this. There are packages help you do this and you can use these to go and get the data you want from a website.  So why would you go ahead and use that? What if you were a real estate agent and you're new to a neighborhood and you want to figure out what's going on in your neighborhood there are lots of websites where you go for information like Zillow or Redfin and there are lots of others and if you wanted to get a sense as to what's going on with the houses being listed in your neighborhood, you could use web scraping to get a bunch of information quickly from all the different sources rather than relying on anyone individually.  Or you can use it for your background research. If you decide you want to start a car dealership, you would want to get some information about what other people are charging and rather than going to all of the websites and clicking on them individually and I figure now and copying the numbers down manually and entering them in a spreadsheet and then looking all them. This is a case where you would use web scraping to do that systematically for you.  Same disclaimer web scraping sound like okay. It is up to you to determine whether or not a website allows you to scrape their information and their ways to be that way. We won't talk about in detail. There are lots of popular platforms that will cut you off if you take too much information from you than at any one point in time so know that that is likely to happen if you don't look into whether or not it's okay to scrape information from a website first.  All right, See you in data science is don't reinvent the wheel if the information is already out there don't work harder than you need to to get it. So always start by looking for available datasets. If those don't exist, your next step would be going to an API and then only go to web scraping if it's allowed and if you can't get it from the initial resources.  So we're going to do a few clicker questions and have discussions about this.  One second. Is it open?  Christmas Eve  one second not working yet.  You guys can't click and right kind of pain.  Okay, reminder frequency code is AD that's on the board.  A croissant even super busy on GitHub and you want to write a blog post summarizing what you've been up to?  Get ready a few more seconds.  3 2 1  All right, so vast majority of you said available data.  Second most popular with a TI and then some people said manual tration are who wants to go with what explain why they pick the option that they did.  Okay, so funny, but you didn't hear the argument was for using the API because if you're making commits to get Hub you would be able to pull that information from the API and then be able to analyze it there. So all the information about everything you didn't get help would be accessible through their API. So on this one I'm going to argue that that is the correct answer. The reason that available data aren't there is cuz the days that you wanted probably doesn't exist as a dataset yet. There is no easy spreadsheet to get you have to go retrieve it from the API.  All right.  next one  You want to analyze where geographically calls to police have occurred in Albuquerque, New Mexico same options?  Closet in a few seconds  3 2 1  All right, so overwhelmingly people said available data, but BNC were possible options here who wants to explain their thought process here.  I'll tell you that the majority got it right on this one. Anybody would defend their thought process there.  What is the date of the data should be available should not Diamond. So I'm going to disappear little bit but agree on the second part of what you said. So I left out a key piece of information here. Is that Albuquerque New Mexico has and open data portal the data are available through their portal and that's what you would access and that would be easier than b or c because the data exist.  the Skip One Night last one  I just had a super interesting article on 5:38 and I want to explore the data myself is the best place to start.  Are closing in a few seconds?  3 2 1  All right. We had some disagreement here who wants to defend their position here. What were your thoughts on choosing the choice to be made?  Okay. So the argument here is for a available data and the statement made was at 5:38 post all of their data online. I'm going to put a slate patio on that. They don't make all of their data available that they make a lot of their data available. So I agree that a is possible here. I'm anybody would explain why they chose be.  Okay, so the conversation there was I mention that it's available but I did say through which Avenue and that was somewhat intentional so that I could ask this question later on and we could have this discussion so you can directly download the data sets from data. Fivethirtyeight.com. But they also host them on GitHub. So you could use an API to access those Sobe is also correct. I'm going to disagree with web scraping because the other to exist and they are easier and then manual curation as this is the last one is always a possibility by Manuel curation. I mean, you can go look for the data and then look at the article and then type in the information yourself or copy and paste but that is prone to making errors cuz we're human and the computers are better at making fewer hours. As long as we tell them the right thing. The manual puration is not going to be the idea for your creation of any dataset, but will sometimes be necessary. I'm so it's not the answer to any of these just because there aren't any of our three 1st Avenues.  Any questions so far on anything I talked about this far and I presented in the beginning.  I already did everybody like us 30. Second minute break talk to each other and then I can have everybody Focus back in cuz I know this is a lot of lecture so you guys have a minute to talk to each other and get it all out and back in.  All right.  For the second half we're going to do the first star. That was very East Coast 2nd half second half we're going to be talking about tidy data and data wrangling. So this is the idea of how to make spreadsheet data at work for you rather than against you and how to get data that aren't good into a good format and we're going to clarify what all of that means today and on Thursday.  Thermostat says the question is what percentage of a data scientist time is spent preparing data that I can be used to answer an interesting question. And that is the definition of data wrangling getting it so you can answer the question you want.  Give me a few more seconds.  3  2  1  okay. So here are the results a lot of you said we're somewhere in the 50 to 75 range. But we have a spread across all of these. I would gather that if you didn't know you would probably guessed because it's in the middle of the range and that's a safe. Guess these people tweet about this all the time data scientist talk about it all the time. And that is at 80 to 90% of their time is working on getting the data from formatted into the form. They can actually use it in and it said hopefully improve over time. So if more people understand how data should look then they can get it right from the beginning and you as a data scientist can spend less time working on this part of her right now and for the past 10 years, this is what it's been. So here's that other caveat to this is that we spend eighty 90% of our time trying to get the data in the form we want.  And we don't get to do the more interesting part at the end that said since a tarantula as a big part of the job. You have to be good at it. You have to not hate it cuz you'll spend a lot of time doing it and you have to be good at it over time. So this is a quote from DJ patil. He was a chief scientist under the Obama Administration and the first Chief data scientist, and he said that good data scientist understand it deep way that the heavy lifting a cleanup and preparation isn't something that gets in the way of solving the problem. It is the problem.  This means that we should all be really good at it. And I think right now since data science isn't evolving feel that we don't always teach it but we expect everybody to know how to do it. So that's why I'm spending a number of a bunch of time talking about how to take data that looks something like this which we're going to all know with Messy in a second and then why and get into a usable form at this is a spreadsheet that came from the Australian government and this is a blog post from so many miles in pain and he showed the fact that you have information kind of all over the place. So you have his header appear that doesn't really include data that you'd want to analyze and then you have what you actually want in your columns over here and I assure you of groups that you would actually want that to be a column itself there lots of missing spots in his junk down here. This is something that would not be very easy to work with. Once you read it into python live process of taking it from this untidy format to a tidy data format, which we're going to Define is through the process of data wrangling.  So I talked about in the programming lecture what the differences are between a data scientist in a computer scientist when it comes to the actual position and data scientist have to be really really good and quick at working with these tabular data.  Imma, spend a little bit of time talking about the terms that were going to use. So the first here's a snapshot of a spreadsheet. These are rose. These are columns.  The information that are stored in columns are going to be variables in this case. So this is information you collect from each in his case person. So variables are stored in columns.  An observation are stored in rows in this case. These observations are people but they don't have to be an everyday to say. So here we could see that we have four observations. So seven variables seven columns for observations for Rose.  And I'm going to talk about types of data here. For example, you might say that this information was information collected from somebody went to the doctor's office and they had to fill out a survey and give this information to the doctor. This may have been measurements taken at your visit that was taken by the nurse or by The Physician and these are two different types of data because this came from is so very and this came from the physician during your meeting.  Okay, so adorable store in Collins operations, Jordan Rose and different types of data.  Generating about this this week for your meeting. Whoever hasn't done that so far. You'll get more details on this for their four rules. The first real tidy data is that each variable should be stored in a single column. This may seem obvious looking at this dataset when you look at other people say they said you'll see that this is not necessarily the case and we'll come back to that and examples of adding a second last name first name one piece of information stored in each column for each variable.  The second is a very observation for each variable should be stored in a different row. So this row includes information for a single observation and that's consistent across the row.  First rule variables in Collins second alterations in Row. The Third is that there should be one table for each type of data. These are the same two types that I showed. The 4 1/4 is 1/10. You have more than one type of data on more than one table. You need a column in each with the same column label that allows them to be joining merged these might seem obvious looking at the spreadsheets were showing right here, but I have seen countless examples where this would be Capital ID in one and then over here this week, maybe being member underscore ID and then it's up to me to figure out already the same for the same people across the date of that says it's a different number that they've used those really were they use consistent labeling and that use consistent number and when you are storing data,  What type of tiny data and spreadsheets tidy data or rectangular what I mean? Is that all of the data fit into a rectangle and I don't have to loop around to avoid cells or stuff missing so I could data set is a rectangular data set and these are the easiest to work with programmatically. So the goal is to take those messy datasets and get them in this format.  Any questions about the general principles of Tidy data and what it means to take a messy dataset into a tidy format.  Okay.  Their number of rules for good spreadsheets. This is also part of your reading for this week. We're going to walk through the examples of the seven of these now and again keep these in mind again. I'll show you obviously examples but this happens all the time in the real world. So it's really important to understand these ahead of time to know what to look for. So consistency is the first rule if it's incredibly important day. I talked about already make sure that your variable names are consistent from one table to the next but also within a variable or variables should be coded the same way. So here if your you have the options of female and male they should be Mount you shouldn't write lowercase female for one uppercase email for another and an F for another individual. It's much easier to summarize information if everything is typed consistently  Plus you so choose good names for things there rules about avoiding Extra Spaces. So programmatically mail without a space is different than male with a space underneath of it underscores should be used instead of spaces and meaningful names of this comes back to our variable naming. So this is a better column header than F1 which doesn't give any information about what stored in it. You will see this all the time and date in the real world. And then it's your job to go figure out what information is stored in there or if you just had the helpful variable name that would save you a lot of time.  Dates are hard workers programmatically. We'll talk about this at some point. But there is a standard and is the standard you should try to use the iso 8601 is the way to approach it where you have four values for year to four months and two four digit in that order. So no other crazy way in which you can enter date or time should be used and reason the dates and times are difficult to work with is because time zones exist. So 2 p.m. Here is not 2 p.m. Worldwide dates are hard because people have chosen lots of different crazy ways to specify dates for automatically will talk about the details later.  So a boy that they sell in spreadsheets you have ID you have date and your glucose the presumably this person's glucose for this ID value was taken on some days are like that shouldn't be missing data. So the Assumption we have to make is that this date still down here and this date still down here and so forth.  The Raven make those assumptions which could be incorrect what if the ordering changed of this it's important to explicitly State what information is in their similarly. This is example of what case worth not rectangular cuz if I were to go around the outfit this week to have like camel humps or something on it. This is an example and in the morning you'll have if something that's untidy that could be entered in a tidy format that's easier to work with.  Schedule, this is Imagine for this is one of the title principles just one thing in a cell is over here on the left. We have one value of a person's weight in pounds on in a Cell over here while this looks like one piece of information really up to you have the value and you have the unit. It's much better to put the unit up at the top. Everybody's clear. This includes weight in pounds rather than having two pieces of information in a single cell.  Don't use Font color and highlighting azada hear. The person is trying to say this value is way different than all these other valleys. It might be a mistake rather than using font color. It's best to have another variable that says hey this one might be an outlier.  Is included for you to look through I won't go through all of this, but it's a summary of everything. We talked about and better approaches to bad naming.  I have this in here to summarize your points that are made in the Tidy data paper from Hadley Wickham This is just so your notes are complete. I won't walk through all of these now.  I'm going to have you all do one of the two here. So the first one take a look carefully at these spreadsheets and it's up to you to determine which of these spreadsheets is there fast. Feel free to talk to each other and discuss why.  I'll give you a few more seconds.  3 2 1  alright, so we have lots of people think B and fewer people saying D in the movie suicide so you can see it going to explain their thinking and choosing their off their answer.  Okay. So the argument here was for choosing D because you if you hear you clarify that your unit is pounds and maybe pounds universally would not be the way in which you would enter weight because most countries would use a kilogram. Okay, so that's one for anybody have a against d  Yes.  Okay, so we are even here is 4B that you just want the value in here and you can specify up here in the column maybe that this is town or somewhere else great. Anybody else have a thought as to why either biordi is not the right answer? Yes.  Okay, so be this is easier to calculate on because it's just the values and then D the option here. It's hard because it has the units. So how do we decide between B&D? So who wants to make the final call on be?  if I show hands anybody for be  Severity so anybody for big for anyone predict?  Highway asleep in class that's fine dancer here is because you would want to as we said put the unit up in the column header. So I wanted to have some discussions are the best option would be you just let the values for your specify the unit in a column header. Okay, we're not going to do that one just as I'm taking data that are a mess and getting them into that format is called Data wrangling will talk about the Technics techniques used to do that next lecture, but I'm going to spend the last few minutes talking about the first exam.  Carfi 30 multiple choice questions last 30 minutes of class a week from today. I wanted to give you all a thought after the process in the way that I develop exam. The Bible and development exam is to take everything we've learned. I take a few questions from each and so I wanted to be balanced across the material that we talked about. There will be a similar number of questions from each topic we talked about so like I said that out the first and then I go check it afterwards. So I'm not just texted testing 1 topic. I am not out to get anybody when I write a dance, but I'm also not out to ask questions like is the sky blue and have one option b B12 be blue when I have to be 17 and then something else that doesn't make sense. So I need to test that were actually learning something and that everything's not obvious. So it's a balance between asking questions that make you recall information and questions that make you apply Concepts. We asked without everything being obvious so that everybody got the 100 cuz then I have no idea whether or not we've actually learnt or learned anything and if I taught anything  BB recall questions where you just have to remember stuff that have been presented to you though the application where I've caught something and then you have to apply the concept.  Again, my goal is not to trick you or to deceive you in any of these. So when you understand the topics, these are the topics covered we're through seven. So we'll finish 8 on Thursday.  I do want you to think during the exam right? Like I don't want it to be so obvious. The question is this guy looks like the sky and you say blue.  How to study the first line of defense for your setting should be the lecture material that can be your notes. It can be you re listening to the podcast. It can be reviewing clicker questions vast majority of the questions on the exam will come from what was discussed in lecture.  The readings are fair game. Only the first two readings will have questions specifically from them, but I don't want you to memorize dates or names or specifics. I really want to make sure did you get the big Concepts from the readings note that while tidy data and data wrangling at the topic or covered? I won't be asked anything specific from that reading because you haven't discussed it in section yet.  You should understand the code for that in class. The best way to do it is go back to the notebooks run the code changed a little bit anticipate what the output should be after you change it see if that happens that will really ensure that you understand it.  I'll actually it's up on Triton head but it's also click link clickable on this link from the slides. There's an exam study guide. It's not comprehensive. It don't have everything on there. But it's a good place to check your understanding. It's broken down by lecture has questions from lecture from the readings from python. If you understand all of those that have a good understanding of all the questions and Concepts on there. You should be okay.  We'll start with questions on Thursday that you have that any material. So feel free to bring them feel any questions about the exam. Those are great for Thursday to be on Thursday.  UC San Diego podcast "
}