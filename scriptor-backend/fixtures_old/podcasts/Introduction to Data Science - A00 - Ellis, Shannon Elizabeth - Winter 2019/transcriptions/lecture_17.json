{
    "Blurbs": {
        "2 1 alright, so we have 52 people saying a and 57 people saying see so I didn't hear lots of you chatting. So I wanted to try and convince your neighbor of the right answer and will revote on this one. So turkey Charter remote see if we can come to a consensus. All right after discussion make your vote. Give it a few more seconds 3 to 1. ": [
            393.6,
            484.4,
            12
        ],
        "200 songs. And because I put this on a slide I change some words to just say expletive so that I didn't show up so that a block of this would look very different with a lot of these words being the most common and we've song count along the x-axis. So! Up a lot and then we have other words as you think about pop music makes sense. This ": [
            3011.3,
            3033.6,
            111
        ],
        "3 2 1 Pizza 70% of people said inconsistent color Choice across label. So if they're indicating is at UK is red in the first part and then that changes to China Inn II plot. She would one use a different color for either you are trying to make sure that people don't think that this red and this red represent the same information. So she is the correct choice. ": [
            303.6,
            326.0,
            9
        ],
        "43% of the class saying and this is the correct answer. I'm going to go through why so this is an example of what not likely a little harder than something. I would include on an exam but there would be one or two questions may be about this level of difficulty to the reason. So we're asking about how become a stylist at Stitch fix which we talked about ": [
            722.2,
            741.8,
            20
        ],
        "But that doesn't tell the sentiments. We have the most frequent words. We know which ones occur more frequently on an Android relative to an iPhone, but now we need to assign sentiment to the words. Do that again. We took an eye exam. We get our individual words from all the tweets and then we look to see how emotion in the Android occurs relative to the iPhone freeze. ": [
            1516.6,
            1540.3,
            56
        ],
        "However, I can see why people say be here. So this gray is too late to project so that would be a fine answer given what you see here, but in general for data visualizations grades are really helpful color to distract you from the stuff. You don't want them to focus on and really hone in or what you do want people to see in your visualization. So red ": [
            326.0,
            347.1,
            10
        ],
        "I know the date of pretty well because I teach the horse and because I read all the responses so I know that people often said they don't like reading or reading quizzes and that they want more time to work on their assignments in discussion section. They this was right after the exam in the midterm. So examine midterm are talking about the same thing here and people mention ": [
            3688.5,
            3714.0,
            136
        ],
        "I learned a lot about pop culture and this way you can see that there are positive and negative words really contributing to the sentiment within this song. I just talked to her she's had but how to come out on top so you can see that sentiment distribution when you look at the sentiments from the words in the song. You break it down by word and look at ": [
            3520.2,
            3543.2,
            129
        ],
        "It could be by grams words next one next to each other. So this will be set of two words and so forth. So people use different ways of breaking down the text so we could be single words more than one word multiple words and then compare that back to the sentiment. How do you break down into its components? You would likely want to remove words that are ": [
            1367.3,
            1388.7,
            49
        ],
        "Listen to a podcast. Alright, everybody. Let's quiet down. We'll get started as people filed in and get their seats. I already decided so. I miss you notes appear at these are just to make sure that everybody is on the same page and knows what's coming up as we come into the close of the quarter. So you have your fourth assignment due this Friday. You do not have ": [
            1.9,
            61.4,
            0
        ],
        "Okay, so we had some movement but not a time. So we have 53% getting the right answer. So as soon as Tampa is a what you would use to generate a bar plot for a categorical variable for histogram. You would use SNS. Just what if you're struggling with this I would say go back to the workbook or to the coding is demonstrating class to make sure you're ": [
            486.9,
            505.5,
            13
        ],
        "So this is really just like do we see what we expect what I was more interested in seeing from this or what words are most important to the least response. And which ones are the most So if you look at the words for least you see things like discussion read exam confusing midterm dislike computer answers examples word picture of life scientist Facebook easy apply example of where ": [
            3659.5,
            3688.5,
            135
        ],
        "The Tokens are going to be what you analyze when doing sentiment analysis to go to text breaking down into tokens. The process of breaking it down into tokens is tokenization and you use this for now, you take the entire bunch of texts known as a corpus split it up into tokens and he's choking Tempe word that I showed you in the last example worth a single word. ": [
            1347.3,
            1367.3,
            48
        ],
        "They also look at the distribution of character for Graham. So number four letters that come up together and it looked forward by grandtheft words that In Paris right after each other how frequently do those words show up in the Cuckoo's calling and how much does that overlap with her other works with regards to common themes in the nineteenth-century literature. They were trying to figure out what topics ": [
            1171.7,
            1195.7,
            40
        ],
        "What can you take away from it is anything weird? Are you can tell me one thing about this graph? What are we looking at? What do you conclude? Yeah. There's one outlier in a 2019 so you're probably talking about this one right here. Okay, so we have one song in 2019 that is far away for the number of words in the song relative to the rest of ": [
            2825.4,
            2886.4,
            105
        ],
        "When you see here when you're looking at the percent increase and Android relative to iPhone. So the further you are to the right the more often, it occurs in an Android and we see the different sentiments from the NRC lexicon here along the left the words that occur most frequently in the Android relative to the iPhone 10 to be negative have a sentiment of disgust sadness fear ": [
            1540.3,
            1563.1,
            57
        ],
        "a few things that they disliked or what that were confusing. So this all made sense to me but it quickly gives us an idea of what the class on mass like the least about the course. Over here a lot of people said they liked the real real world examples. So these capture the same thing largely people said that these examples where applicable to their major and to ": [
            3714.0,
            3735.5,
            137
        ],
        "a male fashion wear gown silk dress lace and ribbons and they did this for hundreds of topics to really get a full picture of what teams were most common in nineteenth-century literature. This is just an example of the types of questions. You can ask with text analysis and it involves not just doing exploratory analysis or inference involves really getting on understanding the words that are in the ": [
            1215.2,
            1235.6,
            42
        ],
        "a middle of the course lecture and then we'll go through a example. You haven't seen before and just walk through a text analysis or two. Imma, go here is that you understand if you were given a bunch of texts on how you could analyze it. We're not going to talk about natural language processing which is an approach. It'll just be one step further than what we're going ": [
            1051.7,
            1072.0,
            34
        ],
        "a more recent song. 1 seconds Atlanta Tech Museum So damn how uncool. I am. I had never heard that song before. I do know who Ariana Grande is any kind of sense in the song and when you look into it and I done had to like look up what the song is about and it's about her and her friends getting Rings after her engagement broke off. So ": [
            3477.2,
            3520.2,
            128
        ],
        "a word is to a document. So we talked about IVF. So just measuring the IDF portion, you take the natural log of the frequency within a document relative to how many documents contain that term. So it's going to decrease this number for words that are frequent and it's going to make this number bigger for words that are unique to a single document in your purpose. So bigger ": [
            1969.1,
            1992.8,
            72
        ],
        "about what we can learn from pop music in the last 3 years. So we can use data and the radiator that I use for this that are not the perfect and houses because I did this on Sunday. And if you were to do this correctly would take a little bit longer. My question is how to change the last three or so. I just took the lyrics to ": [
            2713.2,
            2729.6,
            99
        ],
        "an author's books. You can look at 8 o Clock classic physics text so we have here or Corpus of classic physics text and we break them down by who the text came from what it was Galileo Nikola Tesla Albert Einstein or Christian dragons, and I'm going to have you all take a look at this child each other what you learn from this Emily will chat about this ": [
            2196.9,
            2221.2,
            82
        ],
        "analysis can also up all be done using by grams or three words or forwards and so forth. Okay, that was my music example where I learned Lots about pop culture. So you all are helping me learn more about that here. I want to just summarize what y'all thoughts are about cause nine the river I asked you on the mid course survey what you've enjoyed most about cause ": [
            3562.9,
            3589.2,
            131
        ],
        "and blue are not generally bad for color blindness. And neither of these is true for deer eat. So the answer is c a r b happens to be true here because of the projector. Next on what type of plant would F&S count pot generate. This is an example of how we would use the programming that we talked about in class. It'll be a few more seconds. 3 ": [
            347.1,
            393.6,
            11
        ],
        "and not seen in the rest is what TF IDF is set out to measure. CC the same break down here where each panel is a different novel. And right now we're just plotting the number of word frequency. So we're looking at the frequency distribution. So there are lots of words that don't show up frequently and then there are some that show up much more frequently within a ": [
            2092.7,
            2113.3,
            77
        ],
        "and we'll talk about what that process is. And then you take those individual words or combinations of words and you compare them back to a sentiment lexicon. So send him a Mexican is a dataset where you have words or combination of words and then their corresponding. So how does that word generally make you make someone feel when it appears in text? This is an example of something ": [
            1283.0,
            1305.1,
            45
        ],
        "answer. So we talked about a lot of examples where you could train the data have a model that is accurate that you actually over fit the data which is what this is called on. This is example of me trying to predict your future success. I had high accuracy and the training day does that but below 50% in the test that so this is possible. It just is ": [
            633.8,
            651.0,
            18
        ],
        "anywhere but you do have to turn in your final project which is a compilation of all of your assignments at on an instruction. No big details for this any assignment out on a conclusion and explain who did Wyatt in the project and that will be your final project. So it should be mostly done despite with additional some edits and a bit of more information. I'm sorry, I ": [
            116.9,
            136.7,
            4
        ],
        "at lexical density. How many words are within a song? It's so how many are you Relative to the number of words. I can see it's about half of the song. We saw this previously and Analysis that I showed a while ago looking at the uniqueness of songs and how much they compress decreasing a tiny bit. Three starts now we have a toll number words on the change ": [
            2969.6,
            2992.1,
            109
        ],
        "awesome pipe posted on Triton head. Okay with that we're going to jump into a number of questions just like we did last lecture we'll do this again Thursday. So the first one is a different relaxation question. You're interpreting information from a scatter plot. How many of what type of variable have been plotted reminder? The frequency code is 80 Step at a chat with each other take a ": [
            158.1,
            189.5,
            6
        ],
        "been a lot of really great interactive visualization that look at text in their context. So this is an example that was from the pudding which is an awesome visual essay website and what they did was they took all of the letter to Dear Abby if you never heard of your Abby, she has been an advice columnist for 30 years and some people writing to Dear Abby and ": [
            3851.5,
            3873.1,
            143
        ],
        "books can be summarized through their emotional trajectory. So they're going to look at sentiment analysis for going up there going to look at it all together over time and start to make some connections. So this is somebody who set out to visualize The Hobbit and what you're looking at are the relationships between characters in The Hobbit where the lines demonstrate in a given cat in a given ": [
            3786.8,
            3806.6,
            140
        ],
        "by your big-time fricassee. So if it appears a lot and is unique you will have a larger tf-idf. But it's your turn practice inverse document frequency or tf-idf measures the frequency of a term adjusted for how rarely it is used. This will be bigger than where the word is it the more unique it is to your document which will increase when you multiply the numbers together leading ": [
            2016.3,
            2039.4,
            74
        ],
        "can measure tfidf this way and those words that show up frequently. So have a higher TX or term frequency they show up a lot but they are show up all the time across all of them to these are all those words. They stop words that we talked about. So super kind of words will have zero tf-idf to send save her personally across all documents the words that ": [
            2134.1,
            2154.7,
            79
        ],
        "category words, like smell or grab or crazy 10 to convey anger. Another limitation of using the sentiments is as I mentioned before that. Sometimes words are misclassified. The for example discussed has the word boy and I think that is a Miss classification generally. So these are the things that there are biases within the sentiments that you are looking at. So that is the first one. This is ": [
            3450.0,
            3477.2,
            127
        ],
        "category? The next I'm So today, we're really talking about what if you just have a bunch of texts. What can you do with that to understand what's going in going on in the underlined text? So here to example questions of text analysis of people have done to answer interesting questions. So it's one or two journalists set out to determine if JK Rowling wrote The Cuckoo's calling there ": [
            1089.6,
            1111.6,
            36
        ],
        "change a ton from year to year. When we look at next school diversity. We can see that we're looking at Fox spots here. So we have x-axis is time. We can say that the medians of this black box the black line in the middle of your box plot is consistent so that you have about 200 different words is a typical number of different words and we look ": [
            2947.0,
            2969.6,
            108
        ],
        "chapter who they interact with and this is an interactive graphic if you go to this URL and if you hover over one of the characters, it will show up in Black and it'll show up who it interacts with in the chapter and show the thickness of the interactions would be those that interact with most they also provide this walk through the chapter where it is a positive ": [
            3806.6,
            3829.7,
            141
        ],
        "contacts and start to understand the trends if you were really looking at a histogram, so the number overtime but if you hover over at you keep all of the information in context I didn't just do this for religion related issues. There were lots of issues that they included in this essay. I thought we talked about this previously was looking at artist uniqueness. So you can plant how ": [
            3893.1,
            3914.1,
            145
        ],
        "data set being displayed. Number for diversity of words, what are the most common which ones are the most unique to each of the years? I'm so now we're going to start talking about sentiment. So what sentiment do songs convey most frequently as I've been changed over time where the sentiment is broken down by number one song. I Won't words contribute to the sentiment of these number one ": [
            3246.4,
            3267.1,
            118
        ],
        "demonstrate occurrences between words in a dataset in a simpler way. Okay, so we talked about sentiment analysis. We talked about tf-idf to look at term frequency relative to the rest of the documents. We've also looked at work clouds and networks. And so now I'm going to try to put this all together and show that you can do multiple approaches in a single analysis and I will talk ": [
            2687.2,
            2713.2,
            98
        ],
        "different ingredients and that's depicted by the lines relating the ingredients to the flavor, They used these data to then generate this network. And when you're looking at networks, you need to visually look at a number of different things. The first is the size of the circle. So the size of a circle shows how prevalent ingredient it so you can quickly see garlic shows up a lot in ": [
            2477.1,
            2499.9,
            92
        ],
        "do the analysis forever. If you do go above and beyond to do the analysis, there's a chance for extra hard on the final project. The second exam is a week from today in the last 30 minutes of class will do a few questions with a iclicker to get started to get warmed up to get some practice on what questions on the exam maybe like I don't know ": [
            78.0,
            98.9,
            2
        ],
        "down word for a while if it continues on past this and really do it in a more rigorous way than what I've done here. I'm bored seeing is that across most of the trends most of the sentiments the trend is pretty flat from year to year but positivity seems to be going down across the data that we have. Okay. So this was the number one date of ": [
            3368.2,
            3388.9,
            124
        ],
        "emotional content of the text these words can be removed as they are not helpful for analysis. So you take your text to tokenize it you break it down into Parts you remove the stop words, and then you can do your sentiment analysis. The example I used her leader in the course and then we're going to revisit here was a question of whether the angrier and more hyperbolic ": [
            1407.3,
            1428.6,
            51
        ],
        "ever knows your kid reading grades are out on TriNet. And you're a your third assignment feedback or entree United gradescope for the actual feedback for the assignment? Cape Town out so start to fill those out. Those are really helpful for your night how to adjust his course and where we can improve in the future and the exam 2 study guide if you have the slides are an ": [
            136.7,
            158.1,
            5
        ],
        "everybody about the data visualization stuff we talked about I want to hear is I put most of the sentiments and gray and then just highlighted that Trend that we start to see that we see decreasing positivity. Now if we wanted to really determine if this were true we would have to be more thorough analysis. We want to look back in history. See if that trend has been ": [
            3350.6,
            3368.2,
            123
        ],
        "familiar with how we generate different types of Pop based on what we looked at in class. So, part is what you use for a categorical variable to generate a bar plot. Next one given the weights of lots of different animals, you know that there are a few animals whose weights are a lot bigger than most of the animals in a dataset. How would you measure the central ": [
            505.5,
            525.8,
            14
        ],
        "fix is not a data scientist and I stated in the question. So that is don't interact with algorithm. Their job is separate that's not true because they interact with the algorithms that spot because their client to them and with yogurt and determined to the clothing that they are picking from stay as I was determined which Warehouse to send their selections are well that is a step that ": [
            764.3,
            784.0,
            22
        ],
        "for song do we have more unique words for song see the types of questions you can start to ask when you have a bunch of song lyrics Somebody give you a second to look at this and tell me what you see but this is telling us what you can conclude and if anything weird is going on. The child each other with the x axis. What's the Y. ": [
            2804.9,
            2825.4,
            104
        ],
        "forgot that what words are unique to each year. I'm about to get a sense of what show that most commonly in the songs and is an example of where you can make a word cloud. I think it's a date that yeah shows up a lot and it doesn't change the time from year to year. Okay, so that's an example of using thinner what words occur most often ": [
            3065.0,
            3089.0,
            113
        ],
        "genius.com the problem here. So I'm already finding that my data are limited. And if you're doing this correctly, you would want to make sure that you had all two hundred songs. So some of the songs aren't in this database. So they're going to be missing for my days. That's we're really looking at between 127 and 152 songs from each day. There's funny cheer and this is going ": [
            2746.8,
            2767.3,
            101
        ],
        "grullense and they can be good albums by being simple or complex, but they should aim to be efficient and they don't ever mention with the car symbol. They don't have to accomplish a difficult task and they do not have to be self-contained. They can interact with other algorithms. Alright, okay. So before we get to say topic, I realized that I didn't actually go over there by where ": [
            861.1,
            882.0,
            25
        ],
        "happens that is not the role of stylus stylus run the algorithm to determine who their clients are going to be that happens before the client gets to The Stylist. So that is an algorithm is generated by somebody else at the company Bowie is the correct answer here. At last one I think maybe his own life is so complete the sentence good algorithms are. Think about a child ": [
            784.0,
            815.4,
            23
        ],
        "have a higher TF IDF. Taken plot what? This looks like across the novels and you can see that a lot of the words that are important to the novel happened to be character. So this isn't all that surprising if you understand that we're looking at a novel and that word show up more frequently with regards to proper names. We don't just have to analyze a corpus of ": [
            2173.9,
            2196.9,
            81
        ],
        "his most famous from the 19th century and you want to know what beans come out in a you could go through each and every one of them write down what you think the seams are or you can use the texts themselves to figure out exactly what the themes are without having to painstakingly go through each one and categorize it on your own done this and the way ": [
            1133.7,
            1151.6,
            38
        ],
        "if you look here it says parmesan cheese is authentic to Southern European Cuisine, but does not Coker mean there's not a link between other authentic ingredients. So parmesan cheese is emblematic of Southern European dishes, but there are no links to the other ingredients in that part of the network. So this demonstrates that it's not always the easiest to clean information from a network, but it's hard to ": [
            2660.0,
            2687.2,
            97
        ],
        "in cooking and then you want to see what the lines represent. So why is here representing the co-occurrence of the ingredients? So garlic and scallions 10 to show up in the same recipe whereas soy sauce does not frequently occur with counting cuz there was no line connecting the two when you start to look at that works. You can start to see and glean relationships between them but ": [
            2499.9,
            2524.8,
            93
        ],
        "in the tech than our question is whether or not tweets from the Android which you know Trump is using or more hyperbolic or angrier than the other tweets and what they noticed was that there was a difference between the words used by those trees that came from an iPhone with they presume to be his staff and the Android which we know Trump uses we're looking at here ": [
            1472.4,
            1493.8,
            54
        ],
        "in this data set, but you can see that there are some 27,000 words and there are a lot more than 27,000 words in the English language. Okay. See how does text you break it down into pieces and then you want to assign some sentiment to the text and I said you break it down into words or pairs of words. This is called breaking it down into Tokens. ": [
            1325.4,
            1347.3,
            47
        ],
        "into is the fact that there is an outlier value. Some values are way far apart from the rest of the values and in that case mean would be skewed so you won't use median when you have a bunch of outliers near date is that nothing is the info we talked about in class does an example of a question where you have to understand what central tendency is ": [
            577.9,
            593.6,
            16
        ],
        "is for 2017. But take a look at 2018. So we still have lots of expletive but yeah has really shot up in 2018. So yeah, I was about half and now now we're pretty close. So yeah, it is gaining in popularity has changed a little bit and it's 2019. Yes, and yeah takes over. So yeah shows up a whole lot in the lyrics from 2019. But I ": [
            3033.6,
            3065.0,
            112
        ],
        "is he all of the above? I said this is an example that we've looked at before. I realize that this may not be big enough for y'all to say. Well, then you can read and see if someone else you can help you figure out what the these say. I'll read them quickly for everybody. And this one is Sense and Sensibility. This one is Pride and Prejudice. This ": [
            1763.3,
            1784.0,
            65
        ],
        "is the ratio between the words used on the Android which when there are more frequently on the Android they're over here and when they're more freaking on the iPhone there on the left side. And knowing Trump's lexicon. This likely makes sense that these words would be something that he would type and that these would likely come from staff or there's a lot more hashtags that they're using. ": [
            1493.8,
            1514.7,
            55
        ],
        "it a time or you can take them all at random. And is there a lexicon that you could compare them to an address? That is also yet. Okay, talk about the music example and now your other question. Sorry that I may have missed. Questions that I asked and then I'm going to analyze the text responses to this. So if you do a sentiment analysis on is it ": [
            3618.1,
            3642.4,
            133
        ],
        "it would happen all the time across all the documents and that's not really a word of Interest or trying to figure out what word is most important to one document relative to all the documents in your Corpus. So in order to adjust and account for how frequently and how important the word is in a document you have to adjust by the inverse intended to measure how important ": [
            1948.0,
            1969.1,
            71
        ],
        "joy, like safe winning trupay and love tend to occur more freaking on the iPhone relative to the Android. So having said all of that having told you about what you do what the process is. You take your body of text you break it down into tokens. And then you assign sentiment to it. I want you all to think about what limit limitation of sentiment analysis. Get rid ": [
            1611.2,
            1634.5,
            60
        ],
        "likely doesn't clean that much information for you. I asked you what do you like most and which do you like least? So this is really just a proof of do we see what we expect? So we're broken down by the things you like least are in red and most in Blue, you would expect more positive words to show up in the most negative words to show finale. ": [
            3642.4,
            3659.5,
            134
        ],
        "long time figured out your self first. Can I read a few more seconds? 3 2 1 Okay, so half of the class said that a scatter plot plus two continuous variables and that is the correct answer. So it's kind of what is the plot where you are have to ask these and then their individual points. That show the relationship between two quantitative variables. So this requires you ": [
            189.5,
            249.9,
            7
        ],
        "matter is that words in your data that may not be included in the Lexicon. You have a limited number of words and a word may be really important and convey strong emotion in your date is that if it's not present in the Lexicon, it won't be included in urinalysis. Rihanna contacts in language matters, but it may be lost in sentiment analysis. What did you say something is ": [
            1701.4,
            1720.8,
            62
        ],
        "meaning they like why is he is and discriminatory outcomes. So if and this was what you all right about last week, if you are determining whether or not somebody is likely to go back to jail and then imprisoning them based on this that is likely be found that has been shown that this is based on historical data. So discriminating against individuals, so therefore it is not fair ": [
            903.2,
            923.9,
            27
        ],
        "more freaking it was in the input. And the same goes here so you can quickly see that in this word cloud the word data science and engineering shows up a lot in the day that used to generate the word cloud. Okay, outside of word clouds networks can also be helpful and showing you how words relate to each other. So this starts to get out the context issue ": [
            2409.0,
            2431.5,
            89
        ],
        "networks can get very complicated and I think you would have trouble determining exactly what's going on in this aside from the fact that categories does that you could determine that category 10 to Coker so fruits often show up with one another and vegetables tend to show up and have related compounds. One another so this just demonstrates that there's this big Network things are connected have similar compounds ": [
            2524.8,
            2547.8,
            94
        ],
        "night so far and then what you've liked least, so I did read through a question about cat. Sorry. Yeah, that's a great question. The question is if there's a few questions in there one is when you start to break it down into to work to buy Graham. Is there a way to classify which ones are most important answer is yes, but we're not going to talk about ": [
            3589.2,
            3618.1,
            132
        ],
        "not a very good model when it happen. I hope this one is about the algorithms lecture. So I'll read the question that you read the answers at Stitch fix algorithms are the heart of the company. How do stylus 2 aren't data scientists interact with l. It looks like we have most responses. I'll give you a few more seconds. 3 2 1 so we have almost have to ": [
            651.0,
            722.2,
            19
        ],
        "not great and you break it down into words and suddenly great get the side is positive sentiment aside time. You said something specifically was not great in urinalysis. So by removing the contacts from the language, you don't always get the sentiment assigned correctly. And that is the same explanation for why C is a limitation. The last one I wanted to point out is that I showed you ": [
            1720.8,
            1742.5,
            63
        ],
        "not helpful for analysis of words that don't convey any sentiment. These are going to stop words and there are hundreds of them, but they are extremely kind of words such as of two or words that have nothing to do with getting emotion or conveying emotion or sentiment. They just happen to be in text. So when it is active and Analysis if your goal is to understand the ": [
            1388.7,
            1407.3,
            50
        ],
        "novel. So this long tail would be all those very frequent words, but then those and the A's and the Ants would be out here along with the words that are unique to the novel. So we have to use tf-idf to separate out which ones of these are unique to the novel and show up a lot, but that don't show up in the other books. Southern Mansion you ": [
            2113.3,
            2134.1,
            78
        ],
        "novel. So we're going to use his tf-idf approach to find the important words for the content of each document by decreasing to wait for commonly used words an increase in the weight forward that are not used very much in a collection of Corpus of documents. Again, we're trying to find the words that are important and common but not too, So what words are unique to one novel ": [
            2072.1,
            2092.7,
            76
        ],
        "number for words that are less frequent. This is what we just talked about for this is idea to calculate TF IDF. You multiply the term frequency. So how frequent the word is in your document and then adjusted by that we just talked about. So if it is a comment where that is unique to the document you're looking at this number will be bigger. You'll then multiply it ": [
            1992.8,
            2016.3,
            73
        ],
        "of chat with each other. Give it a few more seconds. 3 2 1 Okay, so we have more than half of the class the majority saying it's e all of the above. So I'm going to walk through each of these the answer is in fact, he all of the above starting to make sure it's clear what some limitations of sentiment analysis are. So the fact of the ": [
            1634.5,
            1701.4,
            61
        ],
        "of people get that be Mansfield Park which is this one is more negative toward the end of the novel as you can see the negative sentiment is down here and we see the progression through the novel on the x-axis. I realized that this was small and that may have contributed to people not being sure and this is an example we looked at before we were talking about ": [
            1872.7,
            1890.3,
            67
        ],
        "of time does uniqueness change over time that leaves looks pretty stable design University change or the density that looks pretty stable. Now we want to look at what words are most common and what words are most unique to each year. So this is where you would use TF IDF. Are we looking at here are the most frequently words are mostly going to use words in a top ": [
            2992.1,
            3011.3,
            110
        ],
        "on Thursday with some more of these and if you have any questions then about the material you'll have the opportunity to ask them before we start talking about Thursday's topic geospatial analysis. Today we're talking about text analysis. We're going to start by going through examples. Y'all have seen before but in more detail, so when was from early way earlier on in the course and one was from ": [
            1032.2,
            1051.7,
            33
        ],
        "on a clicker question on next month. But from this, what are you looking at? And what are the conclusions is anything we are going on. No, it's so quiet today. Can lay okay give you guys another 30 second try and chat with each other about what you're interpreting from this figure. I'll use this question. I'll open it up as y'all chat about it to figure out which ": [
            2221.2,
            2256.7,
            83
        ],
        "one's Mansfield Park Emma under a b and then persuasion and here we have an example of what they're so we've looked at this before and I want you to interpret what's going on with the sentiment in this analysis, which is what's plotted on the y-axis. if a child with each other how to get over a few more seconds 3 2 1 Certainly would most people the majority ": [
            1784.0,
            1872.7,
            66
        ],
        "or anger relative to a more positive sentiments and the conclusion hair was that trumps Android account uses about 40 to 80% more words related to discuss sadness fear anger and other negative sentiments than the iPhone 8 phone account. Does an analysis showing how you can take a bunch of free data and then assign some sentiment to the entire body of text by tokenizing it and then carrying ": [
            1563.1,
            1586.2,
            58
        ],
        "out sentiment analysis? If you want to know what words exactly Drive each of these sentiments, you can see here again with Android being more frequently in the red and iPhone or freezing in the blue. The negative sentiments are here along the top and you can see words dinosaur things like badly crazy lost worst disaster Li bad killing unfair tough. Whereas the word that has to do with ": [
            1586.2,
            1611.2,
            59
        ],
        "over here are movies where a hundred percent of the words are spoken by females over here by mail because he that is largely male bias in Disney movies, which is what we're not if you hover over anyone movie, it will show up down here where the Wild Bunch has 100% male dialogue and 0% female making the correct response here is an example of how you can keep ": [
            4022.8,
            4042.9,
            148
        ],
        "really contributing this or what words went with which songs does an example of where if I had more knowledge about the data itself. I would better be able to interpret my results which will come back to that and that the next example of we're having knowledge about your data that allows you to interpret more So that the higher the tfidf the more unique it is to the ": [
            3226.7,
            3246.4,
            117
        ],
        "really help you in your interpretation. All right. So we've also been removing words from their context a lots of people have spent time recently trying to figure out how to put words back into their contact and how to visualize that and how to analyze it. Are focusing for a little bit longer y'all. Where is Isengard is here? So people haven't started it and determine whether or not ": [
            3755.4,
            3786.8,
            139
        ],
        "relative to its frequency. So you can quickly look and see cognitive learning language and interaction are the most common words people have very strong feelings about word cloud and whether or not they're any good. I wanted you all to know exactly what you're looking at when you see this. So the size of the word is relative to its frequency in the dataset. The larger it is the ": [
            2389.3,
            2409.0,
            88
        ],
        "sense. He came up with the theory of relativity and it doesn't show up in the other physicists are scientists writing. And so the word with the top hot is TF IDF, as you can see, this is TF IDF. Any Einstein would be the word that has most uniquely Einstein. Okay, so far we talked about taking a bunch of texts breaking it down into tokens comparing it back ": [
            2320.8,
            2345.6,
            85
        ],
        "should you be looking at total count or should you be looking at frequency and she I'm looking at frequency of positivity tends to be decreasing. It's kind of hard to interpret cuz there's lots of colors and lots of bars. This is a gentle reminder that when you're looking at a trend of a number over time is often helpful to make these line plot. So she's just reminding ": [
            3328.9,
            3350.6,
            122
        ],
        "so you should always think about who does is Albert if they fail for how do we make sure that that doesn't happen and you can do that by verifying that being sure that you're dating you have is correct and not biased. What this means is remove some individuals from the dataset if your predictions change wildly that means that it is likely unfair and that it's not a ": [
            923.9,
            945.2,
            28
        ],
        "so you should start to think about this. Do you think that knowing anything about pop music which I don't what you learn from. This is do the phone number where they decrease increase over the past three years. Do you think there's more uniqueness in on the pop charts now than there was in 2017 is there less does it said diversity or the density? We have more words ": [
            2785.1,
            2804.9,
            103
        ],
        "song Boss of the words in his song were classified as positive. But there are words that contribute to fear disgust sadness and anger seemed I want to know what words those are. I need to do that. So it's hard to get kind of complicated boy can see are the words along the side here and what sentiment they convey so you can see here that in the anger ": [
            3427.0,
            3450.0,
            126
        ],
        "song and I'm going to briefly mention what we would be talking about more talking about 5 grams. So here take a look at this plot. We have these sentiments from the NRC dataset along the x-axis and then we have the word count here. And then we broken it down by year 2017 2018-2019 and when you see counts be compared to cost time. The first thing you should ": [
            3267.1,
            3291.4,
            119
        ],
        "steep stable algorithm. By the way, you wanted it to be accountable to the people that are subjected to the algorithm. So it requires access to the album. This is where the classes are. The question came in last class is open to everybody. How can you make money and my argument was if you are able to harm somebody that it is more important that people are the people ": [
            945.2,
            966.5,
            29
        ],
        "tendency for animal types? I got yourself in chat with your neighbor. Give every few more seconds. 3 2 1 okay. Okay. So the first three are the only three that are measures of central tendency that we talked about median mean and mode. These are measures of variance. So here we have to determine between these three which is the right one. And at what this should clue you ": [
            525.8,
            577.9,
            15
        ],
        "text in contact and visualize it to convey a point without roll on Thursday. Shut off. UC San Diego podcast ": [
            4042.9,
            4101.1,
            149
        ],
        "texts themselves. What are the ways in which you can do? This is sentiment analysis. We talked about this briefly before and I'll review the example we talked about earlier in the lecture. This was in one of his very first lectures, but to Define sentiment analysis it is to programmatically infer emotional content of text. So take all of the words and your body of text and figure out ": [
            1235.6,
            1258.2,
            43
        ],
        "that has the NRC sentiment lexicon don't even see is when you're looking at words. These would be the words in the Lexicon and these would be the sentiments attached to those words. So you can see that an individual words such as abandonment might have more than one sentiment attached to it. So it's where can have more than one sentiment and there are lots and lots of words ": [
            1305.1,
            1325.4,
            46
        ],
        "that number one song from 2 years in 2017. So very briefly cuz I listen to just a part of the song Maybe. Club isn't the best place to find an opposite the bar is where I go. Okay, so I realize I did notice on what they listen to it. I just didn't know beforehand. So this is from Ed Sheeran and you can see it's a pretty upbeat ": [
            3388.9,
            3427.0,
            125
        ],
        "that they share between within a category and you can start to look at the colors and see that fruits are further away from vegetable. So they tend not to have a ton of overlapping compound. So now works can be helpful, but they can get very involved and be hard to figure out any information the larger they get that that I want you to take a look at ": [
            2547.8,
            2568.6,
            95
        ],
        "that we were talking about before so instead of breaking things down into their individual contacts, maybe want to ask how the words relate to one another. So this has to do with recipes so you can see ingredients in two different recipes here on the left. So you had shrimp scampi and tomato boil and then season muscles at the bottom. So the only ingredient that overlaps between these ": [
            2431.5,
            2454.0,
            90
        ],
        "the difference between them and when you would use them so it took a lot of Concepts together. All rights to this one true false a model can have high accuracy in a training dataset and low accuracy and I test them Add people are clicking and quickly go over the few more seconds 3 to 1. Overwhelmingly people here said this is in fact true, which is the correct ": [
            593.6,
            633.8,
            17
        ],
        "the end of actually there's a question. They just want to make sure that everyone was clear on what this by Matt's where does not algorithms and when they can be dangerous to other individuals and that's when they are important and secret and can harm other individuals and in those cases. It's really important that we make sure that are algorithms should be better known as fat so fair ": [
            882.0,
            903.2,
            26
        ],
        "the most popular songs from the like end of February for the last three years. If you really want to do this you would maybe summarize across the entire year or you would take it from each week, but I only have so much time on Sunday. So what I did was I took the two hundred top songs from Spotify and then I got the lyrics from this website ": [
            2729.6,
            2746.8,
            100
        ],
        "the results from one sentiment lexicon, which is NRC. There's more than one the results you get are sensitive to the Lexicon you use some leprechauns only if I were to be positive or negative so you wouldn't get anything about anger or fear or Joy. It would just step out of my them into positive or negative. So the Lexicon you use affect the results you get Santa where ": [
            1742.5,
            1763.3,
            64
        ],
        "the types of data through talking about Text data and that you can use a corpus of text a bunch of novels from a single author and look at sentiment overtime, but we didn't talk about in this data set with the fact that there is entirely different type of textual analysis you can do and has nothing to do with the sentiment attach two words, but has to do ": [
            1890.3,
            1909.6,
            68
        ],
        "their life a lot of people like the examples about scientist and Facebook example that we talked about the ethics lecture. So because I know the day that I know what to interpret from this unlike in the lyrics where I didn't really know exactly what lyrics came from which songs are with a matter of whether or not they color curtain stop having an understanding of the data can ": [
            3735.5,
            3755.4,
            138
        ],
        "them have any idea what that would be a popular song right now with lots of words. I ready. What else do we learn from this? Any general Transit change over time? What's what's a typical number of words per song? Originated you think a hundred words of typical / song. 300 500 some interpretations about 500 is the number of words that is typical about we do see that ": [
            2886.4,
            2925.2,
            106
        ],
        "then conclude which of these is true. I like if I read a few more seconds. 3 2 1 Okay, so the correct answer is be somebody who chose be explain to me or anybody who can figure it out why a is not the correct choice. Okay. Okay. So what is overlap between the data sets? They just aren't the water that show up in the top most unique ": [
            3130.6,
            3208.5,
            115
        ],
        "then she publishes their question and then her answer and we're looking here is that time and how many were religion related issues over time so you can see it's generally there's been a decrease from the mid-80s and you hover over any one individual you start to get a snapshot of what the person wrote to do a b this is a way to keep all the information and ": [
            3873.1,
            3893.1,
            144
        ],
        "then we talked about before getting into an example of something you haven't seen or two ways to visualize text you all day that we haven't talked about yet. We all probably seen something like this. This is a word cloud and they did for workout is you would give the computer a bunch of texts and it would determine which words occur most frequently and then size the word ": [
            2369.3,
            2389.3,
            87
        ],
        "there's some distribution between very few and a lot more words and this song is new Patek, which I did know how to pronounce and had to listen to and didn't know what that was and it's a watch and now I know this doesn't change a time from your ear is always between 31 and 33% So the words the average words unique number of words per song doesn't ": [
            2925.2,
            2947.0,
            107
        ],
        "they determine a JK Rowling with a likely author of The Cuckoo's calling and they looked at the distribution of word length. So others tend to have a similar vocabulary from one book to the next and similar Styles and they're looking to see if he's style in the Cuckoo's calling Baxter other works. They looked at the distribution of word length and 100. Most common words in the text. ": [
            1151.6,
            1171.7,
            39
        ],
        "think of is are the same number of words in each of these data sets. And if you remember back to the beginning, we had a different number of songs on Whose lyrics we were able to get and that there different numbers of words for song so you don't really want this to be count. You would rather want this to be frequency. So when you look at the ": [
            3291.4,
            3309.2,
            120
        ],
        "this where we're looking at the six most most authentic ingredients for each Region's Cuisine the colors and to get the region and connections indicate relative co-occurrence. And I want you all to chat and determine which of the following is true. Give it a few more seconds. Thing 2 1 play the beginning there is pretty even clicking in between a through D. But then Dee took off. So ": [
            2568.6,
            2660.0,
            96
        ],
        "throughout a long example in class interact with algorithm. So we talked about the fact that a data scientist generate the platform that stylists use to figure out how to best do their job and figure out what styles people should use. So albums are behind the experiment is a b testing used to determine the best system and The Stylist use it. So this one's true everybody at Stitch ": [
            741.8,
            764.3,
            21
        ],
        "to a bigger tf-idf forward that are important to a single document and not found in all the other documents. So this allows us instead of just looking at the sentiment just looking at the sentiment across the novel's week and then say what words really captured the differences between the novels. Forgot the question. What are the most commonly used words in Jane? Austen novels are relative to each ": [
            2039.4,
            2072.1,
            75
        ],
        "to a sentiment lexicon to get an idea of what emotions are conveyed in the text. We don't talk about tf-idf theme text possibly and determining how frequently word show up relative to an entire set of documents. We will find the words that are most important to that text while filter out all those like words like the and the ones that are not unique to that text but ": [
            2345.6,
            2369.3,
            86
        ],
        "to discuss today and you should be able to discuss the limitations to analyzing data with the approaches that I'm talking about today in this so far in this course, he's really focus on data that are either continuously of a bunch of numbers. You look to see if those numbers differ between groups or is what the categories C do the how does that differ on the cream one ": [
            1072.0,
            1089.6,
            35
        ],
        "to do an analysis. So you have to propose how you would analyze the data. So if it is a question of inference, you would have to stay that and explain what variables you would use what if you got certain results what you would expect to see what you would be looking for their just walk through how you would do an analysis. It is not so that you ": [
            61.4,
            78.0,
            1
        ],
        "to know that quantitative and continuous variables are what are plotted on a scatter plot and to identify what how many different variables are on that type of pot. Our next question is what is wrong with the colors in His image base when we talked about in class. So we threw them tried to get out yourself and chat with your neighbor. Going to be a few more seconds. ": [
            249.9,
            301.7,
            8
        ],
        "to make up a dataset 200 songs and I get all of the lyrics for each of the songs in that data sets from 2017 and 2018 and 2019. I need to try to ask lots of questions. So we're going to go through all this so I won't read them all. Now the first three that were going to focus on his do the total number of words change ": [
            2767.3,
            2785.1,
            102
        ],
        "to that year. So it would be inaccurate to say that no words overlap across the years in the data just like the fact that there are no words overlapping in this chart. So when we look here these three words are the most unique words to the 2019 data as an example here where I looked at this and I realized that I had no idea which songs were ": [
            3208.5,
            3226.7,
            116
        ],
        "tweets come from Trump himself rather than his staff and we know that Trump uses an Android phone so they looked at all the tweets coming from Trump's official account. And this was when he was on the campaign Trail Emily look here first. It's just at what words are most common so you can see the words here along the y-axis and how frequently they occur in all the ": [
            1428.6,
            1449.4,
            52
        ],
        "tweets that they analyzed they can see that some words as you would expect like crooked Hillary Clinton, which Trump said Lots on the campaign Trail and hashtag Trump 2016 make sense. You can see that these words all kind of makes sense to be the most common in a bunch of tweets from Donald Trump on the campaign Trail. Listen to really talk anything about the sentiment of what's ": [
            1449.4,
            1472.4,
            53
        ],
        "two is garlic but this is like a scientific papers there has to be more to it than just looking what ingredients are black overlap and they really are interested in what flavor compounds come from each of these ingredients. So what if they do the compounds differ across Cuisine types the lines connect between the flavor compounds and the ingredients so a single compound could be in lots of ": [
            2454.0,
            2477.1,
            91
        ],
        "unique and individual artists words are and then you can put it relative to other famous works like Moby-Dick or Shakespeare and you can see that some artists are have much more much larger vocabulary relative to most individuals would fall somewhere along here. Alexis Rojas one I want to take a look at this figure out what it is saying and then we will do a clicker question interpreting ": [
            3914.1,
            3937.9,
            146
        ],
        "was a rumor that JK Rowling was the person who wrote This Book despite the fact you used the pen name Robert Robert Galbraith, and so they did a text analysis. And found out to determine that most likely JK Rowling was Robert Galbraith and she later confirmed that was true. And then but things are common in the 19th century literature. What a list of what was written and ": [
            1111.6,
            1133.7,
            37
        ],
        "were common they had to go and find co-occurring word me idea. There was that words that co-worker are topics and then they looked at those topics where they occur and look at what other words occur around them to one of the topics. They found that was common in the 19th century literature was the co-occurring word female fashion and then the word they found that came along with ": [
            1195.7,
            1215.2,
            41
        ],
        "were made. And this has to do with the fact that it's who is used to being used on any information about code. It doesn't count. If you just give them the Code if they have no idea what it does to be think carefully about what transparency is just giving the code may not be enough. You might need to have some extra documentation to explain. What it is ": [
            989.3,
            1007.8,
            31
        ],
        "what it conveys emotionally is it a happy text is angry is it sad is it frustrated and you can do this programmatically meaning you put into a computer and you get out your results based on the text that you're analyzing so very simply 2 to its simplest form. You have your text Aida. These are the input you break these down into individual words or combination of words ": [
            1258.2,
            1283.0,
            44
        ],
        "what word you see the most in Emma? But not seen in all of the other. Other novels and the way you can measure this is what the term frequency inverse document frequency or going to briefly walkthrough. What TF IDF is So Donald Trump or you can say that is what you would expect it is helping only a word occurs in a document. But I mentioned that though ": [
            1928.0,
            1948.0,
            70
        ],
        "what words are contributing to each of the sentiments. Plus I want to mention just very briefly is we've only been looking at single words, but you can look at what by grants so when you have two words, you can't open eyes at two look at two words at a time and see which of those that most people in each year Davis at the TF IDF and sentiment ": [
            3543.2,
            3562.9,
            130
        ],
        "what's going on. So this is just a snapshot of an interactive visualization. And the question is to determine which of the following is true. What is it telling you? And what is what if we learn from it? Give it a few more seconds. I'd rather people get their final responses in closet in 3 2 1. Tickets to hear that we're looking at the breakdown of movies and ": [
            3937.9,
            4022.8,
            147
        ],
        "which ones and then visually looking at it in a word cloud. So now I want you to take a look at this figure out what's going on in this plot. And then we will have a quick question the next one to chat with each other. What can you conclude from each ear regarding unique? All right, as you do that take a look at the sticker question and ": [
            3089.0,
            3130.6,
            114
        ],
        "will hire cfids you can see I'm wet book they came from and what the word was just now as you start seeing if you're familiar with Jane Austen, these will look like characters in the books. So in the case of Jane Austen and makes sense that character is which are important to a specific novel and occur only in that novel not relative to all the other novels ": [
            2154.7,
            2173.9,
            80
        ],
        "with what words are most important to a document relative to other documents. So this case for example, we are looking at Jane Austen novels and you want to say in a novel what words are most important to that novel so would be a word like the that would happen and occur frequently across all of them. So what are we what where do you see the most of ": [
            1909.6,
            1928.0,
            69
        ],
        "with your neighbor. Very few more seconds. 3 2 1 Okay, so good algorithms are in fact efficient. We talk about this in should accomplish your goal in a few steps is possible algorithm can be both complex or simple a simple one that would sort number still count as an algorithm and one that determines a complicated problem as to which clothing a client should wear are both El ": [
            815.4,
            861.1,
            24
        ],
        "word is most uniquely Einstein. Give it a few more seconds. 3 2 1 okay, so by and large b-class said that relativity is the word that is most uniquely Einstein. So to come to that conclusion, they looked and they said okay. This one's Einstein and then which one has the largest tf-idf so this would be the word that shows up a lot and Einstein's writings which make ": [
            2256.7,
            2320.8,
            84
        ],
        "word. It's red and if it's a negative it's black and if you hover over anyone part, it tells you what line of dialogue on Bernard. It happens at that point and you can follow it a long layer time. What's an example of how sometimes you want to keep words in the context of a matter over time or how they relate to one? Another is another example there ": [
            3829.7,
            3851.5,
            142
        ],
        "workout, it looks like there's a big drop-off in negative. I'm over the years and you see a decrease overtime and positivity when you look at frequency, so you normalize about how many words and how many songs you have you can see that this decrease still stay there, but that decrease is now gone that's important thing to keep in mind is when you're looking at total count overtime ": [
            3309.2,
            3328.9,
            121
        ],
        "you are using album on that information rather than you make money that said for Stitch fix. You people are going to be harmed by the clothing decisions that are being made their so companies like that. It's fine either Our Lives aren't out there and that they are making money off of those algorithms. Welcome to being transparent being open about how why and how and why particular decisions ": [
            966.5,
            989.3,
            30
        ],
        "you questions on behalf of your final project is due on the day of your final exam, but it's not due until 11:59 p.m. I just want to make sure that everybody is clear that that is a Thursday since most of the deadlines have been Fridays in the class of your final project is due the day of the final exam and you do not have to show up ": [
            98.9,
            116.9,
            3
        ],
        "you're doing with the algorithm when there is harm that could result from your algorithm. You have to admit to make sure that it is fair accountable and transparent. I'm in that is a bare minimum and I could still be biased even if it is fair accountable and transparent. So any questions about the exam about the first questions, we had their or generally about algorithms. Will start lecture ": [
            1007.8,
            1032.2,
            32
        ]
    },
    "File Name": "Introduction to Data Science - A00 - Ellis, Shannon Elizabeth - Winter 2019-lecture_17.flac",
    "Full Transcript": "Listen to a podcast. Alright, everybody. Let's quiet down. We'll get started as people filed in and get their seats.  I already decided so.  I miss you notes appear at these are just to make sure that everybody is on the same page and knows what's coming up as we come into the close of the quarter. So you have your fourth assignment due this Friday. You do not have to do an analysis. So you have to propose how you would analyze the data. So if it is a question of inference, you would have to stay that and explain what variables you would use what if you got certain results what you would expect to see what you would be looking for their just walk through how you would do an analysis. It is not so that you do the analysis forever. If you do go above and beyond to do the analysis, there's a chance for extra hard on the final project.  The second exam is a week from today in the last 30 minutes of class will do a few questions with a iclicker to get started to get warmed up to get some practice on what questions on the exam maybe like I don't know you questions on behalf of your final project is due on the day of your final exam, but it's not due until 11:59 p.m. I just want to make sure that everybody is clear that that is a Thursday since most of the deadlines have been Fridays in the class of your final project is due the day of the final exam and you do not have to show up anywhere but you do have to turn in your final project which is a compilation of all of your assignments at on an instruction. No big details for this any assignment out on a conclusion and explain who did Wyatt in the project and that will be your final project. So it should be mostly done despite with additional some edits and a bit of more information.  I'm sorry, I ever knows your kid reading grades are out on TriNet. And you're a your third assignment feedback or entree United gradescope for the actual feedback for the assignment?  Cape Town out so start to fill those out. Those are really helpful for your night how to adjust his course and where we can improve in the future and the exam 2 study guide if you have the slides are an awesome pipe posted on Triton head.  Okay with that we're going to jump into a number of questions just like we did last lecture we'll do this again Thursday. So the first one is a different relaxation question. You're interpreting information from a scatter plot. How many of what type of variable have been plotted reminder? The frequency code is 80  Step at a chat with each other take a long time figured out your self first.  Can I read a few more seconds?  3 2 1  Okay, so half of the class said that a scatter plot plus two continuous variables and that is the correct answer. So it's kind of what is the plot where you are have to ask these and then their individual points.  That show the relationship between two quantitative variables.  So this requires you to know that quantitative and continuous variables are what are plotted on a scatter plot and to identify what how many different variables are on that type of pot.  Our next question is what is wrong with the colors in His image base when we talked about in class. So we threw them tried to get out yourself and chat with your neighbor.  Going to be a few more seconds.  3 2 1  Pizza 70% of people said inconsistent color Choice across label. So if they're indicating is at UK is red in the first part and then that changes to China Inn II plot. She would one use a different color for either you are trying to make sure that people don't think that this red and this red represent the same information. So she is the correct choice. However, I can see why people say be here. So this gray is too late to project so that would be a fine answer given what you see here, but in general for data visualizations grades are really helpful color to distract you from the stuff. You don't want them to focus on and really hone in or what you do want people to see in your visualization. So red and blue are not generally bad for color blindness. And neither of these is true for deer eat. So the answer is c a r b happens to be true here because of the projector.  Next on what type of plant would F&S count pot generate. This is an example of how we would use the programming that we talked about in class.  It'll be a few more seconds.  3 2 1  alright, so we have 52 people saying a and 57 people saying see so I didn't hear lots of you chatting. So I wanted to try and convince your neighbor of the right answer and will revote on this one.  So turkey Charter remote see if we can come to a consensus.  All right after discussion make your vote.  Give it a few more seconds 3 to 1.  Okay, so we had some movement but not a time. So we have 53% getting the right answer. So as soon as Tampa is a what you would use to generate a bar plot for a categorical variable for histogram. You would use SNS. Just what if you're struggling with this I would say go back to the workbook or to the coding is demonstrating class to make sure you're familiar with how we generate different types of Pop based on what we looked at in class. So, part is what you use for a categorical variable to generate a bar plot.  Next one given the weights of lots of different animals, you know that there are a few animals whose weights are a lot bigger than most of the animals in a dataset. How would you measure the central tendency for animal types?  I got yourself in chat with your neighbor.  Give every few more seconds.  3 2  1  okay. Okay. So the first three are the only three that are measures of central tendency that we talked about median mean and mode. These are measures of variance. So here we have to determine between these three which is the right one. And at what this should clue you into is the fact that there is an outlier value. Some values are way far apart from the rest of the values and in that case mean would be skewed so you won't use median when you have a bunch of outliers near date is that nothing is the info we talked about in class does an example of a question where you have to understand what central tendency is the difference between them and when you would use them so it took a lot of Concepts together.  All rights to this one true false a model can have high accuracy in a training dataset and low accuracy and I test them  Add people are clicking and quickly go over the few more seconds 3 to 1.  Overwhelmingly people here said this is in fact true, which is the correct answer. So we talked about a lot of examples where you could train the data have a model that is accurate that you actually over fit the data which is what this is called on. This is example of me trying to predict your future success. I had high accuracy and the training day does that but below 50% in the test that so this is possible. It just is not a very good model when it happen.  I hope this one is about the algorithms lecture. So I'll read the question that you read the answers at Stitch fix algorithms are the heart of the company. How do stylus 2 aren't data scientists interact with l.  It looks like we have most responses. I'll give you a few more seconds.  3 2 1  so we have almost have to 43% of the class saying and this is the correct answer. I'm going to go through why so this is an example of what not likely a little harder than something. I would include on an exam but there would be one or two questions may be about this level of difficulty to the reason. So we're asking about how become a stylist at Stitch fix which we talked about throughout a long example in class interact with algorithm.  So we talked about the fact that a data scientist generate the platform that stylists use to figure out how to best do their job and figure out what styles people should use. So albums are behind the experiment is a b testing used to determine the best system and The Stylist use it. So this one's true everybody at Stitch fix is not a data scientist and I stated in the question. So that is don't interact with algorithm. Their job is separate that's not true because they interact with the algorithms that spot because their client to them and with yogurt and determined to the clothing that they are picking from stay as I was determined which Warehouse to send their selections are well that is a step that happens that is not the role of stylus stylus run the algorithm to determine who their clients are going to be that happens before the client gets to The Stylist. So that is an algorithm is generated by somebody else at the company Bowie is the correct answer here.  At last one I think maybe his own life is so complete the sentence good algorithms are.  Think about a child with your neighbor.  Very few more seconds.  3 2 1  Okay, so good algorithms are in fact efficient. We talk about this in should accomplish your goal in a few steps is possible algorithm can be both complex or simple a simple one that would sort number still count as an algorithm and one that determines a complicated problem as to which clothing a client should wear are both El grullense and they can be good albums by being simple or complex, but they should aim to be efficient and they don't ever mention with the car symbol. They don't have to accomplish a difficult task and they do not have to be self-contained. They can interact with other algorithms.  Alright, okay. So before we get to say topic, I realized that I didn't actually go over there by where the end of actually there's a question. They just want to make sure that everyone was clear on what this by Matt's where does not algorithms and when they can be dangerous to other individuals and that's when they are important and secret and can harm other individuals and in those cases. It's really important that we make sure that are algorithms should be better known as fat so fair meaning they like why is he is and discriminatory outcomes. So if and this was what you all right about last week, if you are determining whether or not somebody is likely to go back to jail and then imprisoning them based on this that is likely be found that has been shown that this is based on historical data. So discriminating against individuals, so therefore it is not fair so you should always think about who does is Albert if they fail for how do we make sure that that doesn't happen and you can do that by verifying that being sure that you're dating you have is correct and not biased.  What this means is remove some individuals from the dataset if your predictions change wildly that means that it is likely unfair and that it's not a steep stable algorithm.  By the way, you wanted it to be accountable to the people that are subjected to the algorithm. So it requires access to the album. This is where the classes are. The question came in last class is open to everybody. How can you make money and my argument was if you are able to harm somebody that it is more important that people are the people you are using album on that information rather than you make money that said for Stitch fix. You people are going to be harmed by the clothing decisions that are being made their so companies like that. It's fine either Our Lives aren't out there and that they are making money off of those algorithms. Welcome to being transparent being open about how why and how and why particular decisions were made. And this has to do with the fact that it's who is used to being used on any information about code. It doesn't count. If you just give them the Code if they have no idea what it does to be think carefully about what transparency is just giving the code may not be enough. You might need to have some extra documentation to explain.  What it is you're doing with the algorithm when there is harm that could result from your algorithm. You have to admit to make sure that it is fair accountable and transparent. I'm in that is a bare minimum and I could still be biased even if it is fair accountable and transparent. So any questions about the exam about the first questions, we had their or generally about algorithms.  Will start lecture on Thursday with some more of these and if you have any questions then about the material you'll have the opportunity to ask them before we start talking about Thursday's topic geospatial analysis.  Today we're talking about text analysis. We're going to start by going through examples. Y'all have seen before but in more detail, so when was from early way earlier on in the course and one was from a middle of the course lecture and then we'll go through a example. You haven't seen before and just walk through a text analysis or two.  Imma, go here is that you understand if you were given a bunch of texts on how you could analyze it. We're not going to talk about natural language processing which is an approach. It'll just be one step further than what we're going to discuss today and you should be able to discuss the limitations to analyzing data with the approaches that I'm talking about today in this so far in this course, he's really focus on data that are either continuously of a bunch of numbers. You look to see if those numbers differ between groups or is what the categories C do the how does that differ on the cream one category? The next I'm So today, we're really talking about what if you just have a bunch of texts. What can you do with that to understand what's going in going on in the underlined text? So here to example questions of text analysis of people have done to answer interesting questions. So it's one or two journalists set out to determine if JK Rowling wrote The Cuckoo's calling there was a rumor that JK Rowling was the person who wrote This Book despite the fact you used the pen name Robert Robert Galbraith, and so they did a text analysis.  And found out to determine that most likely JK Rowling was Robert Galbraith and she later confirmed that was true. And then but things are common in the 19th century literature. What a list of what was written and his most famous from the 19th century and you want to know what beans come out in a you could go through each and every one of them write down what you think the seams are or you can use the texts themselves to figure out exactly what the themes are without having to painstakingly go through each one and categorize it on your own done this and the way they determine a JK Rowling with a likely author of The Cuckoo's calling and they looked at the distribution of word length. So others tend to have a similar vocabulary from one book to the next and similar Styles and they're looking to see if he's style in the Cuckoo's calling Baxter other works. They looked at the distribution of word length and 100. Most common words in the text. They also look at the distribution of character for Graham. So number four letters that come up together and it looked forward by grandtheft words that  In Paris right after each other how frequently do those words show up in the Cuckoo's calling and how much does that overlap with her other works with regards to common themes in the nineteenth-century literature. They were trying to figure out what topics were common they had to go and find co-occurring word me idea. There was that words that co-worker are topics and then they looked at those topics where they occur and look at what other words occur around them to one of the topics. They found that was common in the 19th century literature was the co-occurring word female fashion and then the word they found that came along with a male fashion wear gown silk dress lace and ribbons and they did this for hundreds of topics to really get a full picture of what teams were most common in nineteenth-century literature. This is just an example of the types of questions. You can ask with text analysis and it involves not just doing exploratory analysis or inference involves really getting on understanding the words that are in the texts themselves.  What are the ways in which you can do? This is sentiment analysis. We talked about this briefly before and I'll review the example we talked about earlier in the lecture. This was in one of his very first lectures, but to Define sentiment analysis it is to programmatically infer emotional content of text. So take all of the words and your body of text and figure out what it conveys emotionally is it a happy text is angry is it sad is it frustrated and you can do this programmatically meaning you put into a computer and you get out your results based on the text that you're analyzing so very simply 2 to its simplest form. You have your text Aida. These are the input you break these down into individual words or combination of words and we'll talk about what that process is. And then you take those individual words or combinations of words and you compare them back to a sentiment lexicon. So send him a Mexican is a dataset where you have words or combination of words and then their corresponding.  So how does that word generally make you make someone feel when it appears in text?  This is an example of something that has the NRC sentiment lexicon don't even see is when you're looking at words. These would be the words in the Lexicon and these would be the sentiments attached to those words. So you can see that an individual words such as abandonment might have more than one sentiment attached to it. So it's where can have more than one sentiment and there are lots and lots of words in this data set, but you can see that there are some 27,000 words and there are a lot more than 27,000 words in the English language.  Okay. See how does text you break it down into pieces and then you want to assign some sentiment to the text and I said you break it down into words or pairs of words. This is called breaking it down into Tokens. The Tokens are going to be what you analyze when doing sentiment analysis to go to text breaking down into tokens. The process of breaking it down into tokens is tokenization and you use this for now, you take the entire bunch of texts known as a corpus split it up into tokens and he's choking Tempe word that I showed you in the last example worth a single word. It could be by grams words next one next to each other. So this will be set of two words and so forth. So people use different ways of breaking down the text so we could be single words more than one word multiple words and then compare that back to the sentiment.  How do you break down into its components? You would likely want to remove words that are not helpful for analysis of words that don't convey any sentiment. These are going to stop words and there are hundreds of them, but they are extremely kind of words such as of two or words that have nothing to do with getting emotion or conveying emotion or sentiment. They just happen to be in text. So when it is active and Analysis if your goal is to understand the emotional content of the text these words can be removed as they are not helpful for analysis. So you take your text to tokenize it you break it down into Parts you remove the stop words, and then you can do your sentiment analysis.  The example I used her leader in the course and then we're going to revisit here was a question of whether the angrier and more hyperbolic tweets come from Trump himself rather than his staff and we know that Trump uses an Android phone so they looked at all the tweets coming from Trump's official account. And this was when he was on the campaign Trail Emily look here first. It's just at what words are most common so you can see the words here along the y-axis and how frequently they occur in all the tweets that they analyzed they can see that some words as you would expect like crooked Hillary Clinton, which Trump said Lots on the campaign Trail and hashtag Trump 2016 make sense. You can see that these words all kind of makes sense to be the most common in a bunch of tweets from Donald Trump on the campaign Trail.  Listen to really talk anything about the sentiment of what's in the tech than our question is whether or not tweets from the Android which you know Trump is using or more hyperbolic or angrier than the other tweets and what they noticed was that there was a difference between the words used by those trees that came from an iPhone with they presume to be his staff and the Android which we know Trump uses we're looking at here is the ratio between the words used on the Android which when there are more frequently on the Android they're over here and when they're more freaking on the iPhone there on the left side.  And knowing Trump's lexicon. This likely makes sense that these words would be something that he would type and that these would likely come from staff or there's a lot more hashtags that they're using.  But that doesn't tell the sentiments. We have the most frequent words. We know which ones occur more frequently on an Android relative to an iPhone, but now we need to assign sentiment to the words.  Do that again. We took an eye exam. We get our individual words from all the tweets and then we look to see how emotion in the Android occurs relative to the iPhone freeze. When you see here when you're looking at the percent increase and Android relative to iPhone. So the further you are to the right the more often, it occurs in an Android and we see the different sentiments from the NRC lexicon here along the left the words that occur most frequently in the Android relative to the iPhone 10 to be negative have a sentiment of disgust sadness fear or anger relative to a more positive sentiments and the conclusion hair was that trumps Android account uses about 40 to 80% more words related to discuss sadness fear anger and other negative sentiments than the iPhone 8 phone account.  Does an analysis showing how you can take a bunch of free data and then assign some sentiment to the entire body of text by tokenizing it and then carrying out sentiment analysis?  If you want to know what words exactly Drive each of these sentiments, you can see here again with Android being more frequently in the red and iPhone or freezing in the blue. The negative sentiments are here along the top and you can see words dinosaur things like badly crazy lost worst disaster Li bad killing unfair tough. Whereas the word that has to do with joy, like safe winning trupay and love tend to occur more freaking on the iPhone relative to the Android.  So having said all of that having told you about what you do what the process is. You take your body of text you break it down into tokens. And then you assign sentiment to it. I want you all to think about what limit limitation of sentiment analysis.  Get rid of chat with each other.  Give it a few more seconds.  3 2 1  Okay, so we have more than half of the class the majority saying it's e all of the above. So I'm going to walk through each of these the answer is in fact, he all of the above starting to make sure it's clear what some limitations of sentiment analysis are.  So the fact of the matter is that words in your data that may not be included in the Lexicon. You have a limited number of words and a word may be really important and convey strong emotion in your date is that if it's not present in the Lexicon, it won't be included in urinalysis.  Rihanna contacts in language matters, but it may be lost in sentiment analysis. What did you say something is not great and you break it down into words and suddenly great get the side is positive sentiment aside time. You said something specifically was not great in urinalysis. So by removing the contacts from the language, you don't always get the sentiment assigned correctly. And that is the same explanation for why C is a limitation. The last one I wanted to point out is that I showed you the results from one sentiment lexicon, which is NRC. There's more than one the results you get are sensitive to the Lexicon you use some leprechauns only if I were to be positive or negative so you wouldn't get anything about anger or fear or Joy. It would just step out of my them into positive or negative. So the Lexicon you use affect the results you get Santa where is he all of the above?  I said this is an example that we've looked at before. I realize that this may not be big enough for y'all to say.  Well, then you can read and see if someone else you can help you figure out what the these say. I'll read them quickly for everybody. And this one is Sense and Sensibility. This one is Pride and Prejudice. This one's Mansfield Park Emma under a b and then persuasion and here we have an example of what they're so we've looked at this before and I want you to interpret what's going on with the sentiment in this analysis, which is what's plotted on the y-axis.  if a child with each other  how to get over a few more seconds  3 2 1  Certainly would most people the majority of people get that be Mansfield Park which is this one is more negative toward the end of the novel as you can see the negative sentiment is down here and we see the progression through the novel on the x-axis. I realized that this was small and that may have contributed to people not being sure and this is an example we looked at before we were talking about the types of data through talking about Text data and that you can use a corpus of text a bunch of novels from a single author and look at sentiment overtime, but we didn't talk about in this data set with the fact that there is entirely different type of textual analysis you can do and has nothing to do with the sentiment attach two words, but has to do with what words are most important to a document relative to other documents. So this case for example, we are looking at Jane Austen novels and you want to say in a novel what words are most important to that novel so would be a word like the that would happen and occur frequently across all of them. So what are we what where do you see the most of what word you see the most in Emma?  But not seen in all of the other. Other novels and the way you can measure this is what the term frequency inverse document frequency or going to briefly walkthrough. What TF IDF is  So Donald Trump or you can say that is what you would expect it is helping only a word occurs in a document. But I mentioned that though it would happen all the time across all the documents and that's not really a word of Interest or trying to figure out what word is most important to one document relative to all the documents in your Corpus. So in order to adjust and account for how frequently and how important the word is in a document you have to adjust by the inverse intended to measure how important a word is to a document. So we talked about IVF. So just measuring the IDF portion, you take the natural log of the frequency within a document relative to how many documents contain that term. So it's going to decrease this number for words that are frequent and it's going to make this number bigger for words that are unique to a single document in your purpose. So bigger number for words that are less frequent.  This is what we just talked about for this is idea to calculate TF IDF. You multiply the term frequency. So how frequent the word is in your document and then adjusted by that we just talked about. So if it is a comment where that is unique to the document you're looking at this number will be bigger. You'll then multiply it by your big-time fricassee. So if it appears a lot and is unique you will have a larger tf-idf.  But it's your turn practice inverse document frequency or tf-idf measures the frequency of a term adjusted for how rarely it is used. This will be bigger than where the word is it the more unique it is to your document which will increase when you multiply the numbers together leading to a bigger tf-idf forward that are important to a single document and not found in all the other documents.  So this allows us instead of just looking at the sentiment just looking at the sentiment across the novel's week and then say what words really captured the differences between the novels.  Forgot the question. What are the most commonly used words in Jane? Austen novels are relative to each novel. So we're going to use his tf-idf approach to find the important words for the content of each document by decreasing to wait for commonly used words an increase in the weight forward that are not used very much in a collection of Corpus of documents. Again, we're trying to find the words that are important and common but not too, So what words are unique to one novel and not seen in the rest is what TF IDF is set out to measure.  CC the same break down here where each panel is a different novel. And right now we're just plotting the number of word frequency. So we're looking at the frequency distribution. So there are lots of words that don't show up frequently and then there are some that show up much more frequently within a novel.  So this long tail would be all those very frequent words, but then those and the A's and the Ants would be out here along with the words that are unique to the novel. So we have to use tf-idf to separate out which ones of these are unique to the novel and show up a lot, but that don't show up in the other books.  Southern Mansion you can measure tfidf this way and those words that show up frequently. So have a higher TX or term frequency they show up a lot but they are show up all the time across all of them to these are all those words. They stop words that we talked about. So super kind of words will have zero tf-idf to send save her personally across all documents the words that will hire cfids you can see I'm wet book they came from and what the word was just now as you start seeing if you're familiar with Jane Austen, these will look like characters in the books. So in the case of Jane Austen and makes sense that character is which are important to a specific novel and occur only in that novel not relative to all the other novels have a higher TF IDF.  Taken plot what? This looks like across the novels and you can see that a lot of the words that are important to the novel happened to be character. So this isn't all that surprising if you understand that we're looking at a novel and that word show up more frequently with regards to proper names. We don't just have to analyze a corpus of an author's books.  You can look at 8 o Clock classic physics text so we have here or Corpus of classic physics text and we break them down by who the text came from what it was Galileo Nikola Tesla Albert Einstein or Christian dragons, and I'm going to have you all take a look at this child each other what you learn from this Emily will chat about this on a clicker question on next month. But from this, what are you looking at? And what are the conclusions is anything we are going on.  No, it's so quiet today.  Can lay okay give you guys another 30 second try and chat with each other about what you're interpreting from this figure.  I'll use this question. I'll open it up as y'all chat about it to figure out which word is most uniquely Einstein.  Give it a few more seconds.  3 2 1  okay, so by and large b-class said that relativity is the word that is most uniquely Einstein. So to come to that conclusion, they looked and they said okay. This one's Einstein and then which one has the largest tf-idf so this would be the word that shows up a lot and Einstein's writings which make sense. He came up with the theory of relativity and it doesn't show up in the other physicists are scientists writing. And so the word with the top hot is TF IDF, as you can see, this is TF IDF. Any Einstein would be the word that has most uniquely Einstein.  Okay, so far we talked about taking a bunch of texts breaking it down into tokens comparing it back to a sentiment lexicon to get an idea of what emotions are conveyed in the text. We don't talk about tf-idf theme text possibly and determining how frequently word show up relative to an entire set of documents. We will find the words that are most important to that text while filter out all those like words like the and the ones that are not unique to that text but then we talked about before getting into an example of something you haven't seen or two ways to visualize text you all day that we haven't talked about yet. We all probably seen something like this. This is a word cloud and they did for workout is you would give the computer a bunch of texts and it would determine which words occur most frequently and then size the word relative to its frequency. So you can quickly look and see cognitive learning language and interaction are the most common words people have very strong feelings about word cloud and whether or not they're any good.  I wanted you all to know exactly what you're looking at when you see this. So the size of the word is relative to its frequency in the dataset. The larger it is the more freaking it was in the input.  And the same goes here so you can quickly see that in this word cloud the word data science and engineering shows up a lot in the day that used to generate the word cloud.  Okay, outside of word clouds networks can also be helpful and showing you how words relate to each other. So this starts to get out the context issue that we were talking about before so instead of breaking things down into their individual contacts, maybe want to ask how the words relate to one another. So this has to do with recipes so you can see ingredients in two different recipes here on the left. So you had shrimp scampi and tomato boil and then season muscles at the bottom. So the only ingredient that overlaps between these two is garlic but this is like a scientific papers there has to be more to it than just looking what ingredients are black overlap and they really are interested in what flavor compounds come from each of these ingredients. So what if they do the compounds differ across Cuisine types the lines connect between the flavor compounds and the ingredients so a single compound could be in lots of different ingredients and that's depicted by the lines relating the ingredients to the flavor,  They used these data to then generate this network. And when you're looking at networks, you need to visually look at a number of different things. The first is the size of the circle. So the size of a circle shows how prevalent ingredient it so you can quickly see garlic shows up a lot in in cooking and then you want to see what the lines represent. So why is here representing the co-occurrence of the ingredients? So garlic and scallions 10 to show up in the same recipe whereas soy sauce does not frequently occur with counting cuz there was no line connecting the two when you start to look at that works. You can start to see and glean relationships between them but networks can get very complicated and I think you would have trouble determining exactly what's going on in this aside from the fact that categories does that you could determine that category 10 to Coker so fruits often show up with one another and vegetables tend to show up and have related compounds.  One another so this just demonstrates that there's this big Network things are connected have similar compounds that they share between within a category and you can start to look at the colors and see that fruits are further away from vegetable. So they tend not to have a ton of overlapping compound. So now works can be helpful, but they can get very involved and be hard to figure out any information the larger they get that that I want you to take a look at this where we're looking at the six most most authentic ingredients for each Region's Cuisine the colors and to get the region and connections indicate relative co-occurrence. And I want you all to chat and determine which of the following is true.  Give it a few more seconds.  Thing 2  1  play the beginning there is pretty even clicking in between a through D. But then Dee took off. So if you look here it says parmesan cheese is authentic to Southern European Cuisine, but does not Coker mean there's not a link between other authentic ingredients. So parmesan cheese is emblematic of Southern European dishes, but there are no links to the other ingredients in that part of the network. So this demonstrates that it's not always the easiest to clean information from a network, but it's hard to demonstrate occurrences between words in a dataset in a simpler way.  Okay, so we talked about sentiment analysis. We talked about tf-idf to look at term frequency relative to the rest of the documents. We've also looked at work clouds and networks. And so now I'm going to try to put this all together and show that you can do multiple approaches in a single analysis and I will talk about what we can learn from pop music in the last 3 years.  So we can use data and the radiator that I use for this that are not the perfect and houses because I did this on Sunday. And if you were to do this correctly would take a little bit longer. My question is how to change the last three or so. I just took the lyrics to the most popular songs from the like end of February for the last three years. If you really want to do this you would maybe summarize across the entire year or you would take it from each week, but I only have so much time on Sunday. So what I did was I took the two hundred top songs from Spotify and then I got the lyrics from this website genius.com the problem here. So I'm already finding that my data are limited. And if you're doing this correctly, you would want to make sure that you had all two hundred songs. So some of the songs aren't in this database. So they're going to be missing for my days. That's we're really looking at between 127 and 152 songs from each day. There's funny cheer and this is going to make up a dataset 200 songs and I get all of the lyrics for each of the songs in that data sets from 2017 and 2018 and 2019.  I need to try to ask lots of questions. So we're going to go through all this so I won't read them all. Now the first three that were going to focus on his do the total number of words change so you should start to think about this. Do you think that knowing anything about pop music which I don't what you learn from. This is do the phone number where they decrease increase over the past three years. Do you think there's more uniqueness in on the pop charts now than there was in 2017 is there less does it said diversity or the density? We have more words for song do we have more unique words for song see the types of questions you can start to ask when you have a bunch of song lyrics  Somebody give you a second to look at this and tell me what you see but this is telling us what you can conclude and if anything weird is going on.  The child each other with the x axis. What's the Y. What can you take away from it is anything weird?  Are you can tell me one thing about this graph?  What are we looking at? What do you conclude?  Yeah.  There's one outlier in a 2019 so you're probably talking about this one right here. Okay, so we have one song in 2019 that is far away for the number of words in the song relative to the rest of them have any idea what that would be a popular song right now with lots of words.  I ready.  What else do we learn from this?  Any general Transit change over time? What's what's a typical number of words per song?  Originated you think a hundred words of typical / song.  300 500 some interpretations about 500 is the number of words that is typical about we do see that there's some distribution between very few and a lot more words and this song is new Patek, which I did know how to pronounce and had to listen to and didn't know what that was and it's a watch and now I know this doesn't change a time from your ear is always between 31 and 33% So the words the average words unique number of words per song doesn't change a ton from year to year. When we look at next school diversity. We can see that we're looking at Fox spots here. So we have x-axis is time. We can say that the medians of this black box the black line in the middle of your box plot is consistent so that you have about 200 different words is a typical number of different words and we look at lexical density. How many words are within a song? It's so how many are you  Relative to the number of words. I can see it's about half of the song. We saw this previously and Analysis that I showed a while ago looking at the uniqueness of songs and how much they compress decreasing a tiny bit.  Three starts now we have a toll number words on the change of time does uniqueness change over time that leaves looks pretty stable design University change or the density that looks pretty stable. Now we want to look at what words are most common and what words are most unique to each year. So this is where you would use TF IDF.  Are we looking at here are the most frequently words are mostly going to use words in a top 200 songs. And because I put this on a slide I change some words to just say expletive so that I didn't show up so that a block of this would look very different with a lot of these words being the most common and we've song count along the x-axis. So! Up a lot and then we have other words as you think about pop music makes sense.  This is for 2017.  But take a look at 2018. So we still have lots of expletive but yeah has really shot up in 2018. So yeah, I was about half and now now we're pretty close. So yeah, it is gaining in popularity has changed a little bit and it's 2019. Yes, and yeah takes over.  So yeah shows up a whole lot in the lyrics from 2019.  But I forgot that what words are unique to each year.  I'm about to get a sense of what show that most commonly in the songs and is an example of where you can make a word cloud.  I think it's a date that yeah shows up a lot and it doesn't change the time from year to year.  Okay, so that's an example of using thinner what words occur most often which ones and then visually looking at it in a word cloud.  So now I want you to take a look at this figure out what's going on in this plot. And then we will have a quick question the next one to chat with each other. What can you conclude from each ear regarding unique?  All right, as you do that take a look at the sticker question and then conclude which of these is true.  I like if I read a few more seconds.  3 2 1  Okay, so the correct answer is be somebody who chose be explain to me or anybody who can figure it out why a is not the correct choice.  Okay. Okay. So what is overlap between the data sets? They just aren't the water that show up in the top most unique to that year. So it would be inaccurate to say that no words overlap across the years in the data just like the fact that there are no words overlapping in this chart. So when we look here these three words are the most unique words to the 2019 data as an example here where I looked at this and I realized that I had no idea which songs were really contributing this or what words went with which songs does an example of where if I had more knowledge about the data itself. I would better be able to interpret my results which will come back to that and that the next example of we're having knowledge about your data that allows you to interpret more  So that the higher the tfidf the more unique it is to the data set being displayed.  Number for diversity of words, what are the most common which ones are the most unique to each of the years? I'm so now we're going to start talking about sentiment. So what sentiment do songs convey most frequently as I've been changed over time where the sentiment is broken down by number one song. I Won't words contribute to the sentiment of these number one song and I'm going to briefly mention what we would be talking about more talking about 5 grams.  So here take a look at this plot. We have these sentiments from the NRC dataset along the x-axis and then we have the word count here. And then we broken it down by year 2017 2018-2019 and when you see counts be compared to cost time. The first thing you should think of is are the same number of words in each of these data sets. And if you remember back to the beginning, we had a different number of songs on Whose lyrics we were able to get and that there different numbers of words for song so you don't really want this to be count. You would rather want this to be frequency. So when you look at the workout, it looks like there's a big drop-off in negative. I'm over the years and you see a decrease overtime and positivity when you look at frequency, so you normalize about how many words and how many songs you have you can see that this decrease still stay there, but that decrease is now gone that's important thing to keep in mind is when you're looking at total count overtime should you be looking at total count or should you be looking at frequency and she  I'm looking at frequency of positivity tends to be decreasing. It's kind of hard to interpret cuz there's lots of colors and lots of bars. This is a gentle reminder that when you're looking at a trend of a number over time is often helpful to make these line plot. So she's just reminding everybody about the data visualization stuff we talked about I want to hear is I put most of the sentiments and gray and then just highlighted that Trend that we start to see that we see decreasing positivity. Now if we wanted to really determine if this were true we would have to be more thorough analysis. We want to look back in history. See if that trend has been down word for a while if it continues on past this and really do it in a more rigorous way than what I've done here. I'm bored seeing is that across most of the trends most of the sentiments the trend is pretty flat from year to year but positivity seems to be going down across the data that we have.  Okay. So this was the number one date of that number one song from 2 years in 2017. So very briefly cuz I  listen to just a part of the song Maybe.  Club isn't the best place to find an opposite the bar is where I go.  Okay, so I realize I did notice on what they listen to it. I just didn't know beforehand. So this is from Ed Sheeran and you can see it's a pretty upbeat song Boss of the words in his song were classified as positive. But there are words that contribute to fear disgust sadness and anger seemed I want to know what words those are.  I need to do that. So it's hard to get kind of complicated boy can see are the words along the side here and what sentiment they convey so you can see here that in the anger category words, like smell or grab or crazy 10 to convey anger. Another limitation of using the sentiments is as I mentioned before that. Sometimes words are misclassified. The for example discussed has the word boy and I think that is a Miss classification generally. So these are the things that there are biases within the sentiments that you are looking at.  So that is the first one. This is a more recent song.  1 seconds  Atlanta Tech Museum  So damn how uncool. I am. I had never heard that song before. I do know who Ariana Grande is any kind of sense in the song and when you look into it and I done had to like look up what the song is about and it's about her and her friends getting Rings after her engagement broke off. So I learned a lot about pop culture and this way you can see that there are positive and negative words really contributing to the sentiment within this song. I just talked to her she's had but how to come out on top so you can see that sentiment distribution when you look at the sentiments from the words in the song.  You break it down by word and look at what words are contributing to each of the sentiments.  Plus I want to mention just very briefly is we've only been looking at single words, but you can look at what by grants so when you have two words, you can't open eyes at two look at two words at a time and see which of those that most people in each year Davis at the TF IDF and sentiment analysis can also up all be done using by grams or three words or forwards and so forth.  Okay, that was my music example where I learned Lots about pop culture. So you all are helping me learn more about that here. I want to just summarize what y'all thoughts are about cause nine the river I asked you on the mid course survey what you've enjoyed most about cause night so far and then what you've liked least, so I did read through a question about cat. Sorry.  Yeah, that's a great question. The question is if there's a few questions in there one is when you start to break it down into to work to buy Graham. Is there a way to classify which ones are most important answer is yes, but we're not going to talk about it a time or you can take them all at random. And is there a lexicon that you could compare them to an address? That is also yet.  Okay, talk about the music example and now your other question. Sorry that I may have missed.  Questions that I asked and then I'm going to analyze the text responses to this. So if you do a sentiment analysis on is it likely doesn't clean that much information for you. I asked you what do you like most and which do you like least? So this is really just a proof of do we see what we expect? So we're broken down by the things you like least are in red and most in Blue, you would expect more positive words to show up in the most negative words to show finale. So this is really just like do we see what we expect what I was more interested in seeing from this or what words are most important to the least response. And which ones are the most  So if you look at the words for least you see things like discussion read exam confusing midterm dislike computer answers examples word picture of life scientist Facebook easy apply example of where I know the date of pretty well because I teach the horse and because I read all the responses so I know that people often said they don't like reading or reading quizzes and that they want more time to work on their assignments in discussion section. They this was right after the exam in the midterm. So examine midterm are talking about the same thing here and people mention a few things that they disliked or what that were confusing. So this all made sense to me but it quickly gives us an idea of what the class on mass like the least about the course.  Over here a lot of people said they liked the real real world examples. So these capture the same thing largely people said that these examples where applicable to their major and to their life a lot of people like the examples about scientist and Facebook example that we talked about the ethics lecture. So because I know the day that I know what to interpret from this unlike in the lyrics where I didn't really know exactly what lyrics came from which songs are with a matter of whether or not they color curtain stop having an understanding of the data can really help you in your interpretation.  All right. So we've also been removing words from their context a lots of people have spent time recently trying to figure out how to put words back into their contact and how to visualize that and how to analyze it.  Are focusing for a little bit longer y'all.  Where is Isengard is here?  So people haven't started it and determine whether or not books can be summarized through their emotional trajectory. So they're going to look at sentiment analysis for going up there going to look at it all together over time and start to make some connections. So this is somebody who set out to visualize The Hobbit and what you're looking at are the relationships between characters in The Hobbit where the lines demonstrate in a given cat in a given chapter who they interact with and this is an interactive graphic if you go to this URL and if you hover over one of the characters, it will show up in Black and it'll show up who it interacts with in the chapter and show the thickness of the interactions would be those that interact with most they also provide this walk through the chapter where it is a positive word. It's red and if it's a negative it's black and if you hover over anyone part, it tells you what line of dialogue on Bernard. It happens at that point and you can follow it a long layer time.  What's an example of how sometimes you want to keep words in the context of a matter over time or how they relate to one? Another is another example there been a lot of really great interactive visualization that look at text in their context. So this is an example that was from the pudding which is an awesome visual essay website and what they did was they took all of the letter to Dear Abby if you never heard of your Abby, she has been an advice columnist for 30 years and some people writing to Dear Abby and then she publishes their question and then her answer and we're looking here is that time and how many were religion related issues over time so you can see it's generally there's been a decrease from the mid-80s and you hover over any one individual you start to get a snapshot of what the person wrote to do a b this is a way to keep all the information and contacts and start to understand the trends if you were really looking at a histogram, so the number overtime but if you hover over at you keep all of the information in context  I didn't just do this for religion related issues. There were lots of issues that they included in this essay.  I thought we talked about this previously was looking at artist uniqueness. So you can plant how unique and individual artists words are and then you can put it relative to other famous works like Moby-Dick or Shakespeare and you can see that some artists are have much more much larger vocabulary relative to most individuals would fall somewhere along here.  Alexis Rojas one I want to take a look at this figure out what it is saying and then we will do a clicker question interpreting what's going on. So this is just a snapshot of an interactive visualization.  And the question is to determine which of the following is true. What is it telling you? And what is what if we learn from it?  Give it a few more seconds.  I'd rather people get their final responses in closet in 3 2 1.  Tickets to hear that we're looking at the breakdown of movies and over here are movies where a hundred percent of the words are spoken by females over here by mail because he that is largely male bias in Disney movies, which is what we're not if you hover over anyone movie, it will show up down here where the Wild Bunch has 100% male dialogue and 0% female making the correct response here is an example of how you can keep text in contact and visualize it to convey a point without roll on Thursday.  Shut off.  UC San Diego podcast "
}